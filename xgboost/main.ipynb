{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnc-br/ddf-isoscapes/blob/furthest_points_test/xgboost/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfXLNaDjlIhs"
      },
      "outputs": [],
      "source": [
        "#@title Imports and modules.\n",
        "%pip install opencv-python\n",
        "%pip install matplotlib\n",
        "%pip install pandas\n",
        "\n",
        "from osgeo import gdal, gdal_array\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib import rc\n",
        "from typing import List\n",
        "from numpy.random import MT19937, RandomState, SeedSequence\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from io import StringIO\n",
        "import xgboost as xgb\n",
        "import os\n",
        "import math\n",
        "import glob\n",
        "\n",
        "rc('animation', html='jshtml')\n",
        "\n",
        "import sys\n",
        "!if [ ! -d \"/content/ddf_common_stub\" ] ; then git clone -b test https://github.com/tnc-br/ddf_common_stub.git; fi\n",
        "sys.path.append(\"/content/ddf_common_stub/\")\n",
        "import ddfimport\n",
        "# ddfimport.ddf_source_control_pane()\n",
        "ddfimport.ddf_import_common()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Options.\n",
        "import raster\n",
        "import dataset\n",
        "import importlib\n",
        "import gaussian\n",
        "\n",
        "importlib.reload(raster)\n",
        "importlib.reload(dataset)\n",
        "importlib.reload(gaussian)\n",
        "\n",
        "# Raster directory. Contains:\n",
        "# iso_O_cellulose.tif: Isoscape of 18O from Precipitation; <-- MODELING TARGET\n",
        "# Iso_Oxi_Stack.tif: Isoscape of 18O from Precipitation; <-- Model input\n",
        "# R.rh_Stack.tif: Atmospheric Relative humidity <-- Model input\n",
        "# R.vpd_Stack.tif: Vapor Pressure Deficit - VPD <-- Model input\n",
        "# Temperature_Stack.tif: Atmospheric Temperature <-- Model input\n",
        "raster.RASTER_BASE = \"/MyDrive/amazon_rainforest_files/amazon_rasters/\" #@param\n",
        "raster.SAMPLE_DATA_BASE = \"/MyDrive/amazon_rainforest_files/amazon_sample_data/\" #@param\n",
        "raster.TEST_DATA_BASE = \"/MyDrive/amazon_rainforest_files/amazon_test_data/\" #@param\n",
        "raster.ANIMATIONS_BASE = \"/MyDrive/amazon_rainforest_files/amazon_animations/\" #@param\n",
        "raster.GDRIVE_BASE = \"/content/gdrive\" #@param\n",
        "\n",
        "REBUILD_MODEL = True #@param {type:\"boolean\"}\n",
        "raster.MODEL_BASE = \"/MyDrive/amazon_rainforest_files/amazon_isoscape_models/\" #@param\n",
        "\n",
        "# How often should XGB log training metadata? 0 is the default, which indicates never.\n",
        "XGB_VERBOSITY_LEVEL = 0 #@param\n",
        "\n",
        "# @markdown If True (the default), then we train using a CSV of reference samples.\n",
        "# @markdown If False, we simulate the training set by sampling training data\n",
        "# @markdown using points from a tiff isoscape.\n",
        "USE_REFERENCE_SAMPLES_FOR_TRAINING = True #@param {type:\"boolean\"}\n",
        "# @markdown The filename that contains the reference samples\n",
        "REFERENCE_CSV_FILENAME = \"2023_06_23_Results_Google.csv\" #@param\n",
        "\n",
        "# If false, requires XGB oxygen isoscape in MODEL_BASE/predicted_isoscape_xgboost.tiff\n",
        "REGENERATE_OXYGEN_XGB_ISOSCAPE = True #@param {type:\"boolean\"}\n",
        "\n",
        "# If false, requires MODEL_BASE/xgb_means_oxygen_isoscape.tiff and MODEL_BASE/xgb_variances_oxygen_isoscape.tiff\n",
        "REGENERATE_OXYGEN_XGB_MEANS_VARIANCES = True #@param {type:\"boolean\"}\n",
        "\n",
        "# See https://zohaib.me/debugging-in-google-collab-notebook/ for tips,\n",
        "# as well as docs for pdb and ipdb.\n",
        "DEBUG = False #@param {type:\"boolean\"}\n",
        "if DEBUG:\n",
        "    %pip install -Uqq ipdb\n",
        "    import ipdb\n",
        "    %pdb on\n",
        "\n",
        "if not USE_REFERENCE_SAMPLES_FOR_TRAINING:\n",
        "  REFERENCE_CSV_FILENAME = \"\""
      ],
      "metadata": {
        "id": "ny_NHZibsULh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "importlib.reload(dataset)"
      ],
      "metadata": {
        "id": "cODS2JQt84W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train Model.\n",
        "import raster\n",
        "import dataset\n",
        "\n",
        "def train_xgb(data: dataset.PartitionedDataset, booster: str, rounds: int) -> xgb.XGBRegressor:\n",
        "  '''Trains a model using XGBoost on the PartitionedDataSet given and using the number of rounds\n",
        "  specified'''\n",
        "  xgb_model = xgb.XGBRegressor(n_estimators=rounds, eta=0.1, max_depth=2, objective='reg:squarederror', booster=booster)\n",
        "  # split data into input and output columns\n",
        "  X, y = data.train.iloc[:, :-1], data.train.iloc[:, -1]\n",
        "  X_val, y_val = data.validation.iloc[:, :-1], data.validation.iloc[:, -1]\n",
        "  print(f\"Predicting: {data.train.columns[-1]}\")\n",
        "  assert data.train.columns[-1] == \"cellulose_oxygen_ratio\"\n",
        "  xgb_model.fit(X, y, eval_set=[(X_val, y_val)], verbose=XGB_VERBOSITY_LEVEL)\n",
        "  return xgb_model\n",
        "\n",
        "def save_xgb(basename: str, model: xgb.XGBRegressor):\n",
        "  '''Saves the given model to a json file so it may be loaded later with running\n",
        "    training again.'''\n",
        "  with open(f\"{basename}_config_xgb.json\", \"w\") as f:\n",
        "    f.write(model.get_booster().save_config())\n",
        "  model.save_model(f\"{basename}_xgb.json\")\n",
        "\n",
        "def load_xgb(basename: str) -> xgb.XGBRegressor:\n",
        "  '''Loads a previously saved xgb model.'''\n",
        "  model = xgb.XGBRegressor()\n",
        "  model.load_model(f\"{basename}_xgb.json\")\n",
        "  with open(f\"{basename}_config_xgb.json\", \"r\") as f:\n",
        "    model.get_booster().load_config(f.read())\n",
        "  return model\n",
        "\n",
        "def train_or_load_xgboost(basename: str, data: dataset.PartitionedDataset, rounds: int=100000):\n",
        "  '''Either trains and saves a new XGBoost model or loads an existing model\n",
        "    depending on the initial form parameters of this colab.'''\n",
        "  if REBUILD_MODEL:\n",
        "    print(\"Training model\")\n",
        "    model = train_xgb(data, booster='gblinear', rounds=rounds)\n",
        "    print(model.evals_result())\n",
        "    print(\"Saving model \", basename)\n",
        "    save_xgb(basename, model)\n",
        "  else:\n",
        "    print(\"Loading model \", basename)\n",
        "    model = load_xgb(basename)\n",
        "\n",
        "  print(f\"RMSE (validation): {model.evals_result()['validation_0']['rmse'][-1]}\")\n",
        "  return model\n",
        "\n",
        "# Validation RMSE xgboost: 0.306059 w/ 100,000 rounds\n",
        "# Validation RMSE google internal tooling: 0.39386\n",
        "xgb_model = train_or_load_xgboost(\n",
        "  raster.get_model_path(\"oxygen_isoscape_model\"),\n",
        "  dataset.partitioned_reference_data(REFERENCE_CSV_FILENAME),\n",
        "  rounds=100000)\n"
      ],
      "metadata": {
        "id": "tUuAc-mCxOpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test train_or_load_xgboost\n",
        "\n",
        "# Test data created as follows:\n",
        "# # Create data for unit tests\n",
        "# from io import StringIO\n",
        "\n",
        "# train_text = StringIO()\n",
        "# partitioned_reference_data.validation.iloc[:10].to_csv(train_text, index=False)\n",
        "# print(train_text.getvalue())\n",
        "\n",
        "import os\n",
        "import dataset\n",
        "\n",
        "def create_test_data():\n",
        "  train_txt = \"\"\"lat,lon,rh,temp,vpd,atmosphere_oxygen_ratio,cellulose_oxygen_ratio\n",
        "  -4.880332787307218,-69.95800610699372,0.8044400215148926,26.225001017252605,0.6966667175292969,-4.451689084370931,37.122222900390625\n",
        "  -4.688096349322666,-70.44263021829333,0.80293075243632,26.35833485921224,0.7074999809265137,-4.41741943359375,37.17825063069662\n",
        "  -4.872397683046066,-69.63990597562567,0.8054693539937338,26.308329264322918,0.6958333651224772,-4.417543411254883,37.1220448811849\n",
        "  -4.8247274690858735,-69.81806665464333,0.8040264447530111,26.308331807454426,0.7016665935516357,-4.424846013387044,37.14974721272787\n",
        "  -4.765274838163909,-70.01923594475969,0.8022874991099039,26.337496439615887,0.709166685740153,-4.418321291605632,37.20004526774088\n",
        "  -4.771462642125715,-70.34365888186157,0.8011360963185629,26.508333841959637,0.7199999491373698,-4.385458946228027,37.247047424316406\n",
        "  -4.798305195940103,-70.28306090761369,0.802595059076945,26.366666158040363,0.709166685740153,-4.411936124165853,37.20568339029948\n",
        "  -5.223217462581197,-69.53591146126529,0.8077573776245117,26.10833231608073,0.6808333396911621,-4.4716800053914385,37.02557881673177\n",
        "  -4.613341938104102,-69.7943386465883,0.8022151788075765,26.400001525878906,0.7116666634877523,-4.423205057779948,37.17969512939453\n",
        "  -4.527212807226629,-69.8817482523093,0.8018482526143392,26.4499994913737,0.7149999936421713,-4.395961443583171,37.22857411702474\"\"\"\n",
        "  train_df = pd.read_csv(StringIO(train_txt))\n",
        "\n",
        "  val_txt = \"\"\"lat,lon,rh,temp,vpd,atmosphere_oxygen_ratio,cellulose_oxygen_ratio\n",
        "  -3.6696957825007046,-54.87948135669049,0.8142155011494955,25.770833333333332,0.6483333110809326,-3.3461217880249023,38.01644388834635\n",
        "  -3.8993546066489224,-54.86859101648585,0.805713415145874,26.149998982747395,0.6933333079020182,-3.261778195699056,38.26288604736328\n",
        "  -3.159593531700783,-54.93297940204632,0.819889227549235,25.912501017252605,0.6333333253860474,-3.287173271179199,37.95276896158854\n",
        "  -3.9960354096696906,-54.87203413927777,0.8026766777038574,26.149998982747395,0.7041666507720947,-3.2518555323282876,38.3813222249349\n",
        "  -3.80255400058822,-54.751942535999156,0.8073338667551676,26.054166158040363,0.6833333174387614,-3.302551587422689,38.1731923421224\n",
        "  -12.828852720078402,-52.607319143523036,0.7082154750823975,25.258333841959637,1.0200000603993733,-3.547501564025879,39.927050272623696\n",
        "  -12.532258968752565,-52.097391445126696,0.7110532919565836,25.520833333333332,1.019166628519694,-3.43365478515625,40.047627766927086\n",
        "  -13.427351375947753,-52.060761037543834,0.7014106909434,24.8249994913737,1.005833387374878,-3.589900334676107,40.11137390136719\n",
        "  -13.349866138079692,-52.65445230256682,0.7073808511098226,24.958333333333332,1.00083327293396,-3.5771010716756186,39.98369598388672\n",
        "  -12.730453380778542,-52.375592581693155,0.7120146751403809,25.2375005086263,1.0024999777475994,-3.494396209716797,40.02317810058594\"\"\"\n",
        "  val_df = pd.read_csv(StringIO(val_txt))\n",
        "\n",
        "  test_df = pd.DataFrame()\n",
        "\n",
        "  return dataset.PartitionedDataset(train=train_df, test=test_df, validation=val_df)\n",
        "\n",
        "# This function override REBUILD_MODEL for testing.\n",
        "def test_train_or_load_xgboost__load_succeeds():\n",
        "  for f in glob.glob(\"/tmp/foobar_model*\"):\n",
        "    os.remove(f)\n",
        "\n",
        "  # TODO: Probably better to have a function to load xgboost models instead of train and load sharing a function.\n",
        "  global REBUILD_MODEL\n",
        "  REBUILD_MODEL_tmp = REBUILD_MODEL\n",
        "  REBUILD_MODEL = True\n",
        "  model_under_test = train_or_load_xgboost(\"/tmp/foobar_model\", create_test_data(), rounds=100)\n",
        "\n",
        "  final_loss = model_under_test.evals_result()['validation_0']['rmse'][-1]\n",
        "  initial_loss = model_under_test.evals_result()['validation_0']['rmse'][0]\n",
        "  assert final_loss < initial_loss\n",
        "\n",
        "  original_prediction = model_under_test.predict(pd.DataFrame(np.array([[1,2,3,4,5,6]], dtype=float), columns=['lat', 'lon', 'rh', 'temp', 'vpd', 'atmosphere_oxygen_ratio']))[0]\n",
        "\n",
        "  REBUILD_MODEL = False\n",
        "  model_under_test = train_or_load_xgboost(\"/tmp/foobar_model\", create_test_data(), rounds=100)\n",
        "  REBUILD_MODEL = REBUILD_MODEL_tmp\n",
        "\n",
        "  loaded_prediction = model_under_test.predict(pd.DataFrame(np.array([[1,2,3,4,5,6]], dtype=float), columns=['lat', 'lon', 'rh', 'temp', 'vpd', 'atmosphere_oxygen_ratio']))[0]\n",
        "  assert original_prediction == loaded_prediction\n",
        "\n",
        "\n",
        "test_train_or_load_xgboost__load_succeeds()\n",
        "\n",
        "# We also trained a model assuming 255 trees sampled monthly.\n",
        "\n",
        "# Preserving this as text only because it is not realistic as of 2023.\n",
        "\n",
        "# Validation RMSE xgboost: 0.29072\n",
        "# Validation RMSE Google internal tooling: 0.29183\n",
        "# monthly_255_trees_xgb_model = train_xgb(monthly_255_trees_partitioned, booster='gbtree', rounds=15000)\n",
        "\n",
        "# For the best results here, add max_depth=2 to XGBRegressor params."
      ],
      "metadata": {
        "id": "2lFZDbNXBMNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate Means GeoTif.\n",
        "\n",
        "def get_xgb_isoscape_prediction(model: xgb.XGBRegressor):\n",
        "  '''returns an isoscape with a set of means predicted using the XGBoost\n",
        "  model.'''\n",
        "  bounds = raster.get_extent(raster.cellulose_isoscape_geotiff().gdal_dataset)\n",
        "  features = [raster.relative_humidity_geotiff(),\n",
        "              raster.temperature_geotiff(),\n",
        "              raster.vapor_pressure_deficit_geotiff(),\n",
        "              raster.atmosphere_isoscape_geotiff()]\n",
        "  image_feature_names = [\"rh\", \"temp\", \"vpd\", \"atmosphere_oxygen_ratio\"]\n",
        "  feature_names = [\"lat\", \"lon\"] + image_feature_names\n",
        "  predicted_isoscape = np.ma.array(np.zeros([bounds.raster_size_x, bounds.raster_size_y, 1], dtype=float), mask=np.ones([bounds.raster_size_x, bounds.raster_size_y, 1], dtype=bool))\n",
        "\n",
        "  for x_idx, x in enumerate(tqdm(np.arange(bounds.minx, bounds.maxx, bounds.pixel_size_x, dtype=float))):\n",
        "    rows = []\n",
        "    row_indexes = []\n",
        "    for y_idx, y in enumerate(np.arange(bounds.miny, bounds.maxy, -bounds.pixel_size_y, dtype=float)):\n",
        "      month = 0\n",
        "      row = {}\n",
        "      try:\n",
        "        for feature, feature_name in zip(features, image_feature_names):\n",
        "          row[feature_name] = raster.get_data_at_coords(feature, x, y, month)\n",
        "        row[\"long\"] = x\n",
        "        row[\"lat\"] = y\n",
        "      except ValueError:\n",
        "        # masked and out-of-bounds coordinates\n",
        "        continue\n",
        "      except IndexError:\n",
        "        continue\n",
        "      rows.append(row)\n",
        "      row_indexes.append((y_idx,month,))\n",
        "    if len(rows) > 0:\n",
        "      reordered = pd.DataFrame(rows)[model.get_booster().feature_names]\n",
        "      predictions = model.predict(reordered)\n",
        "      predictions_np = predictions\n",
        "      for prediction, (y_idx, month_idx) in zip(predictions_np, row_indexes):\n",
        "        predicted_isoscape.mask[x_idx,y_idx,month_idx] = False # unmask since we have data\n",
        "        predicted_isoscape.data[x_idx,y_idx,month_idx] = prediction\n",
        "\n",
        "  return predicted_isoscape\n",
        "\n",
        "if REGENERATE_OXYGEN_XGB_ISOSCAPE:\n",
        "  xgb_isoscape_prediction = get_xgb_isoscape_prediction(xgb_model)\n",
        "  raster.save_numpy_to_geotiff(raster.get_extent(\n",
        "      raster.cellulose_isoscape_geotiff().gdal_dataset),\n",
        "                               xgb_isoscape_prediction,\n",
        "                               raster.get_model_path(\"predicted_isoscape_xgboost.tiff\"))\n",
        "  plt.imshow(xgb_isoscape_prediction)\n",
        "\n",
        "# TODO: TESTME!"
      ],
      "metadata": {
        "id": "2dR7B5YY3w12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate Variance GeoTif.\n",
        "import gaussian\n",
        "\n",
        "predicted_cellulose_isoscape_geotiff = raster.load_raster(raster.get_model_path(\"predicted_isoscape_xgboost.tiff\"))\n",
        "plt.imshow(predicted_cellulose_isoscape_geotiff.yearly_masked_image)\n",
        "\n",
        "rv = gaussian.get_2d_gaussian(-60.16, 4.11, 1)\n",
        "gaussian.plot_gaussian(rv, raster.get_extent(predicted_cellulose_isoscape_geotiff.gdal_dataset))\n",
        "\n",
        "if REGENERATE_OXYGEN_XGB_MEANS_VARIANCES:\n",
        "  xgb_means, xgb_variances = gaussian.gaussian_kernel(predicted_cellulose_isoscape_geotiff, stdev_in_degrees=0.1)\n",
        "  bds = raster.get_extent(predicted_cellulose_isoscape_geotiff.gdal_dataset)\n",
        "  raster.save_numpy_to_geotiff(bds, np.expand_dims(xgb_means, axis=2),\n",
        "                               raster.get_model_path(\"xgb_means_oxygen_isoscape.tiff\"))\n",
        "  raster.save_numpy_to_geotiff(bds, np.expand_dims(xgb_variances, axis=2),\n",
        "                                raster.get_model_path(\"xgb_variances_oxygen_isoscape.tiff\"))\n"
      ],
      "metadata": {
        "id": "tevFMMvE_jZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display Results.\n",
        "xgb_means_oxygen_geotiff = raster.load_raster(raster.get_model_path(\"xgb_means_oxygen_isoscape.tiff\"))\n",
        "xgb_variances_oxygen_geotiff = raster.load_raster(raster.get_model_path(\"xgb_variances_oxygen_isoscape.tiff\"))\n",
        "\n",
        "# Until we re-generate the map\n",
        "xgb_variances_oxygen_geotiff.yearly_masked_image *= (5 / 4)\n",
        "xgb_variances_oxygen_geotiff.masked_image *= (5 / 4)\n",
        "raster.plot_band(xgb_means_oxygen_geotiff, 0)\n",
        "raster.plot_band(xgb_variances_oxygen_geotiff, 0)\n"
      ],
      "metadata": {
        "id": "UgpTUT-7BmJv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}