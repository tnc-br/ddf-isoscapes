{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyB5N9yngMo-"
      },
      "source": [
        "Copyright 2023 Google LLC.\n",
        "SPDX-License-Identifier: Apache-2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qCnFGsvx39s"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Running on hosted runtimes currently not fully supported. The relevant code will be skipped unless you set `RUNNING_ON_HOSTED_RUNTIME = True`. If `RUNNING_ON_HOSTED_RUNTIME == False`, it's best to just skip this heading and got straight to **Imports**, as the `%%writefile` line magic isn't available in all environments."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running on hosted runtime"
      ],
      "metadata": {
        "id": "y_QpBucV083W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set `RUNNING_ON_HOSTED_RUNTIME`\n",
        "RUNNING_ON_HOSTED_RUNTIME = False #@param {type:\"boolean\"}"
      ],
      "metadata": {
        "id": "uZHDaIhKtgrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Write requirements.txt file\n",
        "%%writefile requirements.txt\n",
        "jupyter\n",
        "pandas\n",
        "tqdm\n",
        "xgboost\n",
        "opencv-python\n",
        "matplotlib"
      ],
      "metadata": {
        "id": "-vpENPEXszhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install requirements\n",
        "if RUNNING_ON_HOSTED_RUNTIME:\n",
        "  %pip install -r requirements.txt\n",
        "  # Used to install GDAL\n",
        "  !sudo apt-get install gdal-bin\n",
        "  !sudo apt-get install libgdal-dev\n",
        "  !export CPLUS_INCLUDE_PATH=/usr/include/gdal\n",
        "  !export C_INCLUDE_PATH=/usr/include/gdal\n",
        "  !pip install GDAL==$(gdal-config --version) --global-option=build_ext --global- option=\"-I/usr/include/gdal\""
      ],
      "metadata": {
        "id": "sjL6Ruvks9EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Write partition.py\n",
        "%%writefile partition.py\n",
        "\"\"\"\n",
        "Module for helper functions for manipulating data and datasets.\n",
        "\"\"\"\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "\n",
        "@dataclass\n",
        "class PartitionedDataset:\n",
        "  train: pd.DataFrame\n",
        "  test: pd.DataFrame\n",
        "  validation: pd.DataFrame\n",
        "\n",
        "def partition(df) -> PartitionedDataset:\n",
        "  train = df[df[\"lon\"] < -55]\n",
        "  test = df[(df[\"lon\"] >= -55) & (df[\"lat\"] > -2.85)]\n",
        "  validation = df[(df[\"lon\"] >= -55) & (df[\"lat\"] <= -2.85)]\n",
        "  return PartitionedDataset(train, test, validation)\n",
        "\n",
        "def print_split(dataset: PartitionedDataset) -> None:\n",
        "  total_len = len(dataset.train)+len(dataset.validation)+len(dataset.test)\n",
        "  print(f\"Train: {100*len(dataset.train)/total_len:.2f}% ({len(dataset.train)})\")\n",
        "  print(f\"Test: {100*len(dataset.test)/total_len:.2f}% ({len(dataset.test)})\")\n",
        "  print(f\"Validation: {100*len(dataset.validation)/total_len:.2f}% ({len(dataset.validation)})\")"
      ],
      "metadata": {
        "id": "uC9-HGzvz3Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Write geotiffs.py\n",
        "%%writefile geotiffs.py\n",
        "\"\"\"\n",
        "Helper functions for GeoTIFFs.\n",
        "\"\"\"\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import raster\n",
        "import numpy as np\n",
        "\n",
        "@dataclass\n",
        "class Geotiffs:\n",
        "  \"\"\"\n",
        "  Class to pass around GeoTIFFs.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, load_water_mask: bool = False, load_tree_mask: bool = False, regenerate_plant_nitrogen: bool = False, path: str = \"\"):\n",
        "    self.load_water_mask = load_water_mask\n",
        "    self.load_tree_mask = load_tree_mask\n",
        "    self.regenerate_plant_nitrogen = regenerate_plant_nitrogen\n",
        "    self.get_raster_path = lambda filename : f'{path}{filename}'\n",
        "\n",
        "  def brazil_map_geotiff(self):\n",
        "    return raster.load_raster(self.get_raster_path(\"brasil_clim_raster.tiff\"))\n",
        "\n",
        "  def relative_humidity_geotiff(self):\n",
        "    return raster.load_raster(self.get_raster_path(\"R.rh_Stack.tif\"))\n",
        "\n",
        "  def temperature_geotiff(self):\n",
        "    return raster.load_raster(self.get_raster_path(\"Temperatura_Stack.tif\"))\n",
        "\n",
        "  def vapor_pressure_deficit_geotiff(self):\n",
        "    return raster.load_raster(self.get_raster_path(\"R.vpd_Stack.tif\"))\n",
        "\n",
        "  def atmosphere_isoscape_geotiff(self):\n",
        "    return raster.load_raster(self.get_raster_path(\"Iso_Oxi_Stack.tif\"))\n",
        "\n",
        "  def cellulose_isoscape_geotiff(self):\n",
        "    return raster.load_raster(self.get_raster_path(\"iso_O_cellulose.tif\"))\n",
        "\n",
        "  def soil_plant_nitrogen_difference_isoscape_geotiff(self):\n",
        "    return raster.load_raster(self.get_raster_path(\"raster_krig_d15N_soil_plant.tiff\"))\n",
        "\n",
        "  def soil_nitrogen_isoscape_geotiff(self):\n",
        "    return raster.load_raster(self.get_raster_path(\"raster_krig_d15N_soil.tiff\"))\n",
        "\n",
        "  def plant_nitrogen_isoscape_geotiff(self):\n",
        "    return raster.load_raster(self.get_raster_path(\"plant_nitrogen_isoscape.tiff\"))\n",
        "\n",
        "  def carbon_means_krig_isoscape_geotiff(self):\n",
        "    return raster.load_raster(self.get_raster_path(\"Brasil_Raster_Krig_iso_d13C.tiff\"))\n",
        "\n",
        "  def land_water_mask_geotiff(self):\n",
        "    return raster.load_raster(self.get_raster_path(\"Land_Water_Brazil_MODIS.tif\")) if self.load_water_mask else None\n",
        "\n",
        "  def possible_tree_mask_geotiff(self):\n",
        "    return raster.load_raster(self.get_raster_path(\"Possible_Trees_Brazil_MODIS.tif\")) if self.load_tree_mask else None\n",
        "\n",
        "  def plant_nitrogen_isoscape_geotiff(self):\n",
        "    if self.regenerate_plant_nitrogen:\n",
        "      plant_nitrogen_array = self.soil_nitrogen_isoscape_geotiff().yearly_masked_image - self.soil_plant_nitrogen_difference_isoscape_geotiff().yearly_masked_image\n",
        "      raster.save_numpy_to_geotiff(self.soil_plant_nitrogen_difference_isoscape_geotiff(),\n",
        "                                   np.expand_dims(np.flip(plant_nitrogen_array.T, axis=1), axis=2),\n",
        "                                   self.get_raster_path(\"plant_nitrogen_isoscape.tiff\"))\n",
        "    return raster.load_raster(self.get_raster_path(\"plant_nitrogen_isoscape.tiff\"))"
      ],
      "metadata": {
        "id": "qeT676nG0OAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Write raster.py\n",
        "%%writefile raster.py\n",
        "\"\"\"\n",
        "Package for raster functions.\n",
        "\"\"\"\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import math\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from osgeo import gdal, gdal_array\n",
        "from typing import List\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class AmazonGeoTiff:\n",
        "  \"\"\"Represents a geotiff from our dataset.\"\"\"\n",
        "  gdal_dataset: gdal.Dataset\n",
        "  image_value_array: np.ndarray # ndarray of floats\n",
        "  image_mask_array: np.ndarray # ndarray of uint8\n",
        "  masked_image: np.ma.masked_array\n",
        "  yearly_masked_image: np.ma.masked_array\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Bounds:\n",
        "  \"\"\"Represents geographic bounds and size information.\"\"\"\n",
        "  minx: float\n",
        "  maxx: float\n",
        "  miny: float\n",
        "  maxy: float\n",
        "  pixel_size_x: float\n",
        "  pixel_size_y: float\n",
        "  raster_size_x: float\n",
        "  raster_size_y: float\n",
        "\n",
        "  def to_matplotlib(self) -> List[float]:\n",
        "    return [self.minx, self.maxx, self.miny, self.maxy]\n",
        "\n",
        "\n",
        "def print_raster_info(raster):\n",
        "  dataset = raster\n",
        "  print(\"Driver: {}/{}\".format(dataset.GetDriver().ShortName,\n",
        "                               dataset.GetDriver().LongName))\n",
        "  print(\"Size is {} x {} x {}\".format(dataset.RasterXSize,\n",
        "                                      dataset.RasterYSize,\n",
        "                                      dataset.RasterCount))\n",
        "  print(\"Projection is {}\".format(dataset.GetProjection()))\n",
        "  geotransform = dataset.GetGeoTransform()\n",
        "  if geotransform:\n",
        "    print(\"Origin = ({}, {})\".format(geotransform[0], geotransform[3]))\n",
        "    print(\"Pixel Size = ({}, {})\".format(geotransform[1], geotransform[5]))\n",
        "\n",
        "  for band in range(dataset.RasterCount):\n",
        "    band = dataset.GetRasterBand(band+1)\n",
        "    #print(\"Band Type={}\".format(gdal.GetDataTypeName(band.DataType)))\n",
        "\n",
        "    min = band.GetMinimum()\n",
        "    max = band.GetMaximum()\n",
        "    if not min or not max:\n",
        "      (min,max) = band.ComputeRasterMinMax(False)\n",
        "    #print(\"Min={:.3f}, Max={:.3f}\".format(min,max))\n",
        "\n",
        "    if band.GetOverviewCount() > 0:\n",
        "      print(\"Band has {} overviews\".format(band.GetOverviewCount()))\n",
        "\n",
        "    if band.GetRasterColorTable():\n",
        "      print(\"Band has a color table with {} entries\".format(band.GetRasterColorTable().GetCount()))\n",
        "\n",
        "\n",
        "def load_raster(path: str, use_only_band_index: int = -1) -> AmazonGeoTiff:\n",
        "  \"\"\"\n",
        "  TODO: Refactor (is_single_band, etc., should be a better design)\n",
        "  --> Find a way to simplify this logic. Maybe it needs to be more abstract.\n",
        "  \"\"\"\n",
        "  dataset = gdal.Open(path, gdal.GA_ReadOnly)\n",
        "  try:\n",
        "    print_raster_info(dataset)\n",
        "  except AttributeError as e:\n",
        "    raise OSError(\"Failed to print raster. This likely means it did not load properly from \"+ path)\n",
        "  image_datatype = dataset.GetRasterBand(1).DataType\n",
        "  mask_datatype = dataset.GetRasterBand(1).GetMaskBand().DataType\n",
        "  image = np.zeros((dataset.RasterYSize, dataset.RasterXSize, 12),\n",
        "                   dtype=gdal_array.GDALTypeCodeToNumericTypeCode(image_datatype))\n",
        "  mask = np.zeros((dataset.RasterYSize, dataset.RasterXSize, 12),\n",
        "                  dtype=gdal_array.GDALTypeCodeToNumericTypeCode(image_datatype))\n",
        "\n",
        "  if use_only_band_index == -1:\n",
        "    if dataset.RasterCount != 12 and dataset.RasterCount != 1:\n",
        "      raise ValueError(f\"Expected 12 raster bands (one for each month) or one annual average, but found {dataset.RasterCount}\")\n",
        "    if dataset.RasterCount == 1:\n",
        "      use_only_band_index = 0\n",
        "\n",
        "  is_single_band = use_only_band_index != -1\n",
        "\n",
        "  if is_single_band and use_only_band_index >= dataset.RasterCount:\n",
        "    raise IndexError(f\"Specified raster band index {use_only_band_index}\"\n",
        "                     f\" but there are only {dataset.RasterCount} rasters\")\n",
        "\n",
        "  for band_index in range(12):\n",
        "    band = dataset.GetRasterBand(use_only_band_index+1 if is_single_band else band_index+1)\n",
        "    image[:, :, band_index] = band.ReadAsArray()\n",
        "    mask[:, :, band_index] = band.GetMaskBand().ReadAsArray()\n",
        "  masked_image = np.ma.masked_where(mask == 0, image)\n",
        "  yearly_masked_image = masked_image.mean(axis=2)\n",
        "\n",
        "  return AmazonGeoTiff(dataset, image, mask, masked_image, yearly_masked_image)\n",
        "\n",
        "def get_extent(dataset):\n",
        "  geoTransform = dataset.GetGeoTransform()\n",
        "  minx = geoTransform[0]\n",
        "  maxy = geoTransform[3]\n",
        "  maxx = minx + geoTransform[1] * dataset.RasterXSize\n",
        "  miny = maxy + geoTransform[5] * dataset.RasterYSize\n",
        "  return Bounds(minx, maxx, miny, maxy, geoTransform[1], geoTransform[5], dataset.RasterXSize, dataset.RasterYSize)\n",
        "\n",
        "def plot_band(geotiff: AmazonGeoTiff, month_index, figsize=None):\n",
        "  if figsize:\n",
        "    plt.figure(figsize=figsize)\n",
        "  im = plt.imshow(geotiff.masked_image[:,:,month_index], extent=get_extent(geotiff.gdal_dataset).to_matplotlib(), interpolation='none')\n",
        "  plt.colorbar(im)\n",
        "\n",
        "def animate(geotiff: AmazonGeoTiff, nSeconds, fps):\n",
        "  fig = plt.figure( figsize=(8,8) )\n",
        "\n",
        "  months = []\n",
        "  labels = []\n",
        "  for m in range(12):\n",
        "    months.append(geotiff.masked_image[:,:,m])\n",
        "    labels.append(f\"Month: {m+1}\")\n",
        "  a = months[0]\n",
        "  extent = get_extent(geotiff.gdal_dataset).to_matplotlib()\n",
        "  ax = fig.add_subplot()\n",
        "  im = fig.axes[0].imshow(a, interpolation='none', aspect='auto', extent = extent)\n",
        "  txt = fig.text(0.3,0,\"\", fontsize=24)\n",
        "  fig.colorbar(im)\n",
        "\n",
        "  def animate_func(i):\n",
        "    if i % fps == 0:\n",
        "      print( '.', end ='' )\n",
        "\n",
        "    im.set_array(months[i])\n",
        "    txt.set_text(labels[i])\n",
        "    return [im, txt]\n",
        "\n",
        "  anim = animation.FuncAnimation(\n",
        "      fig,\n",
        "      animate_func,\n",
        "      frames = nSeconds * fps,\n",
        "      interval = 1000 / fps, # in ms\n",
        "  )\n",
        "  plt.close()\n",
        "\n",
        "  return anim\n",
        "\n",
        "def save_numpy_to_geotiff(bounds: Bounds, prediction: np.ma.MaskedArray, path: str):\n",
        "  \"\"\"Copy metadata from a base geotiff and write raster data + mask from `data`\"\"\"\n",
        "  driver = gdal.GetDriverByName(\"GTiff\")\n",
        "  metadata = driver.GetMetadata()\n",
        "  if metadata.get(gdal.DCAP_CREATE) != \"YES\":\n",
        "    raise RuntimeError(\"GTiff driver does not support required method Create().\")\n",
        "  if metadata.get(gdal.DCAP_CREATECOPY) != \"YES\":\n",
        "    raise RuntimeError(\"GTiff driver does not support required method CreateCopy().\")\n",
        "\n",
        "  dataset = driver.Create(path, bounds.raster_size_x, bounds.raster_size_y, prediction.shape[2], eType=gdal.GDT_Float64)\n",
        "  dataset.SetGeoTransform([bounds.minx, bounds.pixel_size_x, 0, bounds.maxy, 0, bounds.pixel_size_y])\n",
        "  dataset.SetProjection('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433],AUTHORITY[\"EPSG\",\"4326\"]]')\n",
        "\n",
        "  #dataset = driver.CreateCopy(path, base.gdal_dataset, strict=0)\n",
        "  if len(prediction.shape) != 3 or prediction.shape[0] != bounds.raster_size_x or prediction.shape[1] != bounds.raster_size_y:\n",
        "    raise ValueError(\"Shape of prediction does not match base geotiff\")\n",
        "  #if prediction.shape[2] > base.gdal_dataset.RasterCount:\n",
        "  #  raise ValueError(f\"Expected fewer than {dataset.RasterCount} bands in prediction but found {prediction.shape[2]}\")\n",
        "\n",
        "  prediction_transformed = np.flip(np.transpose(prediction, axes=[1,0,2]), axis=0)\n",
        "  for band_index in range(dataset.RasterCount):\n",
        "    band = dataset.GetRasterBand(band_index+1)\n",
        "    if band.CreateMaskBand(0) == gdal.CE_Failure:\n",
        "      raise RuntimeError(\"Failed to create mask band\")\n",
        "    mask_band = band.GetMaskBand()\n",
        "    band.WriteArray(np.choose(prediction_transformed[:, :, band_index].mask, (prediction_transformed[:, :, band_index].data,np.array(band.GetNoDataValue()),)))\n",
        "    mask_band.WriteArray(np.logical_not(prediction_transformed[:, :, band_index].mask))\n",
        "\n",
        "def coords_to_indices(bounds: Bounds, x: float, y: float):\n",
        "  if x < bounds.minx or x > bounds.maxx or y < bounds.miny or y > bounds.maxy:\n",
        "    raise ValueError(\"Coordinates out of bounds\")\n",
        "\n",
        "  # X => lat, Y => lon\n",
        "  x_idx = bounds.raster_size_y - int(math.ceil((y - bounds.miny) / abs(bounds.pixel_size_y)))\n",
        "  y_idx = int((x - bounds.minx) / abs(bounds.pixel_size_x))\n",
        "\n",
        "  return x_idx, y_idx\n",
        "\n",
        "\n",
        "def get_data_at_coords(dataset: AmazonGeoTiff, x: float, y: float, month: int) -> float:\n",
        "  # x = longitude\n",
        "  # y = latitude\n",
        "  bounds = get_extent(dataset.gdal_dataset)\n",
        "  x_idx, y_idx = coords_to_indices(bounds, x, y)\n",
        "  if month == -1:\n",
        "    value = dataset.yearly_masked_image[x_idx, y_idx]\n",
        "  else:\n",
        "    value = dataset.masked_image[x_idx, y_idx, month]\n",
        "  if np.ma.is_masked(value):\n",
        "    raise ValueError(\"Coordinates are masked\")\n",
        "  else:\n",
        "    return value"
      ],
      "metadata": {
        "id": "UetkQUIJ0nsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile xgb_lib.py\n",
        "import partition\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "def train_xgb(dataset: partition.PartitionedDataset, booster: str, rounds: int, verbose: int=0) -> xgb.XGBRegressor:\n",
        "  xgb_model = xgb.XGBRegressor(n_estimators=rounds, eta=0.1, max_depth=2, objective='reg:squarederror', booster=booster)\n",
        "  # split data into input and output columns\n",
        "  X, y = dataset.train.iloc[:, :-1], dataset.train.iloc[:, -1]\n",
        "  X_val, y_val = dataset.validation.iloc[:, :-1], dataset.validation.iloc[:, -1]\n",
        "  print(f\"Predicting: {dataset.train.columns[-1]}\")\n",
        "  xgb_model.fit(X, y, eval_set=[(X_val, y_val)], verbose=verbose)\n",
        "  return xgb_model\n",
        "\n",
        "\n",
        "def train_or_load_xgboost(basename: str, dataset: partition.PartitionedDataset, rounds: int=100000, verbose: int=0, rebuild: bool=False):\n",
        "  if rebuild:\n",
        "    print(\"Training model\")\n",
        "    model = train_xgb(dataset, booster='gblinear', rounds=rounds)\n",
        "    with open(f\"{basename}_config_xgb.json\", \"w\") as f:\n",
        "      f.write(model.get_booster().save_config())\n",
        "    model.save_model(f\"{basename}_xgb.json\")\n",
        "  else:\n",
        "    print(\"Loading model\")\n",
        "    model = xgb.XGBRegressor()\n",
        "    model.load_model(f\"{basename}_xgb.json\")\n",
        "    with open(f\"{basename}_config_xgb.json\", \"r\") as f:\n",
        "      model.get_booster().load_config(f.read())\n",
        "  print(f\"RMSE (validation): {model.evals_result()['validation_0']['rmse'][-1]}\")\n",
        "  return model"
      ],
      "metadata": {
        "id": "AbyWY3mG0XK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "7KN62u2n241x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8kW8zuCdZ6W",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Local modules\n",
        "# These modules are stored in the GitHub repo at ddf-isoscapes/xgboost\n",
        "import partition\n",
        "import geotiffs as gts\n",
        "import raster\n",
        "import xgb_lib as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Packaged modules\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from numpy.random import MT19937, RandomState, SeedSequence\n",
        "import pandas as pd\n",
        "import os.path\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "rc('animation', html='jshtml')"
      ],
      "metadata": {
        "id": "9VMeDchFsZuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fIR3BFLz2tzL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hfYnmaIq5-A"
      },
      "outputs": [],
      "source": [
        "#@title Debugging\n",
        "# See https://zohaib.me/debugging-in-google-collab-notebook/ for tips,\n",
        "# as well as docs for pdb and ipdb.\n",
        "DEBUG = False #@param {type:\"boolean\"}\n",
        "if DEBUG:\n",
        "    %pip install -Uqq ipdb\n",
        "    import ipdb\n",
        "    %pdb on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Parent Directories\n",
        "\n",
        "# Whether to use GDrive. Currently unsupported.\n",
        "USE_GDRIVE = False #@param {type:\"boolean\"}\n",
        "# Parent directory of data if not using GDrive.\n",
        "PARENT_DIR = os.path.expanduser(\"~/ddf/amazon_rainforest_files\") #@param\n",
        "\n",
        "# Whether to use MyDrive or a shared drive.\n",
        "USE_SHARED_GDRIVE = False #@param {type:\"boolean\"}\n",
        "# Parent directory of data in MyDrive.\n",
        "PERSONAL_GDRIVE_PARENT_DIR = \"amazon_rainforest_files\" #@param\n",
        "# Parent directory of data in shared drive. Should be prefixed with the shared drive name.\n",
        "SHARED_GDRIVE_PARENT_DIR = \"TNC Fellowship 🌳/4. Isotope Research & Signals/code/amazon_rainforest_files\" #@param\n",
        "\n",
        "_GDRIVE_BASE = None\n",
        "if USE_GDRIVE:\n",
        "  if USE_SHARED_GDRIVE:\n",
        "    _GDRIVE_BASE = f\"/content/drive/Shareddrives/{SHARED_GDRIVE_PARENT_DIR}\"\n",
        "  else:\n",
        "    _GDRIVE_BASE = f\"/content/drive/MyDrive/{PERSONAL_GDRIVE_PARENT_DIR}\"\n",
        "  _ROOT = _GDRIVE_BASE\n",
        "else:\n",
        "  _ROOT = PARENT_DIR\n"
      ],
      "metadata": {
        "id": "wIbsUjaAsZu1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Data Directories\n",
        "# Raster directory. Contains:\n",
        "# iso_O_cellulose.tif: Isoscape of 18O from Precipitation; <-- MODELING TARGET\n",
        "# Iso_Oxi_Stack.tif: Isoscape of 18O from Precipitation; <-- Model input\n",
        "# R.rh_Stack.tif: Atmospheric Relative humidity <-- Model input\n",
        "# R.vpd_Stack.tif: Vapor Pressure Deficit - VPD <-- Model input\n",
        "# Temperature_Stack.tif: Atmospheric Temperature <-- Model input\n",
        "RASTER_BASE = \"amazon_rasters\" #@param\n",
        "\n",
        "SAMPLE_DATA_BASE = \"amazon_sample_data\" #@param\n",
        "TEST_DATA_BASE = \"amazon_test_data\" #@param\n",
        "\n",
        "ANIMATIONS_BASE = \"amazon_animations\" #@param\n",
        "\n",
        "REBUILD_MODEL = False #@param {type:\"boolean\"}\n",
        "MODEL_BASE = \"amazon_isoscape_models\" #@param\n",
        "\n",
        "# How often should XGB log training metadata? 0 is the default, which indicates never.\n",
        "XGB_VERBOSITY_LEVEL = 0 #@param\n",
        "\n",
        "# Used to compute invalid terrain when making predictions. Leave disabled if on a low memory.\n",
        "LOAD_WATER_MASK_GEOTIFF = False #@param {type:\"boolean\"}\n",
        "LOAD_TREE_MASK_GEOTIFF = False #@param {type:\"boolean\"}\n",
        "\n",
        "# If true, requires soil and plant soil nitrogen geotiffs. Also requires the following files:\n",
        "# RASTER_BASE/raster_krig_d15N_soil_plant.tiff\n",
        "# RASTER_BASE/raster_krig_d15N_soil.tiff\n",
        "REGENERATE_PLANT_NITROGEN_GEOTIFF = False #@param {type:\"boolean\"}\n",
        "\n",
        "# If false, requires XGB oxygen isoscape in MODEL_BASE/predicted_isoscape_xgboost.tiff\n",
        "REGENERATE_OXYGEN_XGB_ISOSCAPE = False #@param {type:\"boolean\"}\n",
        "\n",
        "# If false, requires MODEL_BASE/xgb_means_oxygen_isoscape.tiff and MODEL_BASE/xgb_variances_oxygen_isoscape.tiff\n",
        "REGENERATE_OXYGEN_XGB_MEANS_VARIANCES = False #@param {type:\"boolean\"}\n",
        "\n",
        "# If false, requires MODEL_BASE/plant_nitrogen_isoscape_pooled_samples.numpy.mask and MODEL_BASE/plant_nitrogen_isoscape_pooled_samples.numpy.data\n",
        "REGENERATE_NITROGEN_ISOSCAPE = False #@param {type:\"boolean\"}"
      ],
      "metadata": {
        "id": "VVw24xuKsZu3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG24Qt6rq5-B"
      },
      "source": [
        "# Use Global Params to access files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oar1cwI2q5-B"
      },
      "outputs": [],
      "source": [
        "def get_raster_path(filename: str) -> str:\n",
        "  return f\"{_ROOT}/{RASTER_BASE}/{filename}\"\n",
        "\n",
        "def get_model_path(filename: str) -> str:\n",
        "  return f\"{_ROOT}/{MODEL_BASE}/{filename}\"\n",
        "\n",
        "def get_sample_db_path(filename: str) -> str:\n",
        "  return f\"{_ROOT}/{SAMPLE_DATA_BASE}/{filename}\"\n",
        "\n",
        "def get_animations_path(filename: str) -> str:\n",
        "  return f\"{_ROOT}/{ANIMATIONS_BASE}/{filename}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Mount GDrive\n",
        "# Access data stored on Google Drive\n",
        "if _GDRIVE_BASE:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "B1vqro0cq5-C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Se4DoSq5-B"
      },
      "source": [
        "# Load Rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwrDKcwmIu4f"
      },
      "outputs": [],
      "source": [
        "geotiffs = gts.Geotiffs(LOAD_WATER_MASK_GEOTIFF, LOAD_TREE_MASK_GEOTIFF, REGENERATE_PLANT_NITROGEN_GEOTIFF, get_raster_path(''))\n",
        "\n",
        "brazil_map_geotiff = geotiffs.brazil_map_geotiff() # mean annual precipitation\n",
        "# Will be used to compute isoscapes for carbon and nitrogen\n",
        "\n",
        "relative_humidity_geotiff = geotiffs.relative_humidity_geotiff()\n",
        "temperature_geotiff = geotiffs.temperature_geotiff()\n",
        "vapor_pressure_deficit_geotiff = geotiffs.vapor_pressure_deficit_geotiff()\n",
        "atmosphere_isoscape_geotiff = geotiffs.atmosphere_isoscape_geotiff()\n",
        "cellulose_isoscape_geotiff = geotiffs.cellulose_isoscape_geotiff()\n",
        "\n",
        "# Soil Geotiffs are not necessary to load, but required to build plant nitrogen geotiff.\n",
        "soil_plant_nitrogen_difference_isoscape_geotiff = geotiffs.soil_plant_nitrogen_difference_isoscape_geotiff()\n",
        "soil_nitrogen_isoscape_geotiff = geotiffs.soil_nitrogen_isoscape_geotiff()\n",
        "plant_nitrogen_isoscape_geotiff = geotiffs.plant_nitrogen_isoscape_geotiff()\n",
        "\n",
        "carbon_means_krig_isoscape_geotiff = geotiffs.carbon_means_krig_isoscape_geotiff()\n",
        "\n",
        "land_water_mask_geotiff = geotiffs.land_water_mask_geotiff()\n",
        "possible_tree_mask_geotiff = geotiffs.possible_tree_mask_geotiff()\n",
        "\n",
        "plant_nitrogen_isoscape_geotiff = geotiffs.plant_nitrogen_isoscape_geotiff()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-CHLVHQyD9U"
      },
      "source": [
        "# Train Isoscape Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTieS4NxKetw"
      },
      "source": [
        "## Preprocess\n",
        "\n",
        "Sample data from Martinelli's map of measurement sites to train fake isoscape models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNXqjSsTsMYy"
      },
      "outputs": [],
      "source": [
        "def gen_tabular_dataset(monthly: bool, samples_per_site: int) -> pd.DataFrame:\n",
        "  sample_site_coordinates = [(-70,-5,),(-67.5,0,),(-66,-4.5,),(-63,-9.5,),(-63,-9,),(-62,-6,),(-60,-2.5,),(-60,1,),(-60,-12.5,),(-59,-2.5,),(-57.5,-4,),(-55,-3.5,),(-54,-1,),(-52.5,-13,),(-51.5,-2.5,)]\n",
        "  sample_radius = 0.5\n",
        "  features = [relative_humidity_geotiff, temperature_geotiff, vapor_pressure_deficit_geotiff, atmosphere_isoscape_geotiff, cellulose_isoscape_geotiff]\n",
        "  image_feature_names = [\"rh\", \"temp\", \"vpd\", \"atmosphere_oxygen_ratio\", \"cellulose_oxygen_ratio\"]\n",
        "  feature_names = [\"lat\", \"lon\", \"month_of_year\"] + image_feature_names\n",
        "  rs = RandomState(MT19937(SeedSequence(42)))\n",
        "\n",
        "  feature_values = {}\n",
        "  for name in feature_names:\n",
        "    feature_values[name] = []\n",
        "\n",
        "  for coord in tqdm(sample_site_coordinates):\n",
        "    month_start = 0 if monthly else -1\n",
        "    month_end = 12 if monthly else 0\n",
        "    for month in range(month_start, month_end):\n",
        "      samples_collected = 0\n",
        "      while samples_collected < samples_per_site:\n",
        "        row = {}\n",
        "        sample_x, sample_y = 2*(rs.rand(2) - 0.5) * sample_radius\n",
        "        sample_x += coord[0]\n",
        "        sample_y += coord[1]\n",
        "\n",
        "        try:\n",
        "          for feature, feature_name in zip(features, image_feature_names):\n",
        "            row[feature_name] = raster.get_data_at_coords(feature, sample_x, sample_y, month)\n",
        "          row[\"month_of_year\"] = month\n",
        "          row[\"lon\"] = sample_x\n",
        "          row[\"lat\"] = sample_y\n",
        "          samples_collected += 1\n",
        "        except ValueError:\n",
        "          # masked and out-of-bounds coordinates\n",
        "          continue\n",
        "        for key, value in row.items():\n",
        "          feature_values[key].append(value)\n",
        "\n",
        "  samples = pd.DataFrame(feature_values)\n",
        "\n",
        "  if not monthly:\n",
        "    samples.drop(\"month_of_year\", axis=1, inplace=True)\n",
        "\n",
        "  return samples\n",
        "\n",
        "monthly_data_large = gen_tabular_dataset(monthly=True, samples_per_site=30)\n",
        "monthly_data_255_trees = gen_tabular_dataset(monthly=True, samples_per_site=17)\n",
        "yearly_data_large = gen_tabular_dataset(monthly=False, samples_per_site=30*12)\n",
        "yearly_data_255_trees = gen_tabular_dataset(monthly=False, samples_per_site=17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0dAH2eCmegR"
      },
      "outputs": [],
      "source": [
        "leaf_data = pd.read_csv(get_sample_db_path(\"pontos-vasp-cluster.csv\"))\n",
        "leaf_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eitP1hjX335"
      },
      "outputs": [],
      "source": [
        "def load_leaf_dataframe(db_path: str, isotope_col: str):\n",
        "  leaf_data = pd.read_csv(db_path)\n",
        "  leaf_data = leaf_data.rename(columns={\"latitude\": \"lat\", \"longitude\": \"lon\"})\n",
        "  leaf_df = leaf_data[[\"lon\", \"lat\", \"MAP\", \"MAT\", \"vap\", \"d15N_soil\", \"dem\", \"pa\", \"pet\", \"ph\", isotope_col]]\n",
        "  return leaf_df\n",
        "\n",
        "carbon_df = load_leaf_dataframe(get_sample_db_path(\"pontos-vasp-cluster.csv\"), \"d13C\")\n",
        "nitrogen_df = load_leaf_dataframe(get_sample_db_path(\"pontos-vasp-cluster.csv\"), \"d15N\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EW3f_q_sQbL"
      },
      "source": [
        "### Partition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FEXJ9fQtLM9"
      },
      "outputs": [],
      "source": [
        "yearly_large_partitioned = data.partition(yearly_data_large)\n",
        "data.print_split(yearly_large_partitioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DKNx1ckzaYN"
      },
      "outputs": [],
      "source": [
        "yearly_255_trees_partitioned = data.partition(yearly_data_255_trees)\n",
        "data.print_split(yearly_255_trees_partitioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06mCyO82w_n7"
      },
      "outputs": [],
      "source": [
        "monthly_large_partitioned = data.partition(monthly_data_large)\n",
        "data.print_split(monthly_large_partitioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9DQikZExELq"
      },
      "outputs": [],
      "source": [
        "monthly_255_trees_partitioned = data.partition(monthly_data_255_trees)\n",
        "data.print_split(monthly_255_trees_partitioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVTzinGguSFj"
      },
      "outputs": [],
      "source": [
        "nitrogen_df_partitioned = data.partition(nitrogen_df)\n",
        "data.print_split(nitrogen_df_partitioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T73lHHC9u6ol"
      },
      "outputs": [],
      "source": [
        "carbon_df_partitioned = data.partition(carbon_df)\n",
        "data.print_split(carbon_df_partitioned)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIu9xdqPBI_v"
      },
      "source": [
        "## XGBoost: Train XGBoost Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3btpGCb_q5-F"
      },
      "outputs": [],
      "source": [
        "# Validation RMSE xgboost: 0.306059 w/ 100,000 rounds\n",
        "# Validation RMSE google internal tooling: 0.39386\n",
        "yearly_255_trees_xgb_model = xgb.train_or_load_xgboost(\n",
        "  get_model_path(\"oxygen_isoscape_model\"),\n",
        "  yearly_255_trees_partitioned,\n",
        "  rounds=100000,\n",
        "  verbose=XGB_VERBOSITY_LEVEL,\n",
        "  rebuild=REBUILD_MODEL\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKHBDwd0w3SD"
      },
      "outputs": [],
      "source": [
        "# HMM, post-bugfix, Carbon might diverge too.\n",
        "carbon_isoscape_model = xgb.train_or_load_xgboost(get_model_path(\"carbon_isoscape_model\"), carbon_df_partitioned, verbose=XGB_VERBOSITY_LEVEL, rebuild=REBUILD_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y56PuvBSySRG"
      },
      "outputs": [],
      "source": [
        "# Validation loss seems to diverge\n",
        "nitrogen_isoscape_model = xgb.train_or_load_xgboost(get_model_path(\"nitrogen_isoscape_model\"), nitrogen_df_partitioned, rounds=10000, verbose=XGB_VERBOSITY_LEVEL, rebuild=REBUILD_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLigqWVAEGiP"
      },
      "source": [
        "### Test XGBoost Model Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN9XREX4-zc-"
      },
      "source": [
        "**In addition to unit tests, we also trained a model assuming 255 trees sampled monthly.**\n",
        "\n",
        "Preserving this as text only because it is not realistic as of 2023.\n",
        "\n",
        "Validation RMSE xgboost: 0.29072 \\\n",
        "Validation RMSE Google internal tooling: 0.29183 \\\n",
        "`monthly_255_trees_xgb_model = train_xgb(monthly_255_trees_partitioned, booster='gbtree', rounds=15000, XGB_VERBOSITY_LEVEL)`\n",
        "\n",
        "For the best results here, add `max_depth=2` to XGBRegressor params."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn1bp9YzJXDW"
      },
      "source": [
        "# Compute Isoscapes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSSdzTLbfBWS"
      },
      "source": [
        "## XGBoost: Compute AI-Predicted Isoscape\n",
        "\n",
        "Required: REGENERATE_OXYGEN_XGB_ISOSCAPE == true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBUsHhz2n51q"
      },
      "outputs": [],
      "source": [
        "def get_xgb_isoscape_prediction():\n",
        "  bounds = raster.get_extent(cellulose_isoscape_geotiff.gdal_dataset)\n",
        "  features = [relative_humidity_geotiff, temperature_geotiff, vapor_pressure_deficit_geotiff, atmosphere_isoscape_geotiff]\n",
        "  image_feature_names = [\"rh\", \"temp\", \"vpd\", \"atmosphere_oxygen_ratio\"]\n",
        "  #feature_names = [\"lat\", \"lon\", \"month_of_year\"] + image_feature_names\n",
        "  feature_names = [\"lat\", \"lon\"] + image_feature_names\n",
        "  predicted_isoscape = np.ma.array(np.zeros([bounds.raster_size_x, bounds.raster_size_y, 1], dtype=float), mask=np.ones([bounds.raster_size_x, bounds.raster_size_y, 1], dtype=bool))\n",
        "\n",
        "  for x_idx, x in enumerate(tqdm(np.arange(bounds.minx, bounds.maxx, bounds.pixel_size_x, dtype=float))):\n",
        "    rows = []\n",
        "    row_indexes = []\n",
        "    for y_idx, y in enumerate(np.arange(bounds.miny, bounds.maxy, -bounds.pixel_size_y, dtype=float)):\n",
        "      #for month in range(12):\n",
        "      month = 0\n",
        "      row = {}\n",
        "      try:\n",
        "        for feature, feature_name in zip(features, image_feature_names):\n",
        "          row[feature_name] = raster.get_data_at_coords(feature, x, y, month)\n",
        "        #row[\"month_of_year\"] = month\n",
        "        row[\"lon\"] = x\n",
        "        row[\"lat\"] = y\n",
        "      except ValueError:\n",
        "        # masked and out-of-bounds coordinates\n",
        "        continue\n",
        "      except IndexError:\n",
        "        continue\n",
        "      rows.append(row)\n",
        "      row_indexes.append((y_idx,month,))\n",
        "    if (len(rows) > 0):\n",
        "      reordered = pd.DataFrame(rows)[yearly_255_trees_xgb_model.get_booster().feature_names]\n",
        "      predictions = yearly_255_trees_xgb_model.predict(reordered)\n",
        "      predictions_np = predictions\n",
        "      for prediction, (y_idx, month_idx) in zip(predictions_np, row_indexes):\n",
        "        predicted_isoscape.mask[x_idx,y_idx,month_idx] = False # unmask since we have data\n",
        "        predicted_isoscape.data[x_idx,y_idx,month_idx] = prediction\n",
        "\n",
        "  return predicted_isoscape\n",
        "\n",
        "if REGENERATE_OXYGEN_XGB_ISOSCAPE:\n",
        "  xgb_isoscape_prediction = get_xgb_isoscape_prediction()\n",
        "  raster.save_numpy_to_geotiff(raster.get_extent(cellulose_isoscape_geotiff.gdal_dataset), xgb_isoscape_prediction, get_model_path(\"predicted_isoscape_xgboost.tiff\"))\n",
        "  plt.imshow(xgb_isoscape_prediction)\n",
        "\n",
        "# TODO: TESTME!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuExDB3lEXV9"
      },
      "source": [
        "## Turn XGBoost isoscape into a Gaussian distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igc1BbzJEW7b"
      },
      "outputs": [],
      "source": [
        "predicted_cellulose_isoscape_geotiff = raster.load_raster(get_model_path(\"predicted_isoscape_xgboost.tiff\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLOJhVNqFUV_"
      },
      "outputs": [],
      "source": [
        "plt.imshow(predicted_cellulose_isoscape_geotiff.yearly_masked_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YN8UnykFXs5"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "def get_2d_gaussian(center_lon: float, center_lat: float, stdev: float):\n",
        "  \"\"\"Quick-and-dirty function to get a PDF for sampling from an image\n",
        "  to turn it into a distribution. Intended for use with isoscapes.\n",
        "\n",
        "  Major room for improvement! This framing assumes no distortion from the\n",
        "  projection, i.e. that 1 deg latitude == 1 deg longitude == 111 km everywhere.\n",
        "  This should probably be fine for Brazil, for now, since it's near the Equator.\n",
        "  \"\"\"\n",
        "  rv = multivariate_normal([center_lat, center_lon], [[stdev, 0], [0, stdev]])\n",
        "\n",
        "  return rv\n",
        "\n",
        "# x = longitude\n",
        "# y = latitude\n",
        "def plot_gaussian(rv, shape: raster.Bounds):\n",
        "  \"\"\"Informative, for debugging and visualizing get_2d_gaussian().\"\"\"\n",
        "  x = np.linspace(shape.minx, shape.maxx, shape.raster_size_x)\n",
        "  y = np.linspace(shape.maxy, shape.miny, shape.raster_size_y) # inverted y axis\n",
        "  X, Y = np.meshgrid(x,y)\n",
        "  target = np.empty((shape.raster_size_y, shape.raster_size_x,2,), dtype=float)\n",
        "  target[:, :, 0] = Y\n",
        "  target[:, :, 1] = X\n",
        "  pd = rv.pdf(target)\n",
        "  plt.imshow(pd)\n",
        "  plt.colorbar()\n",
        "\n",
        "# TODO: For each pixel in the predicted isoscape\n",
        "# Yeah, this is basically Gaussian blur, huh...\n",
        "# BUT, it does give us a distribution.\n",
        "def gaussian_kernel(input: raster.AmazonGeoTiff, stdev_in_degrees: float=1):\n",
        "  bounds = raster.get_extent(input.gdal_dataset)\n",
        "  means = np.ma.zeros((bounds.raster_size_x, bounds.raster_size_y,), dtype=float)\n",
        "  means.mask = np.ones((bounds.raster_size_x, bounds.raster_size_y), dtype=bool)\n",
        "  variances = np.ma.zeros((bounds.raster_size_x, bounds.raster_size_y,), dtype=float)\n",
        "  variances.mask = np.ones((bounds.raster_size_x, bounds.raster_size_y), dtype=bool)\n",
        "  for map_y in tqdm(range(0, bounds.raster_size_y, 1)):\n",
        "    y_coord = map_y * abs(bounds.pixel_size_y) + bounds.miny\n",
        "    for map_x in range(0, bounds.raster_size_x, 1):\n",
        "      x_coord = map_x * abs(bounds.pixel_size_x) + bounds.minx\n",
        "      rv = get_2d_gaussian(y_coord, x_coord, stdev_in_degrees)\n",
        "      rsamp = rv.rvs(1000)\n",
        "      values = []\n",
        "      for coordinate_pair in rsamp:\n",
        "        try:\n",
        "          #print(coordinate_pair)\n",
        "          values.append(raster.get_data_at_coords(input, coordinate_pair[0], coordinate_pair[1], 0))\n",
        "        except ValueError:\n",
        "          pass\n",
        "        if len(values) == 30:\n",
        "          break\n",
        "      if len(values) == 30:\n",
        "        # Set the mean and stdev pixels\n",
        "        #print(x_coord, y_coord)\n",
        "        means[map_x, map_y] = np.mean(values)\n",
        "        variances[map_x, map_y] = np.var(values)\n",
        "        # Apply sample corrective factor to variance\n",
        "        variances[map_x, map_y] *= len(values) / (len(values)-1)\n",
        "        means.mask[map_x, map_y] = False\n",
        "        variances.mask[map_x, map_y] = False\n",
        "  return means, variances\n",
        "\n",
        "rv = get_2d_gaussian(-60.16, 4.11, 1)\n",
        "plot_gaussian(rv, raster.get_extent(predicted_cellulose_isoscape_geotiff.gdal_dataset))\n",
        "\n",
        "if REGENERATE_OXYGEN_XGB_MEANS_VARIANCES:\n",
        "  xgb_means, xgb_variances = gaussian_kernel(predicted_cellulose_isoscape_geotiff, stdev_in_degrees=0.1)\n",
        "  bds = raster.get_extent(predicted_cellulose_isoscape_geotiff.gdal_dataset)\n",
        "  raster.save_numpy_to_geotiff(bds, np.expand_dims(xgb_means, axis=2), get_model_path(\"xgb_means_oxygen_isoscape.tiff\"))\n",
        "  raster.save_numpy_to_geotiff(bds, np.expand_dims(xgb_variances, axis=2), get_model_path(\"xgb_variances_oxygen_isoscape.tiff\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-D1eG5P1T02"
      },
      "outputs": [],
      "source": [
        "xgb_means_oxygen_geotiff = raster.load_raster(get_model_path(\"xgb_means_oxygen_isoscape.tiff\"))\n",
        "xgb_variances_oxygen_geotiff = raster.load_raster(get_model_path(\"xgb_variances_oxygen_isoscape.tiff\"))\n",
        "\n",
        "# Until we re-generate the map\n",
        "xgb_variances_oxygen_geotiff.yearly_masked_image *= (5 / 4)\n",
        "xgb_variances_oxygen_geotiff.masked_image *= (5 / 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QqM7wL51X0o"
      },
      "outputs": [],
      "source": [
        "raster.plot_band(xgb_means_oxygen_geotiff, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl_G90Be3nxn"
      },
      "outputs": [],
      "source": [
        "raster.plot_band(xgb_variances_oxygen_geotiff, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxh8_X4mvY1F"
      },
      "outputs": [],
      "source": [
        "raster.get_data_at_coords(xgb_means_oxygen_geotiff, -60, 4, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5xxL_SG24E_"
      },
      "source": [
        "# Evaluate Precision, Recall From Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVzLYW_Pc-UM"
      },
      "source": [
        "## Oxygen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D18w0h0229GE"
      },
      "outputs": [],
      "source": [
        "real_samples = pd.read_csv(get_sample_db_path(\"38_ISOTOPE RESULTS _VARIABLES_10032023.csv\"), sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7D4JE2GJQEQ"
      },
      "outputs": [],
      "source": [
        "import scipy.stats\n",
        "import random\n",
        "g = real_samples.groupby(['lat','long'])['d18O sample']\n",
        "print('lat,long,mean,var,n,p_value,reject?')\n",
        "print(\"Real Coordinates, Real Samples\")\n",
        "all_coords = []\n",
        "for coords, x in g:\n",
        "  if x.size > 1:\n",
        "    lat = coords[0]\n",
        "    lon = coords[1]\n",
        "    all_coords.append(coords)\n",
        "    lab_samp_mean = x.mean()\n",
        "    lab_samp_var = x.var()*(x.size / (x.size - 1))\n",
        "    lab_samp_size = x.size\n",
        "    sumauma_samp_mean = raster.get_data_at_coords(xgb_means_oxygen_geotiff, lon, lat, 0)\n",
        "    sumauma_samp_var = raster.get_data_at_coords(xgb_variances_oxygen_geotiff, lon, lat, 0)\n",
        "    sumauma_samp_size = 5\n",
        "    _, p_value = scipy.stats.ttest_ind_from_stats(sumauma_samp_mean, math.sqrt(sumauma_samp_var), sumauma_samp_size, lab_samp_mean, math.sqrt(lab_samp_var), lab_samp_size, equal_var=False, alternative=\"two-sided\")\n",
        "    print(f\"{lat},{lon},{lab_samp_mean},{lab_samp_var},{lab_samp_size},{p_value},{p_value < 0.05}\")\n",
        "\n",
        "print(\"Fake Coordinates, Real Samples\")\n",
        "for _, x in g:\n",
        "  if x.size > 1:\n",
        "    coords = random.choice(all_coords)\n",
        "    lat = coords[0]\n",
        "    lon = coords[1]\n",
        "    lab_samp_mean = x.mean()\n",
        "    lab_samp_var = x.var()*(x.size / (x.size - 1))\n",
        "    lab_samp_size = x.size\n",
        "    sumauma_samp_mean = raster.get_data_at_coords(xgb_means_oxygen_geotiff, lon, lat, 0)\n",
        "    sumauma_samp_var = raster.get_data_at_coords(xgb_variances_oxygen_geotiff, lon, lat, 0)\n",
        "    sumauma_samp_size = 5\n",
        "    _, p_value = scipy.stats.ttest_ind_from_stats(sumauma_samp_mean, math.sqrt(sumauma_samp_var), sumauma_samp_size, lab_samp_mean, math.sqrt(lab_samp_var), lab_samp_size, equal_var=False, alternative=\"two-sided\")\n",
        "    print(f\"{lat},{lon},{lab_samp_mean},{lab_samp_var},{lab_samp_size},{p_value},{p_value < 0.05}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OULPrUeCdBNh"
      },
      "source": [
        "## Carbon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNzQwj5zJ1WB"
      },
      "outputs": [],
      "source": [
        "real_samples_old = pd.read_csv(get_sample_db_path(\"pontos-vasp-cluster.csv\"), sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvzTB2gaGhXL"
      },
      "outputs": [],
      "source": [
        "carbon_means_geotiff = raster.load_raster(get_raster_path(\"iso_d13C_map_wood_stack.tiff\"), use_only_band_index=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXLWv_n9IzUk"
      },
      "outputs": [],
      "source": [
        "carbon_variances_geotiff = raster.load_raster(get_raster_path(\"iso_d13C_map_wood_stack.tiff\"), use_only_band_index=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XoTOHSwKCwp"
      },
      "outputs": [],
      "source": [
        "import scipy.stats\n",
        "g = real_samples_old.groupby(['latitude','longitude'])[['d13C', 'd15N']]\n",
        "# Assume sample variance == Kriging variance b/c we only have means for the samples in this CSV\n",
        "print('lat,long,mean,var,n,p_value,reject?')\n",
        "print(\"Real Coordinates, Real Samples\")\n",
        "all_coords = []\n",
        "for coords, x in g:\n",
        "  try:\n",
        "    if x.size >= 1:\n",
        "      lat = coords[0]\n",
        "      lon = coords[1]\n",
        "      all_coords.append(coords)\n",
        "      lab_samp_size = 5\n",
        "      sumauma_samp_size = 5 # for all we know\n",
        "\n",
        "      # d13C p-value\n",
        "      lab_samp_mean_d13c = x['d13C'].mean()\n",
        "      lab_samp_var_d13c = raster.get_data_at_coords(carbon_variances_geotiff, lon, lat, 0)\n",
        "      sumauma_samp_mean_d13c = raster.get_data_at_coords(carbon_means_geotiff, lon, lat, 0)\n",
        "      sumauma_samp_var_d13c = raster.get_data_at_coords(carbon_variances_geotiff, lon, lat, 0)\n",
        "      _, p_value_d13c = scipy.stats.ttest_ind_from_stats(sumauma_samp_mean_d13c, math.sqrt(sumauma_samp_var_d13c), sumauma_samp_size, lab_samp_mean_d13c, math.sqrt(lab_samp_var_d13c), lab_samp_size, equal_var=True, alternative=\"two-sided\")\n",
        "\n",
        "      # d15N p-value\n",
        "      if False:\n",
        "        # Waiting for real d15N rasters\n",
        "        lab_samp_mean_d15n = x['d15N'].mean()\n",
        "        lab_samp_var_d15n = raster.get_data_at_coords(nitrogen_variances_geotiff, lon, lat, 0)\n",
        "        sumauma_samp_mean_d15n = raster.get_data_at_coords(nitrogen_means_geotiff, lon, lat, 0)\n",
        "        sumauma_samp_var_d15n = raster.get_data_at_coords(nitrogen_variances_geotiff, lon, lat, 0)\n",
        "        _, p_value_d15n = scipy.stats.ttest_ind_from_stats(sumauma_samp_mean_d15n, math.sqrt(sumauma_samp_var_d15n), sumauma_samp_size, lab_samp_mean_d15n, math.sqrt(lab_samp_var_d15n), lab_samp_size, equal_var=True, alternative=\"two-sided\")\n",
        "      print(f\"{lat},{lon},{lab_samp_mean_d13c},{lab_samp_var_d13c},{lab_samp_size},{p_value_d13c},{p_value_d13c < 0.05}\")\n",
        "  except ValueError:\n",
        "    pass\n",
        "\n",
        "print(\"Fake Coordinates, Real Samples\")\n",
        "for real_coords, x in g:\n",
        "  try:\n",
        "    if x.size >= 1:\n",
        "      coords = random.choice(all_coords)\n",
        "      while coords[0] == real_coords[0] and coords[1] == real_coords[1]:\n",
        "        coords = random.choice(all_coords)\n",
        "      lat = coords[0]\n",
        "      lon = coords[1]\n",
        "      lab_samp_mean = x['d13C'].mean()\n",
        "      lab_samp_var = raster.get_data_at_coords(carbon_variances_geotiff, lon, lat, 0)\n",
        "      lab_samp_size = x.size\n",
        "      sumauma_samp_mean = raster.get_data_at_coords(carbon_means_geotiff, lon, lat, 0)\n",
        "      sumauma_samp_var = raster.get_data_at_coords(carbon_variances_geotiff, lon, lat, 0)\n",
        "      sumauma_samp_size = 5\n",
        "      _, p_value = scipy.stats.ttest_ind_from_stats(sumauma_samp_mean, math.sqrt(sumauma_samp_var), sumauma_samp_size, lab_samp_mean, math.sqrt(lab_samp_var), lab_samp_size, equal_var=True, alternative=\"two-sided\")\n",
        "      print(f\"{lat},{lon},{lab_samp_mean},{lab_samp_var},{lab_samp_size},{p_value},{p_value < 0.05}\")\n",
        "  except ValueError:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-3_Jvn90CCw"
      },
      "source": [
        "# A probability that a coordinate matches a sample given a predicted isoscape\n",
        "\n",
        "1. Compute t-intervals for a coordinate pixel on the paperwork based on the output of the AI isoscape model\n",
        "2. For a given sample, compute a z-score from the estimated t-distribution, and turn that into a p-value based on two-sided area under the curve: p($\\in$ isotope distribution | possible coordinates, isoscape). *This can be combined with other knowledge in the future to get, for example, p($\\in$ isoscape distribution $\\land$ tree rings look right for the area | possible coordinates, isoscape, tree ring knowledge).*\n",
        "3. Depending on the p-value, reject the null hypothesis that the paperwork is correct\n",
        "\n",
        "Key challenge with this approach: The statistical bound on false positives is per-pixel, and only guarantees that it falls in a range of isoscape values associated with that location rather than the location itself. We will likely have multiple possible points of origin for each sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i6Ez45-C76Z"
      },
      "source": [
        "## Compute Sample Origin Given AI-Predicted Isoscapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ti0_jhDby1l"
      },
      "outputs": [],
      "source": [
        "predicted_cellulose_isoscape_geotiff = raster.load_raster(get_model_path(\"predicted_isoscape_xgboost.tiff\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygeZCfWtWor8"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    np.sum(predicted_cellulose_isoscape_geotiff.masked_image.mask[0,:,:])\n",
        "except IndexError as err:\n",
        "    raise IndexError(err + \" If you're seeing this error, the image mask is unexpectedly missing. You might need to rerun the Oxygen xgboost trainer.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HnJTO9Ib9hu"
      },
      "outputs": [],
      "source": [
        "raster.plot_band(predicted_cellulose_isoscape_geotiff, 1, figsize=(12,12))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL9f3XYOAZma"
      },
      "outputs": [],
      "source": [
        "plt.imshow(predicted_cellulose_isoscape_geotiff.masked_image.data[:,:,3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJf8ol4x6ZFO"
      },
      "source": [
        "### Compute p-values for a sample\n",
        "Null hypotheses in these tests always involve equality. Our null hypothesis is that the sample could reasonably come from any given location. We reject that null hypothesis when it is very improbably that the sample came from a given location (i.e. p < c).\n",
        "\n",
        "Should be a two-sample test: One sample from the Craig-Gordon or ML approximation, one sample from the tree cellulose, ideally with N >= 30 for each.\n",
        "\n",
        "\\\n",
        "H0 (null hypothesis): pixel group == sample; i.e. the sample could be from x \\\n",
        "Ha (alternative hypothesis): pixel group != sample; i.e. the sample might not be from x\n",
        "\n",
        "\\\n",
        "Many coordinates work well like (-55,-5), but some-- especially those affected by the saturation-- don't, like (-55,-10). We would incorrectly rule those areas out as potential origins of a wood sample by rejecting the null hypothesis at reasonable p-values like 0.05. We could require substantially lower p-values to reject, but that would come at the cost of ruling very little out (i.e. not the most useful model).\n",
        "**TODO: To remedy this, consider using a larger sample size (> 1 pixel) for the isoscape side of the t-test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWX-undaaqab"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9jj3uRgqOoW"
      },
      "outputs": [],
      "source": [
        "def make_isoscape_with_pooled_sample_dimension_from_2d_isoscape(input: np.ma.MaskedArray, radius: int = 5):\n",
        "\n",
        "  canvas = np.zeros((radius*3, radius*3), dtype=float)\n",
        "  x_origin = int((radius*3)/2)\n",
        "  y_origin = int((radius*3)/2)\n",
        "  for y in range(-radius, radius):\n",
        "    for x in range(-radius, radius):\n",
        "      if (x*x+y*y <= radius*radius - radius):\n",
        "        x_pixel = min(max(x+x_origin, 0), radius*3-1)\n",
        "        y_pixel = min(max(y+y_origin, 0), radius*3-1)\n",
        "        canvas[x_pixel, y_pixel] = 1\n",
        "  area = int(sum(canvas.flatten()))\n",
        "\n",
        "  pooled = np.ma.MaskedArray(data=np.zeros((input.shape[0], input.shape[1], area), dtype=float),\n",
        "                             mask=np.repeat(input.mask[:, :, np.newaxis], area, axis=2))\n",
        "\n",
        "\n",
        "  for x_origin in tqdm(range(input.shape[0])):\n",
        "    for y_origin in range(input.shape[1]):\n",
        "      pooling_counter = 0\n",
        "      for y in range(-radius, radius):\n",
        "        for x in range(-radius, radius):\n",
        "          if (x*x+y*y <= radius*radius - radius):\n",
        "            x_pixel = x+x_origin\n",
        "            y_pixel = y+y_origin\n",
        "            x_pixel_clamped = min(max(x_pixel, 0), input.shape[0]-1)\n",
        "            y_pixel_clamped = min(max(y_pixel, 0), input.shape[1]-1)\n",
        "            if x_pixel_clamped == x_pixel and y_pixel_clamped == y_pixel:\n",
        "              pooled[x_origin, y_origin, pooling_counter] = input[x_pixel, y_pixel]\n",
        "              pooling_counter += 1\n",
        "      if pooling_counter < area:\n",
        "        samples_to_mask = area - pooling_counter\n",
        "        pooled.mask[x_origin, y_origin, pooling_counter:] = True\n",
        "  return pooled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS5Kc8_8uqkn"
      },
      "outputs": [],
      "source": [
        "if REGENERATE_NITROGEN_ISOSCAPE:\n",
        "    plant_nitrogen_isoscape_pooled_samples = make_isoscape_with_pooled_sample_dimension_from_2d_isoscape(plant_nitrogen_isoscape_geotiff.yearly_masked_image, 5)\n",
        "    np.save(get_model_path(\"plant_nitrogen_isoscape_pooled_samples.numpy.mask\"), plant_nitrogen_isoscape_pooled_samples.mask)\n",
        "    np.save(get_model_path(\"plant_nitrogen_isoscape_pooled_samples.numpy.data\"), plant_nitrogen_isoscape_pooled_samples.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkCoMp6WpkRT"
      },
      "outputs": [],
      "source": [
        "mask = np.load(get_model_path(\"plant_nitrogen_isoscape_pooled_samples.numpy.mask.npy\"))\n",
        "data = np.load(get_model_path(\"plant_nitrogen_isoscape_pooled_samples.numpy.data.npy\"))\n",
        "plant_nitrogen_isoscape_pooled_samples = np.ma.MaskedArray(data, mask=mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MWjUAXASpJe"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "import scipy.stats\n",
        "import cv2\n",
        "\n",
        "def sample_from_geotiff(x, y, sample_geotiff, sample_radius, num_samples):\n",
        "  \"\"\"\n",
        "  Take a sample from the \"real\" data.\n",
        "  In practice, this would be multiple wood samples from the same furniture\n",
        "  Here, these come from slightly different pixels in a geotiff.\n",
        "  \"\"\"\n",
        "\n",
        "  coord = (x, y,)\n",
        "  rs = RandomState(MT19937(SeedSequence(42)))\n",
        "  samples = []\n",
        "\n",
        "  for _ in range(num_samples):\n",
        "    sample_x, sample_y = 2*(rs.rand(2) - 0.5) * sample_radius\n",
        "    sample_x += coord[0]\n",
        "    sample_y += coord[1]\n",
        "    total_months = sample_geotiff.masked_image.shape[2]\n",
        "    monthly_readings = [raster.get_data_at_coords(sample_geotiff, sample_x, sample_y, month) for month in range(total_months)]\n",
        "    samples.append(np.mean(monthly_readings))\n",
        "  return samples\n",
        "\n",
        "def fake_t_test(isoscape_geotiff_masked_image, cellulose_sample_x, cellulose_sample_y, cellulose_samples_geotiff, num_cellulose_samples):\n",
        "  \"\"\"Called a \"fake\" t-test because we randomly sample points close to\n",
        "     (cellulose_sample_x, cellulose_sample_y) from `cellulose_samples_geotiff`\n",
        "     to mimic taking samples out of the same piece of furniture.\n",
        "\n",
        "     We then perform a series of two-sample t-tests between this fake\n",
        "     furniture/timber sample and each coordinate of isoscape_geotiff\n",
        "  \"\"\"\n",
        "  fake_sample = sample_from_geotiff(cellulose_sample_x, cellulose_sample_y, cellulose_samples_geotiff, raster.get_extent(cellulose_samples_geotiff.gdal_dataset).pixel_size_x*50, num_cellulose_samples)\n",
        "  shape = isoscape_geotiff_masked_image.shape\n",
        "  return scipy.stats.ttest_ind(isoscape_geotiff_masked_image, np.tile(fake_sample,(shape[0],shape[1],1)), axis=2, equal_var=False, alternative='two-sided')\n",
        "\n",
        "def crop_to_coordinates(original_extent: raster.Bounds, new_extent: raster.Bounds, original_image: np.ndarray):\n",
        "  x_off_deg = max((new_extent.minx - original_extent.minx), 0)\n",
        "  x_off_px = int(x_off_deg / abs(original_extent.pixel_size_x))\n",
        "  y_off_deg = max((new_extent.miny - original_extent.miny), 0)\n",
        "  y_off_px = int(y_off_deg / abs(original_extent.pixel_size_y))\n",
        "  x_max_deg = min(new_extent.maxx, original_extent.maxx)\n",
        "  x_max_px = int((x_max_deg - original_extent.minx) / abs(original_extent.pixel_size_x))\n",
        "  y_max_deg = min(new_extent.maxy, original_extent.maxy)\n",
        "  y_max_px = int((y_max_deg - original_extent.miny) / abs(original_extent.pixel_size_y))\n",
        "  # Pixels are stored in descending order if an axis has negative pixel size.\n",
        "  if original_extent.pixel_size_x < 0:\n",
        "    raise RuntimeError(\"Inverted X axis not supported\")\n",
        "  cropped_image = np.flip(original_image, axis=0) if original_extent.pixel_size_y < 0 else original_image\n",
        "  cropped_image = cropped_image[y_off_px:y_max_px, x_off_px:x_max_px]\n",
        "  cropped_image = np.flip(cropped_image, axis=0) if original_extent.pixel_size_y < 0 else cropped_image\n",
        "  resulting_bounds = raster.Bounds(original_extent.minx + x_off_deg,\n",
        "                            x_max_deg,\n",
        "                            original_extent.miny + y_off_deg,\n",
        "                            y_max_deg,\n",
        "                            original_extent.pixel_size_x,\n",
        "                            original_extent.pixel_size_y,\n",
        "                            cropped_image.shape[1],\n",
        "                            cropped_image.shape[0])\n",
        "  return cropped_image, resulting_bounds\n",
        "\n",
        "def pad_to_coordinates(original_extent: raster.Bounds, new_extent: raster.Bounds, original_image: np.ndarray, with_ones: bool = False):\n",
        "  \"\"\"Pads an image to new coordinates larger than the original.\n",
        "  Precondition: Must not specify negative padding (i.e. a crop)\n",
        "  \"\"\"\n",
        "  x_size = max(abs(int((new_extent.maxx - new_extent.minx) / original_extent.pixel_size_x)), original_image.T.shape[0])\n",
        "  y_size = max(abs(int((new_extent.maxy - new_extent.miny) / original_extent.pixel_size_y)), original_image.T.shape[1])\n",
        "  if with_ones:\n",
        "    padded_image = np.ones((x_size, y_size,), dtype=original_image.dtype)\n",
        "  else:\n",
        "    padded_image = np.zeros((x_size, y_size,), dtype=original_image.dtype)\n",
        "  x_offset = max(int((original_extent.minx - new_extent.minx) / abs(original_extent.pixel_size_x)), 0)\n",
        "  y_offset = max(int((original_extent.miny - new_extent.miny) / abs(original_extent.pixel_size_y)), 0)\n",
        "  # Correct rounding errors to satisfy the following by adjusting offsets:\n",
        "  # * x_offset + original_extent.raster_size_x <= padded_image.shape[0]\n",
        "  # --> x_offset <= padded_image.shape[0] - original_extent.raster_size_x\n",
        "  # * y_offset + original_extent.raster_size_y <= padded_image.shape[1]\n",
        "  x_offset = min(x_offset, padded_image.shape[0] - original_image.T.shape[0])\n",
        "  y_offset = min(y_offset, padded_image.shape[1] - original_image.T.shape[1])\n",
        "\n",
        "  # Pixels are stored in descending order if an axis has negative pixel size.\n",
        "  if original_extent.pixel_size_x < 0:\n",
        "    raise RuntimeError(\"Inverted X axis not supported\")\n",
        "  padded_image = np.flip(padded_image, axis=1) if original_extent.pixel_size_y < 0 else padded_image\n",
        "  original_image = np.flip(original_image, axis=0) if original_extent.pixel_size_y < 0 else original_image\n",
        "  padded_image[x_offset:x_offset+original_image.T.shape[0], y_offset:y_offset+original_image.T.shape[1]] = original_image.T\n",
        "  padded_image = np.flip(padded_image, axis=1) if original_extent.pixel_size_y < 0 else padded_image\n",
        "  return padded_image.T\n",
        "\n",
        "def upscale(new_size_x: int, new_size_y: int, original_image: np.ndarray):\n",
        "  \"\"\"Rescales an image of the same geographical extent with nearest-neighbor sampling.\n",
        "\n",
        "  Useful to match pixel size between images.\n",
        "  \"\"\"\n",
        "  resized_data = cv2.resize(original_image, dsize=(new_size_y, new_size_x), interpolation=cv2.INTER_NEAREST_EXACT)\n",
        "  return resized_data\n",
        "\n",
        "def align_to_bounds(original_bounds: raster.Bounds, new_bounds: raster.Bounds, original_image: np.ndarray, pad_with_ones: bool):\n",
        "  # TODO: UNIT TESTS (if we want to use this version in production)!!!\n",
        "  cropped, cropped_bounds = crop_to_coordinates(original_bounds, new_bounds, original_image)\n",
        "  padded = pad_to_coordinates(cropped_bounds, new_bounds, cropped, with_ones=pad_with_ones)\n",
        "  scaled = upscale(new_bounds.raster_size_x, new_bounds.raster_size_y, padded)\n",
        "  return scaled\n",
        "\n",
        "def combine_pvalues(pvalue_nitrogen, pvalue_oxygen, nitrogen_extent, oxygen_extent, nitrogen_mask, oxygen_mask):\n",
        "  # TODO: implement combine_pvalues() in a generic way to combine any two maskedarrays of p-values\n",
        "  # using align_to_bounds()\n",
        "  pvalue_nitrogen = pad_to_coordinates(nitrogen_extent, oxygen_extent, pvalue_nitrogen)\n",
        "  nitrogen_mask = pad_to_coordinates(nitrogen_extent, oxygen_extent, nitrogen_mask.astype(int), True).astype(bool)\n",
        "  pvalue_oxygen = upscale(pvalue_nitrogen.shape[0], pvalue_nitrogen.shape[1], pvalue_oxygen)\n",
        "  oxygen_mask = upscale(pvalue_nitrogen.shape[0], pvalue_nitrogen.shape[1], oxygen_mask.astype(int)).astype(bool)\n",
        "  both_mask = np.logical_or(oxygen_mask, nitrogen_mask)\n",
        "  pvalues_combined = np.ma.MaskedArray(pvalue_oxygen*pvalue_nitrogen, mask=both_mask)\n",
        "  result = np.ma.masked_array(np.where(both_mask, pvalue_oxygen, pvalues_combined), mask=oxygen_mask)\n",
        "  return result\n",
        "\n",
        "def evalutate_on_sample_from_point(x, y, p_threshold=0.05, num_cellulose_samples=10):\n",
        "  print(predicted_cellulose_isoscape_geotiff.masked_image.mask.shape)\n",
        "  default_mask = predicted_cellulose_isoscape_geotiff.masked_image.mask[:,:,0]\n",
        "  oxygen_extent=raster.get_extent(predicted_cellulose_isoscape_geotiff.gdal_dataset)\n",
        "  nitrogen_extent=raster.get_extent(plant_nitrogen_isoscape_geotiff.gdal_dataset)\n",
        "  nitrogen_mask = plant_nitrogen_isoscape_geotiff.yearly_masked_image.mask\n",
        "  oxygen_mask = predicted_cellulose_isoscape_geotiff.yearly_masked_image.mask\n",
        "  # Pessimistic case: Cellulose sample is faked with Craig-Gordon model, and\n",
        "  # the isoscape comes from the AI model. Nitrogen comes from Martinelli's\n",
        "  # geotiff (unknown ultimate source).\n",
        "  statistic_oxygen, pvalue_oxygen = fake_t_test(predicted_cellulose_isoscape_geotiff.masked_image, x, y, cellulose_isoscape_geotiff, num_cellulose_samples)\n",
        "  statistic_nitrogen, pvalue_nitrogen = fake_t_test(plant_nitrogen_isoscape_pooled_samples, x, y, plant_nitrogen_isoscape_geotiff, num_cellulose_samples)\n",
        "  pvalues_combined = combine_pvalues(pvalue_nitrogen, pvalue_oxygen, nitrogen_extent, oxygen_extent, nitrogen_mask, oxygen_mask)\n",
        "\n",
        "  pvalues_bounds = dataclasses.replace(oxygen_extent,\n",
        "                                       raster_size_x = pvalues_combined.shape[0],\n",
        "                                       raster_size_y = pvalues_combined.shape[1],\n",
        "                                       pixel_size_x = nitrogen_extent.pixel_size_x,\n",
        "                                       pixel_size_y = nitrogen_extent.pixel_size_y)\n",
        "  land_water_mask = align_to_bounds(raster.get_extent(land_water_mask_geotiff.gdal_dataset), pvalues_bounds, land_water_mask_geotiff.masked_image[:,:,0], pad_with_ones=True)\n",
        "  possible_tree_mask = align_to_bounds(raster.get_extent(possible_tree_mask_geotiff.gdal_dataset), pvalues_bounds, possible_tree_mask_geotiff.masked_image[:,:,0], pad_with_ones=True)\n",
        "\n",
        "  fig, ax = plt.subplots(2,2, figsize=(30,30))\n",
        "  plt.colorbar(ax[0,0].imshow(pvalues_combined, extent=oxygen_extent.to_matplotlib()), ax=ax[0,0])\n",
        "  ax[0,0].set_title(\"p-values\")\n",
        "  invalid_terrain = np.logical_or(np.logical_or(pvalues_combined.mask, land_water_mask), np.logical_not(possible_tree_mask))\n",
        "  ax[0,1].imshow(np.ma.masked_array(pvalues_combined < p_threshold, mask=invalid_terrain), extent=oxygen_extent.to_matplotlib())\n",
        "  circle1 = plt.Circle((x, y), 1, color='r')\n",
        "  ax[0,1].add_patch(circle1)\n",
        "  ax[0,1].set_title(f\"p < {p_threshold}\")\n",
        "  ax[0,0].set_ylabel(\"Pessimistic Case\")\n",
        "\n",
        "  print(f\"Invalid p-values: {np.sum(pvalues_combined.flatten() > 1)}\")\n",
        "  # Optimistic case: Cellulose sample is faked with Craig-Gordon model, and\n",
        "  # the isoscape also comes from the Craig-Gordon model.\n",
        "  statistic_oxygen, pvalue_oxygen = fake_t_test(cellulose_isoscape_geotiff.masked_image, x, y, cellulose_isoscape_geotiff, num_cellulose_samples)\n",
        "  pvalues_combined = combine_pvalues(pvalue_nitrogen, pvalue_oxygen, nitrogen_extent, oxygen_extent, nitrogen_mask, oxygen_mask)\n",
        "  oxygen_extent=raster.get_extent(cellulose_isoscape_geotiff.gdal_dataset)\n",
        "  plt.colorbar(ax[1,0].imshow(pvalues_combined, extent=oxygen_extent.to_matplotlib()), ax=ax[1,0])\n",
        "  #ax[1,0].imshow(possible_tree_mask, extent=oxygen_extent.to_matplotlib())\n",
        "  #ax[1,0].imshow(invalid_terrain.astype(bool), extent=oxygen_extent.to_matplotlib())\n",
        "  ax[1,1].imshow(np.ma.masked_array(pvalues_combined < p_threshold, invalid_terrain), extent=oxygen_extent.to_matplotlib())\n",
        "  circle1 = plt.Circle((x, y), 1, color='r')\n",
        "  ax[1,1].add_patch(circle1)\n",
        "  ax[1,0].set_ylabel(\"Optimistic Case\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yobubaTYasaj"
      },
      "source": [
        "#### Story\n",
        "\n",
        "**Yellow: We reject the null hypothesis that the sample might have come from here because it is so improbable**\n",
        "\n",
        "##### Pessimistic Example: Outlier point sampled from outlier area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o0KKQ2wa3s2"
      },
      "outputs": [],
      "source": [
        "evalutate_on_sample_from_point(x=-55, y=-10, p_threshold=0.01, num_cellulose_samples=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN40DClvbk-3"
      },
      "source": [
        "##### More Optimistic Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2CgtxSAbnKN"
      },
      "outputs": [],
      "source": [
        "evalutate_on_sample_from_point(x=-55, y=-5, p_threshold=0.01, num_cellulose_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFhsAHZcb7e3"
      },
      "outputs": [],
      "source": [
        "evalutate_on_sample_from_point(x=-65, y=-5, p_threshold=0.01, num_cellulose_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VjyYdLj2VGi"
      },
      "outputs": [],
      "source": [
        "# This is the Amazon biome shapefile we use:\n",
        "# https://services.arcgis.com/F7DSX1DSNSiWmOqh/arcgis/rest/services/lm_bioma_250/FeatureServer\n",
        "# http://geoftp.ibge.gov.br/informacoes_ambientais/estudos_ambientais/biomas/vetores/Biomas_250mil.zip\n",
        "\n",
        "# TODO: Figure out how to produce good isoscapes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6JSQQ2g8gS3"
      },
      "source": [
        "### Compute mean absolute error in km\n",
        "*Compute based on min, max, and median AE for each sample.*\n",
        "Also compute proportion of samples for which nothing passed our threshold.\n",
        "Ideally, plot these against each other on a curve while varying $c$ (0.95, 0.99, etc.).\n",
        "\n",
        "This may not be what we actually want because ultimately this is a binary classification problem. That framing may be more useful as a business metric."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4qCnFGsvx39s",
        "3-CHLVHQyD9U",
        "YTieS4NxKetw",
        "xR2Nz4gcsR6N",
        "ZMhPJFTo1FuI",
        "JbO_o5Gg1LHr",
        "U_2b5nUNxhnB",
        "wjHQ-Asj07XN",
        "i9M9pxxi1KOv",
        "lgfrhThP6KeY",
        "wSSdzTLbfBWS"
      ],
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
