{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnc-br/ddf-isoscapes/blob/UCDavisXGB/xgboost/hosted_runtime/xgboost_hosted_runtime.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyB5N9yngMo-"
      },
      "source": [
        "Copyright 2023 Google LLC.\n",
        "SPDX-License-Identifier: Apache-2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qCnFGsvx39s"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8kW8zuCdZ6W"
      },
      "outputs": [],
      "source": [
        "%pip install opencv-python\n",
        "%pip install matplotlib\n",
        "%pip install pandas\n",
        "\n",
        "from osgeo import gdal, gdal_array\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib import rc\n",
        "from typing import List\n",
        "from numpy.random import MT19937, RandomState, SeedSequence\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from io import StringIO\n",
        "import xgboost as xgb\n",
        "import os\n",
        "import math\n",
        "import glob\n",
        "\n",
        "rc('animation', html='jshtml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0tG92Yw1CYk"
      },
      "outputs": [],
      "source": [
        "# Raster directory. Contains:\n",
        "# iso_O_cellulose.tif: Isoscape of 18O from Precipitation; <-- MODELING TARGET\n",
        "# Iso_Oxi_Stack.tif: Isoscape of 18O from Precipitation; <-- Model input\n",
        "# R.rh_Stack.tif: Atmospheric Relative humidity <-- Model input\n",
        "# R.vpd_Stack.tif: Vapor Pressure Deficit - VPD <-- Model input\n",
        "# Temperature_Stack.tif: Atmospheric Temperature <-- Model input\n",
        "RASTER_BASE = \"/MyDrive/amazon_rainforest_files/amazon_rasters/\" #@param\n",
        "SAMPLE_DATA_BASE = \"/MyDrive/amazon_rainforest_files/amazon_sample_data/\" #@param\n",
        "TEST_DATA_BASE = \"/MyDrive/amazon_rainforest_files/amazon_test_data/\" #@param\n",
        "ANIMATIONS_BASE = \"/MyDrive/amazon_rainforest_files/amazon_animations/\" #@param\n",
        "GDRIVE_BASE = \"/content/drive\" #@param\n",
        "\n",
        "REBUILD_MODEL = True #@param {type:\"boolean\"}\n",
        "MODEL_BASE = \"/MyDrive/amazon_rainforest_files/amazon_isoscape_models/\" #@param\n",
        "\n",
        "# How often should XGB log training metadata? 0 is the default, which indicates never.\n",
        "XGB_VERBOSITY_LEVEL = 0 #@param\n",
        "\n",
        "# Used to compute invalid terrain when making predictions. Leave disabled if on a low memory.\n",
        "LOAD_WATER_MASK_GEOTIFF = False #@param {type:\"boolean\"}\n",
        "LOAD_TREE_MASK_GEOTIFF = False #@param {type:\"boolean\"}\n",
        "\n",
        "# If true, then we use a test set loaded from a CSV of real data.\n",
        "# The default is False, which means to simulate the test set using\n",
        "# a tiff isoscape to sample test points from.\n",
        "REFERENCE_CSV_FILENAME = \"2023_06_23_Results_Google.csv\" #@param\n",
        "USE_REFERENCE_SAMPLES_FOR_TRAINING = True #@param {type:\"boolean\"}\n",
        "\n",
        "# If true, requires soil and plant soil nitrogen geotiffs. Also requires the following files:\n",
        "# RASTER_BASE/raster_krig_d15N_soil_plant.tiff\n",
        "# RASTER_BASE/raster_krig_d15N_soil.tiff\n",
        "REGENERATE_PLANT_NITROGEN_GEOTIFF = False #@param {type:\"boolean\"}\n",
        "\n",
        "# If false, requires XGB oxygen isoscape in MODEL_BASE/predicted_isoscape_xgboost.tiff\n",
        "REGENERATE_OXYGEN_XGB_ISOSCAPE = True #@param {type:\"boolean\"}\n",
        "\n",
        "# If false, requires MODEL_BASE/xgb_means_oxygen_isoscape.tiff and MODEL_BASE/xgb_variances_oxygen_isoscape.tiff\n",
        "REGENERATE_OXYGEN_XGB_MEANS_VARIANCES = True #@param {type:\"boolean\"}\n",
        "\n",
        "# If false, requires MODEL_BASE/plant_nitrogen_isoscape_pooled_samples.numpy.mask and MODEL_BASE/plant_nitrogen_isoscape_pooled_samples.numpy.data\n",
        "REGENERATE_NITROGEN_ISOSCAPE = False #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJg6uDUysBnd"
      },
      "outputs": [],
      "source": [
        "#@title Debugging\n",
        "# See https://zohaib.me/debugging-in-google-collab-notebook/ for tips,\n",
        "# as well as docs for pdb and ipdb.\n",
        "DEBUG = False #@param {type:\"boolean\"}\n",
        "if DEBUG:\n",
        "    %pip install -Uqq ipdb\n",
        "    import ipdb\n",
        "    %pdb on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlM3FIGXx6F2"
      },
      "source": [
        "# Data Types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtFtoKnsx83s"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class AmazonGeoTiff:\n",
        "  \"\"\"Represents a geotiff from our dataset.\"\"\"\n",
        "  gdal_dataset: gdal.Dataset\n",
        "  image_value_array: np.ndarray # ndarray of floats\n",
        "  image_mask_array: np.ndarray # ndarray of uint8\n",
        "  masked_image: np.ma.masked_array\n",
        "  yearly_masked_image: np.ma.masked_array\n",
        "\n",
        "@dataclass\n",
        "class Bounds:\n",
        "  \"\"\"Represents geographic bounds and size information.\"\"\"\n",
        "  minx: float\n",
        "  maxx: float\n",
        "  miny: float\n",
        "  maxy: float\n",
        "  pixel_size_x: float\n",
        "  pixel_size_y: float\n",
        "  raster_size_x: float\n",
        "  raster_size_y: float\n",
        "\n",
        "  def to_matplotlib(self) -> List[float]:\n",
        "    return [self.minx, self.maxx, self.miny, self.maxy]\n",
        "\n",
        "@dataclass\n",
        "class PartitionedDataset:\n",
        "  train: pd.DataFrame\n",
        "  test: pd.DataFrame\n",
        "  validation: pd.DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnZdABlosBne"
      },
      "source": [
        "# Use Global Params to access files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b84LNnYHsBne"
      },
      "outputs": [],
      "source": [
        "def get_raster_path(filename: str) -> str:\n",
        "  root = GDRIVE_BASE if GDRIVE_BASE else \"\"\n",
        "  return f\"{root}{RASTER_BASE}{filename}\"\n",
        "\n",
        "def get_model_path(filename: str) -> str:\n",
        "  root = GDRIVE_BASE if GDRIVE_BASE else \"\"\n",
        "  return f\"{root}{MODEL_BASE}{filename}\"\n",
        "\n",
        "def get_sample_db_path(filename: str) -> str:\n",
        "  root = GDRIVE_BASE if GDRIVE_BASE else \"\"\n",
        "  return f\"{root}{SAMPLE_DATA_BASE}{filename}\"\n",
        "\n",
        "def get_animations_path(filename: str) -> str:\n",
        "  root = GDRIVE_BASE if GDRIVE_BASE else \"\"\n",
        "  return f\"{root}{ANIMATIONS_BASE}{filename}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4eZ_EpvyAcu"
      },
      "source": [
        "## Utils for loading Rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b89tGCn8ztnO"
      },
      "outputs": [],
      "source": [
        "def print_raster_info(raster):\n",
        "  dataset = raster\n",
        "  print(\"Driver: {}/{}\".format(dataset.GetDriver().ShortName,\n",
        "                              dataset.GetDriver().LongName))\n",
        "  print(\"Size is {} x {} x {}\".format(dataset.RasterXSize,\n",
        "                                      dataset.RasterYSize,\n",
        "                                      dataset.RasterCount))\n",
        "  print(\"Projection is {}\".format(dataset.GetProjection()))\n",
        "  geotransform = dataset.GetGeoTransform()\n",
        "  if geotransform:\n",
        "      print(\"Origin = ({}, {})\".format(geotransform[0], geotransform[3]))\n",
        "      print(\"Pixel Size = ({}, {})\".format(geotransform[1], geotransform[5]))\n",
        "\n",
        "  for band in range(dataset.RasterCount):\n",
        "    band = dataset.GetRasterBand(band+1)\n",
        "    #print(\"Band Type={}\".format(gdal.GetDataTypeName(band.DataType)))\n",
        "\n",
        "    min = band.GetMinimum()\n",
        "    max = band.GetMaximum()\n",
        "    if not min or not max:\n",
        "        (min,max) = band.ComputeRasterMinMax(False)\n",
        "    #print(\"Min={:.3f}, Max={:.3f}\".format(min,max))\n",
        "\n",
        "    if band.GetOverviewCount() > 0:\n",
        "        print(\"Band has {} overviews\".format(band.GetOverviewCount()))\n",
        "\n",
        "    if band.GetRasterColorTable():\n",
        "        print(\"Band has a color table with {} entries\".format(band.GetRasterColorTable().GetCount()))\n",
        "\n",
        "def load_raster(path: str, use_only_band_index: int = -1) -> AmazonGeoTiff:\n",
        "  \"\"\"\n",
        "  TODO: Refactor (is_single_band, etc., should be a better design)\n",
        "  --> Find a way to simplify this logic. Maybe it needs to be more abstract.\n",
        "  \"\"\"\n",
        "  dataset = gdal.Open(path, gdal.GA_ReadOnly)\n",
        "  try:\n",
        "    print_raster_info(dataset)\n",
        "  except AttributeError as e:\n",
        "    raise OSError(\"Failed to print raster. This likely means it did not load properly from \"+ path)\n",
        "  image_datatype = dataset.GetRasterBand(1).DataType\n",
        "  mask_datatype = dataset.GetRasterBand(1).GetMaskBand().DataType\n",
        "  image = np.zeros((dataset.RasterYSize, dataset.RasterXSize, 12),\n",
        "                  dtype=gdal_array.GDALTypeCodeToNumericTypeCode(image_datatype))\n",
        "  mask = np.zeros((dataset.RasterYSize, dataset.RasterXSize, 12),\n",
        "                  dtype=gdal_array.GDALTypeCodeToNumericTypeCode(image_datatype))\n",
        "\n",
        "  if use_only_band_index == -1:\n",
        "    if dataset.RasterCount != 12 and dataset.RasterCount != 1:\n",
        "      raise ValueError(f\"Expected 12 raster bands (one for each month) or one annual average, but found {dataset.RasterCount}\")\n",
        "    if dataset.RasterCount == 1:\n",
        "      use_only_band_index = 0\n",
        "\n",
        "  is_single_band = use_only_band_index != -1\n",
        "\n",
        "  if is_single_band and use_only_band_index >= dataset.RasterCount:\n",
        "    raise IndexError(f\"Specified raster band index {use_only_band_index}\"\n",
        "    f\" but there are only {dataset.RasterCount} rasters\")\n",
        "\n",
        "  for band_index in range(12):\n",
        "    band = dataset.GetRasterBand(use_only_band_index+1 if is_single_band else band_index+1)\n",
        "    image[:, :, band_index] = band.ReadAsArray()\n",
        "    mask[:, :, band_index] = band.GetMaskBand().ReadAsArray()\n",
        "  masked_image = np.ma.masked_where(mask == 0, image)\n",
        "  yearly_masked_image = masked_image.mean(axis=2)\n",
        "\n",
        "  return AmazonGeoTiff(dataset, image, mask, masked_image, yearly_masked_image)\n",
        "\n",
        "def get_extent(dataset):\n",
        "  geoTransform = dataset.GetGeoTransform()\n",
        "  minx = geoTransform[0]\n",
        "  maxy = geoTransform[3]\n",
        "  maxx = minx + geoTransform[1] * dataset.RasterXSize\n",
        "  miny = maxy + geoTransform[5] * dataset.RasterYSize\n",
        "  return Bounds(minx, maxx, miny, maxy, geoTransform[1], geoTransform[5], dataset.RasterXSize, dataset.RasterYSize)\n",
        "\n",
        "def plot_band(geotiff: AmazonGeoTiff, month_index, figsize=None):\n",
        "  if figsize:\n",
        "    plt.figure(figsize=figsize)\n",
        "  im = plt.imshow(geotiff.masked_image[:,:,month_index], extent=get_extent(geotiff.gdal_dataset).to_matplotlib(), interpolation='none')\n",
        "  plt.colorbar(im)\n",
        "\n",
        "def animate(geotiff: AmazonGeoTiff, nSeconds, fps):\n",
        "  fig = plt.figure( figsize=(8,8) )\n",
        "\n",
        "  months = []\n",
        "  labels = []\n",
        "  for m in range(12):\n",
        "    months.append(geotiff.masked_image[:,:,m])\n",
        "    labels.append(f\"Month: {m+1}\")\n",
        "  a = months[0]\n",
        "  extent = get_extent(geotiff.gdal_dataset).to_matplotlib()\n",
        "  ax = fig.add_subplot()\n",
        "  im = fig.axes[0].imshow(a, interpolation='none', aspect='auto', extent = extent)\n",
        "  txt = fig.text(0.3,0,\"\", fontsize=24)\n",
        "  fig.colorbar(im)\n",
        "\n",
        "  def animate_func(i):\n",
        "    if i % fps == 0:\n",
        "      print( '.', end ='' )\n",
        "\n",
        "    im.set_array(months[i])\n",
        "    txt.set_text(labels[i])\n",
        "    return [im, txt]\n",
        "\n",
        "  anim = animation.FuncAnimation(\n",
        "                                fig,\n",
        "                                animate_func,\n",
        "                                frames = nSeconds * fps,\n",
        "                                interval = 1000 / fps, # in ms\n",
        "                                )\n",
        "  plt.close()\n",
        "\n",
        "  return anim\n",
        "\n",
        "def save_numpy_to_geotiff(bounds: Bounds, prediction: np.ma.MaskedArray, path: str):\n",
        "  \"\"\"Copy metadata from a base geotiff and write raster data + mask from `data`\"\"\"\n",
        "  driver = gdal.GetDriverByName(\"GTiff\")\n",
        "  metadata = driver.GetMetadata()\n",
        "  if metadata.get(gdal.DCAP_CREATE) != \"YES\":\n",
        "      raise RuntimeError(\"GTiff driver does not support required method Create().\")\n",
        "  if metadata.get(gdal.DCAP_CREATECOPY) != \"YES\":\n",
        "      raise RuntimeError(\"GTiff driver does not support required method CreateCopy().\")\n",
        "\n",
        "  dataset = driver.Create(path, bounds.raster_size_x, bounds.raster_size_y, prediction.shape[2], eType=gdal.GDT_Float64)\n",
        "  dataset.SetGeoTransform([bounds.minx, bounds.pixel_size_x, 0, bounds.maxy, 0, bounds.pixel_size_y])\n",
        "  dataset.SetProjection('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433],AUTHORITY[\"EPSG\",\"4326\"]]')\n",
        "\n",
        "  #dataset = driver.CreateCopy(path, base.gdal_dataset, strict=0)\n",
        "  if len(prediction.shape) != 3 or prediction.shape[0] != bounds.raster_size_x or prediction.shape[1] != bounds.raster_size_y:\n",
        "    raise ValueError(\"Shape of prediction does not match base geotiff\")\n",
        "  #if prediction.shape[2] > base.gdal_dataset.RasterCount:\n",
        "  #  raise ValueError(f\"Expected fewer than {dataset.RasterCount} bands in prediction but found {prediction.shape[2]}\")\n",
        "\n",
        "  prediction_transformed = np.flip(np.transpose(prediction, axes=[1,0,2]), axis=0)\n",
        "  for band_index in range(dataset.RasterCount):\n",
        "    band = dataset.GetRasterBand(band_index+1)\n",
        "    if band.CreateMaskBand(0) == gdal.CE_Failure:\n",
        "      raise RuntimeError(\"Failed to create mask band\")\n",
        "    mask_band = band.GetMaskBand()\n",
        "    band.WriteArray(np.choose(prediction_transformed[:, :, band_index].mask, (prediction_transformed[:, :, band_index].data,np.array(band.GetNoDataValue()),)))\n",
        "    mask_band.WriteArray(np.logical_not(prediction_transformed[:, :, band_index].mask))\n",
        "\n",
        "def coords_to_indices(bounds: Bounds, x: float, y: float):\n",
        "  if x < bounds.minx or x > bounds.maxx or y < bounds.miny or y > bounds.maxy:\n",
        "    raise ValueError(\"Coordinates out of bounds\")\n",
        "\n",
        "  # X => lat, Y => lon\n",
        "  x_idx = bounds.raster_size_y - int(math.ceil((y - bounds.miny) / abs(bounds.pixel_size_y)))\n",
        "  y_idx = int((x - bounds.minx) / abs(bounds.pixel_size_x))\n",
        "\n",
        "  return x_idx, y_idx\n",
        "\n",
        "def test_coords_to_indices():\n",
        "  bounds = Bounds(50, 100, 50, 100, 1, 1, 50, 50)\n",
        "  x, y = coords_to_indices(bounds, 55, 55)\n",
        "  assert x == 45\n",
        "  assert y == 5\n",
        "\n",
        "  bounds = Bounds(-100, -50, -100, -50, 1, 1, 50, 50)\n",
        "  x, y = coords_to_indices(bounds, -55, -55)\n",
        "  assert x == 5\n",
        "  assert y == 45\n",
        "\n",
        "  bounds = Bounds(-10, 50, -10, 50, 1, 1, 60, 60)\n",
        "  x, y = coords_to_indices(bounds, -1, 13)\n",
        "  assert x == 37\n",
        "  assert y == 9\n",
        "\n",
        "  bounds = Bounds(minx=-73.97513931345594, maxx=-34.808472803053895, miny=-33.73347244751509, maxy=5.266527396029211, pixel_size_x=0.04166666650042771, pixel_size_y=-0.041666666499513144, raster_size_x=937, raster_size_y=941)\n",
        "  x, y = coords_to_indices(bounds, -67.14342073173958, -7.273271869467912e-05)\n",
        "  #print(x)\n",
        "  assert x == 131 # was: 132\n",
        "  assert y == 163\n",
        "\n",
        "test_coords_to_indices()\n",
        "\n",
        "def get_data_at_coords(dataset: AmazonGeoTiff, x: float, y: float, month: int) -> float:\n",
        "  # x = longitude\n",
        "  # y = latitude\n",
        "  bounds = get_extent(dataset.gdal_dataset)\n",
        "  x_idx, y_idx = coords_to_indices(bounds, x, y)\n",
        "  if month == -1:\n",
        "    value = dataset.yearly_masked_image[x_idx, y_idx]\n",
        "  else:\n",
        "    value = dataset.masked_image[x_idx, y_idx, month]\n",
        "  if np.ma.is_masked(value):\n",
        "    raise ValueError(\"Coordinates are masked\")\n",
        "  else:\n",
        "    return value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPlgPTLXsBnf"
      },
      "source": [
        "# Load Rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LZUJSK3sBng"
      },
      "outputs": [],
      "source": [
        "# Access data stored on Google Drive\n",
        "if GDRIVE_BASE:\n",
        "    from google.colab import drive\n",
        "    drive.mount(GDRIVE_BASE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PI3Enxboe1u"
      },
      "outputs": [],
      "source": [
        "brazil_map_geotiff = load_raster(get_raster_path(\"brasil_clim_raster.tiff\")) # mean annual precipitation\n",
        "# Will be used to compute isoscapes for carbon and nitrogen\n",
        "\n",
        "relative_humidity_geotiff = load_raster(get_raster_path(\"R.rh_Stack.tif\"))\n",
        "temperature_geotiff = load_raster(get_raster_path(\"Temperatura_Stack.tif\"))\n",
        "vapor_pressure_deficit_geotiff = load_raster(get_raster_path(\"R.vpd_Stack.tif\"))\n",
        "atmosphere_isoscape_geotiff = load_raster(get_raster_path(\"Iso_Oxi_Stack.tif\"))\n",
        "cellulose_isoscape_geotiff = load_raster(get_raster_path(\"iso_O_cellulose.tif\"))\n",
        "\n",
        "# Soil Geotiffs are not necessary to load, but required to build plant nitrogen geotiff.\n",
        "soil_plant_nitrogen_difference_isoscape_geotiff = load_raster(get_raster_path(\"raster_krig_d15N_soil_plant.tiff\"))\n",
        "soil_nitrogen_isoscape_geotiff = load_raster(get_raster_path(\"raster_krig_d15N_soil.tiff\"))\n",
        "plant_nitrogen_isoscape_geotiff = load_raster(get_raster_path(\"plant_nitrogen_isoscape.tiff\"))\n",
        "\n",
        "carbon_means_krig_isoscape_geotiff = load_raster(get_raster_path(\"Brasil_Raster_Krig_iso_d13C.tiff\"))\n",
        "\n",
        "land_water_mask_geotiff = load_raster(get_raster_path(\"Land_Water_Brazil_MODIS.tif\")) if LOAD_WATER_MASK_GEOTIFF else None\n",
        "possible_tree_mask_geotiff = load_raster(get_raster_path(\"Possible_Trees_Brazil_MODIS.tif\")) if LOAD_TREE_MASK_GEOTIFF else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwrDKcwmIu4f"
      },
      "outputs": [],
      "source": [
        "if REGENERATE_PLANT_NITROGEN_GEOTIFF:\n",
        "  plant_nitrogen_array = soil_nitrogen_isoscape_geotiff.yearly_masked_image - soil_plant_nitrogen_difference_isoscape_geotiff.yearly_masked_image\n",
        "  save_numpy_to_geotiff(soil_plant_nitrogen_difference_isoscape_geotiff,\n",
        "    np.expand_dims(np.flip(plant_nitrogen_array.T, axis=1), axis=2),\n",
        "    get_raster_path(\"plant_nitrogen_isoscape.tiff\"))\n",
        "plant_nitrogen_isoscape_geotiff = load_raster(get_raster_path(\"plant_nitrogen_isoscape.tiff\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-CHLVHQyD9U"
      },
      "source": [
        "# Train Isoscape Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTieS4NxKetw"
      },
      "source": [
        "## Preprocess\n",
        "\n",
        "Sample data from Martinelli's map of measurement sites to train fake isoscape models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNXqjSsTsMYy"
      },
      "outputs": [],
      "source": [
        "def gen_tabular_dataset(monthly: bool, samples_per_site: int) -> pd.DataFrame:\n",
        "  return gen_tabular_dataset_with_coords(monthly, samples_per_site,\n",
        "                            [(-70,-5,),(-67.5,0,),(-66,-4.5,),(-63,-9.5,),\n",
        "                             (-63,-9,),(-62,-6,),(-60,-2.5,),(-60,1,),\n",
        "                              (-60,-12.5,),(-59,-2.5,),(-57.5,-4,),\n",
        "                               (-55,-3.5,),(-54,-1,),(-52.5,-13,),(-51.5,-2.5,)],\n",
        "                                         0.5)\n",
        "\n",
        "def gen_tabular_dataset_with_coords(monthly: bool, samples_per_site: int, sample_site_coordinates: list, sample_radius: float) -> pd.DataFrame:\n",
        "  features = [relative_humidity_geotiff, temperature_geotiff, vapor_pressure_deficit_geotiff, atmosphere_isoscape_geotiff, cellulose_isoscape_geotiff]\n",
        "  image_feature_names = [\"rh\", \"temp\", \"vpd\", \"atmosphere_oxygen_ratio\", \"cellulose_oxygen_ratio\"]\n",
        "  feature_names = [\"lat\", \"lon\", \"month_of_year\"] + image_feature_names\n",
        "  rs = RandomState(MT19937(SeedSequence(42)))\n",
        "\n",
        "  feature_values = {}\n",
        "  for name in feature_names:\n",
        "    feature_values[name] = []\n",
        "\n",
        "  for coord in tqdm(sample_site_coordinates):\n",
        "    month_start = 0 if monthly else -1\n",
        "    month_end = 12 if monthly else 0\n",
        "    for month in range(month_start, month_end):\n",
        "      samples_collected = 0\n",
        "      while samples_collected < samples_per_site:\n",
        "        row = {}\n",
        "        sample_x, sample_y = 2*(rs.rand(2) - 0.5) * sample_radius\n",
        "        sample_x += coord[0]\n",
        "        sample_y += coord[1]\n",
        "\n",
        "        try:\n",
        "          for feature, feature_name in zip(features, image_feature_names):\n",
        "            row[feature_name] = get_data_at_coords(feature, sample_x, sample_y, month)\n",
        "          row[\"month_of_year\"] = month\n",
        "          row[\"lon\"] = sample_x\n",
        "          row[\"lat\"] = sample_y\n",
        "          samples_collected += 1\n",
        "\n",
        "        except ValueError as e:\n",
        "          # masked and out-of-bounds coordinates\n",
        "          print(\"!!!!! x={:f}, y={:f}\".format(sample_x, sample_y))\n",
        "          if sample_radius == 0:\n",
        "            samples_collected += 1\n",
        "          continue\n",
        "        for key, value in row.items():\n",
        "          feature_values[key].append(value)\n",
        "\n",
        "  samples = pd.DataFrame(feature_values)\n",
        "\n",
        "  if not monthly:\n",
        "    samples.drop(\"month_of_year\", axis=1, inplace=True)\n",
        "\n",
        "  return samples\n",
        "\n",
        "monthly_data_large = gen_tabular_dataset(monthly=True, samples_per_site=30)\n",
        "monthly_data_255_trees = gen_tabular_dataset(monthly=True, samples_per_site=17)\n",
        "yearly_data_large = gen_tabular_dataset(monthly=False, samples_per_site=30*12)\n",
        "yearly_data_255_trees = gen_tabular_dataset(monthly=False, samples_per_site=17)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sample_data() -> pd.DataFrame:\n",
        "  df = pd.read_csv(get_sample_db_path(REFERENCE_CSV_FILENAME), encoding=\"ISO-8859-1\", sep=',')\n",
        "  print(df.shape)\n",
        "  df = df[['Code', 'lat', 'long', 'd18O_cel']]\n",
        "  df = df[df['d18O_cel'].notna()]\n",
        "\n",
        "  grouped = df.groupby(['lat', 'long'])\n",
        "\n",
        "  means = grouped.mean().reset_index()\n",
        "  locations = list(zip(means[\"long\"], means[\"lat\"]))\n",
        "\n",
        "  sample_data = gen_tabular_dataset_with_coords(monthly=False, samples_per_site=1, sample_site_coordinates=locations, sample_radius = 0)\n",
        "  sample_data = sample_data.drop('cellulose_oxygen_ratio', axis = 1)\n",
        "  sample_data = pd.merge(sample_data, means, how=\"inner\", left_on=['lat', 'lon'], right_on=['lat', 'long'])\n",
        "  sample_data = sample_data.drop('long', axis=1).rename(columns={'d18O_cel': 'cellulose_oxygen_ratio' }).reset_index()\n",
        "  sample_data.drop('index', inplace=True, axis=1)\n",
        "  print()\n",
        "  print(sample_data)\n",
        "\n",
        "\n",
        "  return sample_data\n",
        "\n",
        "# If a real test set is requested, we use that over any simulated data.\n",
        "if USE_REFERENCE_SAMPLES_FOR_TRAINING:\n",
        "  #override yearly data with UC David 40 locations\n",
        "  yearly_data_255_trees = load_sample_data()"
      ],
      "metadata": {
        "id": "Sn5GUfusOeG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0dAH2eCmegR"
      },
      "outputs": [],
      "source": [
        "leaf_data = pd.read_csv(get_sample_db_path(\"pontos-vasp-cluster.csv\"))\n",
        "leaf_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eitP1hjX335"
      },
      "outputs": [],
      "source": [
        "def load_leaf_dataframe(db_path: str, isotope_col: str):\n",
        "  leaf_data = pd.read_csv(db_path)\n",
        "  leaf_data = leaf_data.rename(columns={\"latitude\": \"lat\", \"longitude\": \"lon\"})\n",
        "  leaf_df = leaf_data[[\"lon\", \"lat\", \"MAP\", \"MAT\", \"vap\", \"d15N_soil\", \"dem\", \"pa\", \"pet\", \"ph\", isotope_col]]\n",
        "  return leaf_df\n",
        "\n",
        "carbon_df = load_leaf_dataframe(get_sample_db_path(\"pontos-vasp-cluster.csv\"), \"d13C\")\n",
        "nitrogen_df = load_leaf_dataframe(get_sample_db_path(\"pontos-vasp-cluster.csv\"), \"d15N\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EW3f_q_sQbL"
      },
      "source": [
        "### Partition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX1m5XvIsPyH"
      },
      "outputs": [],
      "source": [
        "def partition(df) -> PartitionedDataset:\n",
        "  train = df[df[\"lon\"] < -55]\n",
        "  test = df[(df[\"lon\"] >= -55) & (df[\"lat\"] > -2.85)]\n",
        "  validation = df[(df[\"lon\"] >= -55) & (df[\"lat\"] <= -2.85)]\n",
        "  return PartitionedDataset(train, test, validation)\n",
        "\n",
        "def print_split(dataset: PartitionedDataset) -> None:\n",
        "  total_len = len(dataset.train)+len(dataset.validation)+len(dataset.test)\n",
        "  print(f\"Train: {100*len(dataset.train)/total_len:.2f}% ({len(dataset.train)})\")\n",
        "  print(f\"Test: {100*len(dataset.test)/total_len:.2f}% ({len(dataset.test)})\")\n",
        "  print(f\"Validation: {100*len(dataset.validation)/total_len:.2f}% ({len(dataset.validation)})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FEXJ9fQtLM9"
      },
      "outputs": [],
      "source": [
        "yearly_large_partitioned = partition(yearly_data_large)\n",
        "print_split(yearly_large_partitioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DKNx1ckzaYN"
      },
      "outputs": [],
      "source": [
        "yearly_255_trees_partitioned = partition(yearly_data_255_trees)\n",
        "print_split(yearly_255_trees_partitioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06mCyO82w_n7"
      },
      "outputs": [],
      "source": [
        "monthly_large_partitioned = partition(monthly_data_large)\n",
        "print_split(monthly_large_partitioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9DQikZExELq"
      },
      "outputs": [],
      "source": [
        "monthly_255_trees_partitioned = partition(monthly_data_255_trees)\n",
        "print_split(monthly_255_trees_partitioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVTzinGguSFj"
      },
      "outputs": [],
      "source": [
        "nitrogen_df_partitioned = partition(nitrogen_df)\n",
        "print_split(nitrogen_df_partitioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T73lHHC9u6ol"
      },
      "outputs": [],
      "source": [
        "carbon_df_partitioned = partition(carbon_df)\n",
        "print_split(carbon_df_partitioned)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIu9xdqPBI_v"
      },
      "source": [
        "## XGBoost: Train XGBoost Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyjRvlzG1J1m"
      },
      "outputs": [],
      "source": [
        "def train_xgb(data: PartitionedDataset, booster: str, rounds: int) -> xgb.XGBRegressor:\n",
        "  xgb_model = xgb.XGBRegressor(n_estimators=rounds, eta=0.1, max_depth=2, objective='reg:squarederror', booster=booster)\n",
        "  # split data into input and output columns\n",
        "  X, y = data.train.iloc[:, :-1], data.train.iloc[:, -1]\n",
        "  X_val, y_val = data.validation.iloc[:, :-1], data.validation.iloc[:, -1]\n",
        "  print(f\"Predicting: {data.train.columns[-1]}\")\n",
        "  xgb_model.fit(X, y, eval_set=[(X_val, y_val)], verbose=XGB_VERBOSITY_LEVEL)\n",
        "  return xgb_model\n",
        "\n",
        "def train_or_load_xgboost(basename: str, data: PartitionedDataset, rounds: int=100000):\n",
        "  if REBUILD_MODEL:\n",
        "    print(\"Training model\")\n",
        "    model = train_xgb(data, booster='gblinear', rounds=rounds)\n",
        "    with open(f\"{basename}_config_xgb.json\", \"w\") as f:\n",
        "      f.write(model.get_booster().save_config())\n",
        "    model.save_model(f\"{basename}_xgb.json\")\n",
        "  else:\n",
        "    print(\"Loading model\")\n",
        "    model = xgb.XGBRegressor()\n",
        "    model.load_model(f\"{basename}_xgb.json\")\n",
        "    with open(f\"{basename}_config_xgb.json\", \"r\") as f:\n",
        "      model.get_booster().load_config(f.read())\n",
        "  print(f\"RMSE (validation): {model.evals_result()['validation_0']['rmse'][-1]}\")\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KspPPr3KsBni"
      },
      "outputs": [],
      "source": [
        "# Validation RMSE xgboost: 0.306059 w/ 100,000 rounds\n",
        "# Validation RMSE google internal tooling: 0.39386\n",
        "yearly_255_trees_xgb_model = train_or_load_xgboost(\n",
        "  get_model_path(\"oxygen_isoscape_model\"),\n",
        "  yearly_255_trees_partitioned,\n",
        "  rounds=100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKHBDwd0w3SD"
      },
      "outputs": [],
      "source": [
        "# HMM, post-bugfix, Carbon might diverge too.\n",
        "carbon_isoscape_model = train_or_load_xgboost(get_model_path(\"carbon_isoscape_model\"), carbon_df_partitioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y56PuvBSySRG"
      },
      "outputs": [],
      "source": [
        "# Validation loss seems to diverge\n",
        "nitrogen_isoscape_model = train_or_load_xgboost(get_model_path(\"nitrogen_isoscape_model\"), nitrogen_df_partitioned, rounds=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLigqWVAEGiP"
      },
      "source": [
        "### Test XGBoost Model Code\n",
        "\n",
        "Test data created as follows:\n",
        "```python\n",
        "# Create data for unit tests\n",
        "from io import StringIO\n",
        "\n",
        "train_text = StringIO()\n",
        "yearly_255_trees_partitioned.validation.iloc[:10].to_csv(train_text, index=False)\n",
        "print(train_text.getvalue())\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLXAbth_CgDf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def create_test_data():\n",
        "  train_txt = \"\"\"lat,lon,rh,temp,vpd,atmosphere_oxygen_ratio,cellulose_oxygen_ratio\n",
        "  -4.880332787307218,-69.95800610699372,0.8044400215148926,26.225001017252605,0.6966667175292969,-4.451689084370931,37.122222900390625\n",
        "  -4.688096349322666,-70.44263021829333,0.80293075243632,26.35833485921224,0.7074999809265137,-4.41741943359375,37.17825063069662\n",
        "  -4.872397683046066,-69.63990597562567,0.8054693539937338,26.308329264322918,0.6958333651224772,-4.417543411254883,37.1220448811849\n",
        "  -4.8247274690858735,-69.81806665464333,0.8040264447530111,26.308331807454426,0.7016665935516357,-4.424846013387044,37.14974721272787\n",
        "  -4.765274838163909,-70.01923594475969,0.8022874991099039,26.337496439615887,0.709166685740153,-4.418321291605632,37.20004526774088\n",
        "  -4.771462642125715,-70.34365888186157,0.8011360963185629,26.508333841959637,0.7199999491373698,-4.385458946228027,37.247047424316406\n",
        "  -4.798305195940103,-70.28306090761369,0.802595059076945,26.366666158040363,0.709166685740153,-4.411936124165853,37.20568339029948\n",
        "  -5.223217462581197,-69.53591146126529,0.8077573776245117,26.10833231608073,0.6808333396911621,-4.4716800053914385,37.02557881673177\n",
        "  -4.613341938104102,-69.7943386465883,0.8022151788075765,26.400001525878906,0.7116666634877523,-4.423205057779948,37.17969512939453\n",
        "  -4.527212807226629,-69.8817482523093,0.8018482526143392,26.4499994913737,0.7149999936421713,-4.395961443583171,37.22857411702474\"\"\"\n",
        "  train_df = pd.read_csv(StringIO(train_txt))\n",
        "\n",
        "  val_txt = \"\"\"lat,lon,rh,temp,vpd,atmosphere_oxygen_ratio,cellulose_oxygen_ratio\n",
        "  -3.6696957825007046,-54.87948135669049,0.8142155011494955,25.770833333333332,0.6483333110809326,-3.3461217880249023,38.01644388834635\n",
        "  -3.8993546066489224,-54.86859101648585,0.805713415145874,26.149998982747395,0.6933333079020182,-3.261778195699056,38.26288604736328\n",
        "  -3.159593531700783,-54.93297940204632,0.819889227549235,25.912501017252605,0.6333333253860474,-3.287173271179199,37.95276896158854\n",
        "  -3.9960354096696906,-54.87203413927777,0.8026766777038574,26.149998982747395,0.7041666507720947,-3.2518555323282876,38.3813222249349\n",
        "  -3.80255400058822,-54.751942535999156,0.8073338667551676,26.054166158040363,0.6833333174387614,-3.302551587422689,38.1731923421224\n",
        "  -12.828852720078402,-52.607319143523036,0.7082154750823975,25.258333841959637,1.0200000603993733,-3.547501564025879,39.927050272623696\n",
        "  -12.532258968752565,-52.097391445126696,0.7110532919565836,25.520833333333332,1.019166628519694,-3.43365478515625,40.047627766927086\n",
        "  -13.427351375947753,-52.060761037543834,0.7014106909434,24.8249994913737,1.005833387374878,-3.589900334676107,40.11137390136719\n",
        "  -13.349866138079692,-52.65445230256682,0.7073808511098226,24.958333333333332,1.00083327293396,-3.5771010716756186,39.98369598388672\n",
        "  -12.730453380778542,-52.375592581693155,0.7120146751403809,25.2375005086263,1.0024999777475994,-3.494396209716797,40.02317810058594\"\"\"\n",
        "  val_df = pd.read_csv(StringIO(val_txt))\n",
        "\n",
        "  test_df = pd.DataFrame()\n",
        "\n",
        "  return PartitionedDataset(train=train_df, test=test_df, validation=val_df)\n",
        "\n",
        "# This function override REBUILD_MODEL for testing.\n",
        "def test_train_or_load_xgboost__load_succeeds():\n",
        "  for f in glob.glob(\"/tmp/foobar_model*\"):\n",
        "    os.remove(f)\n",
        "\n",
        "  # TODO: Probably better to have a function to load xgboost models instead of train and load sharing a function.\n",
        "  global REBUILD_MODEL\n",
        "  REBUILD_MODEL_tmp = REBUILD_MODEL\n",
        "  REBUILD_MODEL = True\n",
        "  model_under_test = yearly_255_trees_xgb_model = train_or_load_xgboost(\"/tmp/foobar_model\", create_test_data(), rounds=100)\n",
        "\n",
        "  final_loss = model_under_test.evals_result()['validation_0']['rmse'][-1]\n",
        "  initial_loss = model_under_test.evals_result()['validation_0']['rmse'][0]\n",
        "  assert final_loss < initial_loss\n",
        "\n",
        "  original_prediction = model_under_test.predict(pd.DataFrame(np.array([[1,2,3,4,5,6]], dtype=float), columns=['lat', 'lon', 'rh', 'temp', 'vpd', 'atmosphere_oxygen_ratio']))[0]\n",
        "\n",
        "  REBUILD_MODEL = False\n",
        "  model_under_test = train_or_load_xgboost(\"/tmp/foobar_model\", create_test_data(), rounds=100)\n",
        "  REBUILD_MODEL = REBUILD_MODEL_tmp\n",
        "\n",
        "  loaded_prediction = model_under_test.predict(pd.DataFrame(np.array([[1,2,3,4,5,6]], dtype=float), columns=['lat', 'lon', 'rh', 'temp', 'vpd', 'atmosphere_oxygen_ratio']))[0]\n",
        "  assert original_prediction == loaded_prediction\n",
        "\n",
        "\n",
        "test_train_or_load_xgboost__load_succeeds()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN9XREX4-zc-"
      },
      "source": [
        "**We also trained a model assuming 255 trees sampled monthly.**\n",
        "\n",
        "Preserving this as text only because it is not realistic as of 2023.\n",
        "\n",
        "Validation RMSE xgboost: 0.29072 \\\n",
        "Validation RMSE Google internal tooling: 0.29183 \\\n",
        "`monthly_255_trees_xgb_model = train_xgb(monthly_255_trees_partitioned, booster='gbtree', rounds=15000)`\n",
        "\n",
        "For the best results here, add `max_depth=2` to XGBRegressor params."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_2b5nUNxhnB"
      },
      "source": [
        "# Data Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxBjEf3iolSI"
      },
      "source": [
        "Do we use coordinates correctly?\n",
        "Ideally, we should create a sample image and make this a unit test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqebAozxoWKZ"
      },
      "outputs": [],
      "source": [
        "get_data_at_coords(relative_humidity_geotiff, -65, -5, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJFqC7iAoqZ9"
      },
      "outputs": [],
      "source": [
        "get_data_at_coords(relative_humidity_geotiff, -43, -10, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn1bp9YzJXDW"
      },
      "source": [
        "# Compute Isoscapes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSSdzTLbfBWS"
      },
      "source": [
        "## XGBoost: Compute AI-Predicted Isoscape\n",
        "\n",
        "Required: REGENERATE_OXYGEN_XGB_ISOSCAPE == true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBUsHhz2n51q"
      },
      "outputs": [],
      "source": [
        "def get_xgb_isoscape_prediction():\n",
        "  bounds = get_extent(cellulose_isoscape_geotiff.gdal_dataset)\n",
        "  features = [relative_humidity_geotiff, temperature_geotiff, vapor_pressure_deficit_geotiff, atmosphere_isoscape_geotiff]\n",
        "  image_feature_names = [\"rh\", \"temp\", \"vpd\", \"atmosphere_oxygen_ratio\"]\n",
        "  #feature_names = [\"lat\", \"lon\", \"month_of_year\"] + image_feature_names\n",
        "  feature_names = [\"lat\", \"lon\"] + image_feature_names\n",
        "  predicted_isoscape = np.ma.array(np.zeros([bounds.raster_size_x, bounds.raster_size_y, 1], dtype=float), mask=np.ones([bounds.raster_size_x, bounds.raster_size_y, 1], dtype=bool))\n",
        "\n",
        "  for x_idx, x in enumerate(tqdm(np.arange(bounds.minx, bounds.maxx, bounds.pixel_size_x, dtype=float))):\n",
        "    rows = []\n",
        "    row_indexes = []\n",
        "    for y_idx, y in enumerate(np.arange(bounds.miny, bounds.maxy, -bounds.pixel_size_y, dtype=float)):\n",
        "      #for month in range(12):\n",
        "      month = 0\n",
        "      row = {}\n",
        "      try:\n",
        "        for feature, feature_name in zip(features, image_feature_names):\n",
        "          row[feature_name] = get_data_at_coords(feature, x, y, month)\n",
        "        #row[\"month_of_year\"] = month\n",
        "        row[\"lon\"] = x\n",
        "        row[\"lat\"] = y\n",
        "      except ValueError:\n",
        "        # masked and out-of-bounds coordinates\n",
        "        continue\n",
        "      except IndexError:\n",
        "        continue\n",
        "      rows.append(row)\n",
        "      row_indexes.append((y_idx,month,))\n",
        "    if (len(rows) > 0):\n",
        "      reordered = pd.DataFrame(rows)[yearly_255_trees_xgb_model.get_booster().feature_names]\n",
        "      predictions = yearly_255_trees_xgb_model.predict(reordered)\n",
        "      predictions_np = predictions\n",
        "      for prediction, (y_idx, month_idx) in zip(predictions_np, row_indexes):\n",
        "        predicted_isoscape.mask[x_idx,y_idx,month_idx] = False # unmask since we have data\n",
        "        predicted_isoscape.data[x_idx,y_idx,month_idx] = prediction\n",
        "\n",
        "  return predicted_isoscape\n",
        "\n",
        "if REGENERATE_OXYGEN_XGB_ISOSCAPE:\n",
        "  xgb_isoscape_prediction = get_xgb_isoscape_prediction()\n",
        "  save_numpy_to_geotiff(get_extent(cellulose_isoscape_geotiff.gdal_dataset), xgb_isoscape_prediction, get_model_path(\"predicted_isoscape_xgboost.tiff\"))\n",
        "  plt.imshow(xgb_isoscape_prediction)\n",
        "\n",
        "# TODO: TESTME!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuExDB3lEXV9"
      },
      "source": [
        "## Turn XGBoost isoscape into a Gaussian distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igc1BbzJEW7b"
      },
      "outputs": [],
      "source": [
        "predicted_cellulose_isoscape_geotiff = load_raster(get_model_path(\"predicted_isoscape_xgboost.tiff\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLOJhVNqFUV_"
      },
      "outputs": [],
      "source": [
        "plt.imshow(predicted_cellulose_isoscape_geotiff.yearly_masked_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YN8UnykFXs5"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "def get_2d_gaussian(center_lon: float, center_lat: float, stdev: float):\n",
        "  \"\"\"Quick-and-dirty function to get a PDF for sampling from an image\n",
        "  to turn it into a distribution. Intended for use with isoscapes.\n",
        "\n",
        "  Major room for improvement! This framing assumes no distortion from the\n",
        "  projection, i.e. that 1 deg latitude == 1 deg longitude == 111 km everywhere.\n",
        "  This should probably be fine for Brazil, for now, since it's near the Equator.\n",
        "  \"\"\"\n",
        "  rv = multivariate_normal([center_lat, center_lon], [[stdev, 0], [0, stdev]])\n",
        "\n",
        "  return rv\n",
        "\n",
        "# x = longitude\n",
        "# y = latitude\n",
        "def plot_gaussian(rv, shape: Bounds):\n",
        "  \"\"\"Informative, for debugging and visualizing get_2d_gaussian().\"\"\"\n",
        "  x = np.linspace(shape.minx, shape.maxx, shape.raster_size_x)\n",
        "  y = np.linspace(shape.maxy, shape.miny, shape.raster_size_y) # inverted y axis\n",
        "  X, Y = np.meshgrid(x,y)\n",
        "  target = np.empty((shape.raster_size_y, shape.raster_size_x,2,), dtype=float)\n",
        "  target[:, :, 0] = Y\n",
        "  target[:, :, 1] = X\n",
        "  pd = rv.pdf(target)\n",
        "  plt.imshow(pd)\n",
        "  plt.colorbar()\n",
        "\n",
        "# TODO: For each pixel in the predicted isoscape\n",
        "# Yeah, this is basically Gaussian blur, huh...\n",
        "# BUT, it does give us a distribution.\n",
        "def gaussian_kernel(input: AmazonGeoTiff, stdev_in_degrees: float=1):\n",
        "  bounds = get_extent(input.gdal_dataset)\n",
        "  means = np.ma.zeros((bounds.raster_size_x, bounds.raster_size_y,), dtype=float)\n",
        "  means.mask = np.ones((bounds.raster_size_x, bounds.raster_size_y), dtype=bool)\n",
        "  variances = np.ma.zeros((bounds.raster_size_x, bounds.raster_size_y,), dtype=float)\n",
        "  variances.mask = np.ones((bounds.raster_size_x, bounds.raster_size_y), dtype=bool)\n",
        "  for map_y in tqdm(range(0, bounds.raster_size_y, 1)):\n",
        "    y_coord = map_y * abs(bounds.pixel_size_y) + bounds.miny\n",
        "    for map_x in range(0, bounds.raster_size_x, 1):\n",
        "      x_coord = map_x * abs(bounds.pixel_size_x) + bounds.minx\n",
        "      rv = get_2d_gaussian(y_coord, x_coord, stdev_in_degrees)\n",
        "      rsamp = rv.rvs(1000)\n",
        "      values = []\n",
        "      for coordinate_pair in rsamp:\n",
        "        try:\n",
        "          #print(coordinate_pair)\n",
        "          values.append(get_data_at_coords(input, coordinate_pair[0], coordinate_pair[1], 0))\n",
        "        except ValueError:\n",
        "          pass\n",
        "        if len(values) == 30:\n",
        "          break\n",
        "      if len(values) == 30:\n",
        "        # Set the mean and stdev pixels\n",
        "        #print(x_coord, y_coord)\n",
        "        means[map_x, map_y] = np.mean(values)\n",
        "        variances[map_x, map_y] = np.var(values)\n",
        "        # Apply sample corrective factor to variance\n",
        "        variances[map_x, map_y] *= len(values) / (len(values)-1)\n",
        "        means.mask[map_x, map_y] = False\n",
        "        variances.mask[map_x, map_y] = False\n",
        "  return means, variances\n",
        "\n",
        "rv = get_2d_gaussian(-60.16, 4.11, 1)\n",
        "plot_gaussian(rv, get_extent(predicted_cellulose_isoscape_geotiff.gdal_dataset))\n",
        "\n",
        "if REGENERATE_OXYGEN_XGB_MEANS_VARIANCES:\n",
        "  xgb_means, xgb_variances = gaussian_kernel(predicted_cellulose_isoscape_geotiff, stdev_in_degrees=0.1)\n",
        "  bds = get_extent(predicted_cellulose_isoscape_geotiff.gdal_dataset)\n",
        "  save_numpy_to_geotiff(bds, np.expand_dims(xgb_means, axis=2), get_model_path(\"xgb_means_oxygen_isoscape.tiff\"))\n",
        "  save_numpy_to_geotiff(bds, np.expand_dims(xgb_variances, axis=2), get_model_path(\"xgb_variances_oxygen_isoscape.tiff\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-D1eG5P1T02"
      },
      "outputs": [],
      "source": [
        "xgb_means_oxygen_geotiff = load_raster(get_model_path(\"xgb_means_oxygen_isoscape.tiff\"))\n",
        "xgb_variances_oxygen_geotiff = load_raster(get_model_path(\"xgb_variances_oxygen_isoscape.tiff\"))\n",
        "\n",
        "# Until we re-generate the map\n",
        "xgb_variances_oxygen_geotiff.yearly_masked_image *= (5 / 4)\n",
        "xgb_variances_oxygen_geotiff.masked_image *= (5 / 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QqM7wL51X0o"
      },
      "outputs": [],
      "source": [
        "plot_band(xgb_means_oxygen_geotiff, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl_G90Be3nxn"
      },
      "outputs": [],
      "source": [
        "plot_band(xgb_variances_oxygen_geotiff, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxh8_X4mvY1F"
      },
      "outputs": [],
      "source": [
        "get_data_at_coords(xgb_means_oxygen_geotiff, -60, 4, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-3_Jvn90CCw"
      },
      "source": [
        "# A probability that a coordinate matches a sample given a predicted isoscape\n",
        "\n",
        "1. Compute t-intervals for a coordinate pixel on the paperwork based on the output of the AI isoscape model\n",
        "2. For a given sample, compute a z-score from the estimated t-distribution, and turn that into a p-value based on two-sided area under the curve: p($\\in$ isotope distribution | possible coordinates, isoscape). *This can be combined with other knowledge in the future to get, for example, p($\\in$ isoscape distribution $\\land$ tree rings look right for the area | possible coordinates, isoscape, tree ring knowledge).*\n",
        "3. Depending on the p-value, reject the null hypothesis that the paperwork is correct\n",
        "\n",
        "Key challenge with this approach: The statistical bound on false positives is per-pixel, and only guarantees that it falls in a range of isoscape values associated with that location rather than the location itself. We will likely have multiple possible points of origin for each sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i6Ez45-C76Z"
      },
      "source": [
        "## Compute Sample Origin Given AI-Predicted Isoscapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ti0_jhDby1l"
      },
      "outputs": [],
      "source": [
        "predicted_cellulose_isoscape_geotiff = load_raster(get_model_path(\"predicted_isoscape_xgboost.tiff\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygeZCfWtWor8"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    np.sum(predicted_cellulose_isoscape_geotiff.masked_image.mask[0,:,:])\n",
        "except IndexError as err:\n",
        "    raise IndexError(err + \" If you're seeing this error, the image mask is unexpectedly missing. You might need to rerun the Oxygen xgboost trainer.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HnJTO9Ib9hu"
      },
      "outputs": [],
      "source": [
        "plot_band(predicted_cellulose_isoscape_geotiff, 1, figsize=(12,12))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL9f3XYOAZma"
      },
      "outputs": [],
      "source": [
        "plt.imshow(predicted_cellulose_isoscape_geotiff.masked_image.data[:,:,3])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4qCnFGsvx39s",
        "3-CHLVHQyD9U",
        "YTieS4NxKetw",
        "U_2b5nUNxhnB",
        "wSSdzTLbfBWS"
      ],
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}