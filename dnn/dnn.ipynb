{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#@title Debugging\n",
    "# See https://zohaib.me/debugging-in-google-collab-notebook/ for tips,\n",
    "# as well as docs for pdb and ipdb.\n",
    "DEBUG = True #@param {type:\"boolean\"}\n",
    "GDRIVE_BASE = \"/content/drive\" #@param\n",
    "DATAFRAME_PATH = \"/MyDrive/monthly_large.csv\"\n",
    "\n",
    "def get_dataframe_path_from_param() -> str:\n",
    "  root = GDRIVE_BASE if GDRIVE_BASE else \"\"\n",
    "  return f\"{root}{DATAFRAME_PATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data stored on Google Drive\n",
    "if GDRIVE_BASE:\n",
    "    from google.colab import drive\n",
    "    drive.mount(GDRIVE_BASE)\n",
    "\n",
    "if DEBUG:\n",
    "    %pip install -Uqq ipdb\n",
    "    import ipdb\n",
    "    %pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def mean_variance_loss(real, predicted):\n",
    "    (real_value, real_variance) = real\n",
    "    (predicted_value, predicted_variance) = predicted\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(real_value - predicted_value))\n",
    "    var_from_point = tf.square(real_value - predicted_value)/2\n",
    "    var_loss = tf.reduce_mean(tf.math.log(var_from_point))\n",
    "    return mean_loss + var_loss\n",
    "\n",
    "def train_nn(\n",
    "        dataset: pd.Dataframe,\n",
    "        hidden_layers: List[int],\n",
    "        epochs: int,\n",
    "        batch_size: int):\n",
    "    # Layers share between mean and variance regressors.\n",
    "    shared_layers = []\n",
    "    for num_nodes in hidden_layers:\n",
    "        shared_layers.append(layers.Dense(num_nodes, activation='relu'))\n",
    "\n",
    "    #Initialize input layers and connect them to shared layers.\n",
    "    num_inputs = dataset.shape[1]\n",
    "    inputs = keras.Input(shape=(num_inputs,))\n",
    "    x = inputs\n",
    "    for shared_layer in shared_layers:\n",
    "        x = shared_layer(x)\n",
    "\n",
    "    # Output is variance and mean, and connect to shared nodes.\n",
    "    mean_output_layer = layers.Dense(1, activation='linear')\n",
    "    mean_output_node = mean_output_layer(x)\n",
    "    variance_output_layer = layers.Dense(1, activation='relu')\n",
    "    variance_output_node = variance_output_layer(x)\n",
    "    outputs = [mean_output_node, variance_output_node]\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer='adam', loss=mean_variance_loss)\n",
    "    X = None #TODO: group_by sample site\n",
    "    Y_mean = None \n",
    "    Y_var = None\n",
    "    model.Train(X, [Y_mean, Y_var], epochs=epochs, batch_size=batch_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(get_dataframe_path_from_param())\n",
    "pd.describe()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
