{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "#@title Debugging\n",
    "# See https://zohaib.me/debugging-in-google-collab-notebook/ for tips,\n",
    "# as well as docs for pdb and ipdb.\n",
    "DEBUG = True #@param {type:\"boolean\"}\n",
    "GDRIVE_BASE = \"/content/drive\" #@param\n",
    "DATAFRAME_PATH = \"/MyDrive/amazon_rainforest_files/monthly_large.csv\" #@param\n",
    "\n",
    "def get_dataframe_path_from_params() -> str:\n",
    "  root = GDRIVE_BASE if GDRIVE_BASE else \"\"\n",
    "  return f\"{root}{DATAFRAME_PATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data stored on Google Drive\n",
    "if GDRIVE_BASE:\n",
    "    from google.colab import drive\n",
    "    drive.mount(GDRIVE_BASE)\n",
    "\n",
    "if DEBUG:\n",
    "    %pip install -Uqq ipdb\n",
    "    import ipdb\n",
    "    %pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def kl_divergence_loss(real, predicted):\n",
    "    real_value = real[0]\n",
    "    real_variance = real[1] + 0.00000001\n",
    "    predicted_value = predicted[0]\n",
    "    predicted_variance = predicted[1] + 0.00000001\n",
    "\n",
    "    kl_loss = -0.5 + tf.math.log(predicted_variance/real_variance) + \\\n",
    "     (tf.square(real_variance) + tf.square(real_value - predicted_value))/ \\\n",
    "     2*tf.square(predicted_variance)\n",
    "    return tf.math.reduce_mean(kl_loss)\n",
    "\n",
    "def train_nn(\n",
    "        X: pd.DataFrame,\n",
    "        Y: pd.DataFrame,\n",
    "        hidden_layers: List[int],\n",
    "        epochs: int,\n",
    "        batch_size: int):\n",
    "    # Layers share between mean and variance regressors.\n",
    "    shared_layers = []\n",
    "    for num_nodes in hidden_layers:\n",
    "        shared_layers.append(layers.Dense(num_nodes, activation='relu'))\n",
    "\n",
    "    #Initialize input layers and connect them to shared layers.\n",
    "    num_inputs = X.shape[1]\n",
    "    inputs = keras.Input(shape=(num_inputs,))\n",
    "    x = inputs\n",
    "    for shared_layer in shared_layers:\n",
    "        x = shared_layer(x)\n",
    "\n",
    "    # Output is variance and mean, and connect to shared nodes.\n",
    "    mean_output_layer = layers.Dense(1, activation='linear', name='mean_output')\n",
    "    mean_output_node = mean_output_layer(x)\n",
    "    variance_output_layer = layers.Dense(1, activation='relu', name='variance_output')\n",
    "    variance_output_node = variance_output_layer(x)\n",
    "    outputs = [mean_output_node, variance_output_node]\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss={'mean_output': 'mse', 'variance_output': kl_divergence_loss})\n",
    "    history = model.fit(X, Y, epochs=epochs, batch_size=batch_size)\n",
    "    return history, model\n",
    "\n",
    "def render_plot_loss(history):\n",
    "  plt.plot(history.history['accuracy'])\n",
    "  plt.plot(history.history['val_accuracy'])\n",
    "  plt.title('model accuracy')\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'val'], loc='upper left')\n",
    "  plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(get_dataframe_path_from_params())\n",
    "\n",
    "group_on = ['sample_site_lon', 'sample_site_lat', 'month_of_year']\n",
    "grouped = df.groupby(group_on)\n",
    "\n",
    "# ASSUMPTION: Taking the mean and variance of a sample site doesn't lower quality of the data.\n",
    "# We need to do this to take advantage of KL-divergence loss.\n",
    "means = grouped.mean()\n",
    "O18_var = grouped.var()['cellulose_oxygen_ratio'] \n",
    "\n",
    "# Merging results in some unreadable column names. Rename the oxygen columns.\n",
    "merged = pd.merge(means, O18_var, on=group_on, how='inner')\n",
    "merged = merged.rename(columns={\n",
    "    'cellulose_oxygen_ratio_x': 'O18_mean',\n",
    "    'cellulose_oxygen_ratio_y' : 'O18_var'})\n",
    "\n",
    "# Target data: Mean and variance\n",
    "Y = merged[[\"O18_mean\", \"O18_var\"]]\n",
    "def format_output():\n",
    "  y1 = Y.pop(\"O18_mean\")\n",
    "  y1 = np.array(y1)\n",
    "  y2 = Y.pop(\"O18_var\")\n",
    "  y2 = np.array(y2)\n",
    "  return y1, y2\n",
    "\n",
    "Y = format_output()\n",
    "\n",
    "# Features: Everything besides mean and variance\n",
    "X = merged.drop([\"O18_mean\", \"O18_var\"], axis=1)\n",
    "X.drop(X.columns[X.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance KL-loss: -14.3\n",
    "Mean MSE loss: 4.11\n",
    "\"General loss\": -10.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "history, model = train_nn(X, Y, hidden_layers=[8, 20], epochs=1000, batch_size=150)\n",
    "render_plot_loss(history)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
