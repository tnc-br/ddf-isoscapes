{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnc-br/ddf-isoscapes/blob/main/dnn/dnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "henIPlAPCb4i"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "\n",
        "#@title Debugging\n",
        "# See https://zohaib.me/debugging-in-google-collab-notebook/ for tips,\n",
        "# as well as docs for pdb and ipdb.\n",
        "DEBUG = False #@param {type:\"boolean\"}\n",
        "GDRIVE_BASE = \"/content/drive\" #@param\n",
        "DATAFRAME_PATH = \"/MyDrive/amazon_rainforest_files/monthly_large.csv\" #@param\n",
        "\n",
        "def get_dataframe_path_from_params() -> str:\n",
        "  root = GDRIVE_BASE if GDRIVE_BASE else \"\"\n",
        "  return f\"{root}{DATAFRAME_PATH}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "xeNtt6exCb4n",
        "outputId": "3cb96f6c-a3cd-4a98-d8d2-f30cc3528c03"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-04642d11bb25>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mGDRIVE_BASE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGDRIVE_BASE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# Access data stored on Google Drive\n",
        "if GDRIVE_BASE:\n",
        "    from google.colab import drive\n",
        "    drive.mount(GDRIVE_BASE)\n",
        "\n",
        "if DEBUG:\n",
        "    %pip install -Uqq ipdb\n",
        "    import ipdb\n",
        "    %pdb on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEhjp8rhCb4o"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def kl_divergence_loss(real, predicted):\n",
        "    real_value = real[0]\n",
        "    real_variance = real[1] + 0.00000001\n",
        "    predicted_value = predicted[0]\n",
        "    predicted_variance = predicted[1] + 0.00000001\n",
        "\n",
        "    kl_loss = -0.5 + tf.math.log(predicted_variance/real_variance) + \\\n",
        "     (tf.square(real_variance) + tf.square(real_value - predicted_value))/ \\\n",
        "     2*tf.square(predicted_variance)\n",
        "    return tf.math.reduce_mean(kl_loss)\n",
        "\n",
        "def train_nn(\n",
        "        X: pd.DataFrame,\n",
        "        Y: pd.DataFrame,\n",
        "        hidden_layers: List[int],\n",
        "        epochs: int,\n",
        "        batch_size: int):\n",
        "    # Layers share between mean and variance regressors.\n",
        "    shared_layers = []\n",
        "    for num_nodes in hidden_layers:\n",
        "        shared_layers.append(layers.Dense(\n",
        "            num_nodes,\n",
        "            activation='relu'))\n",
        "\n",
        "    #Initialize input layers and connect them to shared layers.\n",
        "    num_inputs = X.shape[1]\n",
        "    inputs = keras.Input(shape=(num_inputs,))\n",
        "    x = inputs\n",
        "    for shared_layer in shared_layers:\n",
        "        x = shared_layer(x)\n",
        "\n",
        "    # Output is variance and mean, and connect to shared nodes.\n",
        "    mean_output_layer = layers.Dense(1, activation='linear', name='mean_output')\n",
        "    mean_output_node = mean_output_layer(x)\n",
        "    variance_output_layer = layers.Dense(1, activation='relu', name='variance_output')\n",
        "    variance_output_node = variance_output_layer(x)\n",
        "    outputs = [mean_output_node, variance_output_node]\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimizer, loss={'mean_output': 'mse', 'variance_output': kl_divergence_loss})\n",
        "    history = model.fit(X, Y, epochs=epochs, batch_size=batch_size)\n",
        "    return history, model\n",
        "\n",
        "def render_plot_loss(history):\n",
        "  plt.plot(history.history['mean_output_loss'])\n",
        "  plt.plot(history.history['variance_output_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['mean', 'variance'], loc='upper left')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQrEv57zCb4q"
      },
      "source": [
        "Data preparation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eSNtXphCb4s"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(get_dataframe_path_from_params())\n",
        "\n",
        "group_on = ['sample_site_lon', 'sample_site_lat', 'month_of_year']\n",
        "grouped = df.groupby(group_on)\n",
        "\n",
        "# ASSUMPTION: Taking the mean and variance of a sample site doesn't lower quality of the data.\n",
        "# We need to do this to take advantage of KL-divergence loss.\n",
        "means = grouped.mean()\n",
        "O18_var = grouped.var()['cellulose_oxygen_ratio'] \n",
        "\n",
        "# Merging results in some unreadable column names. Rename the oxygen columns.\n",
        "merged = pd.merge(means, O18_var, on=group_on, how='inner')\n",
        "merged = merged.rename(columns={\n",
        "    'cellulose_oxygen_ratio_x': 'O18_mean',\n",
        "    'cellulose_oxygen_ratio_y' : 'O18_var'})\n",
        "\n",
        "train, test = train_test_split(merged, test_size=0.25, random_state=25)\n",
        "\n",
        "# Target data: Mean and variance\n",
        "Y_train = train[[\"O18_mean\", \"O18_var\"]]\n",
        "Y_test = test[[\"O18_mean\", \"O18_var\"]]\n",
        "\n",
        "def format_output(Y):\n",
        "  y1 = Y.pop(\"O18_mean\")\n",
        "  y1 = np.array(y1)\n",
        "  y2 = Y.pop(\"O18_var\")\n",
        "  y2 = np.array(y2)\n",
        "  return y1, y2\n",
        "\n",
        "Y_train = format_output(Y_train)\n",
        "Y_test = format_output(Y_test)\n",
        "\n",
        "# Features: Everything besides mean and variance\n",
        "X_train = train.drop([\"O18_mean\", \"O18_var\"], axis=1)\n",
        "X_test = train.drop([\"O18_mean\", \"O18_var\"], axis=1)\n",
        "X_train.drop(X_train.columns[X_train.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
        "X_test.drop(X_test.columns[X_test.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUDsOgFJCb4t"
      },
      "source": [
        "Variance KL-loss: -14.3\n",
        "Mean MSE loss: 4.11\n",
        "\"General loss\": -10.17"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0IchVvlCb4u"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "history, model = train_nn(X_train, Y_train, hidden_layers=[12, 26], epochs=500, batch_size=150)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "render_plot_loss(history)"
      ],
      "metadata": {
        "id": "IecLJ7D5DIQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x=X_test, y=Y_test)"
      ],
      "metadata": {
        "id": "oLCwyNCPMkVr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}