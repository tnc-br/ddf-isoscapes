{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnc-br/ddf-isoscapes/blob/new_kl/dnn/weird_result.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational model\n",
        "\n",
        "Find the mean/variance of O18 ratios (as well as N15 and C13 in the future) at a particular lat/lon across Brazil. At the bottom of the colab, train and evaluate 4 different versions of the model with different data partitioning strategies."
      ],
      "metadata": {
        "id": "-0IfT3kGwgK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "henIPlAPCb4i"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import os\n",
        "from typing import List, Tuple, Dict\n",
        "from dataclasses import dataclass\n",
        "from joblib import dump\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.python.ops import math_ops\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "#@title Debugging\n",
        "# See https://zohaib.me/debugging-in-google-collab-notebook/ for tips,\n",
        "# as well as docs for pdb and ipdb.\n",
        "DEBUG = False #@param {type:\"boolean\"}\n",
        "USE_LOCAL_DRIVE = True #@param {type:\"boolean\"}\n",
        "LOCAL_DIR = \"/usr/local/google/home/ruru/Downloads/amazon_sample_data-20230712T203059Z-001\" #@param\n",
        "GDRIVE_DIR = \"MyDrive/amazon_rainforest_files/\" #@param\n",
        "FP_ROOT = LOCAL_DIR\n",
        "\n",
        "MODEL_SAVE_LOCATION = \"/usr/local/google/home/ruru/Downloads/model_builds\" #@param\n",
        "\n",
        "def get_model_save_location(filename) -> str:\n",
        "  root = '' if USE_LOCAL_DRIVE else '/content/drive'\n",
        "  return os.path.join(root, MODEL_SAVE_LOCATION, filename)\n",
        "\n",
        "# Access data stored on Google Drive if not reading data locally.\n",
        "if not USE_LOCAL_DRIVE:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  global FP_ROOT\n",
        "  FP_ROOT = os.path.join('/content/drive', GDRIVE_DIR)\n",
        "\n",
        "if DEBUG:\n",
        "    %pip install -Uqq ipdb\n",
        "    import ipdb\n",
        "    %pdb on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQrEv57zCb4q"
      },
      "source": [
        "# Data preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path: str, columns_to_keep: List[str]):\n",
        "  df = pd.read_csv(path, encoding=\"ISO-8859-1\", sep=',')\n",
        "  df = df[df['d18O_cel_variance'].notna()]\n",
        "  X = df\n",
        "\n",
        "  X['krig_mean_residual'] = X['ordinary_kriging_linear_d18O_predicted_mean'] - X['d18O_cel_mean']\n",
        "  X['krig_variance_residual'] = X['ordinary_kriging_linear_d18O_predicted_variance'] - X['d18O_cel_variance']\n",
        "\n",
        "  columns_to_keep = columns_to_keep + ['krig_mean_residual', 'krig_variance_residual']\n",
        "  X = X.drop(df.columns.difference(columns_to_keep), axis=1)\n",
        "\n",
        "  Y = df[[\"d18O_cel_mean\", \"d18O_cel_variance\"]]\n",
        "  print(X)\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "6XMee1aHfcik"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardization"
      ],
      "metadata": {
        "id": "DtkKhMOtb6cS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class FeaturesToLabels:\n",
        "  def __init__(self, X: pd.DataFrame, Y: pd.DataFrame):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "\n",
        "  def as_tuple(self):\n",
        "    return (self.X, self.Y)\n",
        "\n",
        "\n",
        "def create_feature_scaler(X: pd.DataFrame, columns_to_normalize, columns_to_standardize) -> ColumnTransformer:\n",
        "  feature_scaler = ColumnTransformer(\n",
        "      [('krig_mean_residual_scaler', StandardScaler(), ['krig_mean_residual']),\n",
        "       ('krig_variance_residual_scaler', StandardScaler(), ['krig_variance_residual'])] +\n",
        "      [(column+'_normalizer', Normalizer(), [column]) for column in columns_to_normalize] +\n",
        "      [(column+'_standardizer', StandardScaler(), [column]) for column in columns_to_standardize],\n",
        "      remainder='passthrough')\n",
        "  feature_scaler.fit(X)\n",
        "  print(feature_scaler)\n",
        "  return feature_scaler\n",
        "\n",
        "def create_label_scaler(Y: pd.DataFrame) -> ColumnTransformer:\n",
        "  label_scaler = ColumnTransformer([\n",
        "      ('mean_std_scaler', StandardScaler(), ['d18O_cel_mean']),\n",
        "      ('var_minmax_scaler', MinMaxScaler(), ['d18O_cel_variance'])],\n",
        "      remainder='passthrough')\n",
        "  label_scaler.fit(Y)\n",
        "  return label_scaler\n",
        "\n",
        "def scale(X: pd.DataFrame, Y: pd.DataFrame, feature_scaler, label_scaler):\n",
        "  # transform() outputs numpy arrays :(  need to convert back to DataFrame.\n",
        "  X_standardized = pd.DataFrame(feature_scaler.transform(X),\n",
        "                        index=X.index, columns=X.columns)\n",
        "  return FeaturesToLabels(X_standardized, Y)"
      ],
      "metadata": {
        "id": "XSDwdvMkb7w8"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just a class organization, holds each scaled dataset and the scaler used.\n",
        "# Useful for unscaling predictions.\n",
        "@dataclass\n",
        "class ScaledPartitions():\n",
        "  def __init__(self,\n",
        "               feature_scaler: ColumnTransformer,\n",
        "               label_scaler: ColumnTransformer,\n",
        "               train: FeaturesToLabels, val: FeaturesToLabels,\n",
        "               test: FeaturesToLabels):\n",
        "    self.feature_scaler = feature_scaler\n",
        "    self.label_scaler = label_scaler\n",
        "    self.train = train\n",
        "    self.val = val\n",
        "    self.test = test\n",
        "\n",
        "\n",
        "def load_and_scale(config: Dict,\n",
        "                   columns_to_normalize: List[str],\n",
        "                   columns_to_standardize: List[str]) -> ScaledPartitions:\n",
        "  columns_to_keep = columns_to_normalize + columns_to_standardize\n",
        "  X_train, Y_train = load_dataset(config['TRAIN'], columns_to_keep)\n",
        "  X_val, Y_val = load_dataset(config['VALIDATION'], columns_to_keep)\n",
        "  X_test, Y_test = load_dataset(config['TEST'], columns_to_keep)\n",
        "\n",
        "  feature_scaler = create_feature_scaler(X_train, columns_to_normalize, columns_to_standardize)\n",
        "  label_scaler = create_label_scaler(Y_train)\n",
        "  train = scale(X_train, Y_train, feature_scaler, label_scaler)\n",
        "  val = scale(X_val, Y_val, feature_scaler, label_scaler)\n",
        "  test = scale(X_test, Y_test, feature_scaler, label_scaler)\n",
        "  return ScaledPartitions(feature_scaler, label_scaler, train, val, test)\n"
      ],
      "metadata": {
        "id": "_kf2e_fKon2P"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition\n",
        "\n"
      ],
      "metadata": {
        "id": "usGznR593LZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KL Loss function:"
      ],
      "metadata": {
        "id": "khK7C8WvU8ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_normal_distribution(\n",
        "    mean: tf.Tensor,\n",
        "    stdev: tf.Tensor,\n",
        "    n: int) -> tf.Tensor:\n",
        "    '''\n",
        "    Given a batch of normal distributions described by a mean and stdev in\n",
        "    a tf.Tensor, sample n elements from each distribution and return the mean\n",
        "    and standard deviation per sample.\n",
        "    '''\n",
        "    batch_size = tf.shape(mean)[0]\n",
        "\n",
        "    # Output tensor is (n, batch_size, 1)\n",
        "    sample_values = tfp.distributions.Normal(\n",
        "        loc=mean,\n",
        "        scale=stdev).sample(\n",
        "            sample_shape=n)\n",
        "    # Reshaped tensor will be (batch_size, n)\n",
        "    sample_values = tf.transpose(sample_values)\n",
        "    # Get the mean per sample in the batch.\n",
        "    sample_mean = tf.transpose(tf.math.reduce_mean(sample_values, 2))\n",
        "    sample_stdev = tf.transpose(tf.math.reduce_std(sample_values, 2))\n",
        "\n",
        "    return sample_mean, sample_stdev\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# log(σ2/σ1) + ( σ1^2+(μ1−μ2)^2 ) / 2* σ^2   − 1/2\n",
        "def kl_divergence_helper(real, predicted, sample):\n",
        "    '''\n",
        "    real: tf.Tensor of the real mean and standard deviation of sample to compare\n",
        "    predicted: tf.Tensor of the predicted mean and standard deviation to compare\n",
        "    sample: Whether or not to sample the predicted distribution to get a new\n",
        "            mean and standard deviation.\n",
        "    '''\n",
        "    if real.shape != predicted.shape:\n",
        "      raise ValueError(\n",
        "          f\"real.shape {real.shape} != predicted.shape {predicted.shape}\")\n",
        "\n",
        "    real_value = tf.gather(real, [0], axis=1)\n",
        "    real_std = tf.math.sqrt(tf.gather(real, [1], axis=1))\n",
        "\n",
        "\n",
        "    predicted_value = tf.gather(predicted, [0], axis=1)\n",
        "    predicted_std = tf.math.sqrt(tf.gather(predicted, [1], axis=1))\n",
        "    # If true, sample from the distribution defined by the predicted mean and\n",
        "    # standard deviation to use for mean and stdev used in KL divergence loss.\n",
        "    if sample:\n",
        "      predicted_value, predicted_std = sample_normal_distribution(\n",
        "          mean=predicted_value, stdev=predicted_std, n=15)\n",
        "\n",
        "    kl_loss = -0.5 + tf.math.log(predicted_std/real_std) + \\\n",
        "     (tf.square(real_std) + tf.square(real_value - predicted_value))/ \\\n",
        "     (2*tf.square(predicted_std))\n",
        "\n",
        "    return tf.math.reduce_mean(kl_loss)\n",
        "\n",
        "def kl_divergence(real, predicted):\n",
        "  return kl_divergence_helper(real, predicted, True)"
      ],
      "metadata": {
        "id": "urGjYNNnemX6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the loss function:"
      ],
      "metadata": {
        "id": "fJzBFWQVeqNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "\n",
        "class TensorsDifferShapeTest(unittest.TestCase):\n",
        "   def test(self):\n",
        "      test_real = tf.convert_to_tensor(np.array([[1, 0.02]]))\n",
        "      test_pred = tf.convert_to_tensor(np.array([[0.98]]))\n",
        "      with self.assertRaises(ValueError):\n",
        "         kl_divergence(test_real, test_pred, False)\n",
        "         assert(False) # Triggers if no exception is caught in the previous line.\n",
        "\n",
        "TensorsDifferShapeTest().test()\n",
        "\n",
        "test_real = tf.convert_to_tensor(np.array([[1, 0.02]]))\n",
        "test_pred = tf.convert_to_tensor(np.array([[0.98, 0.021]]))\n",
        "\n",
        "# https://screenshot.googleplex.com/5WM9dinAbhR26ZS\n",
        "assert float(kl_divergence(test_real, test_pred)) == pytest.approx(0.0101094, 1e-5)\n",
        "\n",
        "test_neg_real = tf.convert_to_tensor(np.array([[32.32, 0.0344]]))\n",
        "test_neg_pred = tf.convert_to_tensor(np.array([[32.01, -0.322]]))\n",
        "\n",
        "# Negative variance causes NaN\n",
        "assert tf.math.is_nan(kl_divergence(test_neg_real, test_neg_pred))\n",
        "\n",
        "# Calculated manually by computing the result of this equation in wolfram alpha:\n",
        "# log(σ2/σ1) + ( σ1^2+(μ1−μ2)^2 ) / 2* σ^2   − 1/2\n",
        "test_real_2d = tf.convert_to_tensor(np.array(\n",
        "    [[1.00, 0.020],\n",
        "     [1.01, 0.042]]))\n",
        "test_pred_2d = tf.convert_to_tensor(np.array(\n",
        "    [[0.98, 0.021],\n",
        "     [0.99, 0.012]]))\n",
        "\n",
        "# Should reduce to the average loss of all rows.\n",
        "assert float(kl_divergence(test_real_2d, test_pred_2d)) == pytest.approx(\n",
        "    sum([0.0101094, 0.6402851])/2, 1e-5)"
      ],
      "metadata": {
        "id": "48TaPd70erSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model definition"
      ],
      "metadata": {
        "id": "8rI6qPRh7oO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "def get_early_stopping_callback():\n",
        "  return EarlyStopping(monitor='val_loss', patience=200, min_delta=0.001,\n",
        "                       verbose=1, restore_best_weights=True, start_from_epoch=0)\n",
        "\n",
        "tf.keras.utils.set_random_seed(18731)\n",
        "\n",
        "# I was experimenting with models that took longer to train, and used this\n",
        "# checkpointing callback to periodically save the model. It's optional.\n",
        "def get_checkpoint_callback(model_file):\n",
        "  return ModelCheckpoint(\n",
        "      get_model_save_location(model_file),\n",
        "      monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
        "\n",
        "def train_or_update_variational_model(\n",
        "        sp: ScaledPartitions,\n",
        "        hidden_layers: List[int],\n",
        "        epochs: int,\n",
        "        batch_size: int,\n",
        "        lr: float,\n",
        "        model_file=None,\n",
        "        use_checkpoint=False):\n",
        "  callbacks_list = [get_early_stopping_callback(),\n",
        "                    get_checkpoint_callback(model_file)]\n",
        "  if not use_checkpoint:\n",
        "\n",
        "    # Find the kriging columns and make sure they are at the end of the dataframe.\n",
        "    krig_mean_index = sp.train.X.columns.get_loc(\"krig_mean_residual\")\n",
        "    krig_var_index = sp.train.X.columns.get_loc(\"krig_variance_residual\")\n",
        "    if (krig_mean_index != sp.train.X.shape[1]-2 and krig_var_index != sp.train.X.shape[1]-1):\n",
        "      raise ValueError(\"krig_mean_residual and krig_variance_residual must be\"\n",
        "      \"located in the last two columns of dataframe\")\n",
        "\n",
        "    inputs = keras.Input(shape=(sp.train.X.shape[1],))\n",
        "    krig_mean = tf.expand_dims(inputs[:,-2], 1)\n",
        "    krig_var = tf.expand_dims(inputs[:, -1], 1)\n",
        "\n",
        "    # X contains everything else.\n",
        "    x = inputs[:,0:-2]\n",
        "\n",
        "    for layer_size in hidden_layers:\n",
        "      x = keras.layers.Dense(layer_size, activation='relu')(x)\n",
        "\n",
        "    # Invert the normalization on our outputs, and add kriging predictions as\n",
        "    # constants so the network only predicts the residuals.\n",
        "    mean_residual_output = keras.layers.Dense(1, name='mean_output')(x)\n",
        "    mean_residual_scaler = sp.feature_scaler.named_transformers_['krig_mean_residual_scaler']\n",
        "    unscaled_mean_residual = mean_residual_output * mean_residual_scaler.scale_ + mean_residual_scaler.mean_\n",
        "    untransformed_mean = krig_mean + unscaled_mean_residual\n",
        "\n",
        "    var_residual_output = keras.layers.Dense(1, name='var_output')(x)\n",
        "    var_residual_scaler = sp.feature_scaler.named_transformers_['krig_variance_residual_scaler']\n",
        "    unscaled_var_residual = var_residual_output * var_residual_scaler.scale_  + var_residual_scaler.mean_\n",
        "    untransformed_var = keras.layers.Lambda(lambda t: tf.math.log(1 + tf.exp(t[0]+t[1])))([unscaled_var_residual, krig_var])\n",
        "\n",
        "    # Output mean,  tuples.\n",
        "    outputs = keras.layers.concatenate([untransformed_mean, untransformed_var])\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Later epochs seem to benefit from lower learning rate... but it takes\n",
        "    # a while to get there.\n",
        "    decay = keras.optimizers.schedules.ExponentialDecay(\n",
        "       lr, decay_steps=100, decay_rate=0.5, staircase=True)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss=kl_divergence)\n",
        "    model.summary()\n",
        "  else:\n",
        "    model = keras.models.load_model(\n",
        "        get_model_save_location(model_file),\n",
        "        custom_objects={\"kl_divergence\": kl_divergence})\n",
        "  history = model.fit(sp.train.X, sp.train.Y, verbose=1, epochs=epochs, batch_size=batch_size,\n",
        "                      validation_data=sp.val.as_tuple(),\n",
        "                      shuffle=True, callbacks=callbacks_list)\n",
        "  return history, model"
      ],
      "metadata": {
        "id": "HCkGSPUo3KqY"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def render_plot_loss(history, name):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title(name + ' model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.yscale(\"log\")\n",
        "  plt.ylim((0, 10))\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['loss', 'val_loss'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def destandardize(sd: ScaledPartitions, df: pd.DataFrame):\n",
        "  means = pd.DataFrame(\n",
        "      sd.label_scaler.named_transformers_['var_std_scaler'].inverse_transform(df[['d18O_cel_mean']]),\n",
        "      index=df.index, columns=['d18O_cel_mean'])\n",
        "  vars = df['d18O_cel_variance']\n",
        "  return means.join(vars)\n",
        "\n",
        "def train_and_evaluate(sp: ScaledPartitions, run_id: str, training_batch_size=5):\n",
        "  print(\"==================\")\n",
        "  print(run_id)\n",
        "  history, model = train_or_update_variational_model(\n",
        "      sp, hidden_layers=[20, 20], epochs=10000, batch_size=training_batch_size,\n",
        "      lr=0.001, model_file=run_id+\".h5\", use_checkpoint=False)\n",
        "  render_plot_loss(history, run_id+\" kl_loss\")\n",
        "  model.save(get_model_save_location(run_id+\".h5\"), save_format=\"h5\")\n",
        "\n",
        "  best_epoch_index = history.history['val_loss'].index(min(history.history['val_loss']))\n",
        "  print('Val loss:', history.history['val_loss'][best_epoch_index])\n",
        "  print('Train loss:', history.history['loss'][best_epoch_index])\n",
        "  print('Test loss:', model.evaluate(x=sp.test.X, y=sp.test.Y, verbose=0))\n",
        "\n",
        "  predictions = model.predict_on_batch(sp.test.X)\n",
        "  predictions = pd.DataFrame(predictions, columns=['d18O_cel_mean', 'd18O_cel_variance'])\n",
        "  rmse = np.sqrt(mean_squared_error(sp.test.Y['d18O_cel_mean'], predictions['d18O_cel_mean']))\n",
        "  print(\"dO18 RMSE: \"+ str(rmse))\n",
        "  print(\"EXPECTED:\")\n",
        "  print(sp.test.Y.to_string())\n",
        "  print()\n",
        "  print(\"PREDICTED:\")\n",
        "  print(predictions.to_string())\n",
        "  return model"
      ],
      "metadata": {
        "id": "DALuUm8UOgNu"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Grouped, random\n",
        "\n",
        "We can't (easily) generate isoscapes with these because the isoscapes for 'predkrig_br_lat_ISORG', 'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION' are not easily retrievable... but I'm curious how much better the model is if these columns are included."
      ],
      "metadata": {
        "id": "n8pNR1CrvF2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_random_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/kriging_ensemble_experiment/uc_davis_train_random_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/kriging_ensemble_experiment/uc_davis_test_random_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/kriging_ensemble_experiment/uc_davis_validation_random_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_normalize = []\n",
        "columns_to_standardize = [\n",
        "    'lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature', 'Mean Annual Precipitation']\n",
        "\n",
        "data = load_and_scale(grouped_random_fileset, columns_to_normalize, columns_to_standardize)\n",
        "model = train_and_evaluate(data, \"random_all_boosted\", training_batch_size=3)\n",
        "model.save(get_model_save_location(\"random_all_boosted.keras\"))\n",
        "dump(data.feature_scaler, get_model_save_location('random_all_boosted_transformer.pkl'))\n"
      ],
      "metadata": {
        "id": "rPONfgkjvJWz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b58b563-33c7-4203-e42a-d6b9a819bdd6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         lat       long      VPD       RH       PET  DEM          PA  \\\n",
            "0  -0.121000 -67.013000  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "1  -0.041000 -66.872000  0.62083  0.82559  92.27500   93  1001.84741   \n",
            "2  -3.995545 -57.589273  0.85167  0.77358  99.99167   61  1005.67358   \n",
            "3  -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "4  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "..       ...        ...      ...      ...       ...  ...         ...   \n",
            "94 -2.496000 -59.120000  0.77500  0.78866  98.45000  139   996.36792   \n",
            "95 -2.493000 -59.121000  0.77500  0.78866  98.45000  139   996.36792   \n",
            "96 -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "98 -2.497000 -59.121000  0.77500  0.78866  98.45000  139   996.36792   \n",
            "99 -2.483000 -59.124000  0.77500  0.78866  98.45000   96  1001.48932   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  \\\n",
            "0                  26.20833                       2830   \n",
            "1                  26.37500                       2764   \n",
            "2                  27.16667                       2273   \n",
            "3                  26.20833                       2830   \n",
            "4                  26.29583                       1897   \n",
            "..                      ...                        ...   \n",
            "94                 26.79167                       2253   \n",
            "95                 26.79167                       2253   \n",
            "96                 26.71667                       2795   \n",
            "98                 26.79167                       2253   \n",
            "99                 26.79167                       2253   \n",
            "\n",
            "    Iso_Oxi_Stack_mean_TERZER  isoscape_fullmodel_d18O_prec_REGRESSION  \\\n",
            "0                    -3.96045                                 -4.69293   \n",
            "1                    -3.92301                                 -4.67525   \n",
            "2                    -3.51406                                 -3.73041   \n",
            "3                    -3.96045                                 -4.69293   \n",
            "4                    -3.27639                                 -3.48101   \n",
            "..                        ...                                      ...   \n",
            "94                   -3.70363                                 -3.84010   \n",
            "95                   -3.70363                                 -3.84010   \n",
            "96                   -4.16807                                 -4.86485   \n",
            "98                   -3.70363                                 -3.84010   \n",
            "99                   -3.69263                                 -3.84010   \n",
            "\n",
            "    krig_mean_residual  krig_variance_residual  \n",
            "0             1.589397                0.559822  \n",
            "1            -0.676855                0.474394  \n",
            "2            -0.614146                0.179081  \n",
            "3            -0.941343                0.641567  \n",
            "4            -0.229080                0.225259  \n",
            "..                 ...                     ...  \n",
            "94           -0.621182                0.463616  \n",
            "95           -0.063182               -1.136534  \n",
            "96           -0.246030                0.282990  \n",
            "98            0.182818                0.371679  \n",
            "99           -0.369182                0.424476  \n",
            "\n",
            "[99 rows x 13 columns]\n",
            "         lat       long      VPD       RH       PET  DEM          PA  \\\n",
            "0  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "1  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "2  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "3  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "4  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "5  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "6  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "7  -6.009707 -61.868657  0.77083  0.79509  93.97500   71  1004.47662   \n",
            "8  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "9  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "10 -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "11 -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  \\\n",
            "0                  26.00000                       1840   \n",
            "1                  26.00000                       1840   \n",
            "2                  26.00000                       1840   \n",
            "3                  26.00000                       1840   \n",
            "4                  26.00000                       1840   \n",
            "5                  26.00000                       1840   \n",
            "6                  26.00000                       1840   \n",
            "7                  27.20000                       1996   \n",
            "8                  26.00000                       1840   \n",
            "9                  26.00000                       1840   \n",
            "10                 26.29583                       1897   \n",
            "11                 26.29583                       1897   \n",
            "\n",
            "    Iso_Oxi_Stack_mean_TERZER  isoscape_fullmodel_d18O_prec_REGRESSION  \\\n",
            "0                    -3.30055                                 -3.42629   \n",
            "1                    -3.30055                                 -3.42629   \n",
            "2                    -3.30055                                 -3.42629   \n",
            "3                    -3.30055                                 -3.42629   \n",
            "4                    -3.30055                                 -3.42629   \n",
            "5                    -3.30055                                 -3.42629   \n",
            "6                    -3.30055                                 -3.42629   \n",
            "7                    -4.05694                                 -4.46622   \n",
            "8                    -3.30055                                 -3.42629   \n",
            "9                    -3.30055                                 -3.42629   \n",
            "10                   -3.27639                                 -3.48101   \n",
            "11                   -3.27639                                 -3.48101   \n",
            "\n",
            "    krig_mean_residual  krig_variance_residual  \n",
            "0             1.836516               -1.130660  \n",
            "1             0.408516                0.319060  \n",
            "2             0.856516               -1.771210  \n",
            "3             0.678516               -0.693590  \n",
            "4            -0.191484                0.665760  \n",
            "5             0.956016                0.807252  \n",
            "6             0.728516                0.008160  \n",
            "7             0.343073                0.488285  \n",
            "8             1.610516                0.241840  \n",
            "9             1.468516                0.636760  \n",
            "10            0.090920               -0.256241  \n",
            "11           -0.463080                0.547799  \n",
            "         lat       long      VPD       RH       PET  DEM          PA  \\\n",
            "0  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "1  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "2  -0.121000 -67.013000  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "3  -0.041000 -66.872000  0.62083  0.82559  92.27500   93  1001.84741   \n",
            "4  -3.907000 -66.055000  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "5  -4.304000 -70.291000  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "6  -4.304000 -70.291000  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "7  -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "8  -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "9  -4.303698 -70.291068  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "10 -6.009707 -61.868657  0.77083  0.79509  93.97500   71  1004.47662   \n",
            "11 -3.758534 -66.073754  0.64667  0.82247  90.44167   84  1002.92230   \n",
            "12 -6.009707 -61.868657  0.77083  0.79509  93.97500   71  1004.47662   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  \\\n",
            "0                  26.29583                       1897   \n",
            "1                  26.29583                       1897   \n",
            "2                  26.20833                       2830   \n",
            "3                  26.37500                       2764   \n",
            "4                  26.71667                       2795   \n",
            "5                  26.64583                       2708   \n",
            "6                  26.64583                       2708   \n",
            "7                  26.71667                       2795   \n",
            "8                  26.20833                       2830   \n",
            "9                  26.64583                       2708   \n",
            "10                 27.20000                       1996   \n",
            "11                 26.71667                       2856   \n",
            "12                 27.20000                       1996   \n",
            "\n",
            "    Iso_Oxi_Stack_mean_TERZER  isoscape_fullmodel_d18O_prec_REGRESSION  \\\n",
            "0                    -3.27639                                -3.481010   \n",
            "1                    -3.27639                                -3.481010   \n",
            "2                    -3.96045                                -4.692930   \n",
            "3                    -3.92301                                -4.675250   \n",
            "4                    -4.16807                                -4.864850   \n",
            "5                    -4.36128                                -5.472800   \n",
            "6                    -4.36128                                -5.472800   \n",
            "7                    -4.16807                                -4.864850   \n",
            "8                    -3.96045                                -4.692930   \n",
            "9                    -4.36128                                -5.472804   \n",
            "10                   -4.05694                                -4.466220   \n",
            "11                   -4.13790                                -4.845370   \n",
            "12                   -4.05694                                -4.466220   \n",
            "\n",
            "    krig_mean_residual  krig_variance_residual  \n",
            "0             0.502920                0.524979  \n",
            "1            -0.351080                0.313999  \n",
            "2             0.338657                0.530817  \n",
            "3            -1.307010                0.648582  \n",
            "4             0.987970                0.383700  \n",
            "5            -0.901265                0.554377  \n",
            "6            -1.133265                0.661547  \n",
            "7            -1.596030               -1.450160  \n",
            "8            -0.687343               -0.236623  \n",
            "9            -0.081265                0.254277  \n",
            "10            1.297073                0.566225  \n",
            "11            1.209347                0.203298  \n",
            "12           -0.776927               -0.239815  \n",
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('krig_mean_residual_scaler', StandardScaler(),\n",
            "                                 ['krig_mean_residual']),\n",
            "                                ('krig_variance_residual_scaler',\n",
            "                                 StandardScaler(), ['krig_variance_residual']),\n",
            "                                ('lat_scaler', StandardScaler(), ['lat']),\n",
            "                                ('long_scaler', StandardScaler(), ['long']),\n",
            "                                ('VPD_scaler', StandardScaler(), ['VPD']),\n",
            "                                ('RH_scaler', StandardSca...\n",
            "                                ('Mean Annual Temperature_scaler',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_scaler',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation']),\n",
            "                                ('Iso_Oxi_Stack_mean_TERZER_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Iso_Oxi_Stack_mean_TERZER']),\n",
            "                                ('isoscape_fullmodel_d18O_prec_REGRESSION_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['isoscape_fullmodel_d18O_prec_REGRESSION'])])\n",
            "==================\n",
            "random_all_boosted\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_6 (InputLayer)           [(None, 13)]         0           []                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_17 (S  (None, 11)          0           ['input_6[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 20)           240         ['tf.__operators__.getitem_17[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 20)           420         ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " mean_output (Dense)            (None, 1)            21          ['dense_11[0][0]']               \n",
            "                                                                                                  \n",
            " var_output (Dense)             (None, 1)            21          ['dense_11[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_15 (S  (None,)             0           ['input_6[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " tf.math.multiply_4 (TFOpLambda  (None, 1)           0           ['mean_output[0][0]']            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.multiply_5 (TFOpLambda  (None, 1)           0           ['var_output[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_16 (S  (None,)             0           ['input_6[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " tf.expand_dims_10 (TFOpLambda)  (None, 1)           0           ['tf.__operators__.getitem_15[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TFOpLa  (None, 1)           0           ['tf.math.multiply_4[0][0]']     \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_8 (TFOpLa  (None, 1)           0           ['tf.math.multiply_5[0][0]']     \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.expand_dims_11 (TFOpLambda)  (None, 1)           0           ['tf.__operators__.getitem_16[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TFOpLa  (None, 1)           0           ['tf.expand_dims_10[0][0]',      \n",
            " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1)            0           ['tf.__operators__.add_8[0][0]', \n",
            "                                                                  'tf.expand_dims_11[0][0]']      \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 2)            0           ['tf.__operators__.add_7[0][0]', \n",
            "                                                                  'lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 702\n",
            "Trainable params: 702\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10000\n",
            "33/33 [==============================] - 1s 7ms/step - loss: 1946.6277 - val_loss: 112.4602\n",
            "Epoch 2/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 377.0822 - val_loss: 95.1581\n",
            "Epoch 3/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 232.9733 - val_loss: 65.8177\n",
            "Epoch 4/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 174.3996 - val_loss: 47.1603\n",
            "Epoch 5/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 153.0096 - val_loss: 49.1451\n",
            "Epoch 6/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 116.4665 - val_loss: 44.7991\n",
            "Epoch 7/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 103.1327 - val_loss: 35.5642\n",
            "Epoch 8/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 96.1611 - val_loss: 38.1769\n",
            "Epoch 9/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 94.1444 - val_loss: 36.2565\n",
            "Epoch 10/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 83.1290 - val_loss: 32.3035\n",
            "Epoch 11/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 78.8360 - val_loss: 28.4507\n",
            "Epoch 12/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 65.0814 - val_loss: 35.4875\n",
            "Epoch 13/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 61.2993 - val_loss: 30.0451\n",
            "Epoch 14/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 56.1394 - val_loss: 25.9903\n",
            "Epoch 15/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 52.1674 - val_loss: 24.5366\n",
            "Epoch 16/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 51.1340 - val_loss: 22.0513\n",
            "Epoch 17/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 49.9332 - val_loss: 17.3272\n",
            "Epoch 18/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 47.5011 - val_loss: 19.2494\n",
            "Epoch 19/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 37.7519 - val_loss: 19.7203\n",
            "Epoch 20/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 41.0860 - val_loss: 19.9167\n",
            "Epoch 21/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 39.4567 - val_loss: 15.8177\n",
            "Epoch 22/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 35.1833 - val_loss: 16.8444\n",
            "Epoch 23/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 36.6001 - val_loss: 14.3168\n",
            "Epoch 24/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 37.8076 - val_loss: 13.7995\n",
            "Epoch 25/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 32.1198 - val_loss: 14.5733\n",
            "Epoch 26/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 28.9295 - val_loss: 14.9461\n",
            "Epoch 27/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 28.3021 - val_loss: 12.9839\n",
            "Epoch 28/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 28.2721 - val_loss: 12.1199\n",
            "Epoch 29/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 27.5067 - val_loss: 11.2145\n",
            "Epoch 30/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 27.9148 - val_loss: 11.4019\n",
            "Epoch 31/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 25.0137 - val_loss: 10.2504\n",
            "Epoch 32/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 23.3856 - val_loss: 9.3299\n",
            "Epoch 33/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 22.2830 - val_loss: 10.6068\n",
            "Epoch 34/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 22.1180 - val_loss: 10.5930\n",
            "Epoch 35/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 20.0211 - val_loss: 7.8794\n",
            "Epoch 36/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 21.5558 - val_loss: 8.3010\n",
            "Epoch 37/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 20.2718 - val_loss: 6.4918\n",
            "Epoch 38/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 17.5218 - val_loss: 6.5285\n",
            "Epoch 39/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 16.8393 - val_loss: 7.9902\n",
            "Epoch 40/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 18.3858 - val_loss: 6.0279\n",
            "Epoch 41/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 16.4677 - val_loss: 6.9373\n",
            "Epoch 42/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 17.5881 - val_loss: 6.3668\n",
            "Epoch 43/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 15.0902 - val_loss: 5.1844\n",
            "Epoch 44/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 13.7605 - val_loss: 4.4330\n",
            "Epoch 45/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 16.2106 - val_loss: 4.3664\n",
            "Epoch 46/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 13.6166 - val_loss: 4.2860\n",
            "Epoch 47/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 12.9804 - val_loss: 4.0165\n",
            "Epoch 48/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 14.0206 - val_loss: 4.3175\n",
            "Epoch 49/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 13.6977 - val_loss: 3.8681\n",
            "Epoch 50/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 12.8077 - val_loss: 3.8446\n",
            "Epoch 51/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 12.3254 - val_loss: 3.3476\n",
            "Epoch 52/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 10.9521 - val_loss: 2.9502\n",
            "Epoch 53/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 11.3447 - val_loss: 2.9005\n",
            "Epoch 54/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 11.3286 - val_loss: 3.2969\n",
            "Epoch 55/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 9.6437 - val_loss: 2.9087\n",
            "Epoch 56/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 9.7992 - val_loss: 2.9302\n",
            "Epoch 57/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 8.9293 - val_loss: 2.6367\n",
            "Epoch 58/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 9.4719 - val_loss: 2.6374\n",
            "Epoch 59/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 9.3185 - val_loss: 2.3453\n",
            "Epoch 60/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 9.6422 - val_loss: 2.3376\n",
            "Epoch 61/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 8.0790 - val_loss: 2.3725\n",
            "Epoch 62/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 8.3218 - val_loss: 2.4434\n",
            "Epoch 63/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 7.7139 - val_loss: 2.1900\n",
            "Epoch 64/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 7.7345 - val_loss: 2.2366\n",
            "Epoch 65/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 7.5980 - val_loss: 2.3287\n",
            "Epoch 66/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 7.4941 - val_loss: 2.1092\n",
            "Epoch 67/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 6.8506 - val_loss: 1.9802\n",
            "Epoch 68/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 6.6663 - val_loss: 1.9450\n",
            "Epoch 69/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 7.0942 - val_loss: 2.0717\n",
            "Epoch 70/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 6.2818 - val_loss: 1.9239\n",
            "Epoch 71/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 6.2095 - val_loss: 1.9348\n",
            "Epoch 72/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.9325 - val_loss: 1.9189\n",
            "Epoch 73/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.9072 - val_loss: 1.9366\n",
            "Epoch 74/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 6.0787 - val_loss: 1.8999\n",
            "Epoch 75/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.8696 - val_loss: 1.9043\n",
            "Epoch 76/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.9369 - val_loss: 1.8724\n",
            "Epoch 77/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 5.5891 - val_loss: 1.7968\n",
            "Epoch 78/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.6476 - val_loss: 1.9239\n",
            "Epoch 79/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.7995 - val_loss: 1.8135\n",
            "Epoch 80/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.4403 - val_loss: 2.0028\n",
            "Epoch 81/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.3327 - val_loss: 1.9699\n",
            "Epoch 82/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.8927 - val_loss: 2.0000\n",
            "Epoch 83/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.6054 - val_loss: 1.9457\n",
            "Epoch 84/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.1785 - val_loss: 1.9650\n",
            "Epoch 85/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.6138 - val_loss: 2.0222\n",
            "Epoch 86/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.7615 - val_loss: 1.9703\n",
            "Epoch 87/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.7196 - val_loss: 1.8786\n",
            "Epoch 88/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.8218 - val_loss: 2.0657\n",
            "Epoch 89/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.7852 - val_loss: 2.0737\n",
            "Epoch 90/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.7760 - val_loss: 1.9526\n",
            "Epoch 91/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.7602 - val_loss: 2.0468\n",
            "Epoch 92/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.6243 - val_loss: 2.1193\n",
            "Epoch 93/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.1060 - val_loss: 2.0487\n",
            "Epoch 94/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.5491 - val_loss: 2.1579\n",
            "Epoch 95/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.1278 - val_loss: 2.1128\n",
            "Epoch 96/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.9977 - val_loss: 2.1323\n",
            "Epoch 97/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.2328 - val_loss: 2.0521\n",
            "Epoch 98/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.9585 - val_loss: 2.1071\n",
            "Epoch 99/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.1966 - val_loss: 2.2277\n",
            "Epoch 100/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.8077 - val_loss: 2.1710\n",
            "Epoch 101/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.2701 - val_loss: 2.2765\n",
            "Epoch 102/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.9515 - val_loss: 2.3194\n",
            "Epoch 103/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.0686 - val_loss: 2.2179\n",
            "Epoch 104/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.6293 - val_loss: 2.2271\n",
            "Epoch 105/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.7648 - val_loss: 2.2317\n",
            "Epoch 106/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.5898 - val_loss: 2.1850\n",
            "Epoch 107/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.5059 - val_loss: 2.1745\n",
            "Epoch 108/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.4145 - val_loss: 2.2857\n",
            "Epoch 109/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.4992 - val_loss: 2.2263\n",
            "Epoch 110/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.6642 - val_loss: 2.3414\n",
            "Epoch 111/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.3978 - val_loss: 2.3362\n",
            "Epoch 112/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.5311 - val_loss: 2.2019\n",
            "Epoch 113/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.3796 - val_loss: 2.5427\n",
            "Epoch 114/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.2903 - val_loss: 2.2251\n",
            "Epoch 115/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.3051 - val_loss: 2.3697\n",
            "Epoch 116/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.5154 - val_loss: 2.3867\n",
            "Epoch 117/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.5809 - val_loss: 2.2156\n",
            "Epoch 118/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.1558 - val_loss: 2.2686\n",
            "Epoch 119/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.2892 - val_loss: 2.4102\n",
            "Epoch 120/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.0881 - val_loss: 2.3069\n",
            "Epoch 121/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.9748 - val_loss: 2.2402\n",
            "Epoch 122/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.1357 - val_loss: 2.3831\n",
            "Epoch 123/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.0803 - val_loss: 2.1560\n",
            "Epoch 124/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.2704 - val_loss: 2.4615\n",
            "Epoch 125/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.1640 - val_loss: 2.4197\n",
            "Epoch 126/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.9213 - val_loss: 2.4085\n",
            "Epoch 127/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.8822 - val_loss: 2.2410\n",
            "Epoch 128/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.9274 - val_loss: 2.4783\n",
            "Epoch 129/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.7348 - val_loss: 2.2450\n",
            "Epoch 130/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.8380 - val_loss: 2.3027\n",
            "Epoch 131/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.8540 - val_loss: 2.5596\n",
            "Epoch 132/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.8356 - val_loss: 2.4053\n",
            "Epoch 133/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.8344 - val_loss: 2.3762\n",
            "Epoch 134/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.8373 - val_loss: 2.4115\n",
            "Epoch 135/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.8071 - val_loss: 2.2869\n",
            "Epoch 136/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5813 - val_loss: 2.2937\n",
            "Epoch 137/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.6090 - val_loss: 2.2592\n",
            "Epoch 138/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.6532 - val_loss: 2.3274\n",
            "Epoch 139/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.7053 - val_loss: 2.2403\n",
            "Epoch 140/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.6459 - val_loss: 2.3331\n",
            "Epoch 141/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.7631 - val_loss: 2.3332\n",
            "Epoch 142/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5602 - val_loss: 2.4976\n",
            "Epoch 143/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.6984 - val_loss: 2.2496\n",
            "Epoch 144/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5311 - val_loss: 2.5245\n",
            "Epoch 145/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5249 - val_loss: 2.3228\n",
            "Epoch 146/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5509 - val_loss: 2.4623\n",
            "Epoch 147/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5226 - val_loss: 2.4067\n",
            "Epoch 148/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4604 - val_loss: 2.3692\n",
            "Epoch 149/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4482 - val_loss: 2.3673\n",
            "Epoch 150/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5064 - val_loss: 2.1420\n",
            "Epoch 151/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4159 - val_loss: 2.2229\n",
            "Epoch 152/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4333 - val_loss: 2.4680\n",
            "Epoch 153/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3623 - val_loss: 2.4531\n",
            "Epoch 154/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2929 - val_loss: 2.3942\n",
            "Epoch 155/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4625 - val_loss: 2.3167\n",
            "Epoch 156/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3156 - val_loss: 2.2860\n",
            "Epoch 157/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3271 - val_loss: 2.4058\n",
            "Epoch 158/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2475 - val_loss: 2.3707\n",
            "Epoch 159/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3508 - val_loss: 2.1908\n",
            "Epoch 160/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3139 - val_loss: 2.3716\n",
            "Epoch 161/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2764 - val_loss: 2.1807\n",
            "Epoch 162/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4250 - val_loss: 2.1807\n",
            "Epoch 163/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3667 - val_loss: 2.3789\n",
            "Epoch 164/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3146 - val_loss: 2.0586\n",
            "Epoch 165/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2709 - val_loss: 2.0986\n",
            "Epoch 166/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3122 - val_loss: 2.1314\n",
            "Epoch 167/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2042 - val_loss: 2.1485\n",
            "Epoch 168/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1984 - val_loss: 2.2256\n",
            "Epoch 169/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2525 - val_loss: 2.0305\n",
            "Epoch 170/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1584 - val_loss: 2.1384\n",
            "Epoch 171/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2281 - val_loss: 2.1272\n",
            "Epoch 172/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1917 - val_loss: 2.0313\n",
            "Epoch 173/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2078 - val_loss: 2.0508\n",
            "Epoch 174/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2094 - val_loss: 2.0818\n",
            "Epoch 175/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2109 - val_loss: 2.0546\n",
            "Epoch 176/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1200 - val_loss: 2.0275\n",
            "Epoch 177/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1771 - val_loss: 2.0715\n",
            "Epoch 178/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1317 - val_loss: 2.1348\n",
            "Epoch 179/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1144 - val_loss: 2.1047\n",
            "Epoch 180/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2024 - val_loss: 2.0084\n",
            "Epoch 181/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1357 - val_loss: 2.1068\n",
            "Epoch 182/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0984 - val_loss: 2.0120\n",
            "Epoch 183/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0943 - val_loss: 2.1320\n",
            "Epoch 184/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0997 - val_loss: 2.1194\n",
            "Epoch 185/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0658 - val_loss: 1.9882\n",
            "Epoch 186/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1130 - val_loss: 2.1384\n",
            "Epoch 187/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1153 - val_loss: 1.9875\n",
            "Epoch 188/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1334 - val_loss: 1.9254\n",
            "Epoch 189/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1309 - val_loss: 1.9574\n",
            "Epoch 190/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0739 - val_loss: 2.0005\n",
            "Epoch 191/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0572 - val_loss: 2.2773\n",
            "Epoch 192/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0029 - val_loss: 2.0065\n",
            "Epoch 193/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0501 - val_loss: 2.2037\n",
            "Epoch 194/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0883 - val_loss: 2.1502\n",
            "Epoch 195/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0124 - val_loss: 1.9402\n",
            "Epoch 196/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0383 - val_loss: 1.9831\n",
            "Epoch 197/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0258 - val_loss: 1.9940\n",
            "Epoch 198/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0613 - val_loss: 2.1305\n",
            "Epoch 199/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0107 - val_loss: 1.9585\n",
            "Epoch 200/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9939 - val_loss: 1.9753\n",
            "Epoch 201/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9655 - val_loss: 1.8684\n",
            "Epoch 202/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0028 - val_loss: 1.9763\n",
            "Epoch 203/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9845 - val_loss: 1.9078\n",
            "Epoch 204/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0287 - val_loss: 2.0551\n",
            "Epoch 205/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9938 - val_loss: 1.7869\n",
            "Epoch 206/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0223 - val_loss: 1.9736\n",
            "Epoch 207/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0410 - val_loss: 1.8989\n",
            "Epoch 208/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0187 - val_loss: 1.8813\n",
            "Epoch 209/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9729 - val_loss: 1.9620\n",
            "Epoch 210/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0060 - val_loss: 1.8849\n",
            "Epoch 211/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9260 - val_loss: 1.8906\n",
            "Epoch 212/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9553 - val_loss: 1.9333\n",
            "Epoch 213/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0311 - val_loss: 1.9521\n",
            "Epoch 214/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9500 - val_loss: 1.9279\n",
            "Epoch 215/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9656 - val_loss: 1.8580\n",
            "Epoch 216/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9840 - val_loss: 1.9059\n",
            "Epoch 217/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9691 - val_loss: 1.8757\n",
            "Epoch 218/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9169 - val_loss: 1.8288\n",
            "Epoch 219/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9648 - val_loss: 1.8954\n",
            "Epoch 220/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9495 - val_loss: 1.9128\n",
            "Epoch 221/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8713 - val_loss: 1.8583\n",
            "Epoch 222/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9363 - val_loss: 1.8588\n",
            "Epoch 223/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9401 - val_loss: 1.9732\n",
            "Epoch 224/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9170 - val_loss: 1.9344\n",
            "Epoch 225/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9230 - val_loss: 1.9557\n",
            "Epoch 226/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9238 - val_loss: 1.9025\n",
            "Epoch 227/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9281 - val_loss: 1.7033\n",
            "Epoch 228/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9096 - val_loss: 1.9224\n",
            "Epoch 229/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9236 - val_loss: 1.8754\n",
            "Epoch 230/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8792 - val_loss: 1.6718\n",
            "Epoch 231/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9254 - val_loss: 1.8894\n",
            "Epoch 232/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9782 - val_loss: 1.7450\n",
            "Epoch 233/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8903 - val_loss: 1.8969\n",
            "Epoch 234/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8613 - val_loss: 1.7529\n",
            "Epoch 235/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8729 - val_loss: 1.7862\n",
            "Epoch 236/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8412 - val_loss: 1.8150\n",
            "Epoch 237/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8708 - val_loss: 1.7353\n",
            "Epoch 238/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8659 - val_loss: 1.7603\n",
            "Epoch 239/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8614 - val_loss: 1.7585\n",
            "Epoch 240/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8795 - val_loss: 1.7456\n",
            "Epoch 241/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8487 - val_loss: 1.7667\n",
            "Epoch 242/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8393 - val_loss: 1.7282\n",
            "Epoch 243/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8280 - val_loss: 1.7539\n",
            "Epoch 244/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8148 - val_loss: 1.7332\n",
            "Epoch 245/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9175 - val_loss: 1.7356\n",
            "Epoch 246/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8663 - val_loss: 1.6483\n",
            "Epoch 247/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8502 - val_loss: 1.8619\n",
            "Epoch 248/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8457 - val_loss: 1.6832\n",
            "Epoch 249/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7996 - val_loss: 1.7500\n",
            "Epoch 250/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7959 - val_loss: 1.7782\n",
            "Epoch 251/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7581 - val_loss: 1.7733\n",
            "Epoch 252/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8169 - val_loss: 1.6937\n",
            "Epoch 253/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7787 - val_loss: 1.6126\n",
            "Epoch 254/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8004 - val_loss: 1.6837\n",
            "Epoch 255/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7810 - val_loss: 1.7023\n",
            "Epoch 256/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7511 - val_loss: 1.8814\n",
            "Epoch 257/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7283 - val_loss: 1.6202\n",
            "Epoch 258/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7488 - val_loss: 1.6680\n",
            "Epoch 259/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7346 - val_loss: 1.7447\n",
            "Epoch 260/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7380 - val_loss: 1.6965\n",
            "Epoch 261/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7555 - val_loss: 1.6502\n",
            "Epoch 262/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7702 - val_loss: 1.6073\n",
            "Epoch 263/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7055 - val_loss: 1.8421\n",
            "Epoch 264/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7219 - val_loss: 1.6192\n",
            "Epoch 265/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.6923 - val_loss: 1.5642\n",
            "Epoch 266/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7202 - val_loss: 1.5982\n",
            "Epoch 267/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7049 - val_loss: 1.6709\n",
            "Epoch 268/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6909 - val_loss: 1.5637\n",
            "Epoch 269/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7129 - val_loss: 1.6672\n",
            "Epoch 270/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6626 - val_loss: 1.6142\n",
            "Epoch 271/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6416 - val_loss: 1.4888\n",
            "Epoch 272/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6697 - val_loss: 1.5887\n",
            "Epoch 273/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6613 - val_loss: 1.6176\n",
            "Epoch 274/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6314 - val_loss: 1.5638\n",
            "Epoch 275/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5853 - val_loss: 1.5133\n",
            "Epoch 276/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6099 - val_loss: 1.4462\n",
            "Epoch 277/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5569 - val_loss: 1.5098\n",
            "Epoch 278/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5574 - val_loss: 1.4683\n",
            "Epoch 279/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5710 - val_loss: 1.5407\n",
            "Epoch 280/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4709 - val_loss: 1.6276\n",
            "Epoch 281/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5054 - val_loss: 1.5550\n",
            "Epoch 282/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5168 - val_loss: 1.5647\n",
            "Epoch 283/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4562 - val_loss: 1.5152\n",
            "Epoch 284/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4025 - val_loss: 1.4721\n",
            "Epoch 285/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4140 - val_loss: 1.4107\n",
            "Epoch 286/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3413 - val_loss: 1.5728\n",
            "Epoch 287/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3442 - val_loss: 1.4975\n",
            "Epoch 288/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3032 - val_loss: 1.6327\n",
            "Epoch 289/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2499 - val_loss: 1.5805\n",
            "Epoch 290/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2598 - val_loss: 1.4557\n",
            "Epoch 291/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2020 - val_loss: 1.4665\n",
            "Epoch 292/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.1769 - val_loss: 1.6263\n",
            "Epoch 293/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.0926 - val_loss: 1.8167\n",
            "Epoch 294/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.1340 - val_loss: 1.8046\n",
            "Epoch 295/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.1441 - val_loss: 1.2179\n",
            "Epoch 296/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.0570 - val_loss: 1.5968\n",
            "Epoch 297/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.0793 - val_loss: 1.3595\n",
            "Epoch 298/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.0261 - val_loss: 1.2397\n",
            "Epoch 299/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.0195 - val_loss: 1.8733\n",
            "Epoch 300/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.0064 - val_loss: 1.2656\n",
            "Epoch 301/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.0725 - val_loss: 1.1854\n",
            "Epoch 302/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.9222 - val_loss: 1.5283\n",
            "Epoch 303/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.9766 - val_loss: 1.4027\n",
            "Epoch 304/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.9426 - val_loss: 1.8025\n",
            "Epoch 305/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.9429 - val_loss: 1.6381\n",
            "Epoch 306/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.9086 - val_loss: 1.2324\n",
            "Epoch 307/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.8481 - val_loss: 2.3670\n",
            "Epoch 308/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.9211 - val_loss: 2.2575\n",
            "Epoch 309/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.8607 - val_loss: 1.8195\n",
            "Epoch 310/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.8622 - val_loss: 2.3147\n",
            "Epoch 311/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.9128 - val_loss: 1.6896\n",
            "Epoch 312/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.8327 - val_loss: 1.5639\n",
            "Epoch 313/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.8648 - val_loss: 2.2935\n",
            "Epoch 314/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7699 - val_loss: 2.0224\n",
            "Epoch 315/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.8231 - val_loss: 2.3714\n",
            "Epoch 316/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.8178 - val_loss: 3.0230\n",
            "Epoch 317/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7952 - val_loss: 2.2871\n",
            "Epoch 318/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7904 - val_loss: 2.0922\n",
            "Epoch 319/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7742 - val_loss: 2.3714\n",
            "Epoch 320/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7251 - val_loss: 2.4116\n",
            "Epoch 321/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6961 - val_loss: 2.8431\n",
            "Epoch 322/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6772 - val_loss: 2.8813\n",
            "Epoch 323/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6850 - val_loss: 3.9075\n",
            "Epoch 324/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6586 - val_loss: 3.0894\n",
            "Epoch 325/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6031 - val_loss: 7.1279\n",
            "Epoch 326/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6201 - val_loss: 3.8911\n",
            "Epoch 327/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7159 - val_loss: 2.4152\n",
            "Epoch 328/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6813 - val_loss: 3.5860\n",
            "Epoch 329/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6613 - val_loss: 2.0553\n",
            "Epoch 330/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6037 - val_loss: 4.0405\n",
            "Epoch 331/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6442 - val_loss: 3.3469\n",
            "Epoch 332/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6663 - val_loss: 2.8241\n",
            "Epoch 333/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6260 - val_loss: 2.8871\n",
            "Epoch 334/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5974 - val_loss: 4.3310\n",
            "Epoch 335/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5740 - val_loss: 2.8610\n",
            "Epoch 336/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5784 - val_loss: 4.8310\n",
            "Epoch 337/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5809 - val_loss: 3.5753\n",
            "Epoch 338/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5718 - val_loss: 2.4343\n",
            "Epoch 339/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6455 - val_loss: 3.7535\n",
            "Epoch 340/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5396 - val_loss: 6.0777\n",
            "Epoch 341/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5789 - val_loss: 4.4083\n",
            "Epoch 342/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5336 - val_loss: 5.9445\n",
            "Epoch 343/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5658 - val_loss: 4.5534\n",
            "Epoch 344/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5286 - val_loss: 4.0423\n",
            "Epoch 345/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5731 - val_loss: 3.2486\n",
            "Epoch 346/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5450 - val_loss: 3.3711\n",
            "Epoch 347/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5379 - val_loss: 3.0989\n",
            "Epoch 348/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5541 - val_loss: 2.8969\n",
            "Epoch 349/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4970 - val_loss: 3.1252\n",
            "Epoch 350/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5995 - val_loss: 2.3329\n",
            "Epoch 351/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5120 - val_loss: 3.8945\n",
            "Epoch 352/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5339 - val_loss: 2.5402\n",
            "Epoch 353/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4873 - val_loss: 3.6146\n",
            "Epoch 354/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5295 - val_loss: 3.6971\n",
            "Epoch 355/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4961 - val_loss: 3.7891\n",
            "Epoch 356/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4944 - val_loss: 3.8231\n",
            "Epoch 357/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5496 - val_loss: 2.9713\n",
            "Epoch 358/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5450 - val_loss: 3.4335\n",
            "Epoch 359/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4944 - val_loss: 5.0219\n",
            "Epoch 360/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5215 - val_loss: 3.8542\n",
            "Epoch 361/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4758 - val_loss: 5.3845\n",
            "Epoch 362/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4687 - val_loss: 4.1770\n",
            "Epoch 363/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5092 - val_loss: 3.5002\n",
            "Epoch 364/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5809 - val_loss: 3.8500\n",
            "Epoch 365/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4878 - val_loss: 5.2206\n",
            "Epoch 366/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5150 - val_loss: 3.0035\n",
            "Epoch 367/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4372 - val_loss: 4.6419\n",
            "Epoch 368/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4609 - val_loss: 3.9679\n",
            "Epoch 369/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5200 - val_loss: 5.0563\n",
            "Epoch 370/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4891 - val_loss: 5.2840\n",
            "Epoch 371/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4355 - val_loss: 10.2169\n",
            "Epoch 372/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5435 - val_loss: 4.0117\n",
            "Epoch 373/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4732 - val_loss: 5.0945\n",
            "Epoch 374/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5270 - val_loss: 4.0056\n",
            "Epoch 375/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4768 - val_loss: 4.5212\n",
            "Epoch 376/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4307 - val_loss: 3.5377\n",
            "Epoch 377/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4271 - val_loss: 3.0521\n",
            "Epoch 378/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5102 - val_loss: 4.4652\n",
            "Epoch 379/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4459 - val_loss: 4.5119\n",
            "Epoch 380/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4296 - val_loss: 3.1303\n",
            "Epoch 381/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4306 - val_loss: 5.2651\n",
            "Epoch 382/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4036 - val_loss: 3.2126\n",
            "Epoch 383/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3946 - val_loss: 2.7400\n",
            "Epoch 384/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4647 - val_loss: 3.7340\n",
            "Epoch 385/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4447 - val_loss: 3.4036\n",
            "Epoch 386/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4177 - val_loss: 4.1537\n",
            "Epoch 387/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 5.6115\n",
            "Epoch 388/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6931 - val_loss: 2.9116\n",
            "Epoch 389/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5183 - val_loss: 3.7292\n",
            "Epoch 390/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5165 - val_loss: 3.3690\n",
            "Epoch 391/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4147 - val_loss: 3.3374\n",
            "Epoch 392/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4114 - val_loss: 4.5401\n",
            "Epoch 393/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3839 - val_loss: 4.0168\n",
            "Epoch 394/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3584 - val_loss: 6.9505\n",
            "Epoch 395/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4519 - val_loss: 3.4674\n",
            "Epoch 396/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4620 - val_loss: 5.6547\n",
            "Epoch 397/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3841 - val_loss: 4.6057\n",
            "Epoch 398/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4449 - val_loss: 3.9564\n",
            "Epoch 399/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3903 - val_loss: 5.8229\n",
            "Epoch 400/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4185 - val_loss: 5.7177\n",
            "Epoch 401/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4266 - val_loss: 3.4372\n",
            "Epoch 402/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4201 - val_loss: 3.4418\n",
            "Epoch 403/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4007 - val_loss: 4.8952\n",
            "Epoch 404/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3924 - val_loss: 2.8731\n",
            "Epoch 405/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4238 - val_loss: 3.8947\n",
            "Epoch 406/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4388 - val_loss: 3.8005\n",
            "Epoch 407/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4382 - val_loss: 3.2850\n",
            "Epoch 408/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4480 - val_loss: 4.1476\n",
            "Epoch 409/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4074 - val_loss: 5.8460\n",
            "Epoch 410/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4524 - val_loss: 3.5980\n",
            "Epoch 411/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4005 - val_loss: 4.2748\n",
            "Epoch 412/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3865 - val_loss: 4.0949\n",
            "Epoch 413/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4220 - val_loss: 4.4899\n",
            "Epoch 414/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3709 - val_loss: 5.3241\n",
            "Epoch 415/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3507 - val_loss: 4.1608\n",
            "Epoch 416/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3996 - val_loss: 6.1170\n",
            "Epoch 417/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4282 - val_loss: 3.2085\n",
            "Epoch 418/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7269 - val_loss: 2.0973\n",
            "Epoch 419/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5067 - val_loss: 3.5616\n",
            "Epoch 420/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3940 - val_loss: 3.1661\n",
            "Epoch 421/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4118 - val_loss: 4.1672\n",
            "Epoch 422/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3603 - val_loss: 3.3594\n",
            "Epoch 423/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3875 - val_loss: 5.2689\n",
            "Epoch 424/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3333 - val_loss: 4.0029\n",
            "Epoch 425/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3922 - val_loss: 3.0146\n",
            "Epoch 426/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3426 - val_loss: 3.8356\n",
            "Epoch 427/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3528 - val_loss: 3.8647\n",
            "Epoch 428/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3293 - val_loss: 6.4227\n",
            "Epoch 429/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3753 - val_loss: 3.6290\n",
            "Epoch 430/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3807 - val_loss: 5.3599\n",
            "Epoch 431/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3281 - val_loss: 5.9259\n",
            "Epoch 432/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3241 - val_loss: 4.1142\n",
            "Epoch 433/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3803 - val_loss: 3.9496\n",
            "Epoch 434/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3310 - val_loss: 5.7056\n",
            "Epoch 435/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3088 - val_loss: 4.4605\n",
            "Epoch 436/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3343 - val_loss: 6.0944\n",
            "Epoch 437/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3032 - val_loss: 5.3807\n",
            "Epoch 438/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3877 - val_loss: 4.5339\n",
            "Epoch 439/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3741 - val_loss: 5.2508\n",
            "Epoch 440/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3707 - val_loss: 4.6771\n",
            "Epoch 441/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3395 - val_loss: 5.5613\n",
            "Epoch 442/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3736 - val_loss: 7.9553\n",
            "Epoch 443/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3757 - val_loss: 5.0480\n",
            "Epoch 444/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4095 - val_loss: 6.3856\n",
            "Epoch 445/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3352 - val_loss: 5.6624\n",
            "Epoch 446/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3204 - val_loss: 4.5061\n",
            "Epoch 447/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3524 - val_loss: 4.6272\n",
            "Epoch 448/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3876 - val_loss: 4.2950\n",
            "Epoch 449/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3219 - val_loss: 4.9829\n",
            "Epoch 450/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3529 - val_loss: 3.5582\n",
            "Epoch 451/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2921 - val_loss: 4.4920\n",
            "Epoch 452/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3148 - val_loss: 4.4370\n",
            "Epoch 453/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3420 - val_loss: 3.1726\n",
            "Epoch 454/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3455 - val_loss: 4.3243\n",
            "Epoch 455/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3121 - val_loss: 8.0166\n",
            "Epoch 456/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3212 - val_loss: 6.5993\n",
            "Epoch 457/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3256 - val_loss: 6.7126\n",
            "Epoch 458/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3168 - val_loss: 4.4037\n",
            "Epoch 459/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2722 - val_loss: 4.1639\n",
            "Epoch 460/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3032 - val_loss: 6.9923\n",
            "Epoch 461/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2799 - val_loss: 5.0251\n",
            "Epoch 462/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4106 - val_loss: 5.3876\n",
            "Epoch 463/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3438 - val_loss: 3.3179\n",
            "Epoch 464/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2760 - val_loss: 6.2594\n",
            "Epoch 465/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3373 - val_loss: 3.3504\n",
            "Epoch 466/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3015 - val_loss: 5.3283\n",
            "Epoch 467/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3395 - val_loss: 3.6863\n",
            "Epoch 468/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3145 - val_loss: 4.9229\n",
            "Epoch 469/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2671 - val_loss: 4.7130\n",
            "Epoch 470/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3282 - val_loss: 2.8589\n",
            "Epoch 471/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2851 - val_loss: 3.9821\n",
            "Epoch 472/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3363 - val_loss: 4.3003\n",
            "Epoch 473/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5660 - val_loss: 1.3938\n",
            "Epoch 474/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4069 - val_loss: 7.6286\n",
            "Epoch 475/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3356 - val_loss: 6.1580\n",
            "Epoch 476/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3230 - val_loss: 4.5486\n",
            "Epoch 477/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3747 - val_loss: 8.7241\n",
            "Epoch 478/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2853 - val_loss: 5.5153\n",
            "Epoch 479/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3298 - val_loss: 5.8904\n",
            "Epoch 480/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3036 - val_loss: 4.6780\n",
            "Epoch 481/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3719 - val_loss: 7.1069\n",
            "Epoch 482/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3094 - val_loss: 4.2693\n",
            "Epoch 483/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2917 - val_loss: 5.9714\n",
            "Epoch 484/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2759 - val_loss: 5.8977\n",
            "Epoch 485/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3138 - val_loss: 5.2677\n",
            "Epoch 486/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3187 - val_loss: 4.0296\n",
            "Epoch 487/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2588 - val_loss: 9.2324\n",
            "Epoch 488/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3281 - val_loss: 5.3454\n",
            "Epoch 489/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2927 - val_loss: 6.5357\n",
            "Epoch 490/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2746 - val_loss: 8.1892\n",
            "Epoch 491/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2729 - val_loss: 5.8564\n",
            "Epoch 492/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2983 - val_loss: 7.1794\n",
            "Epoch 493/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3067 - val_loss: 5.6563\n",
            "Epoch 494/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3207 - val_loss: 6.7191\n",
            "Epoch 495/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2726 - val_loss: 5.0273\n",
            "Epoch 496/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2674 - val_loss: 4.2872\n",
            "Epoch 497/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2975 - val_loss: 3.3085\n",
            "Epoch 498/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2194 - val_loss: 6.8751\n",
            "Epoch 499/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3025 - val_loss: 1.5493\n",
            "Epoch 500/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2744 - val_loss: 4.7674\n",
            "Epoch 501/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2786 - val_loss: 5.0800\n",
            "Epoch 502/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3010 - val_loss: 5.9309\n",
            "Epoch 503/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2793 - val_loss: 6.5568\n",
            "Epoch 504/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3005 - val_loss: 3.6547\n",
            "Epoch 505/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2850 - val_loss: 4.5499\n",
            "Epoch 506/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4382 - val_loss: 7.8368\n",
            "Epoch 507/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2995 - val_loss: 10.3834\n",
            "Epoch 508/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3235 - val_loss: 3.0493\n",
            "Epoch 509/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3207 - val_loss: 6.4480\n",
            "Epoch 510/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2787 - val_loss: 7.6225\n",
            "Epoch 511/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3377 - val_loss: 2.8024\n",
            "Epoch 512/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3128 - val_loss: 3.2846\n",
            "Epoch 513/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2172 - val_loss: 4.6099\n",
            "Epoch 514/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2718 - val_loss: 6.7360\n",
            "Epoch 515/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2866 - val_loss: 5.3277\n",
            "Epoch 516/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4369 - val_loss: 6.4225\n",
            "Epoch 517/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2865 - val_loss: 7.2325\n",
            "Epoch 518/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2864 - val_loss: 3.7063\n",
            "Epoch 519/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3989 - val_loss: 3.9859\n",
            "Epoch 520/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3121 - val_loss: 4.5070\n",
            "Epoch 521/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3225 - val_loss: 4.7694\n",
            "Epoch 522/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3340 - val_loss: 7.2288\n",
            "Epoch 523/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3360 - val_loss: 4.7165\n",
            "Epoch 524/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2365 - val_loss: 3.0959\n",
            "Epoch 525/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2512 - val_loss: 4.8903\n",
            "Epoch 526/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2787 - val_loss: 4.2210\n",
            "Epoch 527/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3076 - val_loss: 3.6781\n",
            "Epoch 528/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2979 - val_loss: 6.0873\n",
            "Epoch 529/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2372 - val_loss: 4.6938\n",
            "Epoch 530/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2970 - val_loss: 5.1669\n",
            "Epoch 531/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2469 - val_loss: 5.2410\n",
            "Epoch 532/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2164 - val_loss: 6.9434\n",
            "Epoch 533/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2223 - val_loss: 7.7991\n",
            "Epoch 534/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2701 - val_loss: 3.7583\n",
            "Epoch 535/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2556 - val_loss: 4.4195\n",
            "Epoch 536/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2712 - val_loss: 4.6061\n",
            "Epoch 537/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3073 - val_loss: 4.2399\n",
            "Epoch 538/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2448 - val_loss: 1.9180\n",
            "Epoch 539/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2684 - val_loss: 4.9829\n",
            "Epoch 540/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2602 - val_loss: 4.1997\n",
            "Epoch 541/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2414 - val_loss: 7.4029\n",
            "Epoch 542/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2818 - val_loss: 5.2996\n",
            "Epoch 543/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2624 - val_loss: 5.4991\n",
            "Epoch 544/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2353 - val_loss: 7.1970\n",
            "Epoch 545/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2365 - val_loss: 4.9961\n",
            "Epoch 546/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3428 - val_loss: 11.6406\n",
            "Epoch 547/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2541 - val_loss: 4.9242\n",
            "Epoch 548/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2497 - val_loss: 7.7589\n",
            "Epoch 549/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2933 - val_loss: 4.8939\n",
            "Epoch 550/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2586 - val_loss: 8.2123\n",
            "Epoch 551/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2298 - val_loss: 7.2868\n",
            "Epoch 552/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2389 - val_loss: 4.8410\n",
            "Epoch 553/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2441 - val_loss: 4.3455\n",
            "Epoch 554/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3172 - val_loss: 4.0886\n",
            "Epoch 555/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2284 - val_loss: 5.2749\n",
            "Epoch 556/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3812 - val_loss: 4.8324\n",
            "Epoch 557/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2303 - val_loss: 4.8670\n",
            "Epoch 558/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2113 - val_loss: 5.7642\n",
            "Epoch 559/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3185 - val_loss: 8.5641\n",
            "Epoch 560/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2888 - val_loss: 3.2867\n",
            "Epoch 561/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2588 - val_loss: 3.5286\n",
            "Epoch 562/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2415 - val_loss: 3.6036\n",
            "Epoch 563/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2523 - val_loss: 3.3690\n",
            "Epoch 564/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2791 - val_loss: 2.9995\n",
            "Epoch 565/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2776 - val_loss: 3.8383\n",
            "Epoch 566/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2602 - val_loss: 2.2428\n",
            "Epoch 567/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2916 - val_loss: 4.5415\n",
            "Epoch 568/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2837 - val_loss: 4.2680\n",
            "Epoch 569/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2583 - val_loss: 4.2289\n",
            "Epoch 570/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2666 - val_loss: 4.9091\n",
            "Epoch 571/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2394 - val_loss: 4.4563\n",
            "Epoch 572/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2321 - val_loss: 3.3676\n",
            "Epoch 573/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1968 - val_loss: 6.7557\n",
            "Epoch 574/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2293 - val_loss: 7.1713\n",
            "Epoch 575/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2650 - val_loss: 6.1458\n",
            "Epoch 576/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2278 - val_loss: 4.7438\n",
            "Epoch 577/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2161 - val_loss: 4.0347\n",
            "Epoch 578/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2235 - val_loss: 3.8121\n",
            "Epoch 579/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2222 - val_loss: 6.6441\n",
            "Epoch 580/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2494 - val_loss: 4.7373\n",
            "Epoch 581/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3032 - val_loss: 2.9199\n",
            "Epoch 582/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3186 - val_loss: 7.8077\n",
            "Epoch 583/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3457 - val_loss: 5.2846\n",
            "Epoch 584/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3761 - val_loss: 4.8383\n",
            "Epoch 585/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2211 - val_loss: 3.1697\n",
            "Epoch 586/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2293 - val_loss: 6.3541\n",
            "Epoch 587/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2209 - val_loss: 4.8234\n",
            "Epoch 588/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1931 - val_loss: 4.4350\n",
            "Epoch 589/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2030 - val_loss: 3.6070\n",
            "Epoch 590/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2502 - val_loss: 3.7287\n",
            "Epoch 591/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2385 - val_loss: 5.7613\n",
            "Epoch 592/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2697 - val_loss: 4.4218\n",
            "Epoch 593/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2486 - val_loss: 4.0204\n",
            "Epoch 594/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2433 - val_loss: 5.3466\n",
            "Epoch 595/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2222 - val_loss: 5.9993\n",
            "Epoch 596/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1848 - val_loss: 3.8756\n",
            "Epoch 597/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2295 - val_loss: 3.1576\n",
            "Epoch 598/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1854 - val_loss: 2.0982\n",
            "Epoch 599/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1823 - val_loss: 3.5314\n",
            "Epoch 600/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2294 - val_loss: 5.7530\n",
            "Epoch 601/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2347 - val_loss: 7.4863\n",
            "Epoch 602/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2044 - val_loss: 3.6542\n",
            "Epoch 603/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2166 - val_loss: 5.0112\n",
            "Epoch 604/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1806 - val_loss: 6.5987\n",
            "Epoch 605/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2284 - val_loss: 4.2473\n",
            "Epoch 606/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2550 - val_loss: 4.7481\n",
            "Epoch 607/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2639 - val_loss: 3.9739\n",
            "Epoch 608/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2454 - val_loss: 4.5177\n",
            "Epoch 609/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4392 - val_loss: 12.9464\n",
            "Epoch 610/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4196 - val_loss: 6.1059\n",
            "Epoch 611/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2571 - val_loss: 5.4433\n",
            "Epoch 612/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2436 - val_loss: 2.4231\n",
            "Epoch 613/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2699 - val_loss: 4.5151\n",
            "Epoch 614/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2962 - val_loss: 1.6236\n",
            "Epoch 615/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2878 - val_loss: 2.5561\n",
            "Epoch 616/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2079 - val_loss: 1.9949\n",
            "Epoch 617/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2262 - val_loss: 3.8849\n",
            "Epoch 618/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2549 - val_loss: 2.1116\n",
            "Epoch 619/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2430 - val_loss: 3.0165\n",
            "Epoch 620/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1995 - val_loss: 3.9459\n",
            "Epoch 621/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2157 - val_loss: 5.1041\n",
            "Epoch 622/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2009 - val_loss: 3.3476\n",
            "Epoch 623/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1703 - val_loss: 6.6256\n",
            "Epoch 624/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2563 - val_loss: 3.7042\n",
            "Epoch 625/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2614 - val_loss: 5.3819\n",
            "Epoch 626/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2224 - val_loss: 3.5756\n",
            "Epoch 627/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1895 - val_loss: 5.0193\n",
            "Epoch 628/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2049 - val_loss: 4.0061\n",
            "Epoch 629/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3170 - val_loss: 7.8654\n",
            "Epoch 630/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2236 - val_loss: 6.9920\n",
            "Epoch 631/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1937 - val_loss: 5.7132\n",
            "Epoch 632/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1708 - val_loss: 3.7900\n",
            "Epoch 633/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2374 - val_loss: 4.5805\n",
            "Epoch 634/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2108 - val_loss: 5.8928\n",
            "Epoch 635/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2157 - val_loss: 5.6188\n",
            "Epoch 636/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2262 - val_loss: 5.0531\n",
            "Epoch 637/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1740 - val_loss: 5.5186\n",
            "Epoch 638/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2361 - val_loss: 2.9312\n",
            "Epoch 639/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2078 - val_loss: 5.1359\n",
            "Epoch 640/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2470 - val_loss: 3.1299\n",
            "Epoch 641/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1992 - val_loss: 5.2841\n",
            "Epoch 642/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2153 - val_loss: 3.3793\n",
            "Epoch 643/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2453 - val_loss: 4.5678\n",
            "Epoch 644/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3365 - val_loss: 5.2219\n",
            "Epoch 645/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2377 - val_loss: 2.8181\n",
            "Epoch 646/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3128 - val_loss: 4.9335\n",
            "Epoch 647/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2496 - val_loss: 5.4815\n",
            "Epoch 648/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2207 - val_loss: 4.4142\n",
            "Epoch 649/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2140 - val_loss: 3.5021\n",
            "Epoch 650/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1819 - val_loss: 4.3067\n",
            "Epoch 651/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1662 - val_loss: 5.8369\n",
            "Epoch 652/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1703 - val_loss: 4.6123\n",
            "Epoch 653/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1897 - val_loss: 4.2105\n",
            "Epoch 654/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1672 - val_loss: 5.5680\n",
            "Epoch 655/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1749 - val_loss: 4.5068\n",
            "Epoch 656/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2105 - val_loss: 3.5828\n",
            "Epoch 657/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3252 - val_loss: 12.1924\n",
            "Epoch 658/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2109 - val_loss: 11.6440\n",
            "Epoch 659/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2136 - val_loss: 5.9535\n",
            "Epoch 660/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2031 - val_loss: 4.0670\n",
            "Epoch 661/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2117 - val_loss: 5.9683\n",
            "Epoch 662/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2789 - val_loss: 3.6027\n",
            "Epoch 663/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2586 - val_loss: 3.2378\n",
            "Epoch 664/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2459 - val_loss: 3.8423\n",
            "Epoch 665/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2220 - val_loss: 7.9551\n",
            "Epoch 666/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2003 - val_loss: 2.4569\n",
            "Epoch 667/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2143 - val_loss: 5.0397\n",
            "Epoch 668/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2083 - val_loss: 5.0552\n",
            "Epoch 669/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2455 - val_loss: 4.0039\n",
            "Epoch 670/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2502 - val_loss: 3.7528\n",
            "Epoch 671/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2883 - val_loss: 5.2854\n",
            "Epoch 672/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3139 - val_loss: 1.6319\n",
            "Epoch 673/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6227 - val_loss: 0.6142\n",
            "Epoch 674/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4006 - val_loss: 1.5893\n",
            "Epoch 675/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3275 - val_loss: 1.9366\n",
            "Epoch 676/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2957 - val_loss: 2.4527\n",
            "Epoch 677/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2819 - val_loss: 3.0203\n",
            "Epoch 678/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3530 - val_loss: 1.3125\n",
            "Epoch 679/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2757 - val_loss: 2.9609\n",
            "Epoch 680/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2446 - val_loss: 2.8847\n",
            "Epoch 681/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2655 - val_loss: 2.6569\n",
            "Epoch 682/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2286 - val_loss: 3.1761\n",
            "Epoch 683/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2669 - val_loss: 3.1033\n",
            "Epoch 684/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2488 - val_loss: 2.3129\n",
            "Epoch 685/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3111 - val_loss: 2.1892\n",
            "Epoch 686/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2508 - val_loss: 2.5844\n",
            "Epoch 687/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2206 - val_loss: 3.2813\n",
            "Epoch 688/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1878 - val_loss: 3.7745\n",
            "Epoch 689/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2293 - val_loss: 5.7746\n",
            "Epoch 690/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2614 - val_loss: 5.0236\n",
            "Epoch 691/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2084 - val_loss: 5.0488\n",
            "Epoch 692/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2782 - val_loss: 3.5213\n",
            "Epoch 693/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2213 - val_loss: 4.3532\n",
            "Epoch 694/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2235 - val_loss: 3.2268\n",
            "Epoch 695/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2807 - val_loss: 1.2450\n",
            "Epoch 696/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2786 - val_loss: 3.9091\n",
            "Epoch 697/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2158 - val_loss: 5.5355\n",
            "Epoch 698/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1471 - val_loss: 4.2913\n",
            "Epoch 699/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2124 - val_loss: 6.2936\n",
            "Epoch 700/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1984 - val_loss: 3.8767\n",
            "Epoch 701/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6625 - val_loss: 3.9074\n",
            "Epoch 702/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4357 - val_loss: 12.3163\n",
            "Epoch 703/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3702 - val_loss: 19.1105\n",
            "Epoch 704/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2886 - val_loss: 13.7202\n",
            "Epoch 705/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2504 - val_loss: 8.7006\n",
            "Epoch 706/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2684 - val_loss: 6.7379\n",
            "Epoch 707/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1905 - val_loss: 3.6287\n",
            "Epoch 708/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1937 - val_loss: 2.7058\n",
            "Epoch 709/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1944 - val_loss: 2.9925\n",
            "Epoch 710/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2077 - val_loss: 3.5321\n",
            "Epoch 711/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2837 - val_loss: 2.3373\n",
            "Epoch 712/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2313 - val_loss: 2.6664\n",
            "Epoch 713/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2102 - val_loss: 2.7832\n",
            "Epoch 714/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2073 - val_loss: 3.4243\n",
            "Epoch 715/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1768 - val_loss: 3.1099\n",
            "Epoch 716/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1632 - val_loss: 4.2662\n",
            "Epoch 717/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1755 - val_loss: 2.3182\n",
            "Epoch 718/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2342 - val_loss: 2.1202\n",
            "Epoch 719/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2376 - val_loss: 2.2942\n",
            "Epoch 720/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2792 - val_loss: 3.2639\n",
            "Epoch 721/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1996 - val_loss: 3.0669\n",
            "Epoch 722/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2073 - val_loss: 2.6591\n",
            "Epoch 723/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1997 - val_loss: 4.0704\n",
            "Epoch 724/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2084 - val_loss: 4.1958\n",
            "Epoch 725/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1822 - val_loss: 5.0532\n",
            "Epoch 726/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2037 - val_loss: 3.1571\n",
            "Epoch 727/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3033 - val_loss: 2.3469\n",
            "Epoch 728/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2791 - val_loss: 1.5636\n",
            "Epoch 729/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2312 - val_loss: 4.0257\n",
            "Epoch 730/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2009 - val_loss: 3.6650\n",
            "Epoch 731/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1806 - val_loss: 5.7725\n",
            "Epoch 732/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2270 - val_loss: 2.8986\n",
            "Epoch 733/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2114 - val_loss: 2.1262\n",
            "Epoch 734/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1846 - val_loss: 3.9615\n",
            "Epoch 735/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1831 - val_loss: 4.5578\n",
            "Epoch 736/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1675 - val_loss: 4.1271\n",
            "Epoch 737/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1828 - val_loss: 7.3809\n",
            "Epoch 738/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1828 - val_loss: 6.0409\n",
            "Epoch 739/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2524 - val_loss: 3.6891\n",
            "Epoch 740/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2347 - val_loss: 5.5965\n",
            "Epoch 741/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2343 - val_loss: 3.4132\n",
            "Epoch 742/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1990 - val_loss: 1.6675\n",
            "Epoch 743/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1746 - val_loss: 3.0569\n",
            "Epoch 744/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1905 - val_loss: 4.0364\n",
            "Epoch 745/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1797 - val_loss: 3.2067\n",
            "Epoch 746/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1795 - val_loss: 6.0491\n",
            "Epoch 747/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1996 - val_loss: 5.2223\n",
            "Epoch 748/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2435 - val_loss: 9.1579\n",
            "Epoch 749/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1929 - val_loss: 6.2404\n",
            "Epoch 750/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1923 - val_loss: 5.9346\n",
            "Epoch 751/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1989 - val_loss: 3.2091\n",
            "Epoch 752/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2222 - val_loss: 2.7730\n",
            "Epoch 753/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2029 - val_loss: 5.1390\n",
            "Epoch 754/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2313 - val_loss: 4.4495\n",
            "Epoch 755/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2360 - val_loss: 2.9975\n",
            "Epoch 756/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2286 - val_loss: 8.4302\n",
            "Epoch 757/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1620 - val_loss: 3.3541\n",
            "Epoch 758/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1818 - val_loss: 5.0979\n",
            "Epoch 759/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2253 - val_loss: 3.1327\n",
            "Epoch 760/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1565 - val_loss: 5.8540\n",
            "Epoch 761/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1674 - val_loss: 5.7138\n",
            "Epoch 762/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1619 - val_loss: 4.3878\n",
            "Epoch 763/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2329 - val_loss: 6.3587\n",
            "Epoch 764/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3062 - val_loss: 5.7471\n",
            "Epoch 765/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2077 - val_loss: 3.9976\n",
            "Epoch 766/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2262 - val_loss: 5.0381\n",
            "Epoch 767/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2686 - val_loss: 5.3069\n",
            "Epoch 768/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1572 - val_loss: 7.4797\n",
            "Epoch 769/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1396 - val_loss: 3.9189\n",
            "Epoch 770/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1936 - val_loss: 5.1304\n",
            "Epoch 771/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2305 - val_loss: 4.5231\n",
            "Epoch 772/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2268 - val_loss: 5.0566\n",
            "Epoch 773/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1702 - val_loss: 5.5742\n",
            "Epoch 774/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1927 - val_loss: 6.7918\n",
            "Epoch 775/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1778 - val_loss: 4.1208\n",
            "Epoch 776/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2157 - val_loss: 5.0168\n",
            "Epoch 777/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2233 - val_loss: 5.2914\n",
            "Epoch 778/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2685 - val_loss: 5.0192\n",
            "Epoch 779/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2800 - val_loss: 1.5951\n",
            "Epoch 780/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2971 - val_loss: 3.7626\n",
            "Epoch 781/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2417 - val_loss: 2.5594\n",
            "Epoch 782/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1833 - val_loss: 3.7395\n",
            "Epoch 783/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2801 - val_loss: 7.1685\n",
            "Epoch 784/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2095 - val_loss: 4.6888\n",
            "Epoch 785/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2111 - val_loss: 3.9428\n",
            "Epoch 786/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1941 - val_loss: 4.0964\n",
            "Epoch 787/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1565 - val_loss: 6.1369\n",
            "Epoch 788/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1853 - val_loss: 6.8531\n",
            "Epoch 789/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1798 - val_loss: 6.2121\n",
            "Epoch 790/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1854 - val_loss: 4.9913\n",
            "Epoch 791/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2948 - val_loss: 2.6912\n",
            "Epoch 792/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3016 - val_loss: 5.3084\n",
            "Epoch 793/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2436 - val_loss: 3.7286\n",
            "Epoch 794/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3676 - val_loss: 15.4353\n",
            "Epoch 795/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2678 - val_loss: 6.2822\n",
            "Epoch 796/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2006 - val_loss: 8.2688\n",
            "Epoch 797/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1651 - val_loss: 4.9906\n",
            "Epoch 798/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1632 - val_loss: 6.9961\n",
            "Epoch 799/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2128 - val_loss: 8.6138\n",
            "Epoch 800/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1709 - val_loss: 3.6339\n",
            "Epoch 801/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2473 - val_loss: 2.2843\n",
            "Epoch 802/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3283 - val_loss: 5.5814\n",
            "Epoch 803/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2325 - val_loss: 7.8192\n",
            "Epoch 804/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1821 - val_loss: 4.5779\n",
            "Epoch 805/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2627 - val_loss: 3.8445\n",
            "Epoch 806/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2665 - val_loss: 9.5167\n",
            "Epoch 807/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1888 - val_loss: 6.6689\n",
            "Epoch 808/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1727 - val_loss: 5.6961\n",
            "Epoch 809/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2042 - val_loss: 3.3675\n",
            "Epoch 810/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1751 - val_loss: 3.7194\n",
            "Epoch 811/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2048 - val_loss: 1.5166\n",
            "Epoch 812/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1704 - val_loss: 2.5723\n",
            "Epoch 813/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1821 - val_loss: 2.4262\n",
            "Epoch 814/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4020 - val_loss: 6.6951\n",
            "Epoch 815/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2399 - val_loss: 10.3508\n",
            "Epoch 816/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2076 - val_loss: 4.4224\n",
            "Epoch 817/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2042 - val_loss: 4.0437\n",
            "Epoch 818/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1678 - val_loss: 3.1674\n",
            "Epoch 819/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1843 - val_loss: 1.2250\n",
            "Epoch 820/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2127 - val_loss: 4.6906\n",
            "Epoch 821/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1919 - val_loss: 5.3642\n",
            "Epoch 822/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1896 - val_loss: 2.9570\n",
            "Epoch 823/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1970 - val_loss: 3.2498\n",
            "Epoch 824/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1855 - val_loss: 3.9929\n",
            "Epoch 825/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2162 - val_loss: 5.2934\n",
            "Epoch 826/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1897 - val_loss: 3.3280\n",
            "Epoch 827/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2195 - val_loss: 1.8042\n",
            "Epoch 828/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2935 - val_loss: 3.9117\n",
            "Epoch 829/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1646 - val_loss: 3.9914\n",
            "Epoch 830/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2527 - val_loss: 2.6666\n",
            "Epoch 831/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1963 - val_loss: 5.8409\n",
            "Epoch 832/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1922 - val_loss: 5.2738\n",
            "Epoch 833/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1876 - val_loss: 5.2002\n",
            "Epoch 834/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1834 - val_loss: 3.8564\n",
            "Epoch 835/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2271 - val_loss: 6.0351\n",
            "Epoch 836/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1764 - val_loss: 3.5947\n",
            "Epoch 837/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1871 - val_loss: 2.7223\n",
            "Epoch 838/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2303 - val_loss: 3.5802\n",
            "Epoch 839/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2212 - val_loss: 8.2655\n",
            "Epoch 840/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2020 - val_loss: 3.7748\n",
            "Epoch 841/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2510 - val_loss: 2.3468\n",
            "Epoch 842/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1659 - val_loss: 4.6362\n",
            "Epoch 843/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1718 - val_loss: 4.3487\n",
            "Epoch 844/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2697 - val_loss: 2.8564\n",
            "Epoch 845/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2084 - val_loss: 2.9393\n",
            "Epoch 846/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2821 - val_loss: 3.6849\n",
            "Epoch 847/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2309 - val_loss: 3.6104\n",
            "Epoch 848/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2277 - val_loss: 5.6413\n",
            "Epoch 849/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1920 - val_loss: 3.8280\n",
            "Epoch 850/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1770 - val_loss: 2.5758\n",
            "Epoch 851/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2369 - val_loss: 1.6885\n",
            "Epoch 852/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2204 - val_loss: 4.7068\n",
            "Epoch 853/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2181 - val_loss: 1.2456\n",
            "Epoch 854/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2551 - val_loss: 2.2433\n",
            "Epoch 855/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2610 - val_loss: 1.4687\n",
            "Epoch 856/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4180 - val_loss: 2.5630\n",
            "Epoch 857/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2666 - val_loss: 2.7646\n",
            "Epoch 858/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2237 - val_loss: 1.6131\n",
            "Epoch 859/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2343 - val_loss: 1.6071\n",
            "Epoch 860/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2289 - val_loss: 2.9557\n",
            "Epoch 861/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1832 - val_loss: 3.9083\n",
            "Epoch 862/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2175 - val_loss: 3.9911\n",
            "Epoch 863/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1836 - val_loss: 2.2122\n",
            "Epoch 864/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1569 - val_loss: 3.4885\n",
            "Epoch 865/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1996 - val_loss: 1.6351\n",
            "Epoch 866/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2028 - val_loss: 2.6591\n",
            "Epoch 867/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2915 - val_loss: 3.3799\n",
            "Epoch 868/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2439 - val_loss: 2.7320\n",
            "Epoch 869/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2280 - val_loss: 5.0152\n",
            "Epoch 870/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1712 - val_loss: 3.0987\n",
            "Epoch 871/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2130 - val_loss: 3.7012\n",
            "Epoch 872/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2182 - val_loss: 4.3872\n",
            "Epoch 873/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2170 - val_loss: 2.8170\n",
            "Epoch 874/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2504 - val_loss: 2.5526\n",
            "Epoch 875/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2754 - val_loss: 2.3072\n",
            "Epoch 876/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2149 - val_loss: 2.8501\n",
            "Epoch 877/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1989 - val_loss: 3.3503\n",
            "Epoch 878/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1574 - val_loss: 3.1756\n",
            "Epoch 879/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1666 - val_loss: 4.3082\n",
            "Epoch 880/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2145 - val_loss: 3.4768\n",
            "Epoch 881/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1950 - val_loss: 2.4412\n",
            "Epoch 882/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2282 - val_loss: 3.6067\n",
            "Epoch 883/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1689 - val_loss: 2.2565\n",
            "Epoch 884/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1752 - val_loss: 2.9747\n",
            "Epoch 885/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1717 - val_loss: 1.4431\n",
            "Epoch 886/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2077 - val_loss: 1.8927\n",
            "Epoch 887/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1683 - val_loss: 2.9268\n",
            "Epoch 888/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2212 - val_loss: 1.6567\n",
            "Epoch 889/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1918 - val_loss: 1.8032\n",
            "Epoch 890/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1508 - val_loss: 3.6889\n",
            "Epoch 891/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1806 - val_loss: 3.0687\n",
            "Epoch 892/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1811 - val_loss: 1.2626\n",
            "Epoch 893/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1843 - val_loss: 3.5288\n",
            "Epoch 894/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1512 - val_loss: 5.9119\n",
            "Epoch 895/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2081 - val_loss: 2.8393\n",
            "Epoch 896/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2411 - val_loss: 1.6278\n",
            "Epoch 897/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1693 - val_loss: 2.7170\n",
            "Epoch 898/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1450 - val_loss: 4.7142\n",
            "Epoch 899/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1461 - val_loss: 4.1495\n",
            "Epoch 900/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1873 - val_loss: 2.4785\n",
            "Epoch 901/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1884 - val_loss: 2.9474\n",
            "Epoch 902/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1599 - val_loss: 2.1842\n",
            "Epoch 903/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1561 - val_loss: 3.6694\n",
            "Epoch 904/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3184 - val_loss: 3.4221\n",
            "Epoch 905/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2451 - val_loss: 6.1862\n",
            "Epoch 906/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1596 - val_loss: 1.5278\n",
            "Epoch 907/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2775 - val_loss: 1.2022\n",
            "Epoch 908/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1909 - val_loss: 1.8770\n",
            "Epoch 909/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1817 - val_loss: 2.2869\n",
            "Epoch 910/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1791 - val_loss: 2.8535\n",
            "Epoch 911/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1977 - val_loss: 2.1524\n",
            "Epoch 912/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2062 - val_loss: 3.4680\n",
            "Epoch 913/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1651 - val_loss: 2.8336\n",
            "Epoch 914/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1668 - val_loss: 3.7774\n",
            "Epoch 915/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2024 - val_loss: 3.2276\n",
            "Epoch 916/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1590 - val_loss: 3.1611\n",
            "Epoch 917/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1655 - val_loss: 3.4243\n",
            "Epoch 918/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1991 - val_loss: 1.9099\n",
            "Epoch 919/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2084 - val_loss: 3.3494\n",
            "Epoch 920/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1418 - val_loss: 3.8985\n",
            "Epoch 921/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1615 - val_loss: 4.2931\n",
            "Epoch 922/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1980 - val_loss: 4.8188\n",
            "Epoch 923/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1885 - val_loss: 5.1370\n",
            "Epoch 924/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1871 - val_loss: 2.5800\n",
            "Epoch 925/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1901 - val_loss: 1.9840\n",
            "Epoch 926/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1789 - val_loss: 2.6088\n",
            "Epoch 927/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1305 - val_loss: 3.0662\n",
            "Epoch 928/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1626 - val_loss: 3.8273\n",
            "Epoch 929/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2033 - val_loss: 3.9338\n",
            "Epoch 930/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2323 - val_loss: 2.7423\n",
            "Epoch 931/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2248 - val_loss: 3.1310\n",
            "Epoch 932/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2031 - val_loss: 3.3627\n",
            "Epoch 933/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1826 - val_loss: 3.8169\n",
            "Epoch 934/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1815 - val_loss: 4.9208\n",
            "Epoch 935/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1690 - val_loss: 6.3628\n",
            "Epoch 936/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1584 - val_loss: 3.4477\n",
            "Epoch 937/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1587 - val_loss: 2.8229\n",
            "Epoch 938/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2115 - val_loss: 3.5002\n",
            "Epoch 939/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1170 - val_loss: 6.5847\n",
            "Epoch 940/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1761 - val_loss: 3.4623\n",
            "Epoch 941/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1902 - val_loss: 3.3780\n",
            "Epoch 942/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1921 - val_loss: 5.6344\n",
            "Epoch 943/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1937 - val_loss: 3.2412\n",
            "Epoch 944/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1913 - val_loss: 1.3698\n",
            "Epoch 945/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1800 - val_loss: 2.3523\n",
            "Epoch 946/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2071 - val_loss: 1.5376\n",
            "Epoch 947/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1927 - val_loss: 3.0513\n",
            "Epoch 948/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1608 - val_loss: 4.9600\n",
            "Epoch 949/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1993 - val_loss: 3.7394\n",
            "Epoch 950/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1457 - val_loss: 4.2462\n",
            "Epoch 951/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2000 - val_loss: 3.9273\n",
            "Epoch 952/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2352 - val_loss: 3.8178\n",
            "Epoch 953/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2078 - val_loss: 7.8770\n",
            "Epoch 954/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1581 - val_loss: 4.4925\n",
            "Epoch 955/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1762 - val_loss: 3.2823\n",
            "Epoch 956/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1683 - val_loss: 5.0288\n",
            "Epoch 957/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1853 - val_loss: 3.4661\n",
            "Epoch 958/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2701 - val_loss: 3.0790\n",
            "Epoch 959/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2161 - val_loss: 2.0151\n",
            "Epoch 960/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1839 - val_loss: 3.4303\n",
            "Epoch 961/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2257 - val_loss: 3.9218\n",
            "Epoch 962/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2177 - val_loss: 4.3763\n",
            "Epoch 963/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2190 - val_loss: 5.0829\n",
            "Epoch 964/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2586 - val_loss: 1.4217\n",
            "Epoch 965/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1982 - val_loss: 3.2363\n",
            "Epoch 966/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1704 - val_loss: 3.6031\n",
            "Epoch 967/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1759 - val_loss: 4.4685\n",
            "Epoch 968/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1762 - val_loss: 2.6518\n",
            "Epoch 969/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1771 - val_loss: 1.6858\n",
            "Epoch 970/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1820 - val_loss: 5.1571\n",
            "Epoch 971/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1953 - val_loss: 1.6308\n",
            "Epoch 972/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1831 - val_loss: 2.0137\n",
            "Epoch 973/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1730 - val_loss: 3.2829\n",
            "Epoch 974/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2035 - val_loss: 1.6150\n",
            "Epoch 975/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2533 - val_loss: 1.7553\n",
            "Epoch 976/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1953 - val_loss: 2.2210\n",
            "Epoch 977/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1771 - val_loss: 1.6321\n",
            "Epoch 978/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1651 - val_loss: 2.7131\n",
            "Epoch 979/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1685 - val_loss: 4.2158\n",
            "Epoch 980/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2296 - val_loss: 2.2286\n",
            "Epoch 981/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1857 - val_loss: 1.9820\n",
            "Epoch 982/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1816 - val_loss: 3.5296\n",
            "Epoch 983/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1624 - val_loss: 2.7349\n",
            "Epoch 984/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1631 - val_loss: 2.9069\n",
            "Epoch 985/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1565 - val_loss: 3.0086\n",
            "Epoch 986/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2062 - val_loss: 1.7606\n",
            "Epoch 987/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2740 - val_loss: 2.9777\n",
            "Epoch 988/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3457 - val_loss: 3.7292\n",
            "Epoch 989/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3056 - val_loss: 3.8091\n",
            "Epoch 990/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3162 - val_loss: 2.3192\n",
            "Epoch 991/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2395 - val_loss: 2.9646\n",
            "Epoch 992/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2226 - val_loss: 2.3840\n",
            "Epoch 993/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1648 - val_loss: 5.4305\n",
            "Epoch 994/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1781 - val_loss: 4.5759\n",
            "Epoch 995/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2090 - val_loss: 2.4499\n",
            "Epoch 996/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2805 - val_loss: 4.5070\n",
            "Epoch 997/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2313 - val_loss: 3.1216\n",
            "Epoch 998/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2272 - val_loss: 6.4002\n",
            "Epoch 999/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1767 - val_loss: 5.0943\n",
            "Epoch 1000/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2954 - val_loss: 2.3577\n",
            "Epoch 1001/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2386 - val_loss: 3.7552\n",
            "Epoch 1002/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1766 - val_loss: 1.8571\n",
            "Epoch 1003/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1653 - val_loss: 3.1125\n",
            "Epoch 1004/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1796 - val_loss: 2.4205\n",
            "Epoch 1005/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1689 - val_loss: 2.2920\n",
            "Epoch 1006/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1948 - val_loss: 1.9779\n",
            "Epoch 1007/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1950 - val_loss: 3.3008\n",
            "Epoch 1008/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2308 - val_loss: 2.3136\n",
            "Epoch 1009/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2333 - val_loss: 1.8574\n",
            "Epoch 1010/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1530 - val_loss: 6.9805\n",
            "Epoch 1011/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1475 - val_loss: 4.1432\n",
            "Epoch 1012/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1637 - val_loss: 3.4813\n",
            "Epoch 1013/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1900 - val_loss: 3.7748\n",
            "Epoch 1014/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1813 - val_loss: 3.2131\n",
            "Epoch 1015/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1952 - val_loss: 5.4776\n",
            "Epoch 1016/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1639 - val_loss: 2.3988\n",
            "Epoch 1017/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1588 - val_loss: 5.9197\n",
            "Epoch 1018/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1832 - val_loss: 2.9661\n",
            "Epoch 1019/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2166 - val_loss: 1.9933\n",
            "Epoch 1020/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1561 - val_loss: 4.3458\n",
            "Epoch 1021/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1707 - val_loss: 3.0094\n",
            "Epoch 1022/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1646 - val_loss: 4.7171\n",
            "Epoch 1023/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1447 - val_loss: 2.4906\n",
            "Epoch 1024/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1517 - val_loss: 2.4028\n",
            "Epoch 1025/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1888 - val_loss: 3.6137\n",
            "Epoch 1026/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2258 - val_loss: 3.8417\n",
            "Epoch 1027/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2074 - val_loss: 3.7289\n",
            "Epoch 1028/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1870 - val_loss: 4.3454\n",
            "Epoch 1029/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1921 - val_loss: 2.1133\n",
            "Epoch 1030/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1883 - val_loss: 5.7682\n",
            "Epoch 1031/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2039 - val_loss: 2.6702\n",
            "Epoch 1032/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1935 - val_loss: 9.2792\n",
            "Epoch 1033/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2413 - val_loss: 2.0872\n",
            "Epoch 1034/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1838 - val_loss: 3.4610\n",
            "Epoch 1035/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2152 - val_loss: 3.6695\n",
            "Epoch 1036/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2693 - val_loss: 3.3916\n",
            "Epoch 1037/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1834 - val_loss: 3.0518\n",
            "Epoch 1038/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2347 - val_loss: 2.1453\n",
            "Epoch 1039/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2085 - val_loss: 3.2941\n",
            "Epoch 1040/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2021 - val_loss: 3.9797\n",
            "Epoch 1041/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1979 - val_loss: 1.6461\n",
            "Epoch 1042/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2296 - val_loss: 2.5345\n",
            "Epoch 1043/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1871 - val_loss: 1.8150\n",
            "Epoch 1044/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1484 - val_loss: 3.2045\n",
            "Epoch 1045/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1349 - val_loss: 2.3459\n",
            "Epoch 1046/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1580 - val_loss: 3.4349\n",
            "Epoch 1047/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1672 - val_loss: 2.1300\n",
            "Epoch 1048/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3231 - val_loss: 2.7415\n",
            "Epoch 1049/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2266 - val_loss: 5.2294\n",
            "Epoch 1050/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1743 - val_loss: 3.1276\n",
            "Epoch 1051/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1841 - val_loss: 2.8483\n",
            "Epoch 1052/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1488 - val_loss: 5.7942\n",
            "Epoch 1053/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2365 - val_loss: 4.3300\n",
            "Epoch 1054/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2071 - val_loss: 4.2660\n",
            "Epoch 1055/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2021 - val_loss: 5.2953\n",
            "Epoch 1056/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1500 - val_loss: 6.7720\n",
            "Epoch 1057/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1522 - val_loss: 4.2746\n",
            "Epoch 1058/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1583 - val_loss: 2.9236\n",
            "Epoch 1059/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1588 - val_loss: 2.7814\n",
            "Epoch 1060/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1651 - val_loss: 4.1305\n",
            "Epoch 1061/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1708 - val_loss: 3.3491\n",
            "Epoch 1062/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1313 - val_loss: 2.7440\n",
            "Epoch 1063/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1532 - val_loss: 3.9773\n",
            "Epoch 1064/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2764 - val_loss: 2.9451\n",
            "Epoch 1065/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1680 - val_loss: 3.5389\n",
            "Epoch 1066/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1391 - val_loss: 4.4225\n",
            "Epoch 1067/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2442 - val_loss: 7.7483\n",
            "Epoch 1068/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2815 - val_loss: 1.9515\n",
            "Epoch 1069/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2264 - val_loss: 3.7294\n",
            "Epoch 1070/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1781 - val_loss: 3.7910\n",
            "Epoch 1071/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2568 - val_loss: 3.7958\n",
            "Epoch 1072/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2275 - val_loss: 1.8365\n",
            "Epoch 1073/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2096 - val_loss: 2.8081\n",
            "Epoch 1074/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1710 - val_loss: 2.8975\n",
            "Epoch 1075/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1580 - val_loss: 4.6732\n",
            "Epoch 1076/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1931 - val_loss: 2.0784\n",
            "Epoch 1077/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2965 - val_loss: 7.4975\n",
            "Epoch 1078/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2168 - val_loss: 13.3028\n",
            "Epoch 1079/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1916 - val_loss: 4.5230\n",
            "Epoch 1080/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2098 - val_loss: 3.6718\n",
            "Epoch 1081/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2480 - val_loss: 4.2992\n",
            "Epoch 1082/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1706 - val_loss: 2.5747\n",
            "Epoch 1083/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1509 - val_loss: 3.3621\n",
            "Epoch 1084/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1891 - val_loss: 2.7218\n",
            "Epoch 1085/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2092 - val_loss: 5.0468\n",
            "Epoch 1086/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1455 - val_loss: 4.5265\n",
            "Epoch 1087/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1273 - val_loss: 5.3069\n",
            "Epoch 1088/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1489 - val_loss: 3.9755\n",
            "Epoch 1089/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1402 - val_loss: 3.6395\n",
            "Epoch 1090/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3199 - val_loss: 2.2830\n",
            "Epoch 1091/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2078 - val_loss: 5.2863\n",
            "Epoch 1092/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1629 - val_loss: 10.4092\n",
            "Epoch 1093/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1736 - val_loss: 5.2224\n",
            "Epoch 1094/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1803 - val_loss: 4.1630\n",
            "Epoch 1095/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1885 - val_loss: 3.5638\n",
            "Epoch 1096/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1387 - val_loss: 3.1372\n",
            "Epoch 1097/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1609 - val_loss: 1.9826\n",
            "Epoch 1098/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1563 - val_loss: 3.4197\n",
            "Epoch 1099/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1370 - val_loss: 5.1398\n",
            "Epoch 1100/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1900 - val_loss: 4.9976\n",
            "Epoch 1101/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2764 - val_loss: 4.2401\n",
            "Epoch 1102/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2079 - val_loss: 3.8780\n",
            "Epoch 1103/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2561 - val_loss: 2.5318\n",
            "Epoch 1104/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2245 - val_loss: 3.0850\n",
            "Epoch 1105/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1592 - val_loss: 2.5403\n",
            "Epoch 1106/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1534 - val_loss: 2.4025\n",
            "Epoch 1107/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1967 - val_loss: 2.5936\n",
            "Epoch 1108/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1761 - val_loss: 5.4296\n",
            "Epoch 1109/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1935 - val_loss: 1.2971\n",
            "Epoch 1110/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1833 - val_loss: 3.5138\n",
            "Epoch 1111/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1513 - val_loss: 4.0619\n",
            "Epoch 1112/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2064 - val_loss: 2.8819\n",
            "Epoch 1113/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3729 - val_loss: 3.2139\n",
            "Epoch 1114/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1991 - val_loss: 6.4319\n",
            "Epoch 1115/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2233 - val_loss: 1.8725\n",
            "Epoch 1116/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1785 - val_loss: 3.7123\n",
            "Epoch 1117/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2093 - val_loss: 3.0335\n",
            "Epoch 1118/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1884 - val_loss: 1.6910\n",
            "Epoch 1119/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1946 - val_loss: 2.0413\n",
            "Epoch 1120/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1648 - val_loss: 2.4935\n",
            "Epoch 1121/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1608 - val_loss: 5.0552\n",
            "Epoch 1122/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1537 - val_loss: 4.4795\n",
            "Epoch 1123/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1492 - val_loss: 5.6420\n",
            "Epoch 1124/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1524 - val_loss: 5.8013\n",
            "Epoch 1125/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2052 - val_loss: 1.8645\n",
            "Epoch 1126/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1703 - val_loss: 2.1524\n",
            "Epoch 1127/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2675 - val_loss: 2.6640\n",
            "Epoch 1128/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3852 - val_loss: 1.2525\n",
            "Epoch 1129/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2400 - val_loss: 3.5041\n",
            "Epoch 1130/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2747 - val_loss: 3.9223\n",
            "Epoch 1131/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1891 - val_loss: 2.0015\n",
            "Epoch 1132/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2024 - val_loss: 2.4254\n",
            "Epoch 1133/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2456 - val_loss: 2.6156\n",
            "Epoch 1134/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2729 - val_loss: 6.2643\n",
            "Epoch 1135/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2137 - val_loss: 4.7461\n",
            "Epoch 1136/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1486 - val_loss: 2.9809\n",
            "Epoch 1137/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1859 - val_loss: 3.2716\n",
            "Epoch 1138/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2084 - val_loss: 1.4996\n",
            "Epoch 1139/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1668 - val_loss: 2.8560\n",
            "Epoch 1140/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1588 - val_loss: 2.6186\n",
            "Epoch 1141/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1701 - val_loss: 1.6782\n",
            "Epoch 1142/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1933 - val_loss: 2.5802\n",
            "Epoch 1143/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1767 - val_loss: 3.1076\n",
            "Epoch 1144/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1954 - val_loss: 3.8679\n",
            "Epoch 1145/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1633 - val_loss: 2.4976\n",
            "Epoch 1146/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1887 - val_loss: 2.4778\n",
            "Epoch 1147/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1781 - val_loss: 5.1312\n",
            "Epoch 1148/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1610 - val_loss: 6.2872\n",
            "Epoch 1149/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1278 - val_loss: 5.9006\n",
            "Epoch 1150/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1399 - val_loss: 1.7310\n",
            "Epoch 1151/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1598 - val_loss: 3.1893\n",
            "Epoch 1152/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1784 - val_loss: 4.3253\n",
            "Epoch 1153/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1655 - val_loss: 3.7121\n",
            "Epoch 1154/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1497 - val_loss: 2.2921\n",
            "Epoch 1155/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2143 - val_loss: 5.2572\n",
            "Epoch 1156/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1603 - val_loss: 8.0919\n",
            "Epoch 1157/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2515 - val_loss: 2.9184\n",
            "Epoch 1158/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2710 - val_loss: 3.6614\n",
            "Epoch 1159/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1857 - val_loss: 3.6739\n",
            "Epoch 1160/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1446 - val_loss: 4.3010\n",
            "Epoch 1161/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1813 - val_loss: 3.7825\n",
            "Epoch 1162/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1801 - val_loss: 3.4602\n",
            "Epoch 1163/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2436 - val_loss: 1.8396\n",
            "Epoch 1164/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2618 - val_loss: 6.9422\n",
            "Epoch 1165/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1821 - val_loss: 4.1891\n",
            "Epoch 1166/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1562 - val_loss: 2.3368\n",
            "Epoch 1167/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1543 - val_loss: 2.5778\n",
            "Epoch 1168/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1400 - val_loss: 2.4990\n",
            "Epoch 1169/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1351 - val_loss: 3.0017\n",
            "Epoch 1170/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1467 - val_loss: 1.6869\n",
            "Epoch 1171/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2588 - val_loss: 2.9373\n",
            "Epoch 1172/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2513 - val_loss: 2.3288\n",
            "Epoch 1173/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2240 - val_loss: 4.0456\n",
            "Epoch 1174/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2106 - val_loss: 4.3386\n",
            "Epoch 1175/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1638 - val_loss: 3.0051\n",
            "Epoch 1176/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1897 - val_loss: 2.9798\n",
            "Epoch 1177/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1909 - val_loss: 2.4094\n",
            "Epoch 1178/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1691 - val_loss: 3.0054\n",
            "Epoch 1179/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1998 - val_loss: 2.7178\n",
            "Epoch 1180/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1853 - val_loss: 4.1351\n",
            "Epoch 1181/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1633 - val_loss: 2.7193\n",
            "Epoch 1182/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2863 - val_loss: 1.3767\n",
            "Epoch 1183/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1844 - val_loss: 2.0939\n",
            "Epoch 1184/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1633 - val_loss: 2.2530\n",
            "Epoch 1185/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1669 - val_loss: 1.2930\n",
            "Epoch 1186/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1680 - val_loss: 2.1070\n",
            "Epoch 1187/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1390 - val_loss: 3.1321\n",
            "Epoch 1188/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1584 - val_loss: 2.8826\n",
            "Epoch 1189/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2682 - val_loss: 1.4010\n",
            "Epoch 1190/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2332 - val_loss: 2.8763\n",
            "Epoch 1191/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2133 - val_loss: 2.0814\n",
            "Epoch 1192/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1861 - val_loss: 2.3088\n",
            "Epoch 1193/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2015 - val_loss: 2.0124\n",
            "Epoch 1194/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1573 - val_loss: 1.2757\n",
            "Epoch 1195/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1932 - val_loss: 1.8363\n",
            "Epoch 1196/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2184 - val_loss: 2.7701\n",
            "Epoch 1197/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1761 - val_loss: 5.7872\n",
            "Epoch 1198/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2119 - val_loss: 2.6856\n",
            "Epoch 1199/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1953 - val_loss: 3.1985\n",
            "Epoch 1200/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1685 - val_loss: 4.6381\n",
            "Epoch 1201/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1745 - val_loss: 2.9630\n",
            "Epoch 1202/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1651 - val_loss: 1.9510\n",
            "Epoch 1203/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1436 - val_loss: 3.1856\n",
            "Epoch 1204/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1646 - val_loss: 2.7137\n",
            "Epoch 1205/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2648 - val_loss: 2.2600\n",
            "Epoch 1206/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2787 - val_loss: 1.4992\n",
            "Epoch 1207/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1906 - val_loss: 1.9847\n",
            "Epoch 1208/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2168 - val_loss: 1.8636\n",
            "Epoch 1209/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2199 - val_loss: 4.2716\n",
            "Epoch 1210/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2197 - val_loss: 3.3429\n",
            "Epoch 1211/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1771 - val_loss: 2.8118\n",
            "Epoch 1212/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1849 - val_loss: 2.0923\n",
            "Epoch 1213/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1574 - val_loss: 1.7532\n",
            "Epoch 1214/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1506 - val_loss: 2.0370\n",
            "Epoch 1215/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1911 - val_loss: 3.1809\n",
            "Epoch 1216/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1995 - val_loss: 4.2290\n",
            "Epoch 1217/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1579 - val_loss: 2.4281\n",
            "Epoch 1218/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1809 - val_loss: 5.0013\n",
            "Epoch 1219/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1959 - val_loss: 5.9562\n",
            "Epoch 1220/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2453 - val_loss: 3.8530\n",
            "Epoch 1221/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1770 - val_loss: 2.6573\n",
            "Epoch 1222/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1745 - val_loss: 2.8849\n",
            "Epoch 1223/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1410 - val_loss: 3.9041\n",
            "Epoch 1224/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1612 - val_loss: 3.5718\n",
            "Epoch 1225/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1829 - val_loss: 3.9984\n",
            "Epoch 1226/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1598 - val_loss: 2.5355\n",
            "Epoch 1227/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1826 - val_loss: 2.7021\n",
            "Epoch 1228/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1985 - val_loss: 2.3013\n",
            "Epoch 1229/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2587 - val_loss: 3.2725\n",
            "Epoch 1230/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2199 - val_loss: 2.6066\n",
            "Epoch 1231/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2003 - val_loss: 4.7130\n",
            "Epoch 1232/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1426 - val_loss: 2.1751\n",
            "Epoch 1233/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2393 - val_loss: 2.2742\n",
            "Epoch 1234/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2444 - val_loss: 3.0270\n",
            "Epoch 1235/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1589 - val_loss: 2.1352\n",
            "Epoch 1236/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1697 - val_loss: 3.7196\n",
            "Epoch 1237/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3698 - val_loss: 1.6496\n",
            "Epoch 1238/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2195 - val_loss: 2.7546\n",
            "Epoch 1239/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1530 - val_loss: 2.3546\n",
            "Epoch 1240/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1937 - val_loss: 2.3927\n",
            "Epoch 1241/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1672 - val_loss: 3.2006\n",
            "Epoch 1242/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1465 - val_loss: 4.8938\n",
            "Epoch 1243/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1546 - val_loss: 1.8721\n",
            "Epoch 1244/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2107 - val_loss: 2.7931\n",
            "Epoch 1245/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1757 - val_loss: 4.0340\n",
            "Epoch 1246/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1537 - val_loss: 3.4767\n",
            "Epoch 1247/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1533 - val_loss: 3.7188\n",
            "Epoch 1248/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2168 - val_loss: 1.6038\n",
            "Epoch 1249/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1976 - val_loss: 2.3609\n",
            "Epoch 1250/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1285 - val_loss: 4.7178\n",
            "Epoch 1251/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1555 - val_loss: 3.6832\n",
            "Epoch 1252/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1409 - val_loss: 1.1282\n",
            "Epoch 1253/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1302 - val_loss: 2.5411\n",
            "Epoch 1254/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1741 - val_loss: 3.6946\n",
            "Epoch 1255/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1455 - val_loss: 3.4325\n",
            "Epoch 1256/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1783 - val_loss: 4.8565\n",
            "Epoch 1257/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1804 - val_loss: 3.2701\n",
            "Epoch 1258/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1753 - val_loss: 2.1959\n",
            "Epoch 1259/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1340 - val_loss: 2.8705\n",
            "Epoch 1260/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1359 - val_loss: 2.3290\n",
            "Epoch 1261/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1650 - val_loss: 2.5759\n",
            "Epoch 1262/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2085 - val_loss: 2.6676\n",
            "Epoch 1263/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1800 - val_loss: 4.3022\n",
            "Epoch 1264/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1567 - val_loss: 6.6318\n",
            "Epoch 1265/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1398 - val_loss: 3.7964\n",
            "Epoch 1266/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1336 - val_loss: 2.7649\n",
            "Epoch 1267/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2085 - val_loss: 3.1545\n",
            "Epoch 1268/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1419 - val_loss: 2.7614\n",
            "Epoch 1269/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1909 - val_loss: 2.7970\n",
            "Epoch 1270/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1707 - val_loss: 2.4891\n",
            "Epoch 1271/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1799 - val_loss: 2.3663\n",
            "Epoch 1272/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1654 - val_loss: 6.0092\n",
            "Epoch 1273/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1575 - val_loss: 3.9887\n",
            "Epoch 1274/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2399 - val_loss: 5.4478\n",
            "Epoch 1275/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2629 - val_loss: 1.9635\n",
            "Epoch 1276/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1573 - val_loss: 2.0946\n",
            "Epoch 1277/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1802 - val_loss: 2.5173\n",
            "Epoch 1278/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1756 - val_loss: 2.3740\n",
            "Epoch 1279/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1862 - val_loss: 4.1682\n",
            "Epoch 1280/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1911 - val_loss: 1.2909\n",
            "Epoch 1281/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1831 - val_loss: 2.7382\n",
            "Epoch 1282/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1734 - val_loss: 3.5846\n",
            "Epoch 1283/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2099 - val_loss: 4.2551\n",
            "Epoch 1284/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1629 - val_loss: 4.2795\n",
            "Epoch 1285/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1242 - val_loss: 4.6002\n",
            "Epoch 1286/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1434 - val_loss: 3.0209\n",
            "Epoch 1287/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1179 - val_loss: 4.3893\n",
            "Epoch 1288/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1446 - val_loss: 4.1451\n",
            "Epoch 1289/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1532 - val_loss: 4.4129\n",
            "Epoch 1290/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1782 - val_loss: 3.2019\n",
            "Epoch 1291/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1649 - val_loss: 1.6808\n",
            "Epoch 1292/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2105 - val_loss: 3.1068\n",
            "Epoch 1293/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1818 - val_loss: 3.9449\n",
            "Epoch 1294/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2069 - val_loss: 3.2685\n",
            "Epoch 1295/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1521 - val_loss: 3.6423\n",
            "Epoch 1296/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1599 - val_loss: 3.2541\n",
            "Epoch 1297/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1573 - val_loss: 4.4937\n",
            "Epoch 1298/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1865 - val_loss: 5.5717\n",
            "Epoch 1299/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1654 - val_loss: 4.9553\n",
            "Epoch 1300/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2089 - val_loss: 3.7305\n",
            "Epoch 1301/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2624 - val_loss: 1.7367\n",
            "Epoch 1302/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2738 - val_loss: 2.8286\n",
            "Epoch 1303/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3208 - val_loss: 3.0026\n",
            "Epoch 1304/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1660 - val_loss: 3.7880\n",
            "Epoch 1305/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1778 - val_loss: 2.0336\n",
            "Epoch 1306/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2526 - val_loss: 2.6593\n",
            "Epoch 1307/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3765 - val_loss: 7.7153\n",
            "Epoch 1308/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2122 - val_loss: 4.5298\n",
            "Epoch 1309/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1727 - val_loss: 4.1729\n",
            "Epoch 1310/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1860 - val_loss: 5.1894\n",
            "Epoch 1311/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2293 - val_loss: 6.7538\n",
            "Epoch 1312/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1679 - val_loss: 4.7743\n",
            "Epoch 1313/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2374 - val_loss: 3.3287\n",
            "Epoch 1314/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1663 - val_loss: 3.7911\n",
            "Epoch 1315/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1939 - val_loss: 5.5090\n",
            "Epoch 1316/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1737 - val_loss: 5.1408\n",
            "Epoch 1317/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1744 - val_loss: 3.8667\n",
            "Epoch 1318/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2238 - val_loss: 6.6293\n",
            "Epoch 1319/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1223 - val_loss: 6.0216\n",
            "Epoch 1320/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1342 - val_loss: 4.6797\n",
            "Epoch 1321/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1652 - val_loss: 3.8565\n",
            "Epoch 1322/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1399 - val_loss: 3.2834\n",
            "Epoch 1323/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1689 - val_loss: 5.8900\n",
            "Epoch 1324/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1611 - val_loss: 4.1394\n",
            "Epoch 1325/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1458 - val_loss: 2.4602\n",
            "Epoch 1326/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1523 - val_loss: 2.6498\n",
            "Epoch 1327/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1617 - val_loss: 2.8310\n",
            "Epoch 1328/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1465 - val_loss: 3.3995\n",
            "Epoch 1329/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1726 - val_loss: 3.1938\n",
            "Epoch 1330/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1863 - val_loss: 3.5872\n",
            "Epoch 1331/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1474 - val_loss: 6.1790\n",
            "Epoch 1332/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1876 - val_loss: 4.7539\n",
            "Epoch 1333/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1831 - val_loss: 4.0442\n",
            "Epoch 1334/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1909 - val_loss: 3.5824\n",
            "Epoch 1335/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1983 - val_loss: 4.1922\n",
            "Epoch 1336/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2025 - val_loss: 3.9911\n",
            "Epoch 1337/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2056 - val_loss: 3.4772\n",
            "Epoch 1338/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1935 - val_loss: 3.5653\n",
            "Epoch 1339/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1416 - val_loss: 5.8435\n",
            "Epoch 1340/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2246 - val_loss: 6.0683\n",
            "Epoch 1341/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2260 - val_loss: 2.7985\n",
            "Epoch 1342/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1691 - val_loss: 2.9568\n",
            "Epoch 1343/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1857 - val_loss: 4.0332\n",
            "Epoch 1344/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1920 - val_loss: 3.1522\n",
            "Epoch 1345/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1456 - val_loss: 3.2641\n",
            "Epoch 1346/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1657 - val_loss: 3.1328\n",
            "Epoch 1347/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1582 - val_loss: 1.9117\n",
            "Epoch 1348/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1567 - val_loss: 3.5045\n",
            "Epoch 1349/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1512 - val_loss: 1.7940\n",
            "Epoch 1350/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1486 - val_loss: 3.3241\n",
            "Epoch 1351/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1344 - val_loss: 5.4240\n",
            "Epoch 1352/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1542 - val_loss: 2.5022\n",
            "Epoch 1353/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1705 - val_loss: 1.8205\n",
            "Epoch 1354/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1559 - val_loss: 2.0982\n",
            "Epoch 1355/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1434 - val_loss: 3.1278\n",
            "Epoch 1356/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1725 - val_loss: 2.1872\n",
            "Epoch 1357/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2089 - val_loss: 2.0404\n",
            "Epoch 1358/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1314 - val_loss: 1.3142\n",
            "Epoch 1359/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1162 - val_loss: 2.9573\n",
            "Epoch 1360/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1679 - val_loss: 2.9840\n",
            "Epoch 1361/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1330 - val_loss: 2.9368\n",
            "Epoch 1362/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1507 - val_loss: 1.9664\n",
            "Epoch 1363/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1532 - val_loss: 3.3639\n",
            "Epoch 1364/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1738 - val_loss: 2.2000\n",
            "Epoch 1365/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1921 - val_loss: 3.6963\n",
            "Epoch 1366/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2240 - val_loss: 3.1924\n",
            "Epoch 1367/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1844 - val_loss: 9.3159\n",
            "Epoch 1368/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1839 - val_loss: 6.9531\n",
            "Epoch 1369/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2050 - val_loss: 4.7407\n",
            "Epoch 1370/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1746 - val_loss: 3.9747\n",
            "Epoch 1371/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2161 - val_loss: 3.0103\n",
            "Epoch 1372/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2198 - val_loss: 2.7398\n",
            "Epoch 1373/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2230 - val_loss: 4.8659\n",
            "Epoch 1374/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1909 - val_loss: 2.5142\n",
            "Epoch 1375/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2037 - val_loss: 4.5043\n",
            "Epoch 1376/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2041 - val_loss: 3.8352\n",
            "Epoch 1377/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1630 - val_loss: 3.1844\n",
            "Epoch 1378/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2016 - val_loss: 8.1401\n",
            "Epoch 1379/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2177 - val_loss: 3.9690\n",
            "Epoch 1380/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2297 - val_loss: 7.5537\n",
            "Epoch 1381/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1433 - val_loss: 2.7804\n",
            "Epoch 1382/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1745 - val_loss: 3.2268\n",
            "Epoch 1383/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1943 - val_loss: 2.3756\n",
            "Epoch 1384/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1812 - val_loss: 4.6178\n",
            "Epoch 1385/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1671 - val_loss: 4.8465\n",
            "Epoch 1386/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1521 - val_loss: 4.1975\n",
            "Epoch 1387/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2122 - val_loss: 2.8535\n",
            "Epoch 1388/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2561 - val_loss: 3.4322\n",
            "Epoch 1389/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1632 - val_loss: 3.3640\n",
            "Epoch 1390/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1446 - val_loss: 2.8836\n",
            "Epoch 1391/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1603 - val_loss: 5.6310\n",
            "Epoch 1392/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1612 - val_loss: 3.6921\n",
            "Epoch 1393/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3461 - val_loss: 1.7091\n",
            "Epoch 1394/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2710 - val_loss: 3.3709\n",
            "Epoch 1395/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1682 - val_loss: 5.1103\n",
            "Epoch 1396/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2177 - val_loss: 2.8499\n",
            "Epoch 1397/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2235 - val_loss: 3.2449\n",
            "Epoch 1398/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1926 - val_loss: 3.2649\n",
            "Epoch 1399/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1245 - val_loss: 3.6479\n",
            "Epoch 1400/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1745 - val_loss: 2.8905\n",
            "Epoch 1401/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1373 - val_loss: 2.8425\n",
            "Epoch 1402/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1468 - val_loss: 5.7252\n",
            "Epoch 1403/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1290 - val_loss: 4.0763\n",
            "Epoch 1404/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1319 - val_loss: 6.3693\n",
            "Epoch 1405/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2098 - val_loss: 2.4991\n",
            "Epoch 1406/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2979 - val_loss: 2.5353\n",
            "Epoch 1407/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1991 - val_loss: 2.4025\n",
            "Epoch 1408/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1669 - val_loss: 2.4245\n",
            "Epoch 1409/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1623 - val_loss: 2.7198\n",
            "Epoch 1410/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2254 - val_loss: 2.5763\n",
            "Epoch 1411/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1540 - val_loss: 4.8307\n",
            "Epoch 1412/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1354 - val_loss: 3.4482\n",
            "Epoch 1413/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1444 - val_loss: 3.5912\n",
            "Epoch 1414/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1878 - val_loss: 2.7535\n",
            "Epoch 1415/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1746 - val_loss: 3.0961\n",
            "Epoch 1416/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1426 - val_loss: 2.1891\n",
            "Epoch 1417/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1737 - val_loss: 2.0768\n",
            "Epoch 1418/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1708 - val_loss: 3.0141\n",
            "Epoch 1419/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1567 - val_loss: 4.3170\n",
            "Epoch 1420/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1814 - val_loss: 1.9071\n",
            "Epoch 1421/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2247 - val_loss: 3.4305\n",
            "Epoch 1422/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1714 - val_loss: 2.9476\n",
            "Epoch 1423/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1788 - val_loss: 2.8861\n",
            "Epoch 1424/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1842 - val_loss: 5.8766\n",
            "Epoch 1425/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2174 - val_loss: 2.4148\n",
            "Epoch 1426/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1830 - val_loss: 2.3751\n",
            "Epoch 1427/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1335 - val_loss: 4.2099\n",
            "Epoch 1428/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2414 - val_loss: 1.7323\n",
            "Epoch 1429/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2403 - val_loss: 2.6859\n",
            "Epoch 1430/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1384 - val_loss: 4.0205\n",
            "Epoch 1431/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1734 - val_loss: 4.1121\n",
            "Epoch 1432/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1693 - val_loss: 3.4213\n",
            "Epoch 1433/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1574 - val_loss: 4.5496\n",
            "Epoch 1434/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1950 - val_loss: 4.0962\n",
            "Epoch 1435/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2004 - val_loss: 3.1769\n",
            "Epoch 1436/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1716 - val_loss: 3.1158\n",
            "Epoch 1437/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1563 - val_loss: 3.0981\n",
            "Epoch 1438/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1622 - val_loss: 4.0314\n",
            "Epoch 1439/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1413 - val_loss: 5.4036\n",
            "Epoch 1440/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1333 - val_loss: 3.2362\n",
            "Epoch 1441/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1582 - val_loss: 4.2103\n",
            "Epoch 1442/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1506 - val_loss: 2.2873\n",
            "Epoch 1443/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1939 - val_loss: 2.7942\n",
            "Epoch 1444/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1732 - val_loss: 2.7078\n",
            "Epoch 1445/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2239 - val_loss: 5.0320\n",
            "Epoch 1446/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1850 - val_loss: 2.6023\n",
            "Epoch 1447/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1844 - val_loss: 4.1866\n",
            "Epoch 1448/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1488 - val_loss: 5.2909\n",
            "Epoch 1449/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1331 - val_loss: 3.6054\n",
            "Epoch 1450/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1882 - val_loss: 1.9188\n",
            "Epoch 1451/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1864 - val_loss: 3.1864\n",
            "Epoch 1452/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1595 - val_loss: 4.2368\n",
            "Epoch 1453/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1900 - val_loss: 4.5100\n",
            "Epoch 1454/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2283 - val_loss: 3.2425\n",
            "Epoch 1455/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2799 - val_loss: 2.3081\n",
            "Epoch 1456/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2002 - val_loss: 2.3950\n",
            "Epoch 1457/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2062 - val_loss: 2.7897\n",
            "Epoch 1458/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1530 - val_loss: 2.1921\n",
            "Epoch 1459/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2064 - val_loss: 1.6529\n",
            "Epoch 1460/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2385 - val_loss: 3.2585\n",
            "Epoch 1461/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2089 - val_loss: 2.7238\n",
            "Epoch 1462/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1751 - val_loss: 2.4052\n",
            "Epoch 1463/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7713 - val_loss: 1.1452\n",
            "Epoch 1464/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4207 - val_loss: 1.8860\n",
            "Epoch 1465/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2275 - val_loss: 1.8316\n",
            "Epoch 1466/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2225 - val_loss: 3.1167\n",
            "Epoch 1467/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1920 - val_loss: 2.9987\n",
            "Epoch 1468/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1915 - val_loss: 2.1510\n",
            "Epoch 1469/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1520 - val_loss: 3.1953\n",
            "Epoch 1470/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1689 - val_loss: 3.4617\n",
            "Epoch 1471/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1405 - val_loss: 4.4079\n",
            "Epoch 1472/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1724 - val_loss: 2.8537\n",
            "Epoch 1473/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2232 - val_loss: 2.8848\n",
            "Epoch 1474/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1635 - val_loss: 3.3705\n",
            "Epoch 1475/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1458 - val_loss: 3.4755\n",
            "Epoch 1476/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1467 - val_loss: 2.4222\n",
            "Epoch 1477/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1549 - val_loss: 2.8491\n",
            "Epoch 1478/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1686 - val_loss: 3.9028\n",
            "Epoch 1479/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2287 - val_loss: 3.8175\n",
            "Epoch 1480/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2016 - val_loss: 3.7257\n",
            "Epoch 1481/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1884 - val_loss: 2.3888\n",
            "Epoch 1482/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1935 - val_loss: 3.0505\n",
            "Epoch 1483/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3881 - val_loss: 1.4800\n",
            "Epoch 1484/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3052 - val_loss: 2.5720\n",
            "Epoch 1485/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2154 - val_loss: 2.6673\n",
            "Epoch 1486/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1931 - val_loss: 3.5934\n",
            "Epoch 1487/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2015 - val_loss: 4.9856\n",
            "Epoch 1488/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1968 - val_loss: 2.4276\n",
            "Epoch 1489/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1372 - val_loss: 2.4682\n",
            "Epoch 1490/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1534 - val_loss: 3.5512\n",
            "Epoch 1491/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1533 - val_loss: 2.0885\n",
            "Epoch 1492/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2058 - val_loss: 2.8297\n",
            "Epoch 1493/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1125 - val_loss: 5.0113\n",
            "Epoch 1494/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1819 - val_loss: 1.9273\n",
            "Epoch 1495/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2773 - val_loss: 2.2955\n",
            "Epoch 1496/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2064 - val_loss: 7.2077\n",
            "Epoch 1497/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1692 - val_loss: 5.9513\n",
            "Epoch 1498/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1570 - val_loss: 2.7358\n",
            "Epoch 1499/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1483 - val_loss: 2.8814\n",
            "Epoch 1500/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2727 - val_loss: 3.1140\n",
            "Epoch 1501/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1836 - val_loss: 1.7707\n",
            "Epoch 1502/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1733 - val_loss: 3.5300\n",
            "Epoch 1503/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1545 - val_loss: 2.9712\n",
            "Epoch 1504/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1529 - val_loss: 4.1187\n",
            "Epoch 1505/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1481 - val_loss: 1.6918\n",
            "Epoch 1506/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1718 - val_loss: 4.0806\n",
            "Epoch 1507/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1478 - val_loss: 1.6909\n",
            "Epoch 1508/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1873 - val_loss: 1.9649\n",
            "Epoch 1509/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1783 - val_loss: 3.5778\n",
            "Epoch 1510/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1972 - val_loss: 1.5238\n",
            "Epoch 1511/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1583 - val_loss: 2.5798\n",
            "Epoch 1512/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1788 - val_loss: 2.1069\n",
            "Epoch 1513/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1611 - val_loss: 4.6006\n",
            "Epoch 1514/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1272 - val_loss: 3.2375\n",
            "Epoch 1515/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1380 - val_loss: 1.8195\n",
            "Epoch 1516/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2086 - val_loss: 4.6182\n",
            "Epoch 1517/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1671 - val_loss: 4.9291\n",
            "Epoch 1518/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1921 - val_loss: 4.3716\n",
            "Epoch 1519/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1890 - val_loss: 3.0082\n",
            "Epoch 1520/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2293 - val_loss: 2.3744\n",
            "Epoch 1521/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1918 - val_loss: 1.7565\n",
            "Epoch 1522/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1359 - val_loss: 2.3177\n",
            "Epoch 1523/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1377 - val_loss: 1.7535\n",
            "Epoch 1524/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2037 - val_loss: 4.7908\n",
            "Epoch 1525/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3031 - val_loss: 2.2639\n",
            "Epoch 1526/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2995 - val_loss: 3.6995\n",
            "Epoch 1527/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1682 - val_loss: 4.0038\n",
            "Epoch 1528/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2290 - val_loss: 3.6346\n",
            "Epoch 1529/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1459 - val_loss: 3.9013\n",
            "Epoch 1530/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1463 - val_loss: 2.9353\n",
            "Epoch 1531/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2766 - val_loss: 2.0454\n",
            "Epoch 1532/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1944 - val_loss: 1.4957\n",
            "Epoch 1533/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1545 - val_loss: 2.6820\n",
            "Epoch 1534/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1971 - val_loss: 1.4109\n",
            "Epoch 1535/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1710 - val_loss: 1.5946\n",
            "Epoch 1536/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1736 - val_loss: 1.6045\n",
            "Epoch 1537/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1473 - val_loss: 3.2373\n",
            "Epoch 1538/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1818 - val_loss: 2.4181\n",
            "Epoch 1539/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1750 - val_loss: 2.2968\n",
            "Epoch 1540/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1556 - val_loss: 2.5128\n",
            "Epoch 1541/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1854 - val_loss: 3.1064\n",
            "Epoch 1542/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1437 - val_loss: 4.7141\n",
            "Epoch 1543/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2573 - val_loss: 2.6977\n",
            "Epoch 1544/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2948 - val_loss: 6.9196\n",
            "Epoch 1545/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1776 - val_loss: 5.6491\n",
            "Epoch 1546/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2187 - val_loss: 4.9667\n",
            "Epoch 1547/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1567 - val_loss: 4.6821\n",
            "Epoch 1548/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1653 - val_loss: 3.2946\n",
            "Epoch 1549/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1683 - val_loss: 2.1872\n",
            "Epoch 1550/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1302 - val_loss: 3.7453\n",
            "Epoch 1551/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1599 - val_loss: 2.0792\n",
            "Epoch 1552/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2307 - val_loss: 1.6717\n",
            "Epoch 1553/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1878 - val_loss: 2.7654\n",
            "Epoch 1554/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2343 - val_loss: 4.9928\n",
            "Epoch 1555/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2092 - val_loss: 4.8709\n",
            "Epoch 1556/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1108 - val_loss: 5.2049\n",
            "Epoch 1557/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1508 - val_loss: 6.2015\n",
            "Epoch 1558/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1541 - val_loss: 2.0387\n",
            "Epoch 1559/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1406 - val_loss: 5.3406\n",
            "Epoch 1560/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1530 - val_loss: 2.5731\n",
            "Epoch 1561/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1398 - val_loss: 4.9575\n",
            "Epoch 1562/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2202 - val_loss: 3.1143\n",
            "Epoch 1563/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1754 - val_loss: 4.4861\n",
            "Epoch 1564/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1956 - val_loss: 5.8266\n",
            "Epoch 1565/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1512 - val_loss: 3.6361\n",
            "Epoch 1566/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1606 - val_loss: 5.6585\n",
            "Epoch 1567/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1156 - val_loss: 4.7259\n",
            "Epoch 1568/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1515 - val_loss: 3.5699\n",
            "Epoch 1569/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1573 - val_loss: 5.8877\n",
            "Epoch 1570/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1644 - val_loss: 5.7849\n",
            "Epoch 1571/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1713 - val_loss: 2.7680\n",
            "Epoch 1572/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1648 - val_loss: 2.7058\n",
            "Epoch 1573/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1682 - val_loss: 6.5520\n",
            "Epoch 1574/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1563 - val_loss: 2.7852\n",
            "Epoch 1575/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2110 - val_loss: 4.4347\n",
            "Epoch 1576/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1484 - val_loss: 3.7344\n",
            "Epoch 1577/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1380 - val_loss: 3.1395\n",
            "Epoch 1578/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1376 - val_loss: 2.9157\n",
            "Epoch 1579/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1886 - val_loss: 6.7651\n",
            "Epoch 1580/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1435 - val_loss: 3.6600\n",
            "Epoch 1581/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1316 - val_loss: 2.8026\n",
            "Epoch 1582/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1133 - val_loss: 2.7138\n",
            "Epoch 1583/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1248 - val_loss: 3.3393\n",
            "Epoch 1584/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1565 - val_loss: 2.5381\n",
            "Epoch 1585/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1508 - val_loss: 4.4344\n",
            "Epoch 1586/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2250 - val_loss: 2.8788\n",
            "Epoch 1587/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1618 - val_loss: 1.5130\n",
            "Epoch 1588/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3193 - val_loss: 1.5887\n",
            "Epoch 1589/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2571 - val_loss: 5.1875\n",
            "Epoch 1590/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2041 - val_loss: 3.7462\n",
            "Epoch 1591/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1641 - val_loss: 3.7534\n",
            "Epoch 1592/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1764 - val_loss: 3.4947\n",
            "Epoch 1593/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1407 - val_loss: 2.0470\n",
            "Epoch 1594/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1470 - val_loss: 3.4277\n",
            "Epoch 1595/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1201 - val_loss: 2.3672\n",
            "Epoch 1596/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1458 - val_loss: 3.3149\n",
            "Epoch 1597/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2103 - val_loss: 1.4409\n",
            "Epoch 1598/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1877 - val_loss: 3.6395\n",
            "Epoch 1599/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2610 - val_loss: 8.9516\n",
            "Epoch 1600/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2081 - val_loss: 3.7649\n",
            "Epoch 1601/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1716 - val_loss: 2.6287\n",
            "Epoch 1602/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1301 - val_loss: 3.0854\n",
            "Epoch 1603/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1675 - val_loss: 2.4790\n",
            "Epoch 1604/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1534 - val_loss: 2.9939\n",
            "Epoch 1605/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1793 - val_loss: 3.7235\n",
            "Epoch 1606/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1905 - val_loss: 5.9246\n",
            "Epoch 1607/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1268 - val_loss: 6.3721\n",
            "Epoch 1608/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1252 - val_loss: 2.6002\n",
            "Epoch 1609/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1083 - val_loss: 2.6981\n",
            "Epoch 1610/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1211 - val_loss: 2.6637\n",
            "Epoch 1611/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1507 - val_loss: 3.2860\n",
            "Epoch 1612/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2218 - val_loss: 4.3494\n",
            "Epoch 1613/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1658 - val_loss: 3.1737\n",
            "Epoch 1614/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1733 - val_loss: 3.2669\n",
            "Epoch 1615/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1330 - val_loss: 3.6041\n",
            "Epoch 1616/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1574 - val_loss: 2.2980\n",
            "Epoch 1617/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1688 - val_loss: 3.9004\n",
            "Epoch 1618/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1958 - val_loss: 3.0072\n",
            "Epoch 1619/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1581 - val_loss: 3.6406\n",
            "Epoch 1620/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1513 - val_loss: 3.2250\n",
            "Epoch 1621/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1351 - val_loss: 1.6312\n",
            "Epoch 1622/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1121 - val_loss: 2.6060\n",
            "Epoch 1623/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1661 - val_loss: 2.2211\n",
            "Epoch 1624/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1528 - val_loss: 1.4745\n",
            "Epoch 1625/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1710 - val_loss: 2.3505\n",
            "Epoch 1626/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1927 - val_loss: 2.2625\n",
            "Epoch 1627/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2043 - val_loss: 2.4645\n",
            "Epoch 1628/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1452 - val_loss: 2.9631\n",
            "Epoch 1629/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1822 - val_loss: 4.6979\n",
            "Epoch 1630/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1809 - val_loss: 2.4905\n",
            "Epoch 1631/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2365 - val_loss: 2.1904\n",
            "Epoch 1632/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1638 - val_loss: 2.5439\n",
            "Epoch 1633/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2034 - val_loss: 3.3764\n",
            "Epoch 1634/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1700 - val_loss: 3.5338\n",
            "Epoch 1635/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2795 - val_loss: 3.8254\n",
            "Epoch 1636/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1849 - val_loss: 2.4704\n",
            "Epoch 1637/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1888 - val_loss: 4.7996\n",
            "Epoch 1638/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1927 - val_loss: 3.1702\n",
            "Epoch 1639/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1414 - val_loss: 2.6617\n",
            "Epoch 1640/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1147 - val_loss: 3.9177\n",
            "Epoch 1641/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1075 - val_loss: 3.1413\n",
            "Epoch 1642/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2111 - val_loss: 1.2926\n",
            "Epoch 1643/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1730 - val_loss: 2.8189\n",
            "Epoch 1644/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1687 - val_loss: 2.7194\n",
            "Epoch 1645/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1422 - val_loss: 1.8289\n",
            "Epoch 1646/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1622 - val_loss: 1.9149\n",
            "Epoch 1647/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2277 - val_loss: 3.9214\n",
            "Epoch 1648/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1432 - val_loss: 2.7654\n",
            "Epoch 1649/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1722 - val_loss: 2.7886\n",
            "Epoch 1650/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1223 - val_loss: 5.2704\n",
            "Epoch 1651/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1619 - val_loss: 2.7854\n",
            "Epoch 1652/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1771 - val_loss: 3.3425\n",
            "Epoch 1653/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1990 - val_loss: 4.7590\n",
            "Epoch 1654/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2072 - val_loss: 4.1575\n",
            "Epoch 1655/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2516 - val_loss: 3.4450\n",
            "Epoch 1656/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2146 - val_loss: 9.6855\n",
            "Epoch 1657/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1333 - val_loss: 3.8768\n",
            "Epoch 1658/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1569 - val_loss: 2.7016\n",
            "Epoch 1659/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1231 - val_loss: 6.6192\n",
            "Epoch 1660/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1256 - val_loss: 3.8419\n",
            "Epoch 1661/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1928 - val_loss: 4.9064\n",
            "Epoch 1662/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1665 - val_loss: 2.8068\n",
            "Epoch 1663/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1721 - val_loss: 2.0261\n",
            "Epoch 1664/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1509 - val_loss: 4.8694\n",
            "Epoch 1665/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1503 - val_loss: 3.9741\n",
            "Epoch 1666/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1430 - val_loss: 2.9339\n",
            "Epoch 1667/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1519 - val_loss: 2.6995\n",
            "Epoch 1668/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1378 - val_loss: 2.1977\n",
            "Epoch 1669/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1984 - val_loss: 3.1132\n",
            "Epoch 1670/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2045 - val_loss: 2.9655\n",
            "Epoch 1671/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1310 - val_loss: 3.9608\n",
            "Epoch 1672/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1362 - val_loss: 2.7626\n",
            "Epoch 1673/10000\n",
            " 1/33 [..............................] - ETA: 0s - loss: 0.0359Restoring model weights from the end of the best epoch: 673.\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1707 - val_loss: 2.9185\n",
            "Epoch 1673: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_1912349/3424320398.py:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.6142240762710571\n",
            "Train loss: 0.6227149367332458\n",
            "Test loss: 0.7608699798583984\n",
            "dO18 RMSE: 0.781159656126665\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0          26.130            0.19025\n",
            "1          26.984            0.40123\n",
            "2          24.656            0.17728\n",
            "3          26.356            0.03953\n",
            "4          23.962            0.30992\n",
            "5          24.848            0.15842\n",
            "6          25.080            0.05125\n",
            "7          26.546            2.14378\n",
            "8          25.682            0.94472\n",
            "9          24.028            0.45852\n",
            "10         23.944            0.15813\n",
            "11         23.752            0.52437\n",
            "12         26.018            0.96417\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       25.638020           0.940464\n",
            "1       26.373734           1.112991\n",
            "2       25.083710           1.396025\n",
            "3       26.776901           0.780901\n",
            "4       23.720564           2.286960\n",
            "5       23.468111           2.604398\n",
            "6       23.709505           2.357202\n",
            "7       25.909279           6.316822\n",
            "8       26.012041           3.107356\n",
            "9       22.653301           3.279318\n",
            "10      23.774776           0.825336\n",
            "11      22.989298           2.828616\n",
            "12      25.696999           2.061186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-02 22:10:04.597409: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [13,13]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['MyDrive/amazon_rainforest_files/variational/model/random_all_boosted_transformer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Grouped, random, ablating other models except kriging\n",
        "\n",
        "We can generate isoscapes for this model easily."
      ],
      "metadata": {
        "id": "opmE3xc0vcY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_random_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/kriging_ensemble_experiment/uc_davis_train_random_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/kriging_ensemble_experiment/uc_davis_test_random_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/kriging_ensemble_experiment/uc_davis_validation_random_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_normalize = []\n",
        "columns_to_standardize = [\n",
        "    'lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature', 'Mean Annual Precipitation']\n",
        "\n",
        "data = load_and_scale(grouped_random_fileset, columns_to_normalize, columns_to_standardize)\n",
        "model = train_and_evaluate(data, \"random_ablated_boosted\", training_batch_size=3)\n",
        "model.save(get_model_save_location(\"random_ablated_boosted.keras\"))\n",
        "dump(data.feature_scaler, get_model_save_location('random_ablated_boosted_transformer.pkl'))"
      ],
      "metadata": {
        "id": "KCebsA7XvfRH",
        "outputId": "92baaec6-eb52-4b7a-bb6f-77cd4a54600f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         lat       long      VPD       RH       PET  DEM          PA  \\\n",
            "0  -0.121000 -67.013000  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "1  -0.041000 -66.872000  0.62083  0.82559  92.27500   93  1001.84741   \n",
            "2  -3.995545 -57.589273  0.85167  0.77358  99.99167   61  1005.67358   \n",
            "3  -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "4  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "..       ...        ...      ...      ...       ...  ...         ...   \n",
            "94 -2.496000 -59.120000  0.77500  0.78866  98.45000  139   996.36792   \n",
            "95 -2.493000 -59.121000  0.77500  0.78866  98.45000  139   996.36792   \n",
            "96 -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "98 -2.497000 -59.121000  0.77500  0.78866  98.45000  139   996.36792   \n",
            "99 -2.483000 -59.124000  0.77500  0.78866  98.45000   96  1001.48932   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  krig_mean_residual  \\\n",
            "0                  26.20833                       2830            1.589397   \n",
            "1                  26.37500                       2764           -0.676855   \n",
            "2                  27.16667                       2273           -0.614146   \n",
            "3                  26.20833                       2830           -0.941343   \n",
            "4                  26.29583                       1897           -0.229080   \n",
            "..                      ...                        ...                 ...   \n",
            "94                 26.79167                       2253           -0.621182   \n",
            "95                 26.79167                       2253           -0.063182   \n",
            "96                 26.71667                       2795           -0.246030   \n",
            "98                 26.79167                       2253            0.182818   \n",
            "99                 26.79167                       2253           -0.369182   \n",
            "\n",
            "    krig_variance_residual  \n",
            "0                 0.559822  \n",
            "1                 0.474394  \n",
            "2                 0.179081  \n",
            "3                 0.641567  \n",
            "4                 0.225259  \n",
            "..                     ...  \n",
            "94                0.463616  \n",
            "95               -1.136534  \n",
            "96                0.282990  \n",
            "98                0.371679  \n",
            "99                0.424476  \n",
            "\n",
            "[99 rows x 11 columns]\n",
            "         lat       long      VPD       RH       PET  DEM          PA  \\\n",
            "0  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "1  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "2  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "3  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "4  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "5  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "6  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "7  -6.009707 -61.868657  0.77083  0.79509  93.97500   71  1004.47662   \n",
            "8  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "9  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "10 -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "11 -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  krig_mean_residual  \\\n",
            "0                  26.00000                       1840            1.836516   \n",
            "1                  26.00000                       1840            0.408516   \n",
            "2                  26.00000                       1840            0.856516   \n",
            "3                  26.00000                       1840            0.678516   \n",
            "4                  26.00000                       1840           -0.191484   \n",
            "5                  26.00000                       1840            0.956016   \n",
            "6                  26.00000                       1840            0.728516   \n",
            "7                  27.20000                       1996            0.343073   \n",
            "8                  26.00000                       1840            1.610516   \n",
            "9                  26.00000                       1840            1.468516   \n",
            "10                 26.29583                       1897            0.090920   \n",
            "11                 26.29583                       1897           -0.463080   \n",
            "\n",
            "    krig_variance_residual  \n",
            "0                -1.130660  \n",
            "1                 0.319060  \n",
            "2                -1.771210  \n",
            "3                -0.693590  \n",
            "4                 0.665760  \n",
            "5                 0.807252  \n",
            "6                 0.008160  \n",
            "7                 0.488285  \n",
            "8                 0.241840  \n",
            "9                 0.636760  \n",
            "10               -0.256241  \n",
            "11                0.547799  \n",
            "         lat       long      VPD       RH       PET  DEM          PA  \\\n",
            "0  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "1  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "2  -0.121000 -67.013000  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "3  -0.041000 -66.872000  0.62083  0.82559  92.27500   93  1001.84741   \n",
            "4  -3.907000 -66.055000  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "5  -4.304000 -70.291000  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "6  -4.304000 -70.291000  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "7  -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "8  -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "9  -4.303698 -70.291068  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "10 -6.009707 -61.868657  0.77083  0.79509  93.97500   71  1004.47662   \n",
            "11 -3.758534 -66.073754  0.64667  0.82247  90.44167   84  1002.92230   \n",
            "12 -6.009707 -61.868657  0.77083  0.79509  93.97500   71  1004.47662   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  krig_mean_residual  \\\n",
            "0                  26.29583                       1897            0.502920   \n",
            "1                  26.29583                       1897           -0.351080   \n",
            "2                  26.20833                       2830            0.338657   \n",
            "3                  26.37500                       2764           -1.307010   \n",
            "4                  26.71667                       2795            0.987970   \n",
            "5                  26.64583                       2708           -0.901265   \n",
            "6                  26.64583                       2708           -1.133265   \n",
            "7                  26.71667                       2795           -1.596030   \n",
            "8                  26.20833                       2830           -0.687343   \n",
            "9                  26.64583                       2708           -0.081265   \n",
            "10                 27.20000                       1996            1.297073   \n",
            "11                 26.71667                       2856            1.209347   \n",
            "12                 27.20000                       1996           -0.776927   \n",
            "\n",
            "    krig_variance_residual  \n",
            "0                 0.524979  \n",
            "1                 0.313999  \n",
            "2                 0.530817  \n",
            "3                 0.648582  \n",
            "4                 0.383700  \n",
            "5                 0.554377  \n",
            "6                 0.661547  \n",
            "7                -1.450160  \n",
            "8                -0.236623  \n",
            "9                 0.254277  \n",
            "10                0.566225  \n",
            "11                0.203298  \n",
            "12               -0.239815  \n",
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('krig_mean_residual_scaler', StandardScaler(),\n",
            "                                 ['krig_mean_residual']),\n",
            "                                ('krig_variance_residual_scaler',\n",
            "                                 StandardScaler(), ['krig_variance_residual']),\n",
            "                                ('lat_standardizer', StandardScaler(), ['lat']),\n",
            "                                ('long_standardizer', StandardScaler(),\n",
            "                                 ['long']),\n",
            "                                ('VPD_standardizer', StandardScaler(), ['VPD']),\n",
            "                                ('RH_standardizer', StandardScaler(), ['RH']),\n",
            "                                ('PET_standardizer', StandardScaler(), ['PET']),\n",
            "                                ('DEM_standardizer', StandardScaler(), ['DEM']),\n",
            "                                ('PA_standardizer', StandardScaler(), ['PA']),\n",
            "                                ('Mean Annual Temperature_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation'])])\n",
            "==================\n",
            "random_all_boosted\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_9 (InputLayer)           [(None, 11)]         0           []                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_26 (S  (None, 9)           0           ['input_9[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 20)           200         ['tf.__operators__.getitem_26[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 20)           420         ['dense_16[0][0]']               \n",
            "                                                                                                  \n",
            " mean_output (Dense)            (None, 1)            21          ['dense_17[0][0]']               \n",
            "                                                                                                  \n",
            " var_output (Dense)             (None, 1)            21          ['dense_17[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_24 (S  (None,)             0           ['input_9[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " tf.math.multiply_10 (TFOpLambd  (None, 1)           0           ['mean_output[0][0]']            \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.multiply_11 (TFOpLambd  (None, 1)           0           ['var_output[0][0]']             \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_25 (S  (None,)             0           ['input_9[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " tf.expand_dims_16 (TFOpLambda)  (None, 1)           0           ['tf.__operators__.getitem_24[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_15 (TFOpL  (None, 1)           0           ['tf.math.multiply_10[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_17 (TFOpL  (None, 1)           0           ['tf.math.multiply_11[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.expand_dims_17 (TFOpLambda)  (None, 1)           0           ['tf.__operators__.getitem_25[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_16 (TFOpL  (None, 1)           0           ['tf.expand_dims_16[0][0]',      \n",
            " ambda)                                                           'tf.__operators__.add_15[0][0]']\n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1)            0           ['tf.__operators__.add_17[0][0]',\n",
            "                                                                  'tf.expand_dims_17[0][0]']      \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 2)            0           ['tf.__operators__.add_16[0][0]',\n",
            "                                                                  'lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 662\n",
            "Trainable params: 662\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10000\n",
            "33/33 [==============================] - 1s 7ms/step - loss: 5965.6953 - val_loss: 700.6152\n",
            "Epoch 2/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 898.8491 - val_loss: 427.5547\n",
            "Epoch 3/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 481.7486 - val_loss: 296.0116\n",
            "Epoch 4/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 288.7733 - val_loss: 173.3296\n",
            "Epoch 5/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 284.2606 - val_loss: 146.5928\n",
            "Epoch 6/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 159.0876 - val_loss: 113.3336\n",
            "Epoch 7/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 124.0842 - val_loss: 87.8822\n",
            "Epoch 8/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 89.0760 - val_loss: 94.5820\n",
            "Epoch 9/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 79.4346 - val_loss: 81.8090\n",
            "Epoch 10/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 73.2892 - val_loss: 67.9350\n",
            "Epoch 11/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 68.0414 - val_loss: 62.2119\n",
            "Epoch 12/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 54.3590 - val_loss: 75.6412\n",
            "Epoch 13/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 47.6457 - val_loss: 60.2339\n",
            "Epoch 14/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 46.5117 - val_loss: 50.5132\n",
            "Epoch 15/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 43.6099 - val_loss: 53.7375\n",
            "Epoch 16/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 43.3922 - val_loss: 45.6475\n",
            "Epoch 17/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 38.6100 - val_loss: 35.6559\n",
            "Epoch 18/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 37.3862 - val_loss: 39.3389\n",
            "Epoch 19/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 31.0827 - val_loss: 40.6109\n",
            "Epoch 20/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 30.2863 - val_loss: 41.1096\n",
            "Epoch 21/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 31.6667 - val_loss: 35.1339\n",
            "Epoch 22/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 25.6673 - val_loss: 34.9377\n",
            "Epoch 23/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 29.7011 - val_loss: 28.6697\n",
            "Epoch 24/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 25.7313 - val_loss: 28.2178\n",
            "Epoch 25/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 24.9552 - val_loss: 30.4634\n",
            "Epoch 26/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 23.9954 - val_loss: 31.9797\n",
            "Epoch 27/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 21.7506 - val_loss: 27.3233\n",
            "Epoch 28/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 23.2598 - val_loss: 27.6191\n",
            "Epoch 29/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 21.3111 - val_loss: 23.8078\n",
            "Epoch 30/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 19.9982 - val_loss: 25.8205\n",
            "Epoch 31/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 18.5966 - val_loss: 25.0352\n",
            "Epoch 32/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 17.7175 - val_loss: 21.9508\n",
            "Epoch 33/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 17.1746 - val_loss: 25.9274\n",
            "Epoch 34/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 15.9447 - val_loss: 26.2275\n",
            "Epoch 35/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 14.9709 - val_loss: 19.8315\n",
            "Epoch 36/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 14.3110 - val_loss: 20.6004\n",
            "Epoch 37/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 14.4281 - val_loss: 16.6262\n",
            "Epoch 38/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 13.5702 - val_loss: 17.8326\n",
            "Epoch 39/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 12.5540 - val_loss: 20.7357\n",
            "Epoch 40/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 12.7003 - val_loss: 16.8563\n",
            "Epoch 41/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 12.5928 - val_loss: 19.4872\n",
            "Epoch 42/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 12.0010 - val_loss: 19.5436\n",
            "Epoch 43/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 11.0642 - val_loss: 14.5151\n",
            "Epoch 44/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 10.9394 - val_loss: 12.9287\n",
            "Epoch 45/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 11.2897 - val_loss: 14.0441\n",
            "Epoch 46/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 10.1350 - val_loss: 12.0457\n",
            "Epoch 47/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 9.3486 - val_loss: 12.9793\n",
            "Epoch 48/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 8.8977 - val_loss: 14.3893\n",
            "Epoch 49/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 9.5513 - val_loss: 12.2772\n",
            "Epoch 50/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 9.4577 - val_loss: 11.8691\n",
            "Epoch 51/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 8.8580 - val_loss: 10.4193\n",
            "Epoch 52/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 7.8298 - val_loss: 9.9859\n",
            "Epoch 53/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 7.5260 - val_loss: 11.4075\n",
            "Epoch 54/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 7.4364 - val_loss: 12.3483\n",
            "Epoch 55/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 6.8954 - val_loss: 9.4066\n",
            "Epoch 56/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 6.7541 - val_loss: 12.3791\n",
            "Epoch 57/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 6.4331 - val_loss: 10.9069\n",
            "Epoch 58/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 6.4179 - val_loss: 9.7377\n",
            "Epoch 59/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 6.5485 - val_loss: 8.1913\n",
            "Epoch 60/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 6.0603 - val_loss: 7.5607\n",
            "Epoch 61/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.2792 - val_loss: 9.1703\n",
            "Epoch 62/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.7142 - val_loss: 10.5730\n",
            "Epoch 63/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 6.0142 - val_loss: 7.8132\n",
            "Epoch 64/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.7635 - val_loss: 9.1263\n",
            "Epoch 65/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.4742 - val_loss: 8.9612\n",
            "Epoch 66/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.7605 - val_loss: 9.0143\n",
            "Epoch 67/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.7427 - val_loss: 7.5237\n",
            "Epoch 68/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.4407 - val_loss: 8.8523\n",
            "Epoch 69/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.6427 - val_loss: 8.9782\n",
            "Epoch 70/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.7077 - val_loss: 7.4038\n",
            "Epoch 71/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.2834 - val_loss: 6.4595\n",
            "Epoch 72/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.3045 - val_loss: 6.2170\n",
            "Epoch 73/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.1953 - val_loss: 7.0669\n",
            "Epoch 74/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.3686 - val_loss: 5.8766\n",
            "Epoch 75/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.7703 - val_loss: 7.0696\n",
            "Epoch 76/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.7510 - val_loss: 6.7679\n",
            "Epoch 77/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.8865 - val_loss: 6.9818\n",
            "Epoch 78/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.6880 - val_loss: 5.6922\n",
            "Epoch 79/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.6963 - val_loss: 7.1130\n",
            "Epoch 80/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.7092 - val_loss: 5.0490\n",
            "Epoch 81/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.6469 - val_loss: 5.2256\n",
            "Epoch 82/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.5859 - val_loss: 5.7339\n",
            "Epoch 83/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.3274 - val_loss: 5.2260\n",
            "Epoch 84/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.3269 - val_loss: 4.7581\n",
            "Epoch 85/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.4849 - val_loss: 4.6312\n",
            "Epoch 86/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.2573 - val_loss: 5.3652\n",
            "Epoch 87/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.2585 - val_loss: 5.6577\n",
            "Epoch 88/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.0420 - val_loss: 5.1190\n",
            "Epoch 89/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.9717 - val_loss: 5.3202\n",
            "Epoch 90/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.2289 - val_loss: 5.7799\n",
            "Epoch 91/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.0806 - val_loss: 4.5088\n",
            "Epoch 92/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.9607 - val_loss: 4.2576\n",
            "Epoch 93/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.7852 - val_loss: 4.7314\n",
            "Epoch 94/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.9223 - val_loss: 4.5867\n",
            "Epoch 95/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.8908 - val_loss: 3.9522\n",
            "Epoch 96/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.8213 - val_loss: 4.1064\n",
            "Epoch 97/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.7674 - val_loss: 5.0202\n",
            "Epoch 98/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.8543 - val_loss: 3.9336\n",
            "Epoch 99/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.7902 - val_loss: 3.5633\n",
            "Epoch 100/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.7246 - val_loss: 3.8883\n",
            "Epoch 101/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.6798 - val_loss: 3.8769\n",
            "Epoch 102/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5292 - val_loss: 3.4165\n",
            "Epoch 103/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.6420 - val_loss: 3.6508\n",
            "Epoch 104/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.6873 - val_loss: 3.4389\n",
            "Epoch 105/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.6120 - val_loss: 3.6879\n",
            "Epoch 106/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.7303 - val_loss: 3.7722\n",
            "Epoch 107/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.6264 - val_loss: 3.4260\n",
            "Epoch 108/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.6041 - val_loss: 3.2913\n",
            "Epoch 109/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.6229 - val_loss: 3.5779\n",
            "Epoch 110/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5803 - val_loss: 3.3877\n",
            "Epoch 111/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.6043 - val_loss: 3.3716\n",
            "Epoch 112/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5364 - val_loss: 3.6157\n",
            "Epoch 113/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.6011 - val_loss: 3.0991\n",
            "Epoch 114/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5567 - val_loss: 3.3113\n",
            "Epoch 115/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5898 - val_loss: 3.4758\n",
            "Epoch 116/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.6245 - val_loss: 3.0728\n",
            "Epoch 117/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5441 - val_loss: 3.3543\n",
            "Epoch 118/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4927 - val_loss: 3.2720\n",
            "Epoch 119/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4867 - val_loss: 3.0528\n",
            "Epoch 120/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4997 - val_loss: 3.1392\n",
            "Epoch 121/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4991 - val_loss: 3.0710\n",
            "Epoch 122/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5517 - val_loss: 3.0982\n",
            "Epoch 123/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4444 - val_loss: 3.0731\n",
            "Epoch 124/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4999 - val_loss: 2.9164\n",
            "Epoch 125/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5524 - val_loss: 2.8013\n",
            "Epoch 126/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5547 - val_loss: 2.8878\n",
            "Epoch 127/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5220 - val_loss: 3.2333\n",
            "Epoch 128/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4624 - val_loss: 2.8582\n",
            "Epoch 129/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4110 - val_loss: 3.2838\n",
            "Epoch 130/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4818 - val_loss: 2.8086\n",
            "Epoch 131/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3994 - val_loss: 2.7043\n",
            "Epoch 132/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4993 - val_loss: 2.8563\n",
            "Epoch 133/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4453 - val_loss: 2.9551\n",
            "Epoch 134/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4151 - val_loss: 2.6985\n",
            "Epoch 135/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4109 - val_loss: 2.9517\n",
            "Epoch 136/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4159 - val_loss: 3.0227\n",
            "Epoch 137/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3540 - val_loss: 2.9282\n",
            "Epoch 138/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3635 - val_loss: 3.0959\n",
            "Epoch 139/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3541 - val_loss: 2.6728\n",
            "Epoch 140/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4089 - val_loss: 2.8905\n",
            "Epoch 141/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3237 - val_loss: 3.1023\n",
            "Epoch 142/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3544 - val_loss: 2.7372\n",
            "Epoch 143/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3299 - val_loss: 2.6614\n",
            "Epoch 144/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4085 - val_loss: 2.7140\n",
            "Epoch 145/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3132 - val_loss: 2.6372\n",
            "Epoch 146/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3467 - val_loss: 2.2615\n",
            "Epoch 147/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3422 - val_loss: 2.3609\n",
            "Epoch 148/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3166 - val_loss: 2.6689\n",
            "Epoch 149/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2848 - val_loss: 2.4883\n",
            "Epoch 150/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3640 - val_loss: 2.6296\n",
            "Epoch 151/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3032 - val_loss: 2.7076\n",
            "Epoch 152/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3327 - val_loss: 2.2700\n",
            "Epoch 153/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3334 - val_loss: 2.4150\n",
            "Epoch 154/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2802 - val_loss: 2.3144\n",
            "Epoch 155/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3263 - val_loss: 2.4295\n",
            "Epoch 156/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3649 - val_loss: 2.4250\n",
            "Epoch 157/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2943 - val_loss: 2.3295\n",
            "Epoch 158/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2590 - val_loss: 2.3837\n",
            "Epoch 159/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3198 - val_loss: 2.3726\n",
            "Epoch 160/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3319 - val_loss: 2.3874\n",
            "Epoch 161/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2247 - val_loss: 2.4830\n",
            "Epoch 162/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2912 - val_loss: 2.4755\n",
            "Epoch 163/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2643 - val_loss: 2.4426\n",
            "Epoch 164/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2483 - val_loss: 2.5093\n",
            "Epoch 165/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2434 - val_loss: 2.4512\n",
            "Epoch 166/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2875 - val_loss: 2.2862\n",
            "Epoch 167/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2617 - val_loss: 2.4315\n",
            "Epoch 168/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2276 - val_loss: 2.5699\n",
            "Epoch 169/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2866 - val_loss: 2.5049\n",
            "Epoch 170/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2277 - val_loss: 2.3162\n",
            "Epoch 171/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2223 - val_loss: 2.3832\n",
            "Epoch 172/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2133 - val_loss: 2.3202\n",
            "Epoch 173/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2211 - val_loss: 2.4216\n",
            "Epoch 174/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1812 - val_loss: 2.2530\n",
            "Epoch 175/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2058 - val_loss: 2.4103\n",
            "Epoch 176/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1484 - val_loss: 2.1992\n",
            "Epoch 177/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1959 - val_loss: 2.2196\n",
            "Epoch 178/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2300 - val_loss: 2.2040\n",
            "Epoch 179/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1688 - val_loss: 2.1167\n",
            "Epoch 180/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1851 - val_loss: 2.2354\n",
            "Epoch 181/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2077 - val_loss: 2.1269\n",
            "Epoch 182/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2083 - val_loss: 2.1815\n",
            "Epoch 183/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1649 - val_loss: 2.1687\n",
            "Epoch 184/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1651 - val_loss: 2.0708\n",
            "Epoch 185/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1666 - val_loss: 2.1898\n",
            "Epoch 186/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1687 - val_loss: 2.2195\n",
            "Epoch 187/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1220 - val_loss: 2.1459\n",
            "Epoch 188/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1408 - val_loss: 2.4717\n",
            "Epoch 189/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2062 - val_loss: 2.1563\n",
            "Epoch 190/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1872 - val_loss: 2.0175\n",
            "Epoch 191/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1065 - val_loss: 1.9938\n",
            "Epoch 192/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0725 - val_loss: 2.1121\n",
            "Epoch 193/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1570 - val_loss: 1.8779\n",
            "Epoch 194/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1410 - val_loss: 1.8839\n",
            "Epoch 195/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1221 - val_loss: 2.0672\n",
            "Epoch 196/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0801 - val_loss: 2.0232\n",
            "Epoch 197/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1312 - val_loss: 2.0375\n",
            "Epoch 198/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1087 - val_loss: 1.9764\n",
            "Epoch 199/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0701 - val_loss: 2.0304\n",
            "Epoch 200/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0536 - val_loss: 1.9633\n",
            "Epoch 201/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0662 - val_loss: 2.0143\n",
            "Epoch 202/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0748 - val_loss: 1.9871\n",
            "Epoch 203/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0555 - val_loss: 2.0069\n",
            "Epoch 204/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0788 - val_loss: 1.9737\n",
            "Epoch 205/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0325 - val_loss: 1.9579\n",
            "Epoch 206/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0953 - val_loss: 2.0332\n",
            "Epoch 207/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0586 - val_loss: 1.9118\n",
            "Epoch 208/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0916 - val_loss: 1.8640\n",
            "Epoch 209/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0549 - val_loss: 1.9204\n",
            "Epoch 210/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0649 - val_loss: 1.9649\n",
            "Epoch 211/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0464 - val_loss: 1.8515\n",
            "Epoch 212/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0372 - val_loss: 1.9344\n",
            "Epoch 213/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0534 - val_loss: 1.8475\n",
            "Epoch 214/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9904 - val_loss: 2.0255\n",
            "Epoch 215/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0046 - val_loss: 1.8630\n",
            "Epoch 216/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0501 - val_loss: 1.9983\n",
            "Epoch 217/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0268 - val_loss: 1.8090\n",
            "Epoch 218/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9845 - val_loss: 1.7819\n",
            "Epoch 219/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9832 - val_loss: 1.8142\n",
            "Epoch 220/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0051 - val_loss: 1.8166\n",
            "Epoch 221/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9496 - val_loss: 1.7703\n",
            "Epoch 222/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9607 - val_loss: 1.9378\n",
            "Epoch 223/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9740 - val_loss: 1.8222\n",
            "Epoch 224/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9680 - val_loss: 1.8865\n",
            "Epoch 225/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9740 - val_loss: 1.8051\n",
            "Epoch 226/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9587 - val_loss: 1.6651\n",
            "Epoch 227/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9567 - val_loss: 1.8536\n",
            "Epoch 228/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9273 - val_loss: 1.9149\n",
            "Epoch 229/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9411 - val_loss: 1.7883\n",
            "Epoch 230/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9140 - val_loss: 1.7825\n",
            "Epoch 231/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9473 - val_loss: 1.7218\n",
            "Epoch 232/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9242 - val_loss: 1.7706\n",
            "Epoch 233/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9127 - val_loss: 1.6672\n",
            "Epoch 234/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8857 - val_loss: 1.7365\n",
            "Epoch 235/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8819 - val_loss: 1.6710\n",
            "Epoch 236/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8632 - val_loss: 1.7167\n",
            "Epoch 237/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9134 - val_loss: 1.7534\n",
            "Epoch 238/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9072 - val_loss: 1.7270\n",
            "Epoch 239/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8555 - val_loss: 1.6973\n",
            "Epoch 240/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8948 - val_loss: 1.6150\n",
            "Epoch 241/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8750 - val_loss: 1.6443\n",
            "Epoch 242/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8019 - val_loss: 1.5053\n",
            "Epoch 243/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8278 - val_loss: 1.6665\n",
            "Epoch 244/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8281 - val_loss: 1.6480\n",
            "Epoch 245/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8498 - val_loss: 1.7046\n",
            "Epoch 246/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8368 - val_loss: 1.6301\n",
            "Epoch 247/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8265 - val_loss: 1.6436\n",
            "Epoch 248/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8359 - val_loss: 1.7027\n",
            "Epoch 249/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7801 - val_loss: 1.6754\n",
            "Epoch 250/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7822 - val_loss: 1.6044\n",
            "Epoch 251/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7207 - val_loss: 1.5653\n",
            "Epoch 252/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7546 - val_loss: 1.6159\n",
            "Epoch 253/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7376 - val_loss: 1.5979\n",
            "Epoch 254/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7208 - val_loss: 1.5012\n",
            "Epoch 255/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7402 - val_loss: 1.5522\n",
            "Epoch 256/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6959 - val_loss: 1.6211\n",
            "Epoch 257/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6821 - val_loss: 1.5496\n",
            "Epoch 258/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7117 - val_loss: 1.6039\n",
            "Epoch 259/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6695 - val_loss: 1.5760\n",
            "Epoch 260/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6592 - val_loss: 1.5741\n",
            "Epoch 261/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6459 - val_loss: 1.5653\n",
            "Epoch 262/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6793 - val_loss: 1.4665\n",
            "Epoch 263/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6339 - val_loss: 1.3806\n",
            "Epoch 264/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6427 - val_loss: 1.4161\n",
            "Epoch 265/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5976 - val_loss: 1.4180\n",
            "Epoch 266/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6225 - val_loss: 1.4170\n",
            "Epoch 267/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5715 - val_loss: 1.4114\n",
            "Epoch 268/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5847 - val_loss: 1.3471\n",
            "Epoch 269/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5460 - val_loss: 1.2645\n",
            "Epoch 270/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5085 - val_loss: 1.2917\n",
            "Epoch 271/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4514 - val_loss: 1.3434\n",
            "Epoch 272/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4773 - val_loss: 1.3075\n",
            "Epoch 273/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4662 - val_loss: 1.3172\n",
            "Epoch 274/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4191 - val_loss: 1.2828\n",
            "Epoch 275/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3169 - val_loss: 1.3019\n",
            "Epoch 276/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3281 - val_loss: 1.1368\n",
            "Epoch 277/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2424 - val_loss: 1.1502\n",
            "Epoch 278/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2150 - val_loss: 1.2302\n",
            "Epoch 279/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.1467 - val_loss: 0.9352\n",
            "Epoch 280/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.9899 - val_loss: 0.8254\n",
            "Epoch 281/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.0938 - val_loss: 0.9326\n",
            "Epoch 282/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2242 - val_loss: 1.0384\n",
            "Epoch 283/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.0115 - val_loss: 0.8904\n",
            "Epoch 284/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.9056 - val_loss: 0.7752\n",
            "Epoch 285/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.8369 - val_loss: 0.6932\n",
            "Epoch 286/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7824 - val_loss: 0.6502\n",
            "Epoch 287/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.8325 - val_loss: 0.6096\n",
            "Epoch 288/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7398 - val_loss: 0.5408\n",
            "Epoch 289/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6696 - val_loss: 0.5423\n",
            "Epoch 290/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6772 - val_loss: 0.9055\n",
            "Epoch 291/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5897 - val_loss: 0.7156\n",
            "Epoch 292/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6523 - val_loss: 0.5109\n",
            "Epoch 293/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5351 - val_loss: 0.4922\n",
            "Epoch 294/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5518 - val_loss: 0.4099\n",
            "Epoch 295/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5915 - val_loss: 0.6054\n",
            "Epoch 296/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5221 - val_loss: 0.6295\n",
            "Epoch 297/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5631 - val_loss: 0.6152\n",
            "Epoch 298/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4695 - val_loss: 0.4979\n",
            "Epoch 299/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4988 - val_loss: 0.5561\n",
            "Epoch 300/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5324 - val_loss: 0.5059\n",
            "Epoch 301/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5221 - val_loss: 0.6235\n",
            "Epoch 302/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4353 - val_loss: 0.4872\n",
            "Epoch 303/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4594 - val_loss: 0.5548\n",
            "Epoch 304/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4166 - val_loss: 0.3943\n",
            "Epoch 305/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4302 - val_loss: 0.5270\n",
            "Epoch 306/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5607 - val_loss: 0.4297\n",
            "Epoch 307/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4861 - val_loss: 0.3454\n",
            "Epoch 308/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5010 - val_loss: 0.3486\n",
            "Epoch 309/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4173 - val_loss: 0.4227\n",
            "Epoch 310/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4144 - val_loss: 0.5236\n",
            "Epoch 311/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4290 - val_loss: 0.5276\n",
            "Epoch 312/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4218 - val_loss: 0.4536\n",
            "Epoch 313/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4476 - val_loss: 0.7323\n",
            "Epoch 314/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3742 - val_loss: 0.5851\n",
            "Epoch 315/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4163 - val_loss: 0.6108\n",
            "Epoch 316/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3482 - val_loss: 0.4073\n",
            "Epoch 317/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4202 - val_loss: 0.4937\n",
            "Epoch 318/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4127 - val_loss: 0.4129\n",
            "Epoch 319/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4367 - val_loss: 0.3353\n",
            "Epoch 320/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4570 - val_loss: 0.3879\n",
            "Epoch 321/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4383 - val_loss: 0.3819\n",
            "Epoch 322/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3924 - val_loss: 0.3914\n",
            "Epoch 323/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3995 - val_loss: 0.4016\n",
            "Epoch 324/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4199 - val_loss: 0.5123\n",
            "Epoch 325/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3959 - val_loss: 0.4735\n",
            "Epoch 326/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3527 - val_loss: 0.5709\n",
            "Epoch 327/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5662 - val_loss: 0.4349\n",
            "Epoch 328/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4347 - val_loss: 0.2951\n",
            "Epoch 329/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4177 - val_loss: 0.4526\n",
            "Epoch 330/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3722 - val_loss: 0.2824\n",
            "Epoch 331/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3904 - val_loss: 0.5064\n",
            "Epoch 332/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3525 - val_loss: 0.4103\n",
            "Epoch 333/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3815 - val_loss: 0.3935\n",
            "Epoch 334/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3643 - val_loss: 0.4033\n",
            "Epoch 335/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3385 - val_loss: 0.3702\n",
            "Epoch 336/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3693 - val_loss: 0.5045\n",
            "Epoch 337/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3905 - val_loss: 0.4419\n",
            "Epoch 338/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4077 - val_loss: 0.5534\n",
            "Epoch 339/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4103 - val_loss: 0.3569\n",
            "Epoch 340/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3514 - val_loss: 1.0967\n",
            "Epoch 341/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4529 - val_loss: 0.5027\n",
            "Epoch 342/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3799 - val_loss: 0.4036\n",
            "Epoch 343/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3956 - val_loss: 0.3977\n",
            "Epoch 344/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3673 - val_loss: 0.4434\n",
            "Epoch 345/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3408 - val_loss: 0.7483\n",
            "Epoch 346/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3425 - val_loss: 0.6630\n",
            "Epoch 347/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3573 - val_loss: 0.5919\n",
            "Epoch 348/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3371 - val_loss: 0.4874\n",
            "Epoch 349/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3363 - val_loss: 0.5212\n",
            "Epoch 350/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3826 - val_loss: 0.4018\n",
            "Epoch 351/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3861 - val_loss: 0.3528\n",
            "Epoch 352/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3256 - val_loss: 0.5913\n",
            "Epoch 353/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3653 - val_loss: 0.4781\n",
            "Epoch 354/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3123 - val_loss: 0.5157\n",
            "Epoch 355/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3638 - val_loss: 0.5792\n",
            "Epoch 356/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3386 - val_loss: 0.3894\n",
            "Epoch 357/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3541 - val_loss: 0.4821\n",
            "Epoch 358/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3370 - val_loss: 0.5541\n",
            "Epoch 359/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3549 - val_loss: 0.9349\n",
            "Epoch 360/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3473 - val_loss: 0.3482\n",
            "Epoch 361/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3028 - val_loss: 0.6243\n",
            "Epoch 362/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3467 - val_loss: 0.4361\n",
            "Epoch 363/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3080 - val_loss: 0.3822\n",
            "Epoch 364/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3361 - val_loss: 0.2722\n",
            "Epoch 365/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2746 - val_loss: 0.8813\n",
            "Epoch 366/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3272 - val_loss: 0.6336\n",
            "Epoch 367/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2770 - val_loss: 0.5402\n",
            "Epoch 368/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3010 - val_loss: 0.3087\n",
            "Epoch 369/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3108 - val_loss: 0.6852\n",
            "Epoch 370/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2797 - val_loss: 0.7602\n",
            "Epoch 371/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2342 - val_loss: 1.2012\n",
            "Epoch 372/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3004 - val_loss: 0.4183\n",
            "Epoch 373/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3540 - val_loss: 0.5827\n",
            "Epoch 374/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3412 - val_loss: 0.6330\n",
            "Epoch 375/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3549 - val_loss: 0.5926\n",
            "Epoch 376/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2984 - val_loss: 0.5014\n",
            "Epoch 377/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3031 - val_loss: 0.3495\n",
            "Epoch 378/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3129 - val_loss: 0.5323\n",
            "Epoch 379/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3129 - val_loss: 0.5041\n",
            "Epoch 380/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2605 - val_loss: 0.5368\n",
            "Epoch 381/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2862 - val_loss: 0.7222\n",
            "Epoch 382/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2789 - val_loss: 0.3631\n",
            "Epoch 383/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3015 - val_loss: 0.4458\n",
            "Epoch 384/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2986 - val_loss: 0.4017\n",
            "Epoch 385/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3282 - val_loss: 0.3936\n",
            "Epoch 386/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2622 - val_loss: 0.4453\n",
            "Epoch 387/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2824 - val_loss: 0.5470\n",
            "Epoch 388/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2885 - val_loss: 0.6176\n",
            "Epoch 389/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2758 - val_loss: 0.5816\n",
            "Epoch 390/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4144 - val_loss: 0.7358\n",
            "Epoch 391/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2762 - val_loss: 0.4812\n",
            "Epoch 392/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2967 - val_loss: 0.4234\n",
            "Epoch 393/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2320 - val_loss: 0.5156\n",
            "Epoch 394/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2474 - val_loss: 0.4145\n",
            "Epoch 395/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4421 - val_loss: 0.5732\n",
            "Epoch 396/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3680 - val_loss: 0.6099\n",
            "Epoch 397/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2776 - val_loss: 0.7380\n",
            "Epoch 398/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3176 - val_loss: 0.6872\n",
            "Epoch 399/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2980 - val_loss: 0.6788\n",
            "Epoch 400/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2616 - val_loss: 0.6121\n",
            "Epoch 401/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3000 - val_loss: 0.3049\n",
            "Epoch 402/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2747 - val_loss: 0.5626\n",
            "Epoch 403/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2713 - val_loss: 0.6938\n",
            "Epoch 404/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3134 - val_loss: 0.4365\n",
            "Epoch 405/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2656 - val_loss: 0.5386\n",
            "Epoch 406/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2912 - val_loss: 0.4008\n",
            "Epoch 407/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2648 - val_loss: 0.5537\n",
            "Epoch 408/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2659 - val_loss: 0.8307\n",
            "Epoch 409/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2479 - val_loss: 0.7936\n",
            "Epoch 410/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2876 - val_loss: 0.2979\n",
            "Epoch 411/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2823 - val_loss: 0.6711\n",
            "Epoch 412/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3701 - val_loss: 0.4375\n",
            "Epoch 413/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3269 - val_loss: 0.6266\n",
            "Epoch 414/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3089 - val_loss: 0.5156\n",
            "Epoch 415/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2438 - val_loss: 0.7376\n",
            "Epoch 416/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2531 - val_loss: 0.4206\n",
            "Epoch 417/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2912 - val_loss: 0.5445\n",
            "Epoch 418/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2590 - val_loss: 0.5879\n",
            "Epoch 419/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3524 - val_loss: 0.7755\n",
            "Epoch 420/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3202 - val_loss: 0.5294\n",
            "Epoch 421/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2667 - val_loss: 0.5201\n",
            "Epoch 422/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2307 - val_loss: 0.5339\n",
            "Epoch 423/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2902 - val_loss: 0.3074\n",
            "Epoch 424/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2638 - val_loss: 0.4618\n",
            "Epoch 425/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2667 - val_loss: 0.6233\n",
            "Epoch 426/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2035 - val_loss: 0.4909\n",
            "Epoch 427/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2474 - val_loss: 0.4802\n",
            "Epoch 428/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2566 - val_loss: 0.3644\n",
            "Epoch 429/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2569 - val_loss: 0.8300\n",
            "Epoch 430/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2872 - val_loss: 0.4331\n",
            "Epoch 431/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2326 - val_loss: 0.6917\n",
            "Epoch 432/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1965 - val_loss: 0.5420\n",
            "Epoch 433/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2654 - val_loss: 0.3566\n",
            "Epoch 434/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2575 - val_loss: 0.5043\n",
            "Epoch 435/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2467 - val_loss: 0.4044\n",
            "Epoch 436/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2295 - val_loss: 0.5497\n",
            "Epoch 437/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2594 - val_loss: 0.7139\n",
            "Epoch 438/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2149 - val_loss: 0.6748\n",
            "Epoch 439/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3102 - val_loss: 0.3714\n",
            "Epoch 440/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2708 - val_loss: 0.3496\n",
            "Epoch 441/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2241 - val_loss: 1.0386\n",
            "Epoch 442/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2313 - val_loss: 1.0568\n",
            "Epoch 443/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2219 - val_loss: 0.2671\n",
            "Epoch 444/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2526 - val_loss: 0.4630\n",
            "Epoch 445/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2746 - val_loss: 0.4805\n",
            "Epoch 446/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2343 - val_loss: 0.3291\n",
            "Epoch 447/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3030 - val_loss: 0.4549\n",
            "Epoch 448/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3149 - val_loss: 0.4136\n",
            "Epoch 449/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5273 - val_loss: 0.2902\n",
            "Epoch 450/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3922 - val_loss: 0.5968\n",
            "Epoch 451/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2946 - val_loss: 0.6119\n",
            "Epoch 452/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3580 - val_loss: 0.5735\n",
            "Epoch 453/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2867 - val_loss: 0.5363\n",
            "Epoch 454/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2568 - val_loss: 0.5013\n",
            "Epoch 455/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2565 - val_loss: 0.3815\n",
            "Epoch 456/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2422 - val_loss: 0.6895\n",
            "Epoch 457/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2290 - val_loss: 0.4647\n",
            "Epoch 458/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2354 - val_loss: 0.8641\n",
            "Epoch 459/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2362 - val_loss: 0.4399\n",
            "Epoch 460/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2492 - val_loss: 0.4055\n",
            "Epoch 461/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2314 - val_loss: 0.3917\n",
            "Epoch 462/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2953 - val_loss: 0.5227\n",
            "Epoch 463/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3157 - val_loss: 0.4954\n",
            "Epoch 464/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3713 - val_loss: 0.5045\n",
            "Epoch 465/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3444 - val_loss: 0.8963\n",
            "Epoch 466/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2380 - val_loss: 0.3824\n",
            "Epoch 467/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2115 - val_loss: 0.3148\n",
            "Epoch 468/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4281 - val_loss: 0.7042\n",
            "Epoch 469/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2879 - val_loss: 0.5106\n",
            "Epoch 470/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2685 - val_loss: 0.2848\n",
            "Epoch 471/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2417 - val_loss: 0.2575\n",
            "Epoch 472/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2443 - val_loss: 0.4952\n",
            "Epoch 473/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2549 - val_loss: 0.3752\n",
            "Epoch 474/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2275 - val_loss: 0.3571\n",
            "Epoch 475/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2363 - val_loss: 0.4141\n",
            "Epoch 476/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3574 - val_loss: 0.3187\n",
            "Epoch 477/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3495 - val_loss: 0.3747\n",
            "Epoch 478/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2362 - val_loss: 0.2547\n",
            "Epoch 479/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2372 - val_loss: 0.3726\n",
            "Epoch 480/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2106 - val_loss: 0.3337\n",
            "Epoch 481/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2309 - val_loss: 0.6163\n",
            "Epoch 482/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2165 - val_loss: 0.4336\n",
            "Epoch 483/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1965 - val_loss: 0.3790\n",
            "Epoch 484/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2196 - val_loss: 0.4299\n",
            "Epoch 485/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2508 - val_loss: 0.6882\n",
            "Epoch 486/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2442 - val_loss: 0.6459\n",
            "Epoch 487/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2438 - val_loss: 0.6870\n",
            "Epoch 488/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2397 - val_loss: 0.8328\n",
            "Epoch 489/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2276 - val_loss: 0.3433\n",
            "Epoch 490/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2004 - val_loss: 0.3209\n",
            "Epoch 491/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2005 - val_loss: 0.4099\n",
            "Epoch 492/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2500 - val_loss: 0.3919\n",
            "Epoch 493/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2356 - val_loss: 0.2940\n",
            "Epoch 494/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2590 - val_loss: 0.4147\n",
            "Epoch 495/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1839 - val_loss: 0.3815\n",
            "Epoch 496/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2028 - val_loss: 0.3509\n",
            "Epoch 497/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3324 - val_loss: 0.2796\n",
            "Epoch 498/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2059 - val_loss: 0.4455\n",
            "Epoch 499/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2002 - val_loss: 0.7442\n",
            "Epoch 500/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1835 - val_loss: 0.3600\n",
            "Epoch 501/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2257 - val_loss: 0.3453\n",
            "Epoch 502/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2190 - val_loss: 0.5357\n",
            "Epoch 503/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2849 - val_loss: 0.3050\n",
            "Epoch 504/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2850 - val_loss: 0.6053\n",
            "Epoch 505/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2722 - val_loss: 0.2309\n",
            "Epoch 506/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4974 - val_loss: 0.7158\n",
            "Epoch 507/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2916 - val_loss: 0.3409\n",
            "Epoch 508/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2687 - val_loss: 0.4605\n",
            "Epoch 509/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1927 - val_loss: 0.3313\n",
            "Epoch 510/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1969 - val_loss: 0.3785\n",
            "Epoch 511/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2384 - val_loss: 0.2960\n",
            "Epoch 512/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3336 - val_loss: 0.3161\n",
            "Epoch 513/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1952 - val_loss: 0.4719\n",
            "Epoch 514/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1873 - val_loss: 0.2506\n",
            "Epoch 515/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4610 - val_loss: 0.4747\n",
            "Epoch 516/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6240 - val_loss: 0.3988\n",
            "Epoch 517/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3163 - val_loss: 0.3348\n",
            "Epoch 518/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2761 - val_loss: 0.5017\n",
            "Epoch 519/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2784 - val_loss: 0.5453\n",
            "Epoch 520/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2826 - val_loss: 0.3964\n",
            "Epoch 521/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3740 - val_loss: 0.3964\n",
            "Epoch 522/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3524 - val_loss: 0.1926\n",
            "Epoch 523/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3033 - val_loss: 0.2584\n",
            "Epoch 524/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2518 - val_loss: 0.3059\n",
            "Epoch 525/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2200 - val_loss: 0.3076\n",
            "Epoch 526/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2657 - val_loss: 0.2671\n",
            "Epoch 527/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2701 - val_loss: 0.4308\n",
            "Epoch 528/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2596 - val_loss: 0.2728\n",
            "Epoch 529/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2316 - val_loss: 0.7823\n",
            "Epoch 530/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2152 - val_loss: 0.5084\n",
            "Epoch 531/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1676 - val_loss: 0.4453\n",
            "Epoch 532/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2208 - val_loss: 0.2106\n",
            "Epoch 533/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1781 - val_loss: 0.3671\n",
            "Epoch 534/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2174 - val_loss: 0.3826\n",
            "Epoch 535/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2457 - val_loss: 0.4012\n",
            "Epoch 536/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2409 - val_loss: 0.2470\n",
            "Epoch 537/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1911 - val_loss: 0.2836\n",
            "Epoch 538/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3094 - val_loss: 0.3205\n",
            "Epoch 539/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3073 - val_loss: 0.7551\n",
            "Epoch 540/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2699 - val_loss: 0.2860\n",
            "Epoch 541/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2039 - val_loss: 0.7298\n",
            "Epoch 542/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2366 - val_loss: 0.6168\n",
            "Epoch 543/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2504 - val_loss: 0.2464\n",
            "Epoch 544/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2122 - val_loss: 0.5961\n",
            "Epoch 545/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1989 - val_loss: 0.2580\n",
            "Epoch 546/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2746 - val_loss: 0.5514\n",
            "Epoch 547/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1975 - val_loss: 0.3246\n",
            "Epoch 548/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2406 - val_loss: 0.2726\n",
            "Epoch 549/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2604 - val_loss: 0.4459\n",
            "Epoch 550/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2711 - val_loss: 0.3505\n",
            "Epoch 551/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1850 - val_loss: 0.3482\n",
            "Epoch 552/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2425 - val_loss: 0.3300\n",
            "Epoch 553/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2181 - val_loss: 0.3583\n",
            "Epoch 554/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2319 - val_loss: 0.2787\n",
            "Epoch 555/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1980 - val_loss: 0.3333\n",
            "Epoch 556/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3249 - val_loss: 0.4459\n",
            "Epoch 557/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2668 - val_loss: 0.5154\n",
            "Epoch 558/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1943 - val_loss: 0.4562\n",
            "Epoch 559/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1808 - val_loss: 0.3167\n",
            "Epoch 560/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2458 - val_loss: 0.4156\n",
            "Epoch 561/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2355 - val_loss: 0.4115\n",
            "Epoch 562/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2131 - val_loss: 0.4236\n",
            "Epoch 563/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2326 - val_loss: 0.4761\n",
            "Epoch 564/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2062 - val_loss: 0.3884\n",
            "Epoch 565/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1824 - val_loss: 0.4203\n",
            "Epoch 566/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2578 - val_loss: 0.3656\n",
            "Epoch 567/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2202 - val_loss: 0.2935\n",
            "Epoch 568/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2554 - val_loss: 0.3363\n",
            "Epoch 569/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1954 - val_loss: 0.5165\n",
            "Epoch 570/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2103 - val_loss: 0.2110\n",
            "Epoch 571/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2292 - val_loss: 0.5855\n",
            "Epoch 572/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2496 - val_loss: 0.3009\n",
            "Epoch 573/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2178 - val_loss: 0.3766\n",
            "Epoch 574/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2947 - val_loss: 0.3992\n",
            "Epoch 575/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2907 - val_loss: 0.3101\n",
            "Epoch 576/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2029 - val_loss: 0.3555\n",
            "Epoch 577/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2682 - val_loss: 0.3481\n",
            "Epoch 578/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2088 - val_loss: 0.4520\n",
            "Epoch 579/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2354 - val_loss: 0.2573\n",
            "Epoch 580/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2264 - val_loss: 0.3913\n",
            "Epoch 581/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2206 - val_loss: 0.3054\n",
            "Epoch 582/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2350 - val_loss: 0.4148\n",
            "Epoch 583/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2062 - val_loss: 0.2680\n",
            "Epoch 584/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2038 - val_loss: 0.2484\n",
            "Epoch 585/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1728 - val_loss: 0.2945\n",
            "Epoch 586/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2177 - val_loss: 0.3497\n",
            "Epoch 587/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2334 - val_loss: 0.2439\n",
            "Epoch 588/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2156 - val_loss: 0.3151\n",
            "Epoch 589/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1907 - val_loss: 0.3592\n",
            "Epoch 590/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2328 - val_loss: 0.2388\n",
            "Epoch 591/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2818 - val_loss: 0.2428\n",
            "Epoch 592/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3234 - val_loss: 0.1725\n",
            "Epoch 593/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2629 - val_loss: 0.3349\n",
            "Epoch 594/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2114 - val_loss: 0.6695\n",
            "Epoch 595/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2451 - val_loss: 0.5021\n",
            "Epoch 596/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1987 - val_loss: 0.4096\n",
            "Epoch 597/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1863 - val_loss: 0.2313\n",
            "Epoch 598/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1883 - val_loss: 0.6573\n",
            "Epoch 599/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2743 - val_loss: 0.4102\n",
            "Epoch 600/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4176 - val_loss: 1.2403\n",
            "Epoch 601/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2563 - val_loss: 0.4391\n",
            "Epoch 602/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1930 - val_loss: 0.3733\n",
            "Epoch 603/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2443 - val_loss: 0.2773\n",
            "Epoch 604/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3051 - val_loss: 0.3582\n",
            "Epoch 605/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2154 - val_loss: 0.1703\n",
            "Epoch 606/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1851 - val_loss: 0.3776\n",
            "Epoch 607/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1738 - val_loss: 0.2458\n",
            "Epoch 608/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2044 - val_loss: 0.3376\n",
            "Epoch 609/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4310 - val_loss: 0.2497\n",
            "Epoch 610/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3031 - val_loss: 0.3781\n",
            "Epoch 611/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2363 - val_loss: 0.5794\n",
            "Epoch 612/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2434 - val_loss: 0.2528\n",
            "Epoch 613/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2102 - val_loss: 0.3092\n",
            "Epoch 614/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2173 - val_loss: 0.4206\n",
            "Epoch 615/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2316 - val_loss: 0.1931\n",
            "Epoch 616/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2074 - val_loss: 0.2313\n",
            "Epoch 617/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2117 - val_loss: 0.3025\n",
            "Epoch 618/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2250 - val_loss: 0.3068\n",
            "Epoch 619/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3621 - val_loss: 0.5687\n",
            "Epoch 620/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2366 - val_loss: 0.3492\n",
            "Epoch 621/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2250 - val_loss: 0.1446\n",
            "Epoch 622/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2257 - val_loss: 0.2986\n",
            "Epoch 623/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1962 - val_loss: 0.2773\n",
            "Epoch 624/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2441 - val_loss: 0.2296\n",
            "Epoch 625/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2581 - val_loss: 0.4024\n",
            "Epoch 626/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2200 - val_loss: 0.2246\n",
            "Epoch 627/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1834 - val_loss: 0.2593\n",
            "Epoch 628/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1478 - val_loss: 0.2432\n",
            "Epoch 629/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2596 - val_loss: 0.5379\n",
            "Epoch 630/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1927 - val_loss: 0.2080\n",
            "Epoch 631/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1870 - val_loss: 0.1913\n",
            "Epoch 632/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2510 - val_loss: 0.4617\n",
            "Epoch 633/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2752 - val_loss: 0.3595\n",
            "Epoch 634/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2589 - val_loss: 0.2446\n",
            "Epoch 635/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2226 - val_loss: 0.4129\n",
            "Epoch 636/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1768 - val_loss: 0.3833\n",
            "Epoch 637/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1851 - val_loss: 0.2439\n",
            "Epoch 638/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1808 - val_loss: 0.2648\n",
            "Epoch 639/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1742 - val_loss: 0.1774\n",
            "Epoch 640/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2372 - val_loss: 0.1834\n",
            "Epoch 641/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1798 - val_loss: 0.2464\n",
            "Epoch 642/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1940 - val_loss: 0.1687\n",
            "Epoch 643/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4684 - val_loss: 0.2221\n",
            "Epoch 644/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3284 - val_loss: 0.3000\n",
            "Epoch 645/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2429 - val_loss: 0.2776\n",
            "Epoch 646/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3001 - val_loss: 0.2514\n",
            "Epoch 647/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2798 - val_loss: 0.4593\n",
            "Epoch 648/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2617 - val_loss: 0.2953\n",
            "Epoch 649/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2345 - val_loss: 0.2551\n",
            "Epoch 650/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1960 - val_loss: 0.3179\n",
            "Epoch 651/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1794 - val_loss: 0.6246\n",
            "Epoch 652/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1988 - val_loss: 0.2531\n",
            "Epoch 653/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2427 - val_loss: 0.7625\n",
            "Epoch 654/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2480 - val_loss: 0.1626\n",
            "Epoch 655/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1962 - val_loss: 0.1890\n",
            "Epoch 656/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1658 - val_loss: 0.2930\n",
            "Epoch 657/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1747 - val_loss: 0.3893\n",
            "Epoch 658/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1961 - val_loss: 0.3084\n",
            "Epoch 659/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1807 - val_loss: 0.3369\n",
            "Epoch 660/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1814 - val_loss: 0.2957\n",
            "Epoch 661/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1962 - val_loss: 0.4325\n",
            "Epoch 662/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1886 - val_loss: 0.2351\n",
            "Epoch 663/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2229 - val_loss: 0.2966\n",
            "Epoch 664/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2088 - val_loss: 0.2085\n",
            "Epoch 665/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1656 - val_loss: 0.2001\n",
            "Epoch 666/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1680 - val_loss: 0.2344\n",
            "Epoch 667/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1892 - val_loss: 0.2258\n",
            "Epoch 668/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1735 - val_loss: 0.4461\n",
            "Epoch 669/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2287 - val_loss: 0.1120\n",
            "Epoch 670/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2236 - val_loss: 0.1976\n",
            "Epoch 671/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1907 - val_loss: 0.1713\n",
            "Epoch 672/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2056 - val_loss: 0.4217\n",
            "Epoch 673/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2575 - val_loss: 0.1976\n",
            "Epoch 674/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1958 - val_loss: 0.5033\n",
            "Epoch 675/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2404 - val_loss: 0.1892\n",
            "Epoch 676/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1829 - val_loss: 0.2858\n",
            "Epoch 677/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1674 - val_loss: 0.3874\n",
            "Epoch 678/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2215 - val_loss: 0.2904\n",
            "Epoch 679/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2085 - val_loss: 0.2194\n",
            "Epoch 680/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1794 - val_loss: 0.2472\n",
            "Epoch 681/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2167 - val_loss: 0.1844\n",
            "Epoch 682/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1851 - val_loss: 0.4416\n",
            "Epoch 683/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1920 - val_loss: 0.3649\n",
            "Epoch 684/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1990 - val_loss: 0.2602\n",
            "Epoch 685/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2343 - val_loss: 0.1782\n",
            "Epoch 686/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1678 - val_loss: 0.1611\n",
            "Epoch 687/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1746 - val_loss: 0.1418\n",
            "Epoch 688/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1676 - val_loss: 0.1950\n",
            "Epoch 689/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2269 - val_loss: 0.2572\n",
            "Epoch 690/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1752 - val_loss: 0.4340\n",
            "Epoch 691/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1828 - val_loss: 0.1858\n",
            "Epoch 692/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1808 - val_loss: 0.1258\n",
            "Epoch 693/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1733 - val_loss: 0.2153\n",
            "Epoch 694/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1815 - val_loss: 0.2271\n",
            "Epoch 695/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2448 - val_loss: 0.2446\n",
            "Epoch 696/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3849 - val_loss: 0.1707\n",
            "Epoch 697/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1990 - val_loss: 0.5902\n",
            "Epoch 698/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1217 - val_loss: 0.4524\n",
            "Epoch 699/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2024 - val_loss: 0.3291\n",
            "Epoch 700/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1853 - val_loss: 0.1835\n",
            "Epoch 701/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6134 - val_loss: 1.2925\n",
            "Epoch 702/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3474 - val_loss: 0.4513\n",
            "Epoch 703/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3389 - val_loss: 0.2947\n",
            "Epoch 704/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2538 - val_loss: 0.1663\n",
            "Epoch 705/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1916 - val_loss: 0.2106\n",
            "Epoch 706/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1964 - val_loss: 0.2457\n",
            "Epoch 707/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2120 - val_loss: 0.0970\n",
            "Epoch 708/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1843 - val_loss: 0.1836\n",
            "Epoch 709/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1844 - val_loss: 0.1783\n",
            "Epoch 710/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1805 - val_loss: 0.2092\n",
            "Epoch 711/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1897 - val_loss: 0.1985\n",
            "Epoch 712/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2740 - val_loss: 0.1506\n",
            "Epoch 713/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1792 - val_loss: 0.2105\n",
            "Epoch 714/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1806 - val_loss: 0.3035\n",
            "Epoch 715/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1730 - val_loss: 0.3859\n",
            "Epoch 716/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1424 - val_loss: 0.3027\n",
            "Epoch 717/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1781 - val_loss: 0.2129\n",
            "Epoch 718/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2318 - val_loss: 0.1913\n",
            "Epoch 719/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1784 - val_loss: 0.2637\n",
            "Epoch 720/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1831 - val_loss: 0.1281\n",
            "Epoch 721/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1720 - val_loss: 0.1279\n",
            "Epoch 722/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1461 - val_loss: 0.1320\n",
            "Epoch 723/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2261 - val_loss: 0.1531\n",
            "Epoch 724/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2236 - val_loss: 0.2257\n",
            "Epoch 725/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1882 - val_loss: 0.2064\n",
            "Epoch 726/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1894 - val_loss: 0.2627\n",
            "Epoch 727/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2902 - val_loss: 0.2337\n",
            "Epoch 728/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3060 - val_loss: 0.2485\n",
            "Epoch 729/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1854 - val_loss: 0.2060\n",
            "Epoch 730/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2794 - val_loss: 0.2117\n",
            "Epoch 731/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1921 - val_loss: 0.3477\n",
            "Epoch 732/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2181 - val_loss: 0.3199\n",
            "Epoch 733/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2012 - val_loss: 0.2396\n",
            "Epoch 734/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1791 - val_loss: 0.2352\n",
            "Epoch 735/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1524 - val_loss: 0.2503\n",
            "Epoch 736/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1518 - val_loss: 0.1366\n",
            "Epoch 737/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1838 - val_loss: 0.1931\n",
            "Epoch 738/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2024 - val_loss: 0.1583\n",
            "Epoch 739/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2186 - val_loss: 0.2727\n",
            "Epoch 740/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1973 - val_loss: 0.2647\n",
            "Epoch 741/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2103 - val_loss: 0.1864\n",
            "Epoch 742/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2585 - val_loss: 0.3132\n",
            "Epoch 743/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1951 - val_loss: 0.2248\n",
            "Epoch 744/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2429 - val_loss: 0.1727\n",
            "Epoch 745/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2117 - val_loss: 0.2655\n",
            "Epoch 746/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1471 - val_loss: 0.1796\n",
            "Epoch 747/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1490 - val_loss: 0.1788\n",
            "Epoch 748/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1951 - val_loss: 0.1243\n",
            "Epoch 749/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1821 - val_loss: 0.1987\n",
            "Epoch 750/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2683 - val_loss: 0.1977\n",
            "Epoch 751/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2422 - val_loss: 0.1545\n",
            "Epoch 752/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2019 - val_loss: 0.3165\n",
            "Epoch 753/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1840 - val_loss: 0.2208\n",
            "Epoch 754/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2064 - val_loss: 0.1907\n",
            "Epoch 755/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2066 - val_loss: 0.1255\n",
            "Epoch 756/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2187 - val_loss: 0.1817\n",
            "Epoch 757/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1768 - val_loss: 0.1573\n",
            "Epoch 758/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1547 - val_loss: 0.1959\n",
            "Epoch 759/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1741 - val_loss: 0.2357\n",
            "Epoch 760/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2227 - val_loss: 0.2285\n",
            "Epoch 761/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1641 - val_loss: 0.3499\n",
            "Epoch 762/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1391 - val_loss: 0.1450\n",
            "Epoch 763/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1622 - val_loss: 0.2914\n",
            "Epoch 764/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2753 - val_loss: 0.4448\n",
            "Epoch 765/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2097 - val_loss: 0.1928\n",
            "Epoch 766/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1587 - val_loss: 0.1417\n",
            "Epoch 767/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2544 - val_loss: 0.1747\n",
            "Epoch 768/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1609 - val_loss: 0.2228\n",
            "Epoch 769/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1377 - val_loss: 0.1667\n",
            "Epoch 770/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1400 - val_loss: 0.1229\n",
            "Epoch 771/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1566 - val_loss: 0.1374\n",
            "Epoch 772/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2414 - val_loss: 0.2791\n",
            "Epoch 773/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1556 - val_loss: 0.1254\n",
            "Epoch 774/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2741 - val_loss: 0.3060\n",
            "Epoch 775/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2228 - val_loss: 0.1600\n",
            "Epoch 776/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2364 - val_loss: 0.1683\n",
            "Epoch 777/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2011 - val_loss: 0.1464\n",
            "Epoch 778/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1826 - val_loss: 0.2482\n",
            "Epoch 779/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2336 - val_loss: 0.1926\n",
            "Epoch 780/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2070 - val_loss: 0.1757\n",
            "Epoch 781/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1819 - val_loss: 0.2449\n",
            "Epoch 782/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1710 - val_loss: 0.1884\n",
            "Epoch 783/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2305 - val_loss: 0.1669\n",
            "Epoch 784/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2250 - val_loss: 0.1931\n",
            "Epoch 785/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4098 - val_loss: 0.1391\n",
            "Epoch 786/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2382 - val_loss: 0.1291\n",
            "Epoch 787/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2100 - val_loss: 0.2147\n",
            "Epoch 788/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1918 - val_loss: 0.2213\n",
            "Epoch 789/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1687 - val_loss: 0.2062\n",
            "Epoch 790/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2074 - val_loss: 0.1241\n",
            "Epoch 791/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2130 - val_loss: 0.0839\n",
            "Epoch 792/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2654 - val_loss: 0.1858\n",
            "Epoch 793/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2595 - val_loss: 0.1362\n",
            "Epoch 794/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2239 - val_loss: 0.1736\n",
            "Epoch 795/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1965 - val_loss: 0.1940\n",
            "Epoch 796/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2368 - val_loss: 0.1547\n",
            "Epoch 797/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1876 - val_loss: 0.0568\n",
            "Epoch 798/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1755 - val_loss: 0.1144\n",
            "Epoch 799/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2660 - val_loss: 0.1910\n",
            "Epoch 800/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2218 - val_loss: 0.1379\n",
            "Epoch 801/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1886 - val_loss: 0.1370\n",
            "Epoch 802/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2059 - val_loss: 0.1226\n",
            "Epoch 803/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1588 - val_loss: 0.3019\n",
            "Epoch 804/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1487 - val_loss: 0.1492\n",
            "Epoch 805/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1654 - val_loss: 0.2889\n",
            "Epoch 806/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2072 - val_loss: 0.1668\n",
            "Epoch 807/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1834 - val_loss: 0.1318\n",
            "Epoch 808/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1805 - val_loss: 0.3090\n",
            "Epoch 809/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1706 - val_loss: 0.1769\n",
            "Epoch 810/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1472 - val_loss: 0.1235\n",
            "Epoch 811/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2039 - val_loss: 0.1490\n",
            "Epoch 812/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1789 - val_loss: 0.1832\n",
            "Epoch 813/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1818 - val_loss: 0.1336\n",
            "Epoch 814/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2968 - val_loss: 0.1885\n",
            "Epoch 815/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1975 - val_loss: 0.1880\n",
            "Epoch 816/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1850 - val_loss: 0.1120\n",
            "Epoch 817/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1730 - val_loss: 0.0834\n",
            "Epoch 818/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1791 - val_loss: 0.1143\n",
            "Epoch 819/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1620 - val_loss: 0.1738\n",
            "Epoch 820/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1924 - val_loss: 0.2081\n",
            "Epoch 821/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2959 - val_loss: 0.1868\n",
            "Epoch 822/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3149 - val_loss: 0.1529\n",
            "Epoch 823/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2699 - val_loss: 0.1212\n",
            "Epoch 824/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2557 - val_loss: 0.4124\n",
            "Epoch 825/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2412 - val_loss: 0.2866\n",
            "Epoch 826/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2150 - val_loss: 0.1225\n",
            "Epoch 827/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1831 - val_loss: 0.1543\n",
            "Epoch 828/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2384 - val_loss: 0.1942\n",
            "Epoch 829/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1457 - val_loss: 0.1410\n",
            "Epoch 830/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1940 - val_loss: 0.1499\n",
            "Epoch 831/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1934 - val_loss: 0.3086\n",
            "Epoch 832/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1495 - val_loss: 0.1841\n",
            "Epoch 833/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1965 - val_loss: 0.2860\n",
            "Epoch 834/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1778 - val_loss: 0.2512\n",
            "Epoch 835/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2516 - val_loss: 0.1082\n",
            "Epoch 836/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1948 - val_loss: 0.1956\n",
            "Epoch 837/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1855 - val_loss: 0.1173\n",
            "Epoch 838/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2969 - val_loss: 0.1180\n",
            "Epoch 839/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2590 - val_loss: 0.2182\n",
            "Epoch 840/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2308 - val_loss: 0.1589\n",
            "Epoch 841/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2775 - val_loss: 0.1455\n",
            "Epoch 842/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1955 - val_loss: 0.2120\n",
            "Epoch 843/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1770 - val_loss: 0.1990\n",
            "Epoch 844/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2801 - val_loss: 0.1675\n",
            "Epoch 845/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2543 - val_loss: 0.1231\n",
            "Epoch 846/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2068 - val_loss: 0.1343\n",
            "Epoch 847/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2027 - val_loss: 0.1329\n",
            "Epoch 848/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1870 - val_loss: 0.1436\n",
            "Epoch 849/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1677 - val_loss: 0.1880\n",
            "Epoch 850/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1728 - val_loss: 0.1553\n",
            "Epoch 851/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1732 - val_loss: 0.1632\n",
            "Epoch 852/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1883 - val_loss: 0.1885\n",
            "Epoch 853/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1668 - val_loss: 0.1514\n",
            "Epoch 854/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1694 - val_loss: 0.1338\n",
            "Epoch 855/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2015 - val_loss: 0.1611\n",
            "Epoch 856/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1917 - val_loss: 0.1065\n",
            "Epoch 857/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1930 - val_loss: 0.0990\n",
            "Epoch 858/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1644 - val_loss: 0.1613\n",
            "Epoch 859/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1540 - val_loss: 0.1214\n",
            "Epoch 860/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2180 - val_loss: 0.1610\n",
            "Epoch 861/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1753 - val_loss: 0.1744\n",
            "Epoch 862/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1906 - val_loss: 0.2912\n",
            "Epoch 863/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1469 - val_loss: 0.1728\n",
            "Epoch 864/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1599 - val_loss: 0.1682\n",
            "Epoch 865/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1814 - val_loss: 0.3185\n",
            "Epoch 866/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3260 - val_loss: 0.7977\n",
            "Epoch 867/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6849 - val_loss: 0.2559\n",
            "Epoch 868/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2946 - val_loss: 0.1832\n",
            "Epoch 869/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2226 - val_loss: 0.3042\n",
            "Epoch 870/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1677 - val_loss: 0.1952\n",
            "Epoch 871/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1879 - val_loss: 0.0776\n",
            "Epoch 872/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1530 - val_loss: 0.2729\n",
            "Epoch 873/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2165 - val_loss: 0.0824\n",
            "Epoch 874/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2173 - val_loss: 0.2519\n",
            "Epoch 875/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2295 - val_loss: 0.1029\n",
            "Epoch 876/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1687 - val_loss: 0.1341\n",
            "Epoch 877/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1545 - val_loss: 0.1320\n",
            "Epoch 878/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1653 - val_loss: 0.1128\n",
            "Epoch 879/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1909 - val_loss: 0.1057\n",
            "Epoch 880/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2193 - val_loss: 0.2667\n",
            "Epoch 881/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2126 - val_loss: 0.1362\n",
            "Epoch 882/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1861 - val_loss: 0.1692\n",
            "Epoch 883/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1403 - val_loss: 0.1835\n",
            "Epoch 884/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1496 - val_loss: 0.2082\n",
            "Epoch 885/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2444 - val_loss: 0.1930\n",
            "Epoch 886/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2277 - val_loss: 0.2259\n",
            "Epoch 887/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2431 - val_loss: 0.1285\n",
            "Epoch 888/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1891 - val_loss: 0.1193\n",
            "Epoch 889/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1726 - val_loss: 0.1197\n",
            "Epoch 890/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1953 - val_loss: 0.1939\n",
            "Epoch 891/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1713 - val_loss: 0.2122\n",
            "Epoch 892/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1490 - val_loss: 0.1010\n",
            "Epoch 893/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1376 - val_loss: 0.1600\n",
            "Epoch 894/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2474 - val_loss: 0.2446\n",
            "Epoch 895/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2236 - val_loss: 0.1003\n",
            "Epoch 896/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1826 - val_loss: 0.1093\n",
            "Epoch 897/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1662 - val_loss: 0.1530\n",
            "Epoch 898/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1581 - val_loss: 0.1437\n",
            "Epoch 899/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1314 - val_loss: 0.3055\n",
            "Epoch 900/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2097 - val_loss: 0.1293\n",
            "Epoch 901/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1990 - val_loss: 0.1574\n",
            "Epoch 902/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1529 - val_loss: 0.1685\n",
            "Epoch 903/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1886 - val_loss: 0.1403\n",
            "Epoch 904/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1964 - val_loss: 0.1272\n",
            "Epoch 905/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2440 - val_loss: 0.1617\n",
            "Epoch 906/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1730 - val_loss: 0.1218\n",
            "Epoch 907/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2640 - val_loss: 0.2953\n",
            "Epoch 908/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2075 - val_loss: 0.1564\n",
            "Epoch 909/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1758 - val_loss: 0.0900\n",
            "Epoch 910/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1438 - val_loss: 0.0951\n",
            "Epoch 911/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2903 - val_loss: 0.2302\n",
            "Epoch 912/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2806 - val_loss: 0.1299\n",
            "Epoch 913/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1881 - val_loss: 0.1336\n",
            "Epoch 914/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1597 - val_loss: 0.1596\n",
            "Epoch 915/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1820 - val_loss: 0.1096\n",
            "Epoch 916/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1341 - val_loss: 0.1506\n",
            "Epoch 917/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1677 - val_loss: 0.1668\n",
            "Epoch 918/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1641 - val_loss: 0.1045\n",
            "Epoch 919/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2085 - val_loss: 0.0995\n",
            "Epoch 920/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1733 - val_loss: 0.1083\n",
            "Epoch 921/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2413 - val_loss: 0.1436\n",
            "Epoch 922/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2077 - val_loss: 0.1664\n",
            "Epoch 923/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1713 - val_loss: 0.1918\n",
            "Epoch 924/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1695 - val_loss: 0.1237\n",
            "Epoch 925/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1448 - val_loss: 0.1331\n",
            "Epoch 926/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1475 - val_loss: 0.1084\n",
            "Epoch 927/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1829 - val_loss: 0.1393\n",
            "Epoch 928/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1806 - val_loss: 0.1735\n",
            "Epoch 929/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1471 - val_loss: 0.1548\n",
            "Epoch 930/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1681 - val_loss: 0.2788\n",
            "Epoch 931/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1483 - val_loss: 0.2043\n",
            "Epoch 932/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1708 - val_loss: 0.0922\n",
            "Epoch 933/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1738 - val_loss: 0.1246\n",
            "Epoch 934/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2158 - val_loss: 0.2161\n",
            "Epoch 935/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1684 - val_loss: 0.3228\n",
            "Epoch 936/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2095 - val_loss: 0.1450\n",
            "Epoch 937/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1948 - val_loss: 0.1857\n",
            "Epoch 938/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1812 - val_loss: 0.1365\n",
            "Epoch 939/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1321 - val_loss: 0.1448\n",
            "Epoch 940/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2068 - val_loss: 0.1581\n",
            "Epoch 941/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2042 - val_loss: 0.1232\n",
            "Epoch 942/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1924 - val_loss: 0.1500\n",
            "Epoch 943/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1406 - val_loss: 0.0982\n",
            "Epoch 944/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1820 - val_loss: 0.1103\n",
            "Epoch 945/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1526 - val_loss: 0.1128\n",
            "Epoch 946/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2121 - val_loss: 0.1745\n",
            "Epoch 947/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2602 - val_loss: 0.1706\n",
            "Epoch 948/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1334 - val_loss: 0.1864\n",
            "Epoch 949/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2300 - val_loss: 0.2618\n",
            "Epoch 950/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1701 - val_loss: 0.3626\n",
            "Epoch 951/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1663 - val_loss: 0.1468\n",
            "Epoch 952/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1700 - val_loss: 0.1083\n",
            "Epoch 953/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1369 - val_loss: 0.1915\n",
            "Epoch 954/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1529 - val_loss: 0.2631\n",
            "Epoch 955/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1676 - val_loss: 0.1022\n",
            "Epoch 956/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2540 - val_loss: 0.4905\n",
            "Epoch 957/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3571 - val_loss: 0.2356\n",
            "Epoch 958/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2578 - val_loss: 0.1867\n",
            "Epoch 959/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2160 - val_loss: 0.1002\n",
            "Epoch 960/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2016 - val_loss: 0.2958\n",
            "Epoch 961/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2301 - val_loss: 0.1421\n",
            "Epoch 962/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1777 - val_loss: 0.1547\n",
            "Epoch 963/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2007 - val_loss: 0.1523\n",
            "Epoch 964/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2097 - val_loss: 0.2570\n",
            "Epoch 965/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1691 - val_loss: 0.1938\n",
            "Epoch 966/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1539 - val_loss: 0.1421\n",
            "Epoch 967/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1798 - val_loss: 0.1818\n",
            "Epoch 968/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1987 - val_loss: 0.2267\n",
            "Epoch 969/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1847 - val_loss: 0.1540\n",
            "Epoch 970/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1720 - val_loss: 0.1498\n",
            "Epoch 971/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1840 - val_loss: 0.1890\n",
            "Epoch 972/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2078 - val_loss: 0.1280\n",
            "Epoch 973/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1893 - val_loss: 0.1278\n",
            "Epoch 974/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2277 - val_loss: 0.1137\n",
            "Epoch 975/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1805 - val_loss: 0.0899\n",
            "Epoch 976/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1384 - val_loss: 0.1238\n",
            "Epoch 977/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1642 - val_loss: 0.1003\n",
            "Epoch 978/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1481 - val_loss: 0.1646\n",
            "Epoch 979/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1956 - val_loss: 0.2485\n",
            "Epoch 980/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2046 - val_loss: 0.0871\n",
            "Epoch 981/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1631 - val_loss: 0.1990\n",
            "Epoch 982/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1470 - val_loss: 0.2420\n",
            "Epoch 983/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1388 - val_loss: 0.2055\n",
            "Epoch 984/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2089 - val_loss: 0.1891\n",
            "Epoch 985/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1607 - val_loss: 0.1704\n",
            "Epoch 986/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1719 - val_loss: 0.1029\n",
            "Epoch 987/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1669 - val_loss: 0.1675\n",
            "Epoch 988/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2717 - val_loss: 0.1545\n",
            "Epoch 989/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2898 - val_loss: 0.1353\n",
            "Epoch 990/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2270 - val_loss: 0.1470\n",
            "Epoch 991/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1893 - val_loss: 0.1342\n",
            "Epoch 992/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1869 - val_loss: 0.0850\n",
            "Epoch 993/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1434 - val_loss: 0.1348\n",
            "Epoch 994/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1971 - val_loss: 0.1047\n",
            "Epoch 995/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2023 - val_loss: 0.1934\n",
            "Epoch 996/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1850 - val_loss: 0.1644\n",
            "Epoch 997/10000\n",
            " 1/33 [..............................] - ETA: 0s - loss: 0.0627Restoring model weights from the end of the best epoch: 797.\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2093 - val_loss: 0.1410\n",
            "Epoch 997: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_1912349/3424320398.py:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.056783467531204224\n",
            "Train loss: 0.18755559623241425\n",
            "Test loss: 0.1438456028699875\n",
            "dO18 RMSE: 0.2464965241797656\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0          26.130            0.19025\n",
            "1          26.984            0.40123\n",
            "2          24.656            0.17728\n",
            "3          26.356            0.03953\n",
            "4          23.962            0.30992\n",
            "5          24.848            0.15842\n",
            "6          25.080            0.05125\n",
            "7          26.546            2.14378\n",
            "8          25.682            0.94472\n",
            "9          24.028            0.45852\n",
            "10         23.944            0.15813\n",
            "11         23.752            0.52437\n",
            "12         26.018            0.96417\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       26.159655           0.357570\n",
            "1       27.060946           0.498601\n",
            "2       24.464302           0.311572\n",
            "3       26.079882           0.274227\n",
            "4       23.860586           0.474837\n",
            "5       24.961367           0.199061\n",
            "6       25.182415           0.133716\n",
            "7       26.983627           4.187814\n",
            "8       25.443577           0.918894\n",
            "9       24.174910           0.476869\n",
            "10      24.054922           0.376187\n",
            "11      23.220125           0.657521\n",
            "12      26.285080           1.128190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-02 22:15:02.547774: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [13,11]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/usr/local/google/home/ruru/Downloads/model_builds/random_all_boosted_transformer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Grouped, fixed, all columns\n",
        "\n",
        "We can't (easily) generate isoscapes with these because the isoscapes for 'predkrig_br_lat_ISORG', 'Iso_Oxi_Stack_mean_TERZER',\n",
        "                   'isoscape_fullmodel_d18O_prec_REGRESSION' are not easily retrievable... but I'm curious how much better the model is if these columns are included."
      ],
      "metadata": {
        "id": "wPDSSm__DR53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_fixed_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/kriging_ensemble_experiment/uc_davis_train_fixed_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/kriging_ensemble_experiment/uc_davis_test_fixed_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/kriging_ensemble_experiment/uc_davis_validation_fixed_grouped.csv'),\n",
        "}\n",
        "\n",
        "\n",
        "columns_to_keep = ['d18O_cel_mean', 'd18O_cel_variance',\n",
        "                   'lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "                   'Mean Annual Temperature','Mean Annual Precipitation',\n",
        "                   'predkrig_br_lat_ISORG', 'Iso_Oxi_Stack_mean_TERZER',\n",
        "                   'isoscape_fullmodel_d18O_prec_REGRESSION',\n",
        "                   'ordinary_kriging_linear_d18O_predicted_mean'\n",
        "                   'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "\n",
        "data = load_and_scale(grouped_fixed_fileset, columns_to_keep)\n",
        "model = train_and_evaluate(data, \"grouped_fixed_all\", training_batch_size=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V9_5iUkDVoCV",
        "outputId": "32a38a74-12ae-4de2-8756-24a02bd4b931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================\n",
            "grouped_fixed_all\n",
            "Model: \"model_51\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_56 (InputLayer)          [(None, 14)]         0           []                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_123 (  (None, 12)          0           ['input_56[0][0]']               \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " dense_102 (Dense)              (None, 20)           260         ['tf.__operators__.getitem_123[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " dense_103 (Dense)              (None, 20)           420         ['dense_102[0][0]']              \n",
            "                                                                                                  \n",
            " var_output (Dense)             (None, 1)            21          ['dense_103[0][0]']              \n",
            "                                                                                                  \n",
            " mean_output (Dense)            (None, 1)            21          ['dense_103[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_103 (TFOpLamb  (None, 1)           0           ['var_output[0][0]']             \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_121 (  (None,)             0           ['input_56[0][0]']               \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " tf.math.multiply_102 (TFOpLamb  (None, 1)           0           ['mean_output[0][0]']            \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_122 (  (None,)             0           ['input_56[0][0]']               \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " tf.__operators__.add_176 (TFOp  (None, 1)           0           ['tf.math.multiply_103[0][0]']   \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " tf.expand_dims_24 (TFOpLambda)  (None, 1)           0           ['tf.__operators__.getitem_121[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_174 (TFOp  (None, 1)           0           ['tf.math.multiply_102[0][0]']   \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " tf.expand_dims_25 (TFOpLambda)  (None, 1)           0           ['tf.__operators__.getitem_122[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " lambda_49 (Lambda)             (None, 1)            0           ['tf.__operators__.add_176[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_175 (TFOp  (None, 1)           0           ['tf.expand_dims_24[0][0]',      \n",
            " Lambda)                                                          'tf.__operators__.add_174[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_177 (TFOp  (None, 1)           0           ['tf.expand_dims_25[0][0]',      \n",
            " Lambda)                                                          'lambda_49[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_51 (Concatenate)   (None, 2)            0           ['tf.__operators__.add_175[0][0]'\n",
            "                                                                 , 'tf.__operators__.add_177[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 722\n",
            "Trainable params: 722\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Restoring model weights from the end of the best epoch: 1056.\n",
            "Epoch 2056: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_3673974/1016735986.py:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 1.133312702178955\n",
            "Train loss: 0.8401697278022766\n",
            "Test loss: 4.44401216506958\n",
            "dO18 RMSE: 4.1899053785054035\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       24.042000           0.345970\n",
            "1       25.240000           0.035950\n",
            "2       25.782000           0.372220\n",
            "3       25.076000           0.230280\n",
            "4       25.966000           0.172480\n",
            "5       27.434000           0.501030\n",
            "6       28.156000           0.999030\n",
            "7       26.836000           0.120880\n",
            "8       28.180000           0.778250\n",
            "9       26.834000           0.094930\n",
            "10      26.644000           0.488430\n",
            "11      26.772000           0.373370\n",
            "12      27.684280           1.216389\n",
            "13      27.403235           0.892280\n",
            "14      24.777670           0.736571\n",
            "15      25.551850           0.312355\n",
            "16      25.115885           0.033519\n",
            "17      25.987815           5.280825\n",
            "18      24.132031           0.389042\n",
            "19      24.898000           0.236070\n",
            "20      23.944000           0.158130\n",
            "21      26.018000           0.964170\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       25.380455           3.068546\n",
            "1       25.380455           3.068546\n",
            "2       25.380455           3.068546\n",
            "3       25.380455           3.068546\n",
            "4       25.380455           3.068546\n",
            "5       21.008560           3.366832\n",
            "6       21.067062           3.364165\n",
            "7       21.067062           3.364165\n",
            "8       21.067062           3.364165\n",
            "9       21.067062           3.364165\n",
            "10      21.067062           3.364165\n",
            "11      21.067062           3.364165\n",
            "12      21.008560           3.366832\n",
            "13      21.067062           3.364165\n",
            "14      23.934954           3.302485\n",
            "15      22.905151           3.379938\n",
            "16      22.930801           3.374500\n",
            "17      22.811020           3.392226\n",
            "18      22.910528           3.377648\n",
            "19      25.380455           3.068546\n",
            "20      25.380455           3.068546\n",
            "21      25.380455           3.068546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-01 22:20:57.439759: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [22,14]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Grouped fixed, ablating other columns besides krigin"
      ],
      "metadata": {
        "id": "h1uuaygyDvXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_fixed_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/kriging_ensemble_experiment/uc_davis_train_fixed_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/kriging_ensemble_experiment/uc_davis_test_fixed_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/kriging_ensemble_experiment/uc_davis_validation_fixed_grouped.csv'),\n",
        "}\n",
        "\n",
        "\n",
        "columns_to_keep = ['d18O_cel_mean', 'd18O_cel_variance',\n",
        "                   'lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "                   'Mean Annual Temperature','Mean Annual Precipitation',\n",
        "                   'ordinary_kriging_linear_d18O_predicted_mean'\n",
        "                   'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "\n",
        "data = load_and_scale(grouped_fixed_fileset, columns_to_keep)\n",
        "model = train_and_evaluate(data, \"grouped_fixed_ablated\", training_batch_size=3)"
      ],
      "metadata": {
        "id": "WQ60MVzlD5DJ",
        "outputId": "a962f398-b406-49b0-9523-b7a1bed7f4c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================\n",
            "grouped_fixed_ablated\n",
            "Model: \"model_52\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_57 (InputLayer)          [(None, 11)]         0           []                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_126 (  (None, 9)           0           ['input_57[0][0]']               \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " dense_104 (Dense)              (None, 20)           200         ['tf.__operators__.getitem_126[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " dense_105 (Dense)              (None, 20)           420         ['dense_104[0][0]']              \n",
            "                                                                                                  \n",
            " var_output (Dense)             (None, 1)            21          ['dense_105[0][0]']              \n",
            "                                                                                                  \n",
            " mean_output (Dense)            (None, 1)            21          ['dense_105[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_105 (TFOpLamb  (None, 1)           0           ['var_output[0][0]']             \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_124 (  (None,)             0           ['input_57[0][0]']               \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " tf.math.multiply_104 (TFOpLamb  (None, 1)           0           ['mean_output[0][0]']            \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_125 (  (None,)             0           ['input_57[0][0]']               \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " tf.__operators__.add_180 (TFOp  (None, 1)           0           ['tf.math.multiply_105[0][0]']   \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " tf.expand_dims_26 (TFOpLambda)  (None, 1)           0           ['tf.__operators__.getitem_124[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_178 (TFOp  (None, 1)           0           ['tf.math.multiply_104[0][0]']   \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " tf.expand_dims_27 (TFOpLambda)  (None, 1)           0           ['tf.__operators__.getitem_125[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " lambda_50 (Lambda)             (None, 1)            0           ['tf.__operators__.add_180[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_179 (TFOp  (None, 1)           0           ['tf.expand_dims_26[0][0]',      \n",
            " Lambda)                                                          'tf.__operators__.add_178[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_181 (TFOp  (None, 1)           0           ['tf.expand_dims_27[0][0]',      \n",
            " Lambda)                                                          'lambda_50[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_52 (Concatenate)   (None, 2)            0           ['tf.__operators__.add_179[0][0]'\n",
            "                                                                 , 'tf.__operators__.add_181[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 662\n",
            "Trainable params: 662\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Restoring model weights from the end of the best epoch: 3025.\n",
            "Epoch 4025: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_3673974/1016735986.py:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 6.73103666305542\n",
            "Train loss: 0.8037930130958557\n",
            "Test loss: 19.681373596191406\n",
            "dO18 RMSE: 7.362366926203444\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       24.042000           0.345970\n",
            "1       25.240000           0.035950\n",
            "2       25.782000           0.372220\n",
            "3       25.076000           0.230280\n",
            "4       25.966000           0.172480\n",
            "5       27.434000           0.501030\n",
            "6       28.156000           0.999030\n",
            "7       26.836000           0.120880\n",
            "8       28.180000           0.778250\n",
            "9       26.834000           0.094930\n",
            "10      26.644000           0.488430\n",
            "11      26.772000           0.373370\n",
            "12      27.684280           1.216389\n",
            "13      27.403235           0.892280\n",
            "14      24.777670           0.736571\n",
            "15      25.551850           0.312355\n",
            "16      25.115885           0.033519\n",
            "17      25.987815           5.280825\n",
            "18      24.132031           0.389042\n",
            "19      24.898000           0.236070\n",
            "20      23.944000           0.158130\n",
            "21      26.018000           0.964170\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       33.559029           1.854698\n",
            "1       33.559029           1.854698\n",
            "2       33.559029           1.854698\n",
            "3       33.559029           1.854698\n",
            "4       33.559029           1.854698\n",
            "5       28.666233           2.020664\n",
            "6       29.298073           2.026066\n",
            "7       29.298073           2.026066\n",
            "8       29.298073           2.026066\n",
            "9       29.298073           2.026066\n",
            "10      29.298073           2.026066\n",
            "11      29.298073           2.026066\n",
            "12      28.666233           2.020664\n",
            "13      29.298073           2.026066\n",
            "14      37.236214           1.750267\n",
            "15      35.552994           1.771863\n",
            "16      35.551338           1.773102\n",
            "17      35.455872           1.769845\n",
            "18      35.547958           1.772609\n",
            "19      33.559029           1.854697\n",
            "20      33.559029           1.854697\n",
            "21      33.559029           1.854697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-01 22:24:26.463319: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [22,11]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}