{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnc-br/ddf-isoscapes/blob/boosting/dnn/variational_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational model\n",
        "\n",
        "Find the mean/variance of O18 ratios (as well as N15 and C13 in the future) at a particular lat/lon across Brazil. At the bottom of the colab, train and evaluate 4 different versions of the model with different data partitioning strategies."
      ],
      "metadata": {
        "id": "-0IfT3kGwgK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "henIPlAPCb4i",
        "outputId": "0c36e5fd-e1c5-46f8-89ea-c38bd70efee8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-03 20:38:16.877286: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-08-03 20:38:16.912915: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-08-03 20:38:16.913977: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-03 20:38:17.625778: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import os\n",
        "from typing import List, Tuple, Dict\n",
        "from dataclasses import dataclass\n",
        "from joblib import dump\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.python.ops import math_ops\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "#@title Debugging\n",
        "# See https://zohaib.me/debugging-in-google-collab-notebook/ for tips,\n",
        "# as well as docs for pdb and ipdb.\n",
        "DEBUG = False #@param {type:\"boolean\"}\n",
        "USE_LOCAL_DRIVE = True #@param {type:\"boolean\"}\n",
        "LOCAL_DIR = \"/usr/local/google/home/ruru/Downloads/amazon_sample_data-20230712T203059Z-001\" #@param\n",
        "GDRIVE_DIR = \"MyDrive/amazon_rainforest_files/\" #@param\n",
        "FP_ROOT = LOCAL_DIR\n",
        "\n",
        "MODEL_SAVE_LOCATION = \"/usr/local/google/home/ruru/Downloads/model_builds\" #@param\n",
        "\n",
        "def get_model_save_location(filename) -> str:\n",
        "  root = '' if USE_LOCAL_DRIVE else '/content/drive'\n",
        "  return os.path.join(root, MODEL_SAVE_LOCATION, filename)\n",
        "\n",
        "# Access data stored on Google Drive if not reading data locally.\n",
        "if not USE_LOCAL_DRIVE:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  global FP_ROOT\n",
        "  FP_ROOT = os.path.join('/content/drive', GDRIVE_DIR)\n",
        "\n",
        "if DEBUG:\n",
        "    %pip install -Uqq ipdb\n",
        "    import ipdb\n",
        "    %pdb on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQrEv57zCb4q"
      },
      "source": [
        "# Data preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path: str, columns_to_keep: List[str]):\n",
        "  df = pd.read_csv(path, encoding=\"ISO-8859-1\", sep=',')\n",
        "  df = df[df['d18O_cel_variance'].notna()]\n",
        "\n",
        "  X = df\n",
        "  X = X.drop(df.columns.difference(columns_to_keep), axis=1)\n",
        "\n",
        "  Y = df[[\"d18O_cel_mean\", \"d18O_cel_variance\"]]\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "6XMee1aHfcik"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardization"
      ],
      "metadata": {
        "id": "DtkKhMOtb6cS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class FeaturesToLabels:\n",
        "  def __init__(self, X: pd.DataFrame, Y: pd.DataFrame):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "\n",
        "  def as_tuple(self):\n",
        "    return (self.X, self.Y)\n",
        "\n",
        "\n",
        "def create_feature_scaler(X: pd.DataFrame,\n",
        "                          columns_to_passthrough,\n",
        "                          columns_to_scale,\n",
        "                          columns_to_standardize) -> ColumnTransformer:\n",
        "  feature_scaler = ColumnTransformer(\n",
        "      [(column+'_normalizer', MinMaxScaler(), [column]) for column in columns_to_scale] +\n",
        "      [(column+'_standardizer', StandardScaler(), [column]) for column in columns_to_standardize],\n",
        "      remainder='passthrough')\n",
        "  feature_scaler.fit(X)\n",
        "  print(feature_scaler)\n",
        "  return feature_scaler\n",
        "\n",
        "def scale(X: pd.DataFrame, feature_scaler):\n",
        "  # transform() outputs numpy arrays :(  need to convert back to DataFrame.\n",
        "  X_standardized = pd.DataFrame(feature_scaler.transform(X),\n",
        "                        index=X.index, columns=X.columns)\n",
        "  return X_standardized"
      ],
      "metadata": {
        "id": "XSDwdvMkb7w8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just a class organization, holds each scaled dataset and the scaler used.\n",
        "# Useful for unscaling predictions.\n",
        "@dataclass\n",
        "class ScaledPartitions():\n",
        "  def __init__(self,\n",
        "               feature_scaler: ColumnTransformer,\n",
        "               label_scaler: ColumnTransformer,\n",
        "               train: FeaturesToLabels, val: FeaturesToLabels,\n",
        "               test: FeaturesToLabels):\n",
        "    self.feature_scaler = feature_scaler\n",
        "    self.label_scaler = label_scaler\n",
        "    self.train = train\n",
        "    self.val = val\n",
        "    self.test = test\n",
        "\n",
        "\n",
        "def load_and_scale(config: Dict,\n",
        "                   columns_to_passthrough: List[str],\n",
        "                   columns_to_scale: List[str],\n",
        "                   columns_to_standardize: List[str]) -> ScaledPartitions:\n",
        "  columns_to_keep = columns_to_passthrough + columns_to_scale + columns_to_standardize\n",
        "  X_train, Y_train = load_dataset(config['TRAIN'], columns_to_keep)\n",
        "  X_val, Y_val = load_dataset(config['VALIDATION'], columns_to_keep)\n",
        "  X_test, Y_test = load_dataset(config['TEST'], columns_to_keep)\n",
        "\n",
        "  # Fit the scaler:\n",
        "  feature_scaler = create_feature_scaler(\n",
        "      X_train,\n",
        "      columns_to_passthrough,\n",
        "      columns_to_scale,\n",
        "      columns_to_standardize)\n",
        "\n",
        "  # Apply the scaler:\n",
        "  train = FeaturesToLabels(scale(X_train, feature_scaler), Y_train)\n",
        "  val = FeaturesToLabels(scale(X_val, feature_scaler), Y_val)\n",
        "  test = FeaturesToLabels(scale(X_test, feature_scaler), Y_test)\n",
        "  return ScaledPartitions(feature_scaler, None, train, val, test)\n"
      ],
      "metadata": {
        "id": "_kf2e_fKon2P"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition\n",
        "\n"
      ],
      "metadata": {
        "id": "usGznR593LZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KL Loss function:"
      ],
      "metadata": {
        "id": "khK7C8WvU8ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_normal_distribution(\n",
        "    mean: tf.Tensor,\n",
        "    stdev: tf.Tensor,\n",
        "    n: int) -> tf.Tensor:\n",
        "    '''\n",
        "    Given a batch of normal distributions described by a mean and stdev in\n",
        "    a tf.Tensor, sample n elements from each distribution and return the mean\n",
        "    and standard deviation per sample.\n",
        "    '''\n",
        "    batch_size = tf.shape(mean)[0]\n",
        "\n",
        "    # Output tensor is (n, batch_size, 1)\n",
        "    sample_values = tfp.distributions.Normal(\n",
        "        loc=mean,\n",
        "        scale=stdev).sample(\n",
        "            sample_shape=n)\n",
        "    # Reshaped tensor will be (batch_size, n)\n",
        "    sample_values = tf.transpose(sample_values)\n",
        "    # Get the mean per sample in the batch.\n",
        "    sample_mean = tf.transpose(tf.math.reduce_mean(sample_values, 2))\n",
        "    sample_stdev = tf.transpose(tf.math.reduce_std(sample_values, 2))\n",
        "\n",
        "    return sample_mean, sample_stdev\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# log(σ2/σ1) + ( σ1^2+(μ1−μ2)^2 ) / 2* σ^2   − 1/2\n",
        "def kl_divergence_helper(real, predicted, sample):\n",
        "    '''\n",
        "    real: tf.Tensor of the real mean and standard deviation of sample to compare\n",
        "    predicted: tf.Tensor of the predicted mean and standard deviation to compare\n",
        "    sample: Whether or not to sample the predicted distribution to get a new\n",
        "            mean and standard deviation.\n",
        "    '''\n",
        "    if real.shape != predicted.shape:\n",
        "      raise ValueError(\n",
        "          f\"real.shape {real.shape} != predicted.shape {predicted.shape}\")\n",
        "\n",
        "    real_value = tf.gather(real, [0], axis=1)\n",
        "    real_std = tf.math.sqrt(tf.gather(real, [1], axis=1))\n",
        "\n",
        "\n",
        "    predicted_value = tf.gather(predicted, [0], axis=1)\n",
        "    predicted_std = tf.math.sqrt(tf.gather(predicted, [1], axis=1))\n",
        "    # If true, sample from the distribution defined by the predicted mean and\n",
        "    # standard deviation to use for mean and stdev used in KL divergence loss.\n",
        "    if sample:\n",
        "      predicted_value, predicted_std = sample_normal_distribution(\n",
        "          mean=predicted_value, stdev=predicted_std, n=15)\n",
        "\n",
        "    kl_loss = -0.5 + tf.math.log(predicted_std/real_std) + \\\n",
        "     (tf.square(real_std) + tf.square(real_value - predicted_value))/ \\\n",
        "     (2*tf.square(predicted_std))\n",
        "\n",
        "    if tf.math.is_nan(tf.math.reduce_mean(kl_loss)):\n",
        "       tf.print(predicted)\n",
        "       sess = tf.compat.v1.Session()\n",
        "       sess.close()\n",
        "\n",
        "    return tf.math.reduce_mean(kl_loss)\n",
        "\n",
        "def kl_divergence(real, predicted):\n",
        "  return kl_divergence_helper(real, predicted, True)"
      ],
      "metadata": {
        "id": "urGjYNNnemX6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the loss function:"
      ],
      "metadata": {
        "id": "fJzBFWQVeqNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "\n",
        "class TensorsDifferShapeTest(unittest.TestCase):\n",
        "   def test(self):\n",
        "      test_real = tf.convert_to_tensor(np.array([[1, 0.02]]))\n",
        "      test_pred = tf.convert_to_tensor(np.array([[0.98]]))\n",
        "      with self.assertRaises(ValueError):\n",
        "         kl_divergence(test_real, test_pred, False)\n",
        "         assert(False) # Triggers if no exception is caught in the previous line.\n",
        "\n",
        "TensorsDifferShapeTest().test()\n",
        "\n",
        "test_real = tf.convert_to_tensor(np.array([[1, 0.02]]))\n",
        "test_pred = tf.convert_to_tensor(np.array([[0.98, 0.021]]))\n",
        "\n",
        "# https://screenshot.googleplex.com/5WM9dinAbhR26ZS\n",
        "assert float(kl_divergence(test_real, test_pred)) == pytest.approx(0.0101094, 1e-5)\n",
        "\n",
        "test_neg_real = tf.convert_to_tensor(np.array([[32.32, 0.0344]]))\n",
        "test_neg_pred = tf.convert_to_tensor(np.array([[32.01, -0.322]]))\n",
        "\n",
        "# Negative variance causes NaN\n",
        "assert tf.math.is_nan(kl_divergence(test_neg_real, test_neg_pred))\n",
        "\n",
        "# Calculated manually by computing the result of this equation in wolfram alpha:\n",
        "# log(σ2/σ1) + ( σ1^2+(μ1−μ2)^2 ) / 2* σ^2   − 1/2\n",
        "test_real_2d = tf.convert_to_tensor(np.array(\n",
        "    [[1.00, 0.020],\n",
        "     [1.01, 0.042]]))\n",
        "test_pred_2d = tf.convert_to_tensor(np.array(\n",
        "    [[0.98, 0.021],\n",
        "     [0.99, 0.012]]))\n",
        "\n",
        "# Should reduce to the average loss of all rows.\n",
        "assert float(kl_divergence(test_real_2d, test_pred_2d)) == pytest.approx(\n",
        "    sum([0.0101094, 0.6402851])/2, 1e-5)"
      ],
      "metadata": {
        "id": "48TaPd70erSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model definition"
      ],
      "metadata": {
        "id": "8rI6qPRh7oO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "tf.keras.utils.set_random_seed(18731)\n",
        "\n",
        "def get_early_stopping_callback():\n",
        "  return EarlyStopping(monitor='val_loss', patience=1000, min_delta=0.001,\n",
        "                       verbose=1, restore_best_weights=True, start_from_epoch=0)\n",
        "\n",
        "# I was experimenting with models that took longer to train, and used this\n",
        "# checkpointing callback to periodically save the model. It's optional.\n",
        "def get_checkpoint_callback(model_file):\n",
        "  return ModelCheckpoint(\n",
        "      get_model_save_location(model_file),\n",
        "      monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
        "\n",
        "def train_or_update_variational_model(\n",
        "        sp: ScaledPartitions,\n",
        "        hidden_layers: List[int],\n",
        "        epochs: int,\n",
        "        batch_size: int,\n",
        "        lr: float,\n",
        "        model_file=None,\n",
        "        use_checkpoint=False):\n",
        "  callbacks_list = [get_early_stopping_callback(),\n",
        "                    get_checkpoint_callback(model_file)]\n",
        "  if not use_checkpoint:\n",
        "\n",
        "    # Find the kriging columns and make sure they are at the end of the dataframe.\n",
        "    krig_mean_index = sp.train.X.columns.get_loc('ordinary_kriging_linear_d18O_predicted_mean')\n",
        "    krig_var_index = sp.train.X.columns.get_loc('ordinary_kriging_linear_d18O_predicted_variance')\n",
        "    if (krig_mean_index != sp.train.X.shape[1]-2 and krig_var_index != sp.train.X.shape[1]-1):\n",
        "      raise ValueError(\"ordinary_kriging_linear_d18O_predicted_mean and\"\n",
        "      \"ordinary_kriging_linear_d18O_predicted_variance must be\"\n",
        "      \"located in the last two columns of dataframe\")\n",
        "\n",
        "    inputs = keras.Input(shape=(sp.train.X.shape[1],))\n",
        "    nn = inputs[:,0:-2]\n",
        "    krig_mean = tf.expand_dims(inputs[:,-2], 1)\n",
        "    krig_variance = tf.expand_dims(inputs[:, -1], 1)\n",
        "\n",
        "    for layer_size in hidden_layers:\n",
        "      nn = keras.layers.Dense(layer_size, activation='relu')(nn)\n",
        "\n",
        "    # Invert the normalization on our outputs, and add kriging predictions as\n",
        "    # constants so the network only predicts the residuals.\n",
        "    pred_mean_residual = keras.layers.Dense(1, name='pred_mean_residual')(nn)\n",
        "    pred_var_residual = keras.layers.Dense(1, name='pred_var_output')(nn)\n",
        "\n",
        "    pred_mean = krig_mean + pred_mean_residual\n",
        "    pred_var = keras.layers.Lambda(lambda t: tf.math.log(1 + tf.math.exp(t[0]+t[1])))([krig_variance, pred_var_residual])\n",
        "\n",
        "    # Output mean, variance tuples.\n",
        "    outputs = keras.layers.concatenate([pred_mean, pred_var])\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss=kl_divergence)\n",
        "    model.summary()\n",
        "  else:\n",
        "    model = keras.models.load_model(\n",
        "        get_model_save_location(model_file),\n",
        "        custom_objects={\"kl_divergence\": kl_divergence})\n",
        "  history = model.fit(sp.train.X, sp.train.Y, verbose=0, epochs=epochs,\n",
        "                      batch_size=batch_size,\n",
        "                      validation_data=sp.val.as_tuple(),\n",
        "                      shuffle=True, callbacks=callbacks_list)\n",
        "  return history, model"
      ],
      "metadata": {
        "id": "HCkGSPUo3KqY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def render_plot_loss(history, name):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title(name + ' model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.yscale(\"log\")\n",
        "  plt.ylim((0, 10))\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['loss', 'val_loss'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def destandardize(sd: ScaledPartitions, df: pd.DataFrame):\n",
        "  means = pd.DataFrame(\n",
        "      sd.label_scaler.named_transformers_['var_std_scaler'].inverse_transform(df[['d18O_cel_mean']]),\n",
        "      index=df.index, columns=['d18O_cel_mean'])\n",
        "  vars = df['d18O_cel_variance']\n",
        "  return means.join(vars)\n",
        "\n",
        "def train_and_evaluate(sp: ScaledPartitions, run_id: str, training_batch_size=5):\n",
        "  print(\"==================\")\n",
        "  print(run_id)\n",
        "  history, model = train_or_update_variational_model(\n",
        "      sp, hidden_layers=[20, 20], epochs=5000, batch_size=training_batch_size,\n",
        "      lr=0.001, model_file=run_id+\".h5\", use_checkpoint=False)\n",
        "  render_plot_loss(history, run_id+\" kl_loss\")\n",
        "  model.save(get_model_save_location(run_id+\".h5\"), save_format=\"h5\")\n",
        "\n",
        "  best_epoch_index = history.history['val_loss'].index(min(history.history['val_loss']))\n",
        "  print('Val loss:', history.history['val_loss'][best_epoch_index])\n",
        "  print('Train loss:', history.history['loss'][best_epoch_index])\n",
        "  print('Test loss:', model.evaluate(x=sp.test.X, y=sp.test.Y, verbose=0))\n",
        "\n",
        "  predictions = model.predict_on_batch(sp.test.X)\n",
        "  predictions = pd.DataFrame(predictions, columns=['d18O_cel_mean', 'd18O_cel_variance'])\n",
        "  rmse = np.sqrt(mean_squared_error(sp.test.Y['d18O_cel_mean'], predictions['d18O_cel_mean']))\n",
        "  print(\"dO18 RMSE: \"+ str(rmse))\n",
        "  print(\"EXPECTED:\")\n",
        "  print(sp.test.Y.to_string())\n",
        "  print()\n",
        "  print(\"PREDICTED:\")\n",
        "  print(predictions.to_string())\n",
        "  return model"
      ],
      "metadata": {
        "id": "DALuUm8UOgNu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Grouped, random (jupyter crashed halfway so I reloaded a checkpoint)\n",
        "\n",
        "We can't (easily) generate isoscapes with these because the isoscapes for 'predkrig_br_lat_ISORG', 'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION' are not easily retrievable... but I'm curious how much better the model is if these columns are included."
      ],
      "metadata": {
        "id": "n8pNR1CrvF2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_random_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_train_random_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_test_random_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_validation_random_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_passthrough = [\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "columns_to_scale = []\n",
        "columns_to_standardize = [\n",
        "    'lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature', 'Mean Annual Precipitation',\n",
        "    'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION']\n",
        "\n",
        "data = load_and_scale(grouped_random_fileset, columns_to_passthrough, columns_to_scale, columns_to_standardize)\n",
        "model = train_and_evaluate(data, \"random_all_boosted_residuals\", training_batch_size=3)\n",
        "model.save(get_model_save_location(\"random_all_boosted_residuals.keras\"), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location('random_all_boosted_residuals_transformer.pkl'))\n"
      ],
      "metadata": {
        "id": "rPONfgkjvJWz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "69c239ed-f3df-431c-bebd-2aaa01ff8c51"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 590/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6870 - val_loss: 1.3445\n",
            "Epoch 591/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6955 - val_loss: 1.3687\n",
            "Epoch 592/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6882 - val_loss: 1.3965\n",
            "Epoch 593/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6903 - val_loss: 1.3810\n",
            "Epoch 594/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6888 - val_loss: 1.3446\n",
            "Epoch 595/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6873 - val_loss: 1.3882\n",
            "Epoch 596/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6927 - val_loss: 1.3234\n",
            "Epoch 597/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6872 - val_loss: 1.3480\n",
            "Epoch 598/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6898 - val_loss: 1.3412\n",
            "Epoch 599/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6854 - val_loss: 1.3598\n",
            "Epoch 600/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6888 - val_loss: 1.3869\n",
            "Epoch 601/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6872 - val_loss: 1.3387\n",
            "Epoch 602/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6888 - val_loss: 1.3434\n",
            "Epoch 603/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6915 - val_loss: 1.3192\n",
            "Epoch 604/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6872 - val_loss: 1.3554\n",
            "Epoch 605/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6839 - val_loss: 1.2849\n",
            "Epoch 606/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6861 - val_loss: 1.3918\n",
            "Epoch 607/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6913 - val_loss: 1.2945\n",
            "Epoch 608/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6850 - val_loss: 1.2721\n",
            "Epoch 609/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6875 - val_loss: 1.3056\n",
            "Epoch 610/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6875 - val_loss: 1.2975\n",
            "Epoch 611/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6894 - val_loss: 1.2642\n",
            "Epoch 612/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6883 - val_loss: 1.2813\n",
            "Epoch 613/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6852 - val_loss: 1.3344\n",
            "Epoch 614/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6918 - val_loss: 1.2405\n",
            "Epoch 615/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6863 - val_loss: 1.2779\n",
            "Epoch 616/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6868 - val_loss: 1.2440\n",
            "Epoch 617/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6859 - val_loss: 1.2517\n",
            "Epoch 618/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6847 - val_loss: 1.2943\n",
            "Epoch 619/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 1.2364\n",
            "Epoch 620/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6848 - val_loss: 1.2768\n",
            "Epoch 621/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6854 - val_loss: 1.1979\n",
            "Epoch 622/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6868 - val_loss: 1.2160\n",
            "Epoch 623/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6866 - val_loss: 1.2885\n",
            "Epoch 624/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6869 - val_loss: 1.2277\n",
            "Epoch 625/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6888 - val_loss: 1.2327\n",
            "Epoch 626/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6848 - val_loss: 1.2297\n",
            "Epoch 627/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6869 - val_loss: 1.2604\n",
            "Epoch 628/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6885 - val_loss: 1.2124\n",
            "Epoch 629/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6877 - val_loss: 1.1847\n",
            "Epoch 630/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6882 - val_loss: 1.1916\n",
            "Epoch 631/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6862 - val_loss: 1.2119\n",
            "Epoch 632/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6897 - val_loss: 1.2236\n",
            "Epoch 633/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6877 - val_loss: 1.2319\n",
            "Epoch 634/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6861 - val_loss: 1.1968\n",
            "Epoch 635/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6837 - val_loss: 1.2066\n",
            "Epoch 636/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6884 - val_loss: 1.1575\n",
            "Epoch 637/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6858 - val_loss: 1.1409\n",
            "Epoch 638/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6860 - val_loss: 1.2248\n",
            "Epoch 639/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6843 - val_loss: 1.1578\n",
            "Epoch 640/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6837 - val_loss: 1.1615\n",
            "Epoch 641/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6864 - val_loss: 1.1792\n",
            "Epoch 642/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6860 - val_loss: 1.1627\n",
            "Epoch 643/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6929 - val_loss: 1.1282\n",
            "Epoch 644/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6918 - val_loss: 1.1327\n",
            "Epoch 645/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6886 - val_loss: 1.1317\n",
            "Epoch 646/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6900 - val_loss: 1.1529\n",
            "Epoch 647/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6900 - val_loss: 1.1654\n",
            "Epoch 648/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6873 - val_loss: 1.1287\n",
            "Epoch 649/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6819 - val_loss: 1.1609\n",
            "Epoch 650/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6860 - val_loss: 1.1479\n",
            "Epoch 651/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6839 - val_loss: 1.1480\n",
            "Epoch 652/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6839 - val_loss: 1.1528\n",
            "Epoch 653/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6843 - val_loss: 1.0973\n",
            "Epoch 654/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6836 - val_loss: 1.0991\n",
            "Epoch 655/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6838 - val_loss: 1.1140\n",
            "Epoch 656/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6862 - val_loss: 1.0857\n",
            "Epoch 657/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6871 - val_loss: 1.1066\n",
            "Epoch 658/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6860 - val_loss: 1.1250\n",
            "Epoch 659/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6851 - val_loss: 1.1216\n",
            "Epoch 660/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6849 - val_loss: 1.1178\n",
            "Epoch 661/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6833 - val_loss: 1.0790\n",
            "Epoch 662/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6841 - val_loss: 1.0954\n",
            "Epoch 663/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6843 - val_loss: 1.1347\n",
            "Epoch 664/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6842 - val_loss: 1.0435\n",
            "Epoch 665/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6854 - val_loss: 1.0577\n",
            "Epoch 666/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6844 - val_loss: 1.0755\n",
            "Epoch 667/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6847 - val_loss: 1.0697\n",
            "Epoch 668/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6859 - val_loss: 1.0534\n",
            "Epoch 669/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6822 - val_loss: 1.0736\n",
            "Epoch 670/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6836 - val_loss: 1.0654\n",
            "Epoch 671/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6812 - val_loss: 1.0759\n",
            "Epoch 672/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6853 - val_loss: 1.0952\n",
            "Epoch 673/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6827 - val_loss: 1.0713\n",
            "Epoch 674/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6826 - val_loss: 1.0887\n",
            "Epoch 675/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6854 - val_loss: 1.0838\n",
            "Epoch 676/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6877 - val_loss: 1.0996\n",
            "Epoch 677/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6921 - val_loss: 1.0904\n",
            "Epoch 678/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6832 - val_loss: 1.1077\n",
            "Epoch 679/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6837 - val_loss: 1.0485\n",
            "Epoch 680/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6849 - val_loss: 1.0581\n",
            "Epoch 681/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6824 - val_loss: 1.0730\n",
            "Epoch 682/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6825 - val_loss: 1.0837\n",
            "Epoch 683/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6808 - val_loss: 1.0607\n",
            "Epoch 684/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6817 - val_loss: 1.0479\n",
            "Epoch 685/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6833 - val_loss: 1.1163\n",
            "Epoch 686/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6819 - val_loss: 1.0044\n",
            "Epoch 687/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6882 - val_loss: 1.0265\n",
            "Epoch 688/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6799 - val_loss: 1.0480\n",
            "Epoch 689/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6849 - val_loss: 1.0440\n",
            "Epoch 690/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6836 - val_loss: 1.0297\n",
            "Epoch 691/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6825 - val_loss: 1.0443\n",
            "Epoch 692/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6803 - val_loss: 1.0243\n",
            "Epoch 693/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6816 - val_loss: 1.0376\n",
            "Epoch 694/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6819 - val_loss: 1.0131\n",
            "Epoch 695/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6826 - val_loss: 1.0696\n",
            "Epoch 696/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6800 - val_loss: 1.0019\n",
            "Epoch 697/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6853 - val_loss: 0.9921\n",
            "Epoch 698/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6877 - val_loss: 1.0232\n",
            "Epoch 699/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6850 - val_loss: 1.0154\n",
            "Epoch 700/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6812 - val_loss: 1.0467\n",
            "Epoch 701/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6822 - val_loss: 1.0144\n",
            "Epoch 702/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6812 - val_loss: 0.9821\n",
            "Epoch 703/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6837 - val_loss: 1.0252\n",
            "Epoch 704/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6782 - val_loss: 0.9887\n",
            "Epoch 705/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6814 - val_loss: 0.9724\n",
            "Epoch 706/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6836 - val_loss: 0.9961\n",
            "Epoch 707/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6887 - val_loss: 1.0107\n",
            "Epoch 708/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6812 - val_loss: 1.0054\n",
            "Epoch 709/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6837 - val_loss: 1.0144\n",
            "Epoch 710/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6863 - val_loss: 1.0432\n",
            "Epoch 711/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6817 - val_loss: 0.9560\n",
            "Epoch 712/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6841 - val_loss: 0.9840\n",
            "Epoch 713/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6818 - val_loss: 0.9993\n",
            "Epoch 714/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6832 - val_loss: 1.0290\n",
            "Epoch 715/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6812 - val_loss: 1.0361\n",
            "Epoch 716/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6814 - val_loss: 0.9533\n",
            "Epoch 717/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6833 - val_loss: 1.0010\n",
            "Epoch 718/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6808 - val_loss: 1.0037\n",
            "Epoch 719/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6779 - val_loss: 1.0207\n",
            "Epoch 720/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6816 - val_loss: 0.9771\n",
            "Epoch 721/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6798 - val_loss: 0.9913\n",
            "Epoch 722/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6799 - val_loss: 0.9938\n",
            "Epoch 723/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6790 - val_loss: 0.9843\n",
            "Epoch 724/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6811 - val_loss: 1.0390\n",
            "Epoch 725/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6819 - val_loss: 1.0157\n",
            "Epoch 726/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6861 - val_loss: 1.0611\n",
            "Epoch 727/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6784 - val_loss: 1.0029\n",
            "Epoch 728/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6879 - val_loss: 0.9780\n",
            "Epoch 729/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6793 - val_loss: 0.9747\n",
            "Epoch 730/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6784 - val_loss: 0.9590\n",
            "Epoch 731/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6812 - val_loss: 0.9429\n",
            "Epoch 732/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6832 - val_loss: 0.9890\n",
            "Epoch 733/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6843 - val_loss: 0.9991\n",
            "Epoch 734/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6869 - val_loss: 1.0071\n",
            "Epoch 735/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6794 - val_loss: 0.9808\n",
            "Epoch 736/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6823 - val_loss: 0.9757\n",
            "Epoch 737/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6784 - val_loss: 0.9463\n",
            "Epoch 738/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6881 - val_loss: 1.0583\n",
            "Epoch 739/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6836 - val_loss: 0.9603\n",
            "Epoch 740/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6846 - val_loss: 0.9415\n",
            "Epoch 741/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6805 - val_loss: 0.9581\n",
            "Epoch 742/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6818 - val_loss: 0.9698\n",
            "Epoch 743/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6860 - val_loss: 0.9765\n",
            "Epoch 744/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6777 - val_loss: 0.9642\n",
            "Epoch 745/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6783 - val_loss: 0.9439\n",
            "Epoch 746/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6841 - val_loss: 0.9253\n",
            "Epoch 747/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6810 - val_loss: 1.0024\n",
            "Epoch 748/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6781 - val_loss: 0.9275\n",
            "Epoch 749/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6820 - val_loss: 0.9080\n",
            "Epoch 750/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6800 - val_loss: 0.9496\n",
            "Epoch 751/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6793 - val_loss: 0.9287\n",
            "Epoch 752/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6781 - val_loss: 0.9186\n",
            "Epoch 753/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6888 - val_loss: 0.9052\n",
            "Epoch 754/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7017 - val_loss: 0.8868\n",
            "Epoch 755/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6958 - val_loss: 0.8691\n",
            "Epoch 756/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6923 - val_loss: 0.8671\n",
            "Epoch 757/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6882 - val_loss: 0.9022\n",
            "Epoch 758/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6844 - val_loss: 0.8790\n",
            "Epoch 759/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6866 - val_loss: 0.8659\n",
            "Epoch 760/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6888 - val_loss: 0.8851\n",
            "Epoch 761/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6836 - val_loss: 0.8839\n",
            "Epoch 762/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6847 - val_loss: 0.8362\n",
            "Epoch 763/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6851 - val_loss: 0.8757\n",
            "Epoch 764/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6823 - val_loss: 0.8742\n",
            "Epoch 765/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6824 - val_loss: 0.8513\n",
            "Epoch 766/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6862 - val_loss: 0.9139\n",
            "Epoch 767/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6843 - val_loss: 0.8986\n",
            "Epoch 768/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6802 - val_loss: 0.8667\n",
            "Epoch 769/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6857 - val_loss: 0.8857\n",
            "Epoch 770/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6779 - val_loss: 0.9193\n",
            "Epoch 771/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6821 - val_loss: 0.9116\n",
            "Epoch 772/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6806 - val_loss: 0.9121\n",
            "Epoch 773/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6819 - val_loss: 0.9296\n",
            "Epoch 774/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6820 - val_loss: 0.9279\n",
            "Epoch 775/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6854 - val_loss: 0.9083\n",
            "Epoch 776/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6808 - val_loss: 0.8816\n",
            "Epoch 777/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6807 - val_loss: 0.9093\n",
            "Epoch 778/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6806 - val_loss: 0.9122\n",
            "Epoch 779/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6780 - val_loss: 0.9015\n",
            "Epoch 780/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6814 - val_loss: 0.9150\n",
            "Epoch 781/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6814 - val_loss: 0.8528\n",
            "Epoch 782/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6821 - val_loss: 0.8950\n",
            "Epoch 783/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6803 - val_loss: 0.9045\n",
            "Epoch 784/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6799 - val_loss: 0.8833\n",
            "Epoch 785/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6814 - val_loss: 0.8327\n",
            "Epoch 786/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6789 - val_loss: 0.8476\n",
            "Epoch 787/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6788 - val_loss: 0.8285\n",
            "Epoch 788/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6814 - val_loss: 0.8447\n",
            "Epoch 789/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6821 - val_loss: 0.8423\n",
            "Epoch 790/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6822 - val_loss: 0.8873\n",
            "Epoch 791/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6792 - val_loss: 0.8283\n",
            "Epoch 792/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6784 - val_loss: 0.8396\n",
            "Epoch 793/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6807 - val_loss: 0.8707\n",
            "Epoch 794/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6786 - val_loss: 0.8570\n",
            "Epoch 795/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6792 - val_loss: 0.8318\n",
            "Epoch 796/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6800 - val_loss: 0.8702\n",
            "Epoch 797/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6770 - val_loss: 0.8556\n",
            "Epoch 798/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6792 - val_loss: 0.8115\n",
            "Epoch 799/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6781 - val_loss: 0.8215\n",
            "Epoch 800/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6855 - val_loss: 0.8474\n",
            "Epoch 801/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6804 - val_loss: 0.7989\n",
            "Epoch 802/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6776 - val_loss: 0.8565\n",
            "Epoch 803/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6776 - val_loss: 0.8376\n",
            "Epoch 804/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6792 - val_loss: 0.8743\n",
            "Epoch 805/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6789 - val_loss: 0.8770\n",
            "Epoch 806/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6809 - val_loss: 0.8498\n",
            "Epoch 807/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6775 - val_loss: 0.8789\n",
            "Epoch 808/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6783 - val_loss: 0.8237\n",
            "Epoch 809/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6789 - val_loss: 0.8501\n",
            "Epoch 810/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6793 - val_loss: 0.8359\n",
            "Epoch 811/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6841 - val_loss: 0.8373\n",
            "Epoch 812/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6771 - val_loss: 0.8766\n",
            "Epoch 813/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6781 - val_loss: 0.9097\n",
            "Epoch 814/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6779 - val_loss: 0.8768\n",
            "Epoch 815/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6796 - val_loss: 0.8959\n",
            "Epoch 816/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6755 - val_loss: 0.8710\n",
            "Epoch 817/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6763 - val_loss: 0.8551\n",
            "Epoch 818/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6794 - val_loss: 0.8768\n",
            "Epoch 819/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6781 - val_loss: 0.8776\n",
            "Epoch 820/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6786 - val_loss: 0.8923\n",
            "Epoch 821/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6757 - val_loss: 0.8475\n",
            "Epoch 822/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6718 - val_loss: 0.8673\n",
            "Epoch 823/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6765 - val_loss: 0.8932\n",
            "Epoch 824/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6744 - val_loss: 0.8641\n",
            "Epoch 825/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6764 - val_loss: 0.8450\n",
            "Epoch 826/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6781 - val_loss: 0.8474\n",
            "Epoch 827/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6747 - val_loss: 0.8325\n",
            "Epoch 828/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6722 - val_loss: 0.8257\n",
            "Epoch 829/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6769 - val_loss: 0.8344\n",
            "Epoch 830/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6760 - val_loss: 0.8284\n",
            "Epoch 831/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6730 - val_loss: 0.8729\n",
            "Epoch 832/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6769 - val_loss: 0.8671\n",
            "Epoch 833/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6755 - val_loss: 0.8110\n",
            "Epoch 834/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6804 - val_loss: 0.8503\n",
            "Epoch 835/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6845 - val_loss: 0.8290\n",
            "Epoch 836/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6788 - val_loss: 0.8022\n",
            "Epoch 837/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6744 - val_loss: 0.8260\n",
            "Epoch 838/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6784 - val_loss: 0.8616\n",
            "Epoch 839/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6732 - val_loss: 0.8361\n",
            "Epoch 840/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6748 - val_loss: 0.7937\n",
            "Epoch 841/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6736 - val_loss: 0.8258\n",
            "Epoch 842/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6717 - val_loss: 0.8353\n",
            "Epoch 843/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6750 - val_loss: 0.8387\n",
            "Epoch 844/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6731 - val_loss: 0.8227\n",
            "Epoch 845/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6740 - val_loss: 0.9059\n",
            "Epoch 846/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6754 - val_loss: 0.8014\n",
            "Epoch 847/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6714 - val_loss: 0.8659\n",
            "Epoch 848/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6774 - val_loss: 0.8271\n",
            "Epoch 849/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6748 - val_loss: 0.8314\n",
            "Epoch 850/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6752 - val_loss: 0.8433\n",
            "Epoch 851/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6702 - val_loss: 0.8269\n",
            "Epoch 852/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6753 - val_loss: 0.8303\n",
            "Epoch 853/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6717 - val_loss: 0.8382\n",
            "Epoch 854/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6710 - val_loss: 0.8991\n",
            "Epoch 855/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6702 - val_loss: 0.8287\n",
            "Epoch 856/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6741 - val_loss: 0.8189\n",
            "Epoch 857/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6694 - val_loss: 0.8388\n",
            "Epoch 858/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6805 - val_loss: 0.8479\n",
            "Epoch 859/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6976 - val_loss: 0.8289\n",
            "Epoch 860/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6900 - val_loss: 0.8730\n",
            "Epoch 861/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6878 - val_loss: 0.8526\n",
            "Epoch 862/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6855 - val_loss: 0.8879\n",
            "Epoch 863/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6840 - val_loss: 0.8621\n",
            "Epoch 864/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6791 - val_loss: 0.8642\n",
            "Epoch 865/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6821 - val_loss: 0.8566\n",
            "Epoch 866/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6837 - val_loss: 0.8131\n",
            "Epoch 867/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6805 - val_loss: 0.8180\n",
            "Epoch 868/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6765 - val_loss: 0.7980\n",
            "Epoch 869/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6815 - val_loss: 0.8249\n",
            "Epoch 870/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6780 - val_loss: 0.8447\n",
            "Epoch 871/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6777 - val_loss: 0.8606\n",
            "Epoch 872/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6823 - val_loss: 0.8368\n",
            "Epoch 873/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6778 - val_loss: 0.8150\n",
            "Epoch 874/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6734 - val_loss: 0.8330\n",
            "Epoch 875/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6763 - val_loss: 0.8248\n",
            "Epoch 876/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6758 - val_loss: 0.8954\n",
            "Epoch 877/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6725 - val_loss: 0.8450\n",
            "Epoch 878/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6727 - val_loss: 0.7883\n",
            "Epoch 879/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6712 - val_loss: 0.8330\n",
            "Epoch 880/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6711 - val_loss: 0.8212\n",
            "Epoch 881/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6707 - val_loss: 0.7841\n",
            "Epoch 882/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6718 - val_loss: 0.8077\n",
            "Epoch 883/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6707 - val_loss: 0.8412\n",
            "Epoch 884/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6672 - val_loss: 0.7772\n",
            "Epoch 885/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6724 - val_loss: 0.7724\n",
            "Epoch 886/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6704 - val_loss: 0.8141\n",
            "Epoch 887/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6800 - val_loss: 0.7740\n",
            "Epoch 888/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6735 - val_loss: 0.8047\n",
            "Epoch 889/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6739 - val_loss: 0.8207\n",
            "Epoch 890/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6721 - val_loss: 0.7910\n",
            "Epoch 891/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6763 - val_loss: 0.8413\n",
            "Epoch 892/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6706 - val_loss: 0.7781\n",
            "Epoch 893/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6742 - val_loss: 0.8398\n",
            "Epoch 894/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6716 - val_loss: 0.7714\n",
            "Epoch 895/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6702 - val_loss: 0.7916\n",
            "Epoch 896/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6708 - val_loss: 0.8141\n",
            "Epoch 897/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6701 - val_loss: 0.7701\n",
            "Epoch 898/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6690 - val_loss: 0.7989\n",
            "Epoch 899/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6799 - val_loss: 0.8669\n",
            "Epoch 900/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6943 - val_loss: 0.7970\n",
            "Epoch 901/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6850 - val_loss: 0.8249\n",
            "Epoch 902/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6836 - val_loss: 0.8285\n",
            "Epoch 903/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6898 - val_loss: 0.8170\n",
            "Epoch 904/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6886 - val_loss: 0.8155\n",
            "Epoch 905/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6796 - val_loss: 0.7874\n",
            "Epoch 906/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6831 - val_loss: 0.8204\n",
            "Epoch 907/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6811 - val_loss: 0.8455\n",
            "Epoch 908/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6825 - val_loss: 0.8323\n",
            "Epoch 909/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6799 - val_loss: 0.8146\n",
            "Epoch 910/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6805 - val_loss: 0.8500\n",
            "Epoch 911/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6772 - val_loss: 0.7961\n",
            "Epoch 912/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6799 - val_loss: 0.7930\n",
            "Epoch 913/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6747 - val_loss: 0.8335\n",
            "Epoch 914/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6752 - val_loss: 0.8073\n",
            "Epoch 915/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6727 - val_loss: 0.7678\n",
            "Epoch 916/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6725 - val_loss: 0.7656\n",
            "Epoch 917/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6714 - val_loss: 0.7950\n",
            "Epoch 918/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6730 - val_loss: 0.7575\n",
            "Epoch 919/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6720 - val_loss: 0.7947\n",
            "Epoch 920/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6668 - val_loss: 0.7727\n",
            "Epoch 921/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6690 - val_loss: 0.7825\n",
            "Epoch 922/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6739 - val_loss: 0.7559\n",
            "Epoch 923/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6766 - val_loss: 0.7832\n",
            "Epoch 924/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6692 - val_loss: 0.8159\n",
            "Epoch 925/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6702 - val_loss: 0.7680\n",
            "Epoch 926/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6796 - val_loss: 0.7514\n",
            "Epoch 927/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6701 - val_loss: 0.8112\n",
            "Epoch 928/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6728 - val_loss: 0.7422\n",
            "Epoch 929/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6708 - val_loss: 0.7770\n",
            "Epoch 930/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6722 - val_loss: 0.7536\n",
            "Epoch 931/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6716 - val_loss: 0.7648\n",
            "Epoch 932/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6675 - val_loss: 0.8002\n",
            "Epoch 933/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6698 - val_loss: 0.7462\n",
            "Epoch 934/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6692 - val_loss: 0.7760\n",
            "Epoch 935/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6704 - val_loss: 0.7489\n",
            "Epoch 936/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6756 - val_loss: 0.7392\n",
            "Epoch 937/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6728 - val_loss: 0.7629\n",
            "Epoch 938/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6673 - val_loss: 0.7489\n",
            "Epoch 939/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6715 - val_loss: 0.7475\n",
            "Epoch 940/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6687 - val_loss: 0.7534\n",
            "Epoch 941/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6724 - val_loss: 0.7507\n",
            "Epoch 942/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6718 - val_loss: 0.7439\n",
            "Epoch 943/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6683 - val_loss: 0.7479\n",
            "Epoch 944/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6744 - val_loss: 0.7438\n",
            "Epoch 945/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6737 - val_loss: 0.7605\n",
            "Epoch 946/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6633 - val_loss: 0.7374\n",
            "Epoch 947/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6693 - val_loss: 0.7464\n",
            "Epoch 948/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6821 - val_loss: 0.7870\n",
            "Epoch 949/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6848 - val_loss: 0.7518\n",
            "Epoch 950/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6950 - val_loss: 0.7689\n",
            "Epoch 951/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6811 - val_loss: 0.7346\n",
            "Epoch 952/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6824 - val_loss: 0.7538\n",
            "Epoch 953/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6795 - val_loss: 0.7471\n",
            "Epoch 954/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6769 - val_loss: 0.7494\n",
            "Epoch 955/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6755 - val_loss: 0.7487\n",
            "Epoch 956/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6780 - val_loss: 0.7396\n",
            "Epoch 957/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6761 - val_loss: 0.7334\n",
            "Epoch 958/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6724 - val_loss: 0.7570\n",
            "Epoch 959/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6731 - val_loss: 0.7457\n",
            "Epoch 960/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6688 - val_loss: 0.7414\n",
            "Epoch 961/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6685 - val_loss: 0.7543\n",
            "Epoch 962/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6689 - val_loss: 0.7401\n",
            "Epoch 963/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6643 - val_loss: 0.7429\n",
            "Epoch 964/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6650 - val_loss: 0.7475\n",
            "Epoch 965/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6750 - val_loss: 0.7584\n",
            "Epoch 966/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6737 - val_loss: 0.7458\n",
            "Epoch 967/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6712 - val_loss: 0.7464\n",
            "Epoch 968/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6689 - val_loss: 0.7321\n",
            "Epoch 969/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6696 - val_loss: 0.7232\n",
            "Epoch 970/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6692 - val_loss: 0.7401\n",
            "Epoch 971/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6736 - val_loss: 0.7286\n",
            "Epoch 972/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6733 - val_loss: 0.7439\n",
            "Epoch 973/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6724 - val_loss: 0.7318\n",
            "Epoch 974/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6751 - val_loss: 0.7366\n",
            "Epoch 975/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6713 - val_loss: 0.7440\n",
            "Epoch 976/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6722 - val_loss: 0.7141\n",
            "Epoch 977/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6662 - val_loss: 0.7332\n",
            "Epoch 978/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6657 - val_loss: 0.7325\n",
            "Epoch 979/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6737 - val_loss: 0.7430\n",
            "Epoch 980/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6746 - val_loss: 0.7373\n",
            "Epoch 981/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6709 - val_loss: 0.7342\n",
            "Epoch 982/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6723 - val_loss: 0.7152\n",
            "Epoch 983/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6704 - val_loss: 0.7317\n",
            "Epoch 984/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6673 - val_loss: 0.7151\n",
            "Epoch 985/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6762 - val_loss: 0.7248\n",
            "Epoch 986/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6831 - val_loss: 0.7327\n",
            "Epoch 987/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6775 - val_loss: 0.7344\n",
            "Epoch 988/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6835 - val_loss: 0.7188\n",
            "Epoch 989/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6882 - val_loss: 0.7251\n",
            "Epoch 990/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6836 - val_loss: 0.7264\n",
            "Epoch 991/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6745 - val_loss: 0.7313\n",
            "Epoch 992/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6769 - val_loss: 0.7254\n",
            "Epoch 993/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6763 - val_loss: 0.7067\n",
            "Epoch 994/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6741 - val_loss: 0.7135\n",
            "Epoch 995/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6715 - val_loss: 0.7371\n",
            "Epoch 996/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6725 - val_loss: 0.7004\n",
            "Epoch 997/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6715 - val_loss: 0.7109\n",
            "Epoch 998/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6705 - val_loss: 0.7122\n",
            "Epoch 999/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6710 - val_loss: 0.7145\n",
            "Epoch 1000/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6707 - val_loss: 0.7045\n",
            "Epoch 1001/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6694 - val_loss: 0.7272\n",
            "Epoch 1002/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6671 - val_loss: 0.7080\n",
            "Epoch 1003/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6793 - val_loss: 0.7068\n",
            "Epoch 1004/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6744 - val_loss: 0.7093\n",
            "Epoch 1005/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6758 - val_loss: 0.7059\n",
            "Epoch 1006/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6709 - val_loss: 0.6907\n",
            "Epoch 1007/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6722 - val_loss: 0.7171\n",
            "Epoch 1008/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6649 - val_loss: 0.6962\n",
            "Epoch 1009/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6685 - val_loss: 0.6934\n",
            "Epoch 1010/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6662 - val_loss: 0.6983\n",
            "Epoch 1011/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6658 - val_loss: 0.6949\n",
            "Epoch 1012/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6682 - val_loss: 0.7029\n",
            "Epoch 1013/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6697 - val_loss: 0.6906\n",
            "Epoch 1014/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6663 - val_loss: 0.6933\n",
            "Epoch 1015/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6702 - val_loss: 0.7088\n",
            "Epoch 1016/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6691 - val_loss: 0.6922\n",
            "Epoch 1017/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6703 - val_loss: 0.6883\n",
            "Epoch 1018/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6678 - val_loss: 0.6877\n",
            "Epoch 1019/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6666 - val_loss: 0.6838\n",
            "Epoch 1020/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6689 - val_loss: 0.7109\n",
            "Epoch 1021/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6700 - val_loss: 0.7018\n",
            "Epoch 1022/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6653 - val_loss: 0.6938\n",
            "Epoch 1023/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6649 - val_loss: 0.7021\n",
            "Epoch 1024/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6650 - val_loss: 0.6975\n",
            "Epoch 1025/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6641 - val_loss: 0.6943\n",
            "Epoch 1026/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6643 - val_loss: 0.7029\n",
            "Epoch 1027/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6679 - val_loss: 0.6981\n",
            "Epoch 1028/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6643 - val_loss: 0.6966\n",
            "Epoch 1029/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6621 - val_loss: 0.6980\n",
            "Epoch 1030/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6773 - val_loss: 0.7262\n",
            "Epoch 1031/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6752 - val_loss: 0.6959\n",
            "Epoch 1032/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6717 - val_loss: 0.6893\n",
            "Epoch 1033/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6701 - val_loss: 0.6699\n",
            "Epoch 1034/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6692 - val_loss: 0.6870\n",
            "Epoch 1035/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6655 - val_loss: 0.6842\n",
            "Epoch 1036/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6649 - val_loss: 0.6902\n",
            "Epoch 1037/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6693 - val_loss: 0.6918\n",
            "Epoch 1038/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6708 - val_loss: 0.6925\n",
            "Epoch 1039/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6705 - val_loss: 0.6888\n",
            "Epoch 1040/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6698 - val_loss: 0.6895\n",
            "Epoch 1041/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6638 - val_loss: 0.6842\n",
            "Epoch 1042/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6652 - val_loss: 0.6885\n",
            "Epoch 1043/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6643 - val_loss: 0.6910\n",
            "Epoch 1044/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6671 - val_loss: 0.6898\n",
            "Epoch 1045/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6709 - val_loss: 0.6892\n",
            "Epoch 1046/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6759 - val_loss: 0.6925\n",
            "Epoch 1047/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6709 - val_loss: 0.6884\n",
            "Epoch 1048/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6680 - val_loss: 0.6825\n",
            "Epoch 1049/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6676 - val_loss: 0.6786\n",
            "Epoch 1050/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6668 - val_loss: 0.6731\n",
            "Epoch 1051/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6685 - val_loss: 0.6776\n",
            "Epoch 1052/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6698 - val_loss: 0.6653\n",
            "Epoch 1053/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6661 - val_loss: 0.6758\n",
            "Epoch 1054/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6653 - val_loss: 0.6847\n",
            "Epoch 1055/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6628 - val_loss: 0.6794\n",
            "Epoch 1056/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6617 - val_loss: 0.6828\n",
            "Epoch 1057/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6686 - val_loss: 0.6933\n",
            "Epoch 1058/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6647 - val_loss: 0.6810\n",
            "Epoch 1059/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6674 - val_loss: 0.6713\n",
            "Epoch 1060/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6690 - val_loss: 0.6736\n",
            "Epoch 1061/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6655 - val_loss: 0.6686\n",
            "Epoch 1062/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6632 - val_loss: 0.6853\n",
            "Epoch 1063/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6666 - val_loss: 0.6661\n",
            "Epoch 1064/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6646 - val_loss: 0.6940\n",
            "Epoch 1065/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6722 - val_loss: 0.6811\n",
            "Epoch 1066/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6723 - val_loss: 0.6916\n",
            "Epoch 1067/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6662 - val_loss: 0.6939\n",
            "Epoch 1068/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6716 - val_loss: 0.6887\n",
            "Epoch 1069/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6654 - val_loss: 0.6932\n",
            "Epoch 1070/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6625 - val_loss: 0.7066\n",
            "Epoch 1071/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6661 - val_loss: 0.7001\n",
            "Epoch 1072/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6664 - val_loss: 0.6736\n",
            "Epoch 1073/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6645 - val_loss: 0.6824\n",
            "Epoch 1074/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6675 - val_loss: 0.6808\n",
            "Epoch 1075/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6712 - val_loss: 0.6993\n",
            "Epoch 1076/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6661 - val_loss: 0.6712\n",
            "Epoch 1077/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6613 - val_loss: 0.6790\n",
            "Epoch 1078/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6658 - val_loss: 0.6708\n",
            "Epoch 1079/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6658 - val_loss: 0.6719\n",
            "Epoch 1080/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6739 - val_loss: 0.6612\n",
            "Epoch 1081/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6749 - val_loss: 0.6790\n",
            "Epoch 1082/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6733 - val_loss: 0.6781\n",
            "Epoch 1083/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6669 - val_loss: 0.6695\n",
            "Epoch 1084/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6626 - val_loss: 0.6709\n",
            "Epoch 1085/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6646 - val_loss: 0.6835\n",
            "Epoch 1086/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6627 - val_loss: 0.6773\n",
            "Epoch 1087/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6749 - val_loss: 0.6762\n",
            "Epoch 1088/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6898 - val_loss: 0.6657\n",
            "Epoch 1089/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6872 - val_loss: 0.6781\n",
            "Epoch 1090/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6871 - val_loss: 0.6441\n",
            "Epoch 1091/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6795 - val_loss: 0.6599\n",
            "Epoch 1092/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6772 - val_loss: 0.6606\n",
            "Epoch 1093/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6734 - val_loss: 0.6610\n",
            "Epoch 1094/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6736 - val_loss: 0.6582\n",
            "Epoch 1095/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6698 - val_loss: 0.6423\n",
            "Epoch 1096/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6699 - val_loss: 0.6548\n",
            "Epoch 1097/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6681 - val_loss: 0.6511\n",
            "Epoch 1098/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6663 - val_loss: 0.6454\n",
            "Epoch 1099/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6663 - val_loss: 0.6456\n",
            "Epoch 1100/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6649 - val_loss: 0.6309\n",
            "Epoch 1101/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6647 - val_loss: 0.6384\n",
            "Epoch 1102/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6592 - val_loss: 0.6401\n",
            "Epoch 1103/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6638 - val_loss: 0.6239\n",
            "Epoch 1104/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6606 - val_loss: 0.6495\n",
            "Epoch 1105/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6587 - val_loss: 0.6343\n",
            "Epoch 1106/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6657 - val_loss: 0.6442\n",
            "Epoch 1107/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6648 - val_loss: 0.6503\n",
            "Epoch 1108/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6634 - val_loss: 0.6363\n",
            "Epoch 1109/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6611 - val_loss: 0.6391\n",
            "Epoch 1110/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6713 - val_loss: 0.6462\n",
            "Epoch 1111/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6654 - val_loss: 0.6268\n",
            "Epoch 1112/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6747 - val_loss: 0.6290\n",
            "Epoch 1113/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6774 - val_loss: 0.6394\n",
            "Epoch 1114/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6780 - val_loss: 0.6243\n",
            "Epoch 1115/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6721 - val_loss: 0.6300\n",
            "Epoch 1116/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6662 - val_loss: 0.6322\n",
            "Epoch 1117/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6707 - val_loss: 0.6253\n",
            "Epoch 1118/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6637 - val_loss: 0.6268\n",
            "Epoch 1119/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6618 - val_loss: 0.6260\n",
            "Epoch 1120/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6582 - val_loss: 0.6339\n",
            "Epoch 1121/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6603 - val_loss: 0.6249\n",
            "Epoch 1122/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6651 - val_loss: 0.6254\n",
            "Epoch 1123/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6626 - val_loss: 0.6199\n",
            "Epoch 1124/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6733 - val_loss: 0.6299\n",
            "Epoch 1125/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6722 - val_loss: 0.6305\n",
            "Epoch 1126/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6755 - val_loss: 0.6254\n",
            "Epoch 1127/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6635 - val_loss: 0.6349\n",
            "Epoch 1128/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6628 - val_loss: 0.6228\n",
            "Epoch 1129/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6576 - val_loss: 0.6389\n",
            "Epoch 1130/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6657 - val_loss: 0.6234\n",
            "Epoch 1131/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6636 - val_loss: 0.6249\n",
            "Epoch 1132/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6622 - val_loss: 0.6327\n",
            "Epoch 1133/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6674 - val_loss: 0.6362\n",
            "Epoch 1134/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6976 - val_loss: 0.6303\n",
            "Epoch 1135/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6896 - val_loss: 0.6388\n",
            "Epoch 1136/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6877 - val_loss: 0.6407\n",
            "Epoch 1137/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6850 - val_loss: 0.6382\n",
            "Epoch 1138/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6813 - val_loss: 0.6486\n",
            "Epoch 1139/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6812 - val_loss: 0.6364\n",
            "Epoch 1140/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6811 - val_loss: 0.6361\n",
            "Epoch 1141/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6802 - val_loss: 0.6401\n",
            "Epoch 1142/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6792 - val_loss: 0.6283\n",
            "Epoch 1143/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6804 - val_loss: 0.6221\n",
            "Epoch 1144/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6816 - val_loss: 0.6219\n",
            "Epoch 1145/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6787 - val_loss: 0.6239\n",
            "Epoch 1146/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6807 - val_loss: 0.6118\n",
            "Epoch 1147/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6785 - val_loss: 0.6269\n",
            "Epoch 1148/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6764 - val_loss: 0.6143\n",
            "Epoch 1149/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6768 - val_loss: 0.6101\n",
            "Epoch 1150/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6797 - val_loss: 0.6176\n",
            "Epoch 1151/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6738 - val_loss: 0.6063\n",
            "Epoch 1152/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6804 - val_loss: 0.5913\n",
            "Epoch 1153/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6751 - val_loss: 0.6026\n",
            "Epoch 1154/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6767 - val_loss: 0.6074\n",
            "Epoch 1155/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6754 - val_loss: 0.6005\n",
            "Epoch 1156/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6766 - val_loss: 0.5843\n",
            "Epoch 1157/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6759 - val_loss: 0.5966\n",
            "Epoch 1158/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6748 - val_loss: 0.5921\n",
            "Epoch 1159/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6739 - val_loss: 0.5934\n",
            "Epoch 1160/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6790 - val_loss: 0.5857\n",
            "Epoch 1161/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6793 - val_loss: 0.5947\n",
            "Epoch 1162/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6739 - val_loss: 0.5726\n",
            "Epoch 1163/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6759 - val_loss: 0.5818\n",
            "Epoch 1164/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6704 - val_loss: 0.5735\n",
            "Epoch 1165/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6759 - val_loss: 0.5740\n",
            "Epoch 1166/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6753 - val_loss: 0.5678\n",
            "Epoch 1167/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6717 - val_loss: 0.5680\n",
            "Epoch 1168/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6724 - val_loss: 0.5658\n",
            "Epoch 1169/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6721 - val_loss: 0.5678\n",
            "Epoch 1170/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6732 - val_loss: 0.5510\n",
            "Epoch 1171/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6734 - val_loss: 0.5588\n",
            "Epoch 1172/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6736 - val_loss: 0.5557\n",
            "Epoch 1173/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6757 - val_loss: 0.5713\n",
            "Epoch 1174/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6724 - val_loss: 0.5700\n",
            "Epoch 1175/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6720 - val_loss: 0.5841\n",
            "Epoch 1176/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6708 - val_loss: 0.5534\n",
            "Epoch 1177/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6737 - val_loss: 0.5528\n",
            "Epoch 1178/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6725 - val_loss: 0.5664\n",
            "Epoch 1179/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6723 - val_loss: 0.5591\n",
            "Epoch 1180/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6753 - val_loss: 0.5406\n",
            "Epoch 1181/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6717 - val_loss: 0.5624\n",
            "Epoch 1182/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6716 - val_loss: 0.5719\n",
            "Epoch 1183/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6730 - val_loss: 0.5508\n",
            "Epoch 1184/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6681 - val_loss: 0.5628\n",
            "Epoch 1185/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6700 - val_loss: 0.5571\n",
            "Epoch 1186/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6677 - val_loss: 0.5502\n",
            "Epoch 1187/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6697 - val_loss: 0.5575\n",
            "Epoch 1188/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6706 - val_loss: 0.5513\n",
            "Epoch 1189/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6688 - val_loss: 0.5500\n",
            "Epoch 1190/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6667 - val_loss: 0.5538\n",
            "Epoch 1191/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6692 - val_loss: 0.5438\n",
            "Epoch 1192/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6655 - val_loss: 0.5598\n",
            "Epoch 1193/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6699 - val_loss: 0.5456\n",
            "Epoch 1194/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6747 - val_loss: 0.5515\n",
            "Epoch 1195/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6656 - val_loss: 0.5416\n",
            "Epoch 1196/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6665 - val_loss: 0.5470\n",
            "Epoch 1197/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6713 - val_loss: 0.5513\n",
            "Epoch 1198/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6672 - val_loss: 0.5377\n",
            "Epoch 1199/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6657 - val_loss: 0.5444\n",
            "Epoch 1200/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6672 - val_loss: 0.5391\n",
            "Epoch 1201/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6640 - val_loss: 0.5316\n",
            "Epoch 1202/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6656 - val_loss: 0.5405\n",
            "Epoch 1203/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6662 - val_loss: 0.5401\n",
            "Epoch 1204/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6702 - val_loss: 0.5293\n",
            "Epoch 1205/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6617 - val_loss: 0.5357\n",
            "Epoch 1206/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6649 - val_loss: 0.5311\n",
            "Epoch 1207/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6657 - val_loss: 0.5242\n",
            "Epoch 1208/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6639 - val_loss: 0.5242\n",
            "Epoch 1209/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6648 - val_loss: 0.5220\n",
            "Epoch 1210/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6612 - val_loss: 0.5217\n",
            "Epoch 1211/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6639 - val_loss: 0.5236\n",
            "Epoch 1212/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6649 - val_loss: 0.5166\n",
            "Epoch 1213/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6634 - val_loss: 0.5285\n",
            "Epoch 1214/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6643 - val_loss: 0.5108\n",
            "Epoch 1215/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6646 - val_loss: 0.5167\n",
            "Epoch 1216/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6622 - val_loss: 0.5141\n",
            "Epoch 1217/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6595 - val_loss: 0.5107\n",
            "Epoch 1218/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6599 - val_loss: 0.5205\n",
            "Epoch 1219/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6658 - val_loss: 0.5148\n",
            "Epoch 1220/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6630 - val_loss: 0.5119\n",
            "Epoch 1221/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6615 - val_loss: 0.5037\n",
            "Epoch 1222/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6650 - val_loss: 0.5063\n",
            "Epoch 1223/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6603 - val_loss: 0.5153\n",
            "Epoch 1224/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6668 - val_loss: 0.5125\n",
            "Epoch 1225/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6684 - val_loss: 0.5049\n",
            "Epoch 1226/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6626 - val_loss: 0.5348\n",
            "Epoch 1227/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6606 - val_loss: 0.5216\n",
            "Epoch 1228/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6629 - val_loss: 0.5122\n",
            "Epoch 1229/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6607 - val_loss: 0.5111\n",
            "Epoch 1230/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6592 - val_loss: 0.5123\n",
            "Epoch 1231/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6607 - val_loss: 0.5108\n",
            "Epoch 1232/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6594 - val_loss: 0.5091\n",
            "Epoch 1233/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6601 - val_loss: 0.5106\n",
            "Epoch 1234/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6672 - val_loss: 0.5069\n",
            "Epoch 1235/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6605 - val_loss: 0.5149\n",
            "Epoch 1236/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6590 - val_loss: 0.5077\n",
            "Epoch 1237/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6607 - val_loss: 0.5131\n",
            "Epoch 1238/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6628 - val_loss: 0.5112\n",
            "Epoch 1239/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6604 - val_loss: 0.5075\n",
            "Epoch 1240/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6602 - val_loss: 0.5100\n",
            "Epoch 1241/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6599 - val_loss: 0.5163\n",
            "Epoch 1242/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6604 - val_loss: 0.5097\n",
            "Epoch 1243/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6620 - val_loss: 0.5084\n",
            "Epoch 1244/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6585 - val_loss: 0.5076\n",
            "Epoch 1245/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6622 - val_loss: 0.5121\n",
            "Epoch 1246/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6737 - val_loss: 0.5118\n",
            "Epoch 1247/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6564 - val_loss: 0.5077\n",
            "Epoch 1248/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6602 - val_loss: 0.4988\n",
            "Epoch 1249/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6604 - val_loss: 0.5066\n",
            "Epoch 1250/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6601 - val_loss: 0.5007\n",
            "Epoch 1251/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6617 - val_loss: 0.5064\n",
            "Epoch 1252/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6777 - val_loss: 0.4964\n",
            "Epoch 1253/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6780 - val_loss: 0.5140\n",
            "Epoch 1254/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6951 - val_loss: 0.5039\n",
            "Epoch 1255/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7125 - val_loss: 0.5071\n",
            "Epoch 1256/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7143 - val_loss: 0.5436\n",
            "Epoch 1257/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7058 - val_loss: 0.5676\n",
            "Epoch 1258/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7020 - val_loss: 0.5742\n",
            "Epoch 1259/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6972 - val_loss: 0.5872\n",
            "Epoch 1260/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6926 - val_loss: 0.5914\n",
            "Epoch 1261/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6912 - val_loss: 0.5923\n",
            "Epoch 1262/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6902 - val_loss: 0.5840\n",
            "Epoch 1263/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6859 - val_loss: 0.5798\n",
            "Epoch 1264/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6864 - val_loss: 0.5862\n",
            "Epoch 1265/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6836 - val_loss: 0.5907\n",
            "Epoch 1266/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6828 - val_loss: 0.5959\n",
            "Epoch 1267/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6843 - val_loss: 0.5740\n",
            "Epoch 1268/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6781 - val_loss: 0.5886\n",
            "Epoch 1269/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6801 - val_loss: 0.6059\n",
            "Epoch 1270/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6751 - val_loss: 0.6166\n",
            "Epoch 1271/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6802 - val_loss: 0.5680\n",
            "Epoch 1272/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6710 - val_loss: 0.6071\n",
            "Epoch 1273/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6705 - val_loss: 0.5963\n",
            "Epoch 1274/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6700 - val_loss: 0.5668\n",
            "Epoch 1275/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6715 - val_loss: 0.5876\n",
            "Epoch 1276/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6738 - val_loss: 0.6224\n",
            "Epoch 1277/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6712 - val_loss: 0.5846\n",
            "Epoch 1278/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6654 - val_loss: 0.5815\n",
            "Epoch 1279/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6676 - val_loss: 0.5837\n",
            "Epoch 1280/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6649 - val_loss: 0.6017\n",
            "Epoch 1281/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6687 - val_loss: 0.5680\n",
            "Epoch 1282/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6628 - val_loss: 0.6020\n",
            "Epoch 1283/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6660 - val_loss: 0.5853\n",
            "Epoch 1284/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6683 - val_loss: 0.5816\n",
            "Epoch 1285/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6653 - val_loss: 0.5911\n",
            "Epoch 1286/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6608 - val_loss: 0.5831\n",
            "Epoch 1287/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6644 - val_loss: 0.5917\n",
            "Epoch 1288/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6628 - val_loss: 0.5725\n",
            "Epoch 1289/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6660 - val_loss: 0.5935\n",
            "Epoch 1290/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6672 - val_loss: 0.5552\n",
            "Epoch 1291/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6608 - val_loss: 0.5827\n",
            "Epoch 1292/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6645 - val_loss: 0.5804\n",
            "Epoch 1293/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6613 - val_loss: 0.5785\n",
            "Epoch 1294/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6619 - val_loss: 0.5736\n",
            "Epoch 1295/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6646 - val_loss: 0.5833\n",
            "Epoch 1296/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6628 - val_loss: 0.5884\n",
            "Epoch 1297/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6623 - val_loss: 0.5783\n",
            "Epoch 1298/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6628 - val_loss: 0.5560\n",
            "Epoch 1299/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6620 - val_loss: 0.5657\n",
            "Epoch 1300/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6583 - val_loss: 0.5933\n",
            "Epoch 1301/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6695 - val_loss: 0.5890\n",
            "Epoch 1302/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6602 - val_loss: 0.5672\n",
            "Epoch 1303/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6607 - val_loss: 0.5788\n",
            "Epoch 1304/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6614 - val_loss: 0.5615\n",
            "Epoch 1305/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6625 - val_loss: 0.5658\n",
            "Epoch 1306/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6572 - val_loss: 0.5938\n",
            "Epoch 1307/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6623 - val_loss: 0.5551\n",
            "Epoch 1308/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6607 - val_loss: 0.5625\n",
            "Epoch 1309/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6771 - val_loss: 0.5173\n",
            "Epoch 1310/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6884 - val_loss: 0.5541\n",
            "Epoch 1311/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6875 - val_loss: 0.5699\n",
            "Epoch 1312/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6850 - val_loss: 0.5840\n",
            "Epoch 1313/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6829 - val_loss: 0.5986\n",
            "Epoch 1314/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6785 - val_loss: 0.5943\n",
            "Epoch 1315/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6787 - val_loss: 0.6066\n",
            "Epoch 1316/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6820 - val_loss: 0.6206\n",
            "Epoch 1317/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6802 - val_loss: 0.6249\n",
            "Epoch 1318/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6809 - val_loss: 0.6201\n",
            "Epoch 1319/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6787 - val_loss: 0.6408\n",
            "Epoch 1320/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6757 - val_loss: 0.6439\n",
            "Epoch 1321/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6776 - val_loss: 0.6367\n",
            "Epoch 1322/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6762 - val_loss: 0.6656\n",
            "Epoch 1323/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6753 - val_loss: 0.6556\n",
            "Epoch 1324/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6770 - val_loss: 0.6224\n",
            "Epoch 1325/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6739 - val_loss: 0.6528\n",
            "Epoch 1326/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6743 - val_loss: 0.6176\n",
            "Epoch 1327/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6744 - val_loss: 0.6605\n",
            "Epoch 1328/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6732 - val_loss: 0.6588\n",
            "Epoch 1329/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6722 - val_loss: 0.6308\n",
            "Epoch 1330/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6742 - val_loss: 0.6513\n",
            "Epoch 1331/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6758 - val_loss: 0.6554\n",
            "Epoch 1332/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6721 - val_loss: 0.6424\n",
            "Epoch 1333/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6734 - val_loss: 0.6891\n",
            "Epoch 1334/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6721 - val_loss: 0.6542\n",
            "Epoch 1335/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6710 - val_loss: 0.6538\n",
            "Epoch 1336/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6705 - val_loss: 0.6657\n",
            "Epoch 1337/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6733 - val_loss: 0.6417\n",
            "Epoch 1338/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6738 - val_loss: 0.6623\n",
            "Epoch 1339/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6705 - val_loss: 0.6589\n",
            "Epoch 1340/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6721 - val_loss: 0.6932\n",
            "Epoch 1341/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6698 - val_loss: 0.6376\n",
            "Epoch 1342/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6712 - val_loss: 0.6461\n",
            "Epoch 1343/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6690 - val_loss: 0.6655\n",
            "Epoch 1344/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6698 - val_loss: 0.6166\n",
            "Epoch 1345/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6729 - val_loss: 0.6744\n",
            "Epoch 1346/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6694 - val_loss: 0.6888\n",
            "Epoch 1347/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6700 - val_loss: 0.6536\n",
            "Epoch 1348/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6714 - val_loss: 0.6643\n",
            "Epoch 1349/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6726 - val_loss: 0.6905\n",
            "Epoch 1350/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6689 - val_loss: 0.6769\n",
            "Epoch 1351/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6691 - val_loss: 0.6464\n",
            "Epoch 1352/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6692 - val_loss: 0.6604\n",
            "Epoch 1353/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6674 - val_loss: 0.6712\n",
            "Epoch 1354/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6684 - val_loss: 0.6233\n",
            "Epoch 1355/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6704 - val_loss: 0.6723\n",
            "Epoch 1356/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6698 - val_loss: 0.6681\n",
            "Epoch 1357/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6704 - val_loss: 0.6935\n",
            "Epoch 1358/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6683 - val_loss: 0.6273\n",
            "Epoch 1359/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6695 - val_loss: 0.6527\n",
            "Epoch 1360/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6697 - val_loss: 0.6702\n",
            "Epoch 1361/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6679 - val_loss: 0.6867\n",
            "Epoch 1362/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6682 - val_loss: 0.6704\n",
            "Epoch 1363/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6686 - val_loss: 0.6203\n",
            "Epoch 1364/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6681 - val_loss: 0.7010\n",
            "Epoch 1365/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6699 - val_loss: 0.6650\n",
            "Epoch 1366/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6699 - val_loss: 0.6559\n",
            "Epoch 1367/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6699 - val_loss: 0.6285\n",
            "Epoch 1368/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6703 - val_loss: 0.6564\n",
            "Epoch 1369/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6703 - val_loss: 0.6418\n",
            "Epoch 1370/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6676 - val_loss: 0.7082\n",
            "Epoch 1371/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6654 - val_loss: 0.6818\n",
            "Epoch 1372/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6666 - val_loss: 0.6513\n",
            "Epoch 1373/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6671 - val_loss: 0.6498\n",
            "Epoch 1374/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6680 - val_loss: 0.6770\n",
            "Epoch 1375/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6664 - val_loss: 0.6433\n",
            "Epoch 1376/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6686 - val_loss: 0.6583\n",
            "Epoch 1377/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6695 - val_loss: 0.6612\n",
            "Epoch 1378/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6671 - val_loss: 0.6820\n",
            "Epoch 1379/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6667 - val_loss: 0.6796\n",
            "Epoch 1380/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6693 - val_loss: 0.6269\n",
            "Epoch 1381/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6680 - val_loss: 0.6874\n",
            "Epoch 1382/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6664 - val_loss: 0.6733\n",
            "Epoch 1383/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6672 - val_loss: 0.6841\n",
            "Epoch 1384/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6672 - val_loss: 0.6965\n",
            "Epoch 1385/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6652 - val_loss: 0.7108\n",
            "Epoch 1386/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6690 - val_loss: 0.6481\n",
            "Epoch 1387/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6687 - val_loss: 0.6920\n",
            "Epoch 1388/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6691 - val_loss: 0.6687\n",
            "Epoch 1389/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6652 - val_loss: 0.7192\n",
            "Epoch 1390/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6651 - val_loss: 0.7051\n",
            "Epoch 1391/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6640 - val_loss: 0.6553\n",
            "Epoch 1392/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6645 - val_loss: 0.6959\n",
            "Epoch 1393/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6670 - val_loss: 0.6608\n",
            "Epoch 1394/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6676 - val_loss: 0.6890\n",
            "Epoch 1395/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6658 - val_loss: 0.6830\n",
            "Epoch 1396/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6690 - val_loss: 0.6905\n",
            "Epoch 1397/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6675 - val_loss: 0.6965\n",
            "Epoch 1398/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6643 - val_loss: 0.6677\n",
            "Epoch 1399/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6671 - val_loss: 0.6736\n",
            "Epoch 1400/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6661 - val_loss: 0.7082\n",
            "Epoch 1401/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6656 - val_loss: 0.6398\n",
            "Epoch 1402/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6649 - val_loss: 0.6676\n",
            "Epoch 1403/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6656 - val_loss: 0.6822\n",
            "Epoch 1404/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6635 - val_loss: 0.6563\n",
            "Epoch 1405/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6647 - val_loss: 0.6969\n",
            "Epoch 1406/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6639 - val_loss: 0.6938\n",
            "Epoch 1407/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6625 - val_loss: 0.6881\n",
            "Epoch 1408/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6657 - val_loss: 0.6772\n",
            "Epoch 1409/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6653 - val_loss: 0.6920\n",
            "Epoch 1410/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6632 - val_loss: 0.6442\n",
            "Epoch 1411/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6685 - val_loss: 0.6862\n",
            "Epoch 1412/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6679 - val_loss: 0.6816\n",
            "Epoch 1413/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6660 - val_loss: 0.6647\n",
            "Epoch 1414/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6622 - val_loss: 0.6634\n",
            "Epoch 1415/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6603 - val_loss: 0.6912\n",
            "Epoch 1416/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6656 - val_loss: 0.7021\n",
            "Epoch 1417/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6627 - val_loss: 0.6934\n",
            "Epoch 1418/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6637 - val_loss: 0.7079\n",
            "Epoch 1419/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6610 - val_loss: 0.7165\n",
            "Epoch 1420/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6628 - val_loss: 0.7446\n",
            "Epoch 1421/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6604 - val_loss: 0.7648\n",
            "Epoch 1422/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6638 - val_loss: 0.8122\n",
            "Epoch 1423/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6680 - val_loss: 0.7579\n",
            "Epoch 1424/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6626 - val_loss: 0.7317\n",
            "Epoch 1425/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6616 - val_loss: 0.7684\n",
            "Epoch 1426/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6625 - val_loss: 0.7591\n",
            "Epoch 1427/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6602 - val_loss: 0.7601\n",
            "Epoch 1428/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6623 - val_loss: 0.7618\n",
            "Epoch 1429/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6604 - val_loss: 0.7501\n",
            "Epoch 1430/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6600 - val_loss: 0.7820\n",
            "Epoch 1431/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6624 - val_loss: 0.7556\n",
            "Epoch 1432/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6601 - val_loss: 0.7077\n",
            "Epoch 1433/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6610 - val_loss: 0.7842\n",
            "Epoch 1434/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6626 - val_loss: 0.7718\n",
            "Epoch 1435/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6613 - val_loss: 0.7605\n",
            "Epoch 1436/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6590 - val_loss: 0.7821\n",
            "Epoch 1437/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6621 - val_loss: 0.7839\n",
            "Epoch 1438/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6657 - val_loss: 0.7457\n",
            "Epoch 1439/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6576 - val_loss: 0.7081\n",
            "Epoch 1440/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6619 - val_loss: 0.7980\n",
            "Epoch 1441/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6584 - val_loss: 0.7501\n",
            "Epoch 1442/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6599 - val_loss: 0.7752\n",
            "Epoch 1443/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6610 - val_loss: 0.7650\n",
            "Epoch 1444/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6622 - val_loss: 0.7211\n",
            "Epoch 1445/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6617 - val_loss: 0.7455\n",
            "Epoch 1446/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6600 - val_loss: 0.7286\n",
            "Epoch 1447/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6610 - val_loss: 0.7693\n",
            "Epoch 1448/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6627 - val_loss: 0.7533\n",
            "Epoch 1449/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6583 - val_loss: 0.7549\n",
            "Epoch 1450/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6621 - val_loss: 0.7753\n",
            "Epoch 1451/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6616 - val_loss: 0.7558\n",
            "Epoch 1452/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6589 - val_loss: 0.7581\n",
            "Epoch 1453/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6629 - val_loss: 0.7925\n",
            "Epoch 1454/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6574 - val_loss: 0.7664\n",
            "Epoch 1455/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6582 - val_loss: 0.8144\n",
            "Epoch 1456/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6580 - val_loss: 0.8408\n",
            "Epoch 1457/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6603 - val_loss: 0.8084\n",
            "Epoch 1458/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6619 - val_loss: 0.7586\n",
            "Epoch 1459/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6612 - val_loss: 0.8963\n",
            "Epoch 1460/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6595 - val_loss: 0.8091\n",
            "Epoch 1461/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6596 - val_loss: 0.8405\n",
            "Epoch 1462/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6574 - val_loss: 0.8344\n",
            "Epoch 1463/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6610 - val_loss: 0.8432\n",
            "Epoch 1464/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6643 - val_loss: 0.8000\n",
            "Epoch 1465/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6579 - val_loss: 0.8407\n",
            "Epoch 1466/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6590 - val_loss: 0.7993\n",
            "Epoch 1467/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6616 - val_loss: 0.7513\n",
            "Epoch 1468/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6562 - val_loss: 0.7927\n",
            "Epoch 1469/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6551 - val_loss: 0.8704\n",
            "Epoch 1470/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6632 - val_loss: 0.8844\n",
            "Epoch 1471/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6612 - val_loss: 0.8826\n",
            "Epoch 1472/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6597 - val_loss: 0.9710\n",
            "Epoch 1473/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6613 - val_loss: 1.0073\n",
            "Epoch 1474/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6577 - val_loss: 0.9098\n",
            "Epoch 1475/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6597 - val_loss: 0.8883\n",
            "Epoch 1476/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6621 - val_loss: 1.0529\n",
            "Epoch 1477/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6524 - val_loss: 1.0347\n",
            "Epoch 1478/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6569 - val_loss: 1.0262\n",
            "Epoch 1479/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6584 - val_loss: 1.0688\n",
            "Epoch 1480/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6606 - val_loss: 1.0006\n",
            "Epoch 1481/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6560 - val_loss: 0.9613\n",
            "Epoch 1482/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6539 - val_loss: 0.9492\n",
            "Epoch 1483/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6569 - val_loss: 1.0686\n",
            "Epoch 1484/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6563 - val_loss: 1.1125\n",
            "Epoch 1485/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6594 - val_loss: 1.0471\n",
            "Epoch 1486/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6558 - val_loss: 1.0037\n",
            "Epoch 1487/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6538 - val_loss: 0.9824\n",
            "Epoch 1488/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6549 - val_loss: 1.0134\n",
            "Epoch 1489/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6560 - val_loss: 1.0992\n",
            "Epoch 1490/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6627 - val_loss: 0.9802\n",
            "Epoch 1491/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6555 - val_loss: 1.1396\n",
            "Epoch 1492/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6552 - val_loss: 1.1362\n",
            "Epoch 1493/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6544 - val_loss: 1.0201\n",
            "Epoch 1494/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6580 - val_loss: 1.0685\n",
            "Epoch 1495/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6527 - val_loss: 1.0774\n",
            "Epoch 1496/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6567 - val_loss: 1.1699\n",
            "Epoch 1497/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6529 - val_loss: 1.0073\n",
            "Epoch 1498/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6546 - val_loss: 1.0324\n",
            "Epoch 1499/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6551 - val_loss: 1.1714\n",
            "Epoch 1500/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6551 - val_loss: 1.0653\n",
            "Epoch 1501/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6615 - val_loss: 1.1101\n",
            "Epoch 1502/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6538 - val_loss: 1.0285\n",
            "Epoch 1503/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6557 - val_loss: 1.1910\n",
            "Epoch 1504/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6584 - val_loss: 1.1134\n",
            "Epoch 1505/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6538 - val_loss: 1.0543\n",
            "Epoch 1506/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6549 - val_loss: 1.0922\n",
            "Epoch 1507/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6555 - val_loss: 1.1396\n",
            "Epoch 1508/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6573 - val_loss: 1.0322\n",
            "Epoch 1509/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6534 - val_loss: 1.0950\n",
            "Epoch 1510/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6529 - val_loss: 1.2606\n",
            "Epoch 1511/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6575 - val_loss: 1.2085\n",
            "Epoch 1512/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6509 - val_loss: 1.0664\n",
            "Epoch 1513/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6543 - val_loss: 1.1482\n",
            "Epoch 1514/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6547 - val_loss: 1.1691\n",
            "Epoch 1515/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6524 - val_loss: 1.1040\n",
            "Epoch 1516/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6527 - val_loss: 1.0827\n",
            "Epoch 1517/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6603 - val_loss: 1.2677\n",
            "Epoch 1518/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6520 - val_loss: 1.0450\n",
            "Epoch 1519/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6545 - val_loss: 1.1307\n",
            "Epoch 1520/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6552 - val_loss: 1.1195\n",
            "Epoch 1521/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6501 - val_loss: 1.2161\n",
            "Epoch 1522/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6527 - val_loss: 1.1913\n",
            "Epoch 1523/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6557 - val_loss: 1.1979\n",
            "Epoch 1524/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6500 - val_loss: 1.1387\n",
            "Epoch 1525/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6506 - val_loss: 1.0841\n",
            "Epoch 1526/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6547 - val_loss: 1.2434\n",
            "Epoch 1527/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6487 - val_loss: 1.2998\n",
            "Epoch 1528/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6557 - val_loss: 1.3450\n",
            "Epoch 1529/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6512 - val_loss: 1.1072\n",
            "Epoch 1530/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6496 - val_loss: 1.1848\n",
            "Epoch 1531/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6546 - val_loss: 1.1567\n",
            "Epoch 1532/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6528 - val_loss: 1.1303\n",
            "Epoch 1533/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6573 - val_loss: 1.2699\n",
            "Epoch 1534/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6537 - val_loss: 1.1079\n",
            "Epoch 1535/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6523 - val_loss: 1.2082\n",
            "Epoch 1536/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6639 - val_loss: 1.0590\n",
            "Epoch 1537/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6703 - val_loss: 1.2459\n",
            "Epoch 1538/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6629 - val_loss: 1.2366\n",
            "Epoch 1539/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6575 - val_loss: 1.2461\n",
            "Epoch 1540/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6614 - val_loss: 1.2187\n",
            "Epoch 1541/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6572 - val_loss: 1.2311\n",
            "Epoch 1542/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6563 - val_loss: 1.3500\n",
            "Epoch 1543/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6526 - val_loss: 1.3476\n",
            "Epoch 1544/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6515 - val_loss: 1.5525\n",
            "Epoch 1545/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6499 - val_loss: 1.2465\n",
            "Epoch 1546/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6514 - val_loss: 1.5243\n",
            "Epoch 1547/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6537 - val_loss: 1.2191\n",
            "Epoch 1548/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6504 - val_loss: 1.2009\n",
            "Epoch 1549/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6508 - val_loss: 1.3752\n",
            "Epoch 1550/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6489 - val_loss: 1.4731\n",
            "Epoch 1551/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6501 - val_loss: 1.2935\n",
            "Epoch 1552/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6556 - val_loss: 1.3060\n",
            "Epoch 1553/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6568 - val_loss: 1.2501\n",
            "Epoch 1554/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6554 - val_loss: 1.2294\n",
            "Epoch 1555/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6507 - val_loss: 1.4865\n",
            "Epoch 1556/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6482 - val_loss: 1.4456\n",
            "Epoch 1557/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6514 - val_loss: 1.3657\n",
            "Epoch 1558/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6521 - val_loss: 1.3972\n",
            "Epoch 1559/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6549 - val_loss: 1.3452\n",
            "Epoch 1560/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6577 - val_loss: 1.3708\n",
            "Epoch 1561/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6538 - val_loss: 1.2682\n",
            "Epoch 1562/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6505 - val_loss: 1.6063\n",
            "Epoch 1563/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6491 - val_loss: 1.3210\n",
            "Epoch 1564/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6477 - val_loss: 1.4978\n",
            "Epoch 1565/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6479 - val_loss: 1.3473\n",
            "Epoch 1566/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6462 - val_loss: 1.2644\n",
            "Epoch 1567/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6538 - val_loss: 1.4827\n",
            "Epoch 1568/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6563 - val_loss: 1.2186\n",
            "Epoch 1569/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6548 - val_loss: 1.2551\n",
            "Epoch 1570/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6567 - val_loss: 1.4124\n",
            "Epoch 1571/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6508 - val_loss: 1.5401\n",
            "Epoch 1572/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6516 - val_loss: 1.3527\n",
            "Epoch 1573/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6481 - val_loss: 1.2228\n",
            "Epoch 1574/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6493 - val_loss: 1.7410\n",
            "Epoch 1575/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6483 - val_loss: 1.2838\n",
            "Epoch 1576/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6465 - val_loss: 1.3787\n",
            "Epoch 1577/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6528 - val_loss: 1.2245\n",
            "Epoch 1578/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6598 - val_loss: 1.2604\n",
            "Epoch 1579/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6520 - val_loss: 1.3584\n",
            "Epoch 1580/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6524 - val_loss: 1.4392\n",
            "Epoch 1581/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6485 - val_loss: 1.3215\n",
            "Epoch 1582/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6485 - val_loss: 1.4012\n",
            "Epoch 1583/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6548 - val_loss: 1.3556\n",
            "Epoch 1584/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6549 - val_loss: 1.3519\n",
            "Epoch 1585/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6523 - val_loss: 1.2178\n",
            "Epoch 1586/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6528 - val_loss: 1.6365\n",
            "Epoch 1587/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6491 - val_loss: 1.4974\n",
            "Epoch 1588/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6524 - val_loss: 1.5378\n",
            "Epoch 1589/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6454 - val_loss: 1.4495\n",
            "Epoch 1590/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6431 - val_loss: 1.3852\n",
            "Epoch 1591/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6495 - val_loss: 1.4099\n",
            "Epoch 1592/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6516 - val_loss: 1.4543\n",
            "Epoch 1593/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6572 - val_loss: 1.2822\n",
            "Epoch 1594/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6607 - val_loss: 1.4727\n",
            "Epoch 1595/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6596 - val_loss: 1.3453\n",
            "Epoch 1596/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6553 - val_loss: 1.3118\n",
            "Epoch 1597/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6535 - val_loss: 1.7225\n",
            "Epoch 1598/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6461 - val_loss: 1.3345\n",
            "Epoch 1599/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6487 - val_loss: 1.4721\n",
            "Epoch 1600/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6505 - val_loss: 1.9926\n",
            "Epoch 1601/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6533 - val_loss: 1.4413\n",
            "Epoch 1602/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6445 - val_loss: 1.2737\n",
            "Epoch 1603/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6491 - val_loss: 1.6082\n",
            "Epoch 1604/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6505 - val_loss: 1.4226\n",
            "Epoch 1605/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6475 - val_loss: 1.3596\n",
            "Epoch 1606/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6514 - val_loss: 1.4521\n",
            "Epoch 1607/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6549 - val_loss: 1.4310\n",
            "Epoch 1608/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6507 - val_loss: 1.3952\n",
            "Epoch 1609/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6533 - val_loss: 1.4806\n",
            "Epoch 1610/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6509 - val_loss: 1.5854\n",
            "Epoch 1611/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6489 - val_loss: 1.4606\n",
            "Epoch 1612/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6467 - val_loss: 1.4795\n",
            "Epoch 1613/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6495 - val_loss: 1.3749\n",
            "Epoch 1614/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6569 - val_loss: 1.5439\n",
            "Epoch 1615/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6510 - val_loss: 1.3249\n",
            "Epoch 1616/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6470 - val_loss: 1.4758\n",
            "Epoch 1617/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6489 - val_loss: 1.5965\n",
            "Epoch 1618/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6499 - val_loss: 1.3338\n",
            "Epoch 1619/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6450 - val_loss: 1.7430\n",
            "Epoch 1620/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6486 - val_loss: 1.2822\n",
            "Epoch 1621/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6494 - val_loss: 1.4184\n",
            "Epoch 1622/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6495 - val_loss: 1.4458\n",
            "Epoch 1623/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6471 - val_loss: 1.5488\n",
            "Epoch 1624/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6561 - val_loss: 1.2676\n",
            "Epoch 1625/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6532 - val_loss: 1.6575\n",
            "Epoch 1626/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6513 - val_loss: 1.6980\n",
            "Epoch 1627/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6483 - val_loss: 1.4818\n",
            "Epoch 1628/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6492 - val_loss: 1.8554\n",
            "Epoch 1629/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6493 - val_loss: 1.5892\n",
            "Epoch 1630/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6481 - val_loss: 1.4672\n",
            "Epoch 1631/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6449 - val_loss: 1.5869\n",
            "Epoch 1632/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6506 - val_loss: 1.3626\n",
            "Epoch 1633/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6501 - val_loss: 1.6412\n",
            "Epoch 1634/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6495 - val_loss: 1.5616\n",
            "Epoch 1635/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6460 - val_loss: 1.6912\n",
            "Epoch 1636/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6459 - val_loss: 1.6210\n",
            "Epoch 1637/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6454 - val_loss: 1.5533\n",
            "Epoch 1638/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6458 - val_loss: 1.7546\n",
            "Epoch 1639/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6502 - val_loss: 1.5019\n",
            "Epoch 1640/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6513 - val_loss: 1.5091\n",
            "Epoch 1641/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6498 - val_loss: 1.5452\n",
            "Epoch 1642/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6493 - val_loss: 1.5134\n",
            "Epoch 1643/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6462 - val_loss: 1.6537\n",
            "Epoch 1644/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6498 - val_loss: 1.3659\n",
            "Epoch 1645/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6458 - val_loss: 1.5982\n",
            "Epoch 1646/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6454 - val_loss: 1.5645\n",
            "Epoch 1647/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6498 - val_loss: 1.4212\n",
            "Epoch 1648/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6469 - val_loss: 1.7107\n",
            "Epoch 1649/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6590 - val_loss: 1.4889\n",
            "Epoch 1650/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6473 - val_loss: 1.6639\n",
            "Epoch 1651/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6501 - val_loss: 1.7018\n",
            "Epoch 1652/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6462 - val_loss: 1.7188\n",
            "Epoch 1653/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6445 - val_loss: 1.5383\n",
            "Epoch 1654/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6486 - val_loss: 1.5550\n",
            "Epoch 1655/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6498 - val_loss: 1.4835\n",
            "Epoch 1656/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6481 - val_loss: 1.8705\n",
            "Epoch 1657/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6455 - val_loss: 1.5854\n",
            "Epoch 1658/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6479 - val_loss: 1.5024\n",
            "Epoch 1659/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6485 - val_loss: 1.5767\n",
            "Epoch 1660/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6496 - val_loss: 1.2797\n",
            "Epoch 1661/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6443 - val_loss: 1.4395\n",
            "Epoch 1662/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6500 - val_loss: 1.3330\n",
            "Epoch 1663/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6696 - val_loss: 0.9959\n",
            "Epoch 1664/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6722 - val_loss: 1.2333\n",
            "Epoch 1665/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6800 - val_loss: 1.4984\n",
            "Epoch 1666/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6713 - val_loss: 1.2914\n",
            "Epoch 1667/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6652 - val_loss: 1.5437\n",
            "Epoch 1668/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6618 - val_loss: 1.5686\n",
            "Epoch 1669/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6582 - val_loss: 1.5107\n",
            "Epoch 1670/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6590 - val_loss: 1.3995\n",
            "Epoch 1671/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6589 - val_loss: 1.6042\n",
            "Epoch 1672/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6565 - val_loss: 1.6719\n",
            "Epoch 1673/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6519 - val_loss: 1.4817\n",
            "Epoch 1674/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6577 - val_loss: 1.5248\n",
            "Epoch 1675/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6517 - val_loss: 1.1857\n",
            "Epoch 1676/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6558 - val_loss: 1.3458\n",
            "Epoch 1677/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6569 - val_loss: 1.4943\n",
            "Epoch 1678/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6531 - val_loss: 1.4562\n",
            "Epoch 1679/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6548 - val_loss: 1.4179\n",
            "Epoch 1680/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6555 - val_loss: 1.3757\n",
            "Epoch 1681/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6577 - val_loss: 1.5324\n",
            "Epoch 1682/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6551 - val_loss: 1.7450\n",
            "Epoch 1683/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6526 - val_loss: 1.2660\n",
            "Epoch 1684/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6530 - val_loss: 1.6568\n",
            "Epoch 1685/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6476 - val_loss: 1.6134\n",
            "Epoch 1686/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6470 - val_loss: 1.3821\n",
            "Epoch 1687/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6489 - val_loss: 1.5104\n",
            "Epoch 1688/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6476 - val_loss: 1.5930\n",
            "Epoch 1689/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6470 - val_loss: 1.5181\n",
            "Epoch 1690/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6458 - val_loss: 1.4525\n",
            "Epoch 1691/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6458 - val_loss: 1.4333\n",
            "Epoch 1692/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6463 - val_loss: 1.3972\n",
            "Epoch 1693/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6450 - val_loss: 1.8490\n",
            "Epoch 1694/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6448 - val_loss: 1.6455\n",
            "Epoch 1695/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6439 - val_loss: 1.6690\n",
            "Epoch 1696/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6414 - val_loss: 1.4662\n",
            "Epoch 1697/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6439 - val_loss: 1.7185\n",
            "Epoch 1698/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6473 - val_loss: 1.2609\n",
            "Epoch 1699/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6423 - val_loss: 1.5888\n",
            "Epoch 1700/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6457 - val_loss: 1.2507\n",
            "Epoch 1701/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6534 - val_loss: 1.5407\n",
            "Epoch 1702/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6481 - val_loss: 1.2020\n",
            "Epoch 1703/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6439 - val_loss: 1.5132\n",
            "Epoch 1704/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6427 - val_loss: 1.4944\n",
            "Epoch 1705/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6417 - val_loss: 1.1091\n",
            "Epoch 1706/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6464 - val_loss: 1.4159\n",
            "Epoch 1707/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6443 - val_loss: 1.5101\n",
            "Epoch 1708/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6452 - val_loss: 1.2384\n",
            "Epoch 1709/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6436 - val_loss: 1.1458\n",
            "Epoch 1710/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6462 - val_loss: 1.1157\n",
            "Epoch 1711/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6451 - val_loss: 1.4655\n",
            "Epoch 1712/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6435 - val_loss: 1.5620\n",
            "Epoch 1713/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6491 - val_loss: 1.0690\n",
            "Epoch 1714/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6499 - val_loss: 1.3320\n",
            "Epoch 1715/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6499 - val_loss: 1.2652\n",
            "Epoch 1716/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6468 - val_loss: 1.4570\n",
            "Epoch 1717/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6449 - val_loss: 1.5742\n",
            "Epoch 1718/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6464 - val_loss: 1.3730\n",
            "Epoch 1719/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6454 - val_loss: 1.3827\n",
            "Epoch 1720/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6453 - val_loss: 1.5155\n",
            "Epoch 1721/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6433 - val_loss: 1.5104\n",
            "Epoch 1722/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6451 - val_loss: 1.8366\n",
            "Epoch 1723/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6393 - val_loss: 1.9706\n",
            "Epoch 1724/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6419 - val_loss: 1.4664\n",
            "Epoch 1725/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6443 - val_loss: 1.5478\n",
            "Epoch 1726/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6437 - val_loss: 2.2529\n",
            "Epoch 1727/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6463 - val_loss: 1.8028\n",
            "Epoch 1728/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6454 - val_loss: 1.9324\n",
            "Epoch 1729/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6463 - val_loss: 1.8368\n",
            "Epoch 1730/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6407 - val_loss: 1.8862\n",
            "Epoch 1731/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6442 - val_loss: 1.7788\n",
            "Epoch 1732/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6466 - val_loss: 1.6140\n",
            "Epoch 1733/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6516 - val_loss: 1.3563\n",
            "Epoch 1734/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6494 - val_loss: 1.8480\n",
            "Epoch 1735/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6421 - val_loss: 1.6587\n",
            "Epoch 1736/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6428 - val_loss: 1.5133\n",
            "Epoch 1737/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6395 - val_loss: 1.5607\n",
            "Epoch 1738/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6430 - val_loss: 1.3384\n",
            "Epoch 1739/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6461 - val_loss: 1.4719\n",
            "Epoch 1740/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6515 - val_loss: 1.2183\n",
            "Epoch 1741/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6479 - val_loss: 1.7264\n",
            "Epoch 1742/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6435 - val_loss: 1.6169\n",
            "Epoch 1743/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6437 - val_loss: 1.2652\n",
            "Epoch 1744/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6484 - val_loss: 1.4293\n",
            "Epoch 1745/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6442 - val_loss: 1.6118\n",
            "Epoch 1746/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6422 - val_loss: 2.0582\n",
            "Epoch 1747/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6448 - val_loss: 1.7194\n",
            "Epoch 1748/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6433 - val_loss: 1.5704\n",
            "Epoch 1749/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6395 - val_loss: 1.4277\n",
            "Epoch 1750/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6515 - val_loss: 2.2335\n",
            "Epoch 1751/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6400 - val_loss: 1.2246\n",
            "Epoch 1752/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6478 - val_loss: 1.4249\n",
            "Epoch 1753/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6453 - val_loss: 1.1009\n",
            "Epoch 1754/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6467 - val_loss: 1.1087\n",
            "Epoch 1755/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6463 - val_loss: 1.3310\n",
            "Epoch 1756/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6506 - val_loss: 1.0656\n",
            "Epoch 1757/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6477 - val_loss: 1.1373\n",
            "Epoch 1758/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6446 - val_loss: 1.5263\n",
            "Epoch 1759/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6528 - val_loss: 1.2423\n",
            "Epoch 1760/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6499 - val_loss: 1.1099\n",
            "Epoch 1761/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6496 - val_loss: 0.9369\n",
            "Epoch 1762/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6496 - val_loss: 1.2630\n",
            "Epoch 1763/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6460 - val_loss: 1.3728\n",
            "Epoch 1764/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6445 - val_loss: 1.3110\n",
            "Epoch 1765/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6425 - val_loss: 1.7176\n",
            "Epoch 1766/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6393 - val_loss: 1.4529\n",
            "Epoch 1767/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6423 - val_loss: 1.2712\n",
            "Epoch 1768/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6429 - val_loss: 1.3368\n",
            "Epoch 1769/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6413 - val_loss: 1.1023\n",
            "Epoch 1770/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6452 - val_loss: 1.0366\n",
            "Epoch 1771/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6454 - val_loss: 1.4988\n",
            "Epoch 1772/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7126 - val_loss: 0.5818\n",
            "Epoch 1773/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6736 - val_loss: 0.5398\n",
            "Epoch 1774/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6716 - val_loss: 0.4511\n",
            "Epoch 1775/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6658 - val_loss: 0.4841\n",
            "Epoch 1776/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6621 - val_loss: 0.4986\n",
            "Epoch 1777/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6582 - val_loss: 0.5687\n",
            "Epoch 1778/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6581 - val_loss: 0.5973\n",
            "Epoch 1779/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6543 - val_loss: 0.5684\n",
            "Epoch 1780/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6550 - val_loss: 0.6133\n",
            "Epoch 1781/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6507 - val_loss: 0.6368\n",
            "Epoch 1782/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6530 - val_loss: 0.5968\n",
            "Epoch 1783/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6474 - val_loss: 0.6283\n",
            "Epoch 1784/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6487 - val_loss: 0.6364\n",
            "Epoch 1785/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6478 - val_loss: 0.6165\n",
            "Epoch 1786/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6477 - val_loss: 0.7154\n",
            "Epoch 1787/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6440 - val_loss: 0.6858\n",
            "Epoch 1788/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6415 - val_loss: 0.6914\n",
            "Epoch 1789/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6505 - val_loss: 0.7265\n",
            "Epoch 1790/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6475 - val_loss: 0.7679\n",
            "Epoch 1791/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6436 - val_loss: 0.7364\n",
            "Epoch 1792/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6463 - val_loss: 0.7987\n",
            "Epoch 1793/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6452 - val_loss: 0.7585\n",
            "Epoch 1794/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6420 - val_loss: 0.7932\n",
            "Epoch 1795/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6433 - val_loss: 0.7312\n",
            "Epoch 1796/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6433 - val_loss: 0.7937\n",
            "Epoch 1797/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6399 - val_loss: 0.7845\n",
            "Epoch 1798/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6394 - val_loss: 0.6605\n",
            "Epoch 1799/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6488 - val_loss: 0.7269\n",
            "Epoch 1800/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6449 - val_loss: 0.6927\n",
            "Epoch 1801/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6482 - val_loss: 0.7421\n",
            "Epoch 1802/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6437 - val_loss: 0.6912\n",
            "Epoch 1803/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6432 - val_loss: 0.7989\n",
            "Epoch 1804/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6419 - val_loss: 0.7179\n",
            "Epoch 1805/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6399 - val_loss: 0.6772\n",
            "Epoch 1806/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6402 - val_loss: 0.6299\n",
            "Epoch 1807/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6391 - val_loss: 0.7248\n",
            "Epoch 1808/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6383 - val_loss: 0.8905\n",
            "Epoch 1809/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7113 - val_loss: 1.0237\n",
            "Epoch 1810/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6535 - val_loss: 0.7292\n",
            "Epoch 1811/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6542 - val_loss: 0.8136\n",
            "Epoch 1812/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6538 - val_loss: 0.8124\n",
            "Epoch 1813/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6511 - val_loss: 0.7247\n",
            "Epoch 1814/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6535 - val_loss: 0.8015\n",
            "Epoch 1815/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6499 - val_loss: 0.7780\n",
            "Epoch 1816/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6525 - val_loss: 0.5915\n",
            "Epoch 1817/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6511 - val_loss: 0.5917\n",
            "Epoch 1818/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6494 - val_loss: 0.8570\n",
            "Epoch 1819/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6493 - val_loss: 0.7335\n",
            "Epoch 1820/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6469 - val_loss: 0.8911\n",
            "Epoch 1821/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6503 - val_loss: 0.7023\n",
            "Epoch 1822/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6530 - val_loss: 0.8397\n",
            "Epoch 1823/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6498 - val_loss: 0.6135\n",
            "Epoch 1824/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6494 - val_loss: 0.5246\n",
            "Epoch 1825/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6572 - val_loss: 0.5035\n",
            "Epoch 1826/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6487 - val_loss: 0.7295\n",
            "Epoch 1827/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6496 - val_loss: 0.6371\n",
            "Epoch 1828/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6473 - val_loss: 0.7797\n",
            "Epoch 1829/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6459 - val_loss: 0.7029\n",
            "Epoch 1830/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6470 - val_loss: 0.8175\n",
            "Epoch 1831/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6521 - val_loss: 0.6501\n",
            "Epoch 1832/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6506 - val_loss: 0.7044\n",
            "Epoch 1833/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6527 - val_loss: 0.7624\n",
            "Epoch 1834/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6503 - val_loss: 0.8323\n",
            "Epoch 1835/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6472 - val_loss: 0.7461\n",
            "Epoch 1836/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6472 - val_loss: 0.7584\n",
            "Epoch 1837/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6464 - val_loss: 0.7550\n",
            "Epoch 1838/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6473 - val_loss: 0.6997\n",
            "Epoch 1839/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6449 - val_loss: 0.6443\n",
            "Epoch 1840/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6479 - val_loss: 0.7986\n",
            "Epoch 1841/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6485 - val_loss: 0.7033\n",
            "Epoch 1842/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6484 - val_loss: 0.7271\n",
            "Epoch 1843/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6438 - val_loss: 0.8266\n",
            "Epoch 1844/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6469 - val_loss: 0.7772\n",
            "Epoch 1845/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6448 - val_loss: 0.7536\n",
            "Epoch 1846/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6438 - val_loss: 0.7128\n",
            "Epoch 1847/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6441 - val_loss: 0.6789\n",
            "Epoch 1848/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6515 - val_loss: 0.7388\n",
            "Epoch 1849/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6459 - val_loss: 0.6591\n",
            "Epoch 1850/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6464 - val_loss: 0.7271\n",
            "Epoch 1851/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6457 - val_loss: 0.7364\n",
            "Epoch 1852/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6450 - val_loss: 0.6838\n",
            "Epoch 1853/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6442 - val_loss: 0.6892\n",
            "Epoch 1854/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6483 - val_loss: 0.7242\n",
            "Epoch 1855/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6479 - val_loss: 0.7642\n",
            "Epoch 1856/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6471 - val_loss: 0.7785\n",
            "Epoch 1857/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6468 - val_loss: 0.7549\n",
            "Epoch 1858/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6441 - val_loss: 0.8142\n",
            "Epoch 1859/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6459 - val_loss: 0.8179\n",
            "Epoch 1860/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6489 - val_loss: 0.7592\n",
            "Epoch 1861/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6495 - val_loss: 0.7779\n",
            "Epoch 1862/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6464 - val_loss: 0.6097\n",
            "Epoch 1863/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6460 - val_loss: 0.7084\n",
            "Epoch 1864/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6473 - val_loss: 0.8108\n",
            "Epoch 1865/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6444 - val_loss: 0.7823\n",
            "Epoch 1866/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6474 - val_loss: 0.7713\n",
            "Epoch 1867/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6446 - val_loss: 0.7533\n",
            "Epoch 1868/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6455 - val_loss: 0.8060\n",
            "Epoch 1869/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6498 - val_loss: 0.6907\n",
            "Epoch 1870/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6468 - val_loss: 0.8222\n",
            "Epoch 1871/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6463 - val_loss: 0.8105\n",
            "Epoch 1872/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6465 - val_loss: 0.7084\n",
            "Epoch 1873/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6474 - val_loss: 0.6156\n",
            "Epoch 1874/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6458 - val_loss: 0.7752\n",
            "Epoch 1875/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6469 - val_loss: 0.9990\n",
            "Epoch 1876/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6522 - val_loss: 0.9637\n",
            "Epoch 1877/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6478 - val_loss: 0.7041\n",
            "Epoch 1878/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6460 - val_loss: 0.8363\n",
            "Epoch 1879/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6473 - val_loss: 0.6811\n",
            "Epoch 1880/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6433 - val_loss: 0.7193\n",
            "Epoch 1881/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6450 - val_loss: 0.7794\n",
            "Epoch 1882/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6448 - val_loss: 0.6686\n",
            "Epoch 1883/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6443 - val_loss: 0.6583\n",
            "Epoch 1884/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6472 - val_loss: 0.7460\n",
            "Epoch 1885/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6484 - val_loss: 0.7076\n",
            "Epoch 1886/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6525 - val_loss: 0.6897\n",
            "Epoch 1887/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6521 - val_loss: 0.6978\n",
            "Epoch 1888/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6474 - val_loss: 0.6552\n",
            "Epoch 1889/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6484 - val_loss: 0.7633\n",
            "Epoch 1890/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6471 - val_loss: 0.7431\n",
            "Epoch 1891/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6454 - val_loss: 0.6914\n",
            "Epoch 1892/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6459 - val_loss: 0.7303\n",
            "Epoch 1893/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6467 - val_loss: 0.6286\n",
            "Epoch 1894/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6457 - val_loss: 0.6541\n",
            "Epoch 1895/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6441 - val_loss: 0.6134\n",
            "Epoch 1896/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6449 - val_loss: 0.7320\n",
            "Epoch 1897/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6478 - val_loss: 0.6970\n",
            "Epoch 1898/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6440 - val_loss: 0.6355\n",
            "Epoch 1899/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6466 - val_loss: 0.7158\n",
            "Epoch 1900/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6490 - val_loss: 0.7875\n",
            "Epoch 1901/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6462 - val_loss: 0.7623\n",
            "Epoch 1902/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6486 - val_loss: 0.7892\n",
            "Epoch 1903/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6462 - val_loss: 0.6821\n",
            "Epoch 1904/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6548 - val_loss: 0.5657\n",
            "Epoch 1905/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6528 - val_loss: 0.6662\n",
            "Epoch 1906/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6475 - val_loss: 0.5940\n",
            "Epoch 1907/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6485 - val_loss: 0.6070\n",
            "Epoch 1908/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6491 - val_loss: 0.6546\n",
            "Epoch 1909/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6477 - val_loss: 0.5545\n",
            "Epoch 1910/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6463 - val_loss: 0.6289\n",
            "Epoch 1911/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6455 - val_loss: 0.5619\n",
            "Epoch 1912/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6458 - val_loss: 0.5873\n",
            "Epoch 1913/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6464 - val_loss: 0.5118\n",
            "Epoch 1914/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6435 - val_loss: 0.6982\n",
            "Epoch 1915/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6448 - val_loss: 0.6846\n",
            "Epoch 1916/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6464 - val_loss: 0.6608\n",
            "Epoch 1917/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6460 - val_loss: 0.5751\n",
            "Epoch 1918/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6510 - val_loss: 0.6194\n",
            "Epoch 1919/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6514 - val_loss: 0.5385\n",
            "Epoch 1920/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6515 - val_loss: 0.6483\n",
            "Epoch 1921/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6453 - val_loss: 0.7413\n",
            "Epoch 1922/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6502 - val_loss: 0.6376\n",
            "Epoch 1923/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6501 - val_loss: 0.6172\n",
            "Epoch 1924/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6488 - val_loss: 0.6962\n",
            "Epoch 1925/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6465 - val_loss: 0.5586\n",
            "Epoch 1926/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6485 - val_loss: 0.5109\n",
            "Epoch 1927/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6466 - val_loss: 0.6234\n",
            "Epoch 1928/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6480 - val_loss: 0.6162\n",
            "Epoch 1929/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6456 - val_loss: 0.5867\n",
            "Epoch 1930/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6487 - val_loss: 0.5994\n",
            "Epoch 1931/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6472 - val_loss: 0.6205\n",
            "Epoch 1932/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6474 - val_loss: 0.7002\n",
            "Epoch 1933/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6456 - val_loss: 0.5689\n",
            "Epoch 1934/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6509 - val_loss: 0.5813\n",
            "Epoch 1935/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6446 - val_loss: 0.5937\n",
            "Epoch 1936/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6485 - val_loss: 0.6968\n",
            "Epoch 1937/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6461 - val_loss: 0.5860\n",
            "Epoch 1938/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6457 - val_loss: 0.5355\n",
            "Epoch 1939/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6485 - val_loss: 0.5480\n",
            "Epoch 1940/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6489 - val_loss: 0.5883\n",
            "Epoch 1941/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6436 - val_loss: 0.6229\n",
            "Epoch 1942/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6434 - val_loss: 0.6089\n",
            "Epoch 1943/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6697 - val_loss: 0.6015\n",
            "Epoch 1944/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6521 - val_loss: 0.6057\n",
            "Epoch 1945/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6475 - val_loss: 0.6402\n",
            "Epoch 1946/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6479 - val_loss: 0.5949\n",
            "Epoch 1947/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6463 - val_loss: 0.5845\n",
            "Epoch 1948/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6467 - val_loss: 0.6445\n",
            "Epoch 1949/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6482 - val_loss: 0.5011\n",
            "Epoch 1950/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6488 - val_loss: 0.5351\n",
            "Epoch 1951/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6474 - val_loss: 0.6393\n",
            "Epoch 1952/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6485 - val_loss: 0.6623\n",
            "Epoch 1953/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6463 - val_loss: 0.5157\n",
            "Epoch 1954/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6493 - val_loss: 0.5454\n",
            "Epoch 1955/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6499 - val_loss: 0.5271\n",
            "Epoch 1956/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6507 - val_loss: 0.5849\n",
            "Epoch 1957/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6496 - val_loss: 0.5454\n",
            "Epoch 1958/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6451 - val_loss: 0.5976\n",
            "Epoch 1959/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6549 - val_loss: 0.5990\n",
            "Epoch 1960/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6586 - val_loss: 0.5950\n",
            "Epoch 1961/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6547 - val_loss: 0.5538\n",
            "Epoch 1962/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6526 - val_loss: 0.6439\n",
            "Epoch 1963/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6528 - val_loss: 0.5531\n",
            "Epoch 1964/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6474 - val_loss: 0.5917\n",
            "Epoch 1965/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6512 - val_loss: 0.6569\n",
            "Epoch 1966/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6580 - val_loss: 0.6306\n",
            "Epoch 1967/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6459 - val_loss: 0.6812\n",
            "Epoch 1968/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6965 - val_loss: 0.5933\n",
            "Epoch 1969/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6956 - val_loss: 0.7300\n",
            "Epoch 1970/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6745 - val_loss: 0.6083\n",
            "Epoch 1971/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6710 - val_loss: 0.5652\n",
            "Epoch 1972/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6686 - val_loss: 0.6373\n",
            "Epoch 1973/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6710 - val_loss: 0.5744\n",
            "Epoch 1974/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6701 - val_loss: 0.5444\n",
            "Epoch 1975/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6638 - val_loss: 0.5613\n",
            "Epoch 1976/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6656 - val_loss: 0.5927\n",
            "Epoch 1977/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6660 - val_loss: 0.5728\n",
            "Epoch 1978/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6605 - val_loss: 0.5528\n",
            "Epoch 1979/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6616 - val_loss: 0.5462\n",
            "Epoch 1980/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6608 - val_loss: 0.5219\n",
            "Epoch 1981/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6628 - val_loss: 0.5087\n",
            "Epoch 1982/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6648 - val_loss: 0.4659\n",
            "Epoch 1983/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6622 - val_loss: 0.6181\n",
            "Epoch 1984/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6622 - val_loss: 0.5482\n",
            "Epoch 1985/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6641 - val_loss: 0.5628\n",
            "Epoch 1986/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6598 - val_loss: 0.5208\n",
            "Epoch 1987/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6620 - val_loss: 0.4966\n",
            "Epoch 1988/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6625 - val_loss: 0.4727\n",
            "Epoch 1989/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6594 - val_loss: 0.4817\n",
            "Epoch 1990/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6632 - val_loss: 0.5135\n",
            "Epoch 1991/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6612 - val_loss: 0.4728\n",
            "Epoch 1992/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6631 - val_loss: 0.5608\n",
            "Epoch 1993/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6635 - val_loss: 0.5157\n",
            "Epoch 1994/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6610 - val_loss: 0.5140\n",
            "Epoch 1995/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6628 - val_loss: 0.4809\n",
            "Epoch 1996/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6643 - val_loss: 0.4825\n",
            "Epoch 1997/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6620 - val_loss: 0.4403\n",
            "Epoch 1998/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6647 - val_loss: 0.4642\n",
            "Epoch 1999/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6615 - val_loss: 0.4341\n",
            "Epoch 2000/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6586 - val_loss: 0.4885\n",
            "Epoch 2001/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6609 - val_loss: 0.4713\n",
            "Epoch 2002/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6581 - val_loss: 0.4510\n",
            "Epoch 2003/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6593 - val_loss: 0.4748\n",
            "Epoch 2004/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6587 - val_loss: 0.4854\n",
            "Epoch 2005/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6591 - val_loss: 0.4792\n",
            "Epoch 2006/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6588 - val_loss: 0.4729\n",
            "Epoch 2007/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6576 - val_loss: 0.4691\n",
            "Epoch 2008/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6576 - val_loss: 0.4682\n",
            "Epoch 2009/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6570 - val_loss: 0.4778\n",
            "Epoch 2010/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6575 - val_loss: 0.4609\n",
            "Epoch 2011/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6604 - val_loss: 0.4779\n",
            "Epoch 2012/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6585 - val_loss: 0.4439\n",
            "Epoch 2013/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6585 - val_loss: 0.4943\n",
            "Epoch 2014/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6580 - val_loss: 0.4851\n",
            "Epoch 2015/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6615 - val_loss: 0.4646\n",
            "Epoch 2016/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6613 - val_loss: 0.4537\n",
            "Epoch 2017/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6587 - val_loss: 0.5186\n",
            "Epoch 2018/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6620 - val_loss: 0.5012\n",
            "Epoch 2019/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6608 - val_loss: 0.4639\n",
            "Epoch 2020/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6575 - val_loss: 0.4890\n",
            "Epoch 2021/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6567 - val_loss: 0.4651\n",
            "Epoch 2022/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6667 - val_loss: 0.4480\n",
            "Epoch 2023/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6599 - val_loss: 0.4962\n",
            "Epoch 2024/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6598 - val_loss: 0.4949\n",
            "Epoch 2025/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6574 - val_loss: 0.4681\n",
            "Epoch 2026/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6576 - val_loss: 0.4503\n",
            "Epoch 2027/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6582 - val_loss: 0.4839\n",
            "Epoch 2028/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6584 - val_loss: 0.5357\n",
            "Epoch 2029/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6588 - val_loss: 0.5046\n",
            "Epoch 2030/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6587 - val_loss: 0.4776\n",
            "Epoch 2031/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6590 - val_loss: 0.4685\n",
            "Epoch 2032/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6563 - val_loss: 0.4757\n",
            "Epoch 2033/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6598 - val_loss: 0.4509\n",
            "Epoch 2034/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6561 - val_loss: 0.5168\n",
            "Epoch 2035/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6629 - val_loss: 0.4803\n",
            "Epoch 2036/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6578 - val_loss: 0.4918\n",
            "Epoch 2037/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6611 - val_loss: 0.4281\n",
            "Epoch 2038/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6569 - val_loss: 0.5271\n",
            "Epoch 2039/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6604 - val_loss: 0.5053\n",
            "Epoch 2040/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6586 - val_loss: 0.4694\n",
            "Epoch 2041/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6577 - val_loss: 0.5215\n",
            "Epoch 2042/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6599 - val_loss: 0.4652\n",
            "Epoch 2043/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6605 - val_loss: 0.4501\n",
            "Epoch 2044/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6567 - val_loss: 0.4295\n",
            "Epoch 2045/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6561 - val_loss: 0.4657\n",
            "Epoch 2046/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6566 - val_loss: 0.4558\n",
            "Epoch 2047/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6561 - val_loss: 0.4676\n",
            "Epoch 2048/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6574 - val_loss: 0.4882\n",
            "Epoch 2049/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6568 - val_loss: 0.4656\n",
            "Epoch 2050/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6567 - val_loss: 0.4793\n",
            "Epoch 2051/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6566 - val_loss: 0.4443\n",
            "Epoch 2052/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6579 - val_loss: 0.4374\n",
            "Epoch 2053/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6573 - val_loss: 0.4611\n",
            "Epoch 2054/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6572 - val_loss: 0.4632\n",
            "Epoch 2055/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6605 - val_loss: 0.4947\n",
            "Epoch 2056/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6547 - val_loss: 0.4673\n",
            "Epoch 2057/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6569 - val_loss: 0.4538\n",
            "Epoch 2058/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6578 - val_loss: 0.4561\n",
            "Epoch 2059/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6592 - val_loss: 0.4569\n",
            "Epoch 2060/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6584 - val_loss: 0.5088\n",
            "Epoch 2061/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6588 - val_loss: 0.4775\n",
            "Epoch 2062/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6613 - val_loss: 0.4359\n",
            "Epoch 2063/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6609 - val_loss: 0.4541\n",
            "Epoch 2064/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6548 - val_loss: 0.4206\n",
            "Epoch 2065/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6587 - val_loss: 0.4222\n",
            "Epoch 2066/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6551 - val_loss: 0.4303\n",
            "Epoch 2067/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6638 - val_loss: 0.4476\n",
            "Epoch 2068/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6558 - val_loss: 0.4661\n",
            "Epoch 2069/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6601 - val_loss: 0.4369\n",
            "Epoch 2070/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6646 - val_loss: 0.5005\n",
            "Epoch 2071/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6625 - val_loss: 0.4467\n",
            "Epoch 2072/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6587 - val_loss: 0.4498\n",
            "Epoch 2073/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6600 - val_loss: 0.4442\n",
            "Epoch 2074/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6565 - val_loss: 0.4449\n",
            "Epoch 2075/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6587 - val_loss: 0.4551\n",
            "Epoch 2076/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6558 - val_loss: 0.4766\n",
            "Epoch 2077/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6574 - val_loss: 0.4623\n",
            "Epoch 2078/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6608 - val_loss: 0.4634\n",
            "Epoch 2079/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6616 - val_loss: 0.4429\n",
            "Epoch 2080/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6577 - val_loss: 0.4595\n",
            "Epoch 2081/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6589 - val_loss: 0.4540\n",
            "Epoch 2082/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6630 - val_loss: 0.4693\n",
            "Epoch 2083/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6601 - val_loss: 0.5021\n",
            "Epoch 2084/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6599 - val_loss: 0.4358\n",
            "Epoch 2085/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6570 - val_loss: 0.4506\n",
            "Epoch 2086/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6575 - val_loss: 0.4180\n",
            "Epoch 2087/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6604 - val_loss: 0.4468\n",
            "Epoch 2088/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6684 - val_loss: 0.4101\n",
            "Epoch 2089/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6782 - val_loss: 0.4174\n",
            "Epoch 2090/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6559 - val_loss: 0.4283\n",
            "Epoch 2091/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6610 - val_loss: 0.4177\n",
            "Epoch 2092/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6575 - val_loss: 0.4456\n",
            "Epoch 2093/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6562 - val_loss: 0.4446\n",
            "Epoch 2094/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6577 - val_loss: 0.4398\n",
            "Epoch 2095/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6546 - val_loss: 0.4284\n",
            "Epoch 2096/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6560 - val_loss: 0.4242\n",
            "Epoch 2097/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6613 - val_loss: 0.4194\n",
            "Epoch 2098/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6591 - val_loss: 0.4225\n",
            "Epoch 2099/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6599 - val_loss: 0.4249\n",
            "Epoch 2100/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6569 - val_loss: 0.4262\n",
            "Epoch 2101/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6559 - val_loss: 0.4300\n",
            "Epoch 2102/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6567 - val_loss: 0.4377\n",
            "Epoch 2103/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6544 - val_loss: 0.4118\n",
            "Epoch 2104/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6585 - val_loss: 0.4130\n",
            "Epoch 2105/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6598 - val_loss: 0.4175\n",
            "Epoch 2106/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6618 - val_loss: 0.4148\n",
            "Epoch 2107/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6620 - val_loss: 0.4136\n",
            "Epoch 2108/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6603 - val_loss: 0.4211\n",
            "Epoch 2109/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6651 - val_loss: 0.4299\n",
            "Epoch 2110/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6574 - val_loss: 0.4182\n",
            "Epoch 2111/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6582 - val_loss: 0.4199\n",
            "Epoch 2112/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6596 - val_loss: 0.4172\n",
            "Epoch 2113/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6570 - val_loss: 0.4349\n",
            "Epoch 2114/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6574 - val_loss: 0.4371\n",
            "Epoch 2115/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6567 - val_loss: 0.4497\n",
            "Epoch 2116/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6583 - val_loss: 0.4250\n",
            "Epoch 2117/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6574 - val_loss: 0.4582\n",
            "Epoch 2118/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6616 - val_loss: 0.4522\n",
            "Epoch 2119/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6562 - val_loss: 0.4396\n",
            "Epoch 2120/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6564 - val_loss: 0.4392\n",
            "Epoch 2121/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6577 - val_loss: 0.4281\n",
            "Epoch 2122/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6555 - val_loss: 0.4463\n",
            "Epoch 2123/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6599 - val_loss: 0.4560\n",
            "Epoch 2124/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6558 - val_loss: 0.4266\n",
            "Epoch 2125/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6581 - val_loss: 0.4107\n",
            "Epoch 2126/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6630 - val_loss: 0.4157\n",
            "Epoch 2127/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6560 - val_loss: 0.4308\n",
            "Epoch 2128/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6571 - val_loss: 0.4160\n",
            "Epoch 2129/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6545 - val_loss: 0.4536\n",
            "Epoch 2130/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6585 - val_loss: 0.4449\n",
            "Epoch 2131/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6564 - val_loss: 0.4326\n",
            "Epoch 2132/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6564 - val_loss: 0.4243\n",
            "Epoch 2133/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6572 - val_loss: 0.4353\n",
            "Epoch 2134/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6578 - val_loss: 0.4309\n",
            "Epoch 2135/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6601 - val_loss: 0.4335\n",
            "Epoch 2136/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6592 - val_loss: 0.4169\n",
            "Epoch 2137/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6592 - val_loss: 0.4224\n",
            "Epoch 2138/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6569 - val_loss: 0.4274\n",
            "Epoch 2139/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6578 - val_loss: 0.4304\n",
            "Epoch 2140/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6582 - val_loss: 0.4145\n",
            "Epoch 2141/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6585 - val_loss: 0.4334\n",
            "Epoch 2142/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6599 - val_loss: 0.4328\n",
            "Epoch 2143/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6597 - val_loss: 0.4164\n",
            "Epoch 2144/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6570 - val_loss: 0.4475\n",
            "Epoch 2145/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6620 - val_loss: 0.4307\n",
            "Epoch 2146/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6573 - val_loss: 0.4242\n",
            "Epoch 2147/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6596 - val_loss: 0.4324\n",
            "Epoch 2148/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6566 - val_loss: 0.4164\n",
            "Epoch 2149/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6557 - val_loss: 0.4238\n",
            "Epoch 2150/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6610 - val_loss: 0.4170\n",
            "Epoch 2151/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6539 - val_loss: 0.4281\n",
            "Epoch 2152/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6553 - val_loss: 0.4298\n",
            "Epoch 2153/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6582 - val_loss: 0.4262\n",
            "Epoch 2154/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6567 - val_loss: 0.4185\n",
            "Epoch 2155/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6569 - val_loss: 0.4189\n",
            "Epoch 2156/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6556 - val_loss: 0.4120\n",
            "Epoch 2157/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6610 - val_loss: 0.4172\n",
            "Epoch 2158/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6585 - val_loss: 0.4171\n",
            "Epoch 2159/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6581 - val_loss: 0.4174\n",
            "Epoch 2160/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6594 - val_loss: 0.4315\n",
            "Epoch 2161/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6566 - val_loss: 0.4240\n",
            "Epoch 2162/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6586 - val_loss: 0.4193\n",
            "Epoch 2163/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6561 - val_loss: 0.4113\n",
            "Epoch 2164/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6623 - val_loss: 0.4233\n",
            "Epoch 2165/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6601 - val_loss: 0.4270\n",
            "Epoch 2166/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6545 - val_loss: 0.4357\n",
            "Epoch 2167/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6539 - val_loss: 0.4335\n",
            "Epoch 2168/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6576 - val_loss: 0.4330\n",
            "Epoch 2169/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6543 - val_loss: 0.4259\n",
            "Epoch 2170/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6566 - val_loss: 0.4175\n",
            "Epoch 2171/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6583 - val_loss: 0.4375\n",
            "Epoch 2172/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6590 - val_loss: 0.4250\n",
            "Epoch 2173/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6591 - val_loss: 0.4631\n",
            "Epoch 2174/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6554 - val_loss: 0.4305\n",
            "Epoch 2175/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6578 - val_loss: 0.4270\n",
            "Epoch 2176/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6550 - val_loss: 0.4242\n",
            "Epoch 2177/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6566 - val_loss: 0.4192\n",
            "Epoch 2178/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6587 - val_loss: 0.4179\n",
            "Epoch 2179/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6556 - val_loss: 0.4185\n",
            "Epoch 2180/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6565 - val_loss: 0.4165\n",
            "Epoch 2181/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6593 - val_loss: 0.4159\n",
            "Epoch 2182/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6591 - val_loss: 0.4247\n",
            "Epoch 2183/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6604 - val_loss: 0.4201\n",
            "Epoch 2184/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6588 - val_loss: 0.4273\n",
            "Epoch 2185/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6594 - val_loss: 0.4293\n",
            "Epoch 2186/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6558 - val_loss: 0.4338\n",
            "Epoch 2187/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6542 - val_loss: 0.4217\n",
            "Epoch 2188/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6538 - val_loss: 0.4183\n",
            "Epoch 2189/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6570 - val_loss: 0.4281\n",
            "Epoch 2190/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6607 - val_loss: 0.4353\n",
            "Epoch 2191/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6642 - val_loss: 0.4357\n",
            "Epoch 2192/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6558 - val_loss: 0.4341\n",
            "Epoch 2193/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6569 - val_loss: 0.4177\n",
            "Epoch 2194/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6596 - val_loss: 0.4257\n",
            "Epoch 2195/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6568 - val_loss: 0.4328\n",
            "Epoch 2196/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6620 - val_loss: 0.4286\n",
            "Epoch 2197/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6570 - val_loss: 0.4237\n",
            "Epoch 2198/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6566 - val_loss: 0.4192\n",
            "Epoch 2199/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6562 - val_loss: 0.4335\n",
            "Epoch 2200/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6566 - val_loss: 0.4485\n",
            "Epoch 2201/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6551 - val_loss: 0.4322\n",
            "Epoch 2202/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6600 - val_loss: 0.4178\n",
            "Epoch 2203/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6574 - val_loss: 0.4276\n",
            "Epoch 2204/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6597 - val_loss: 0.4789\n",
            "Epoch 2205/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6575 - val_loss: 0.4276\n",
            "Epoch 2206/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6567 - val_loss: 0.4239\n",
            "Epoch 2207/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6552 - val_loss: 0.4221\n",
            "Epoch 2208/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6599 - val_loss: 0.4347\n",
            "Epoch 2209/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6611 - val_loss: 0.4337\n",
            "Epoch 2210/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6571 - val_loss: 0.4583\n",
            "Epoch 2211/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6574 - val_loss: 0.4447\n",
            "Epoch 2212/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6556 - val_loss: 0.4280\n",
            "Epoch 2213/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6636 - val_loss: 0.4260\n",
            "Epoch 2214/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6554 - val_loss: 0.4262\n",
            "Epoch 2215/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6538 - val_loss: 0.4224\n",
            "Epoch 2216/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6572 - val_loss: 0.4188\n",
            "Epoch 2217/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6551 - val_loss: 0.4164\n",
            "Epoch 2218/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6556 - val_loss: 0.4225\n",
            "Epoch 2219/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6559 - val_loss: 0.4341\n",
            "Epoch 2220/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6586 - val_loss: 0.4309\n",
            "Epoch 2221/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6541 - val_loss: 0.4328\n",
            "Epoch 2222/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6561 - val_loss: 0.4292\n",
            "Epoch 2223/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6570 - val_loss: 0.4292\n",
            "Epoch 2224/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6594 - val_loss: 0.4189\n",
            "Epoch 2225/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6553 - val_loss: 0.4273\n",
            "Epoch 2226/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6554 - val_loss: 0.4335\n",
            "Epoch 2227/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6544 - val_loss: 0.4223\n",
            "Epoch 2228/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6576 - val_loss: 0.4193\n",
            "Epoch 2229/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6542 - val_loss: 0.4204\n",
            "Epoch 2230/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6569 - val_loss: 0.4191\n",
            "Epoch 2231/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6546 - val_loss: 0.4169\n",
            "Epoch 2232/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6581 - val_loss: 0.4234\n",
            "Epoch 2233/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6576 - val_loss: 0.4258\n",
            "Epoch 2234/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6567 - val_loss: 0.4177\n",
            "Epoch 2235/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6623 - val_loss: 0.4578\n",
            "Epoch 2236/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6747 - val_loss: 0.4312\n",
            "Epoch 2237/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6608 - val_loss: 1.4049\n",
            "Epoch 2238/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6669 - val_loss: 0.4246\n",
            "Epoch 2239/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6726 - val_loss: 0.4604\n",
            "Epoch 2240/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6562 - val_loss: 0.4305\n",
            "Epoch 2241/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6587 - val_loss: 0.4227\n",
            "Epoch 2242/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6536 - val_loss: 0.4191\n",
            "Epoch 2243/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6555 - val_loss: 0.4256\n",
            "Epoch 2244/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6536 - val_loss: 0.4256\n",
            "Epoch 2245/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6584 - val_loss: 0.4173\n",
            "Epoch 2246/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6545 - val_loss: 0.4185\n",
            "Epoch 2247/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6584 - val_loss: 0.4135\n",
            "Epoch 2248/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6569 - val_loss: 0.4187\n",
            "Epoch 2249/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6558 - val_loss: 0.4302\n",
            "Epoch 2250/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6534 - val_loss: 0.4263\n",
            "Epoch 2251/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6538 - val_loss: 0.4161\n",
            "Epoch 2252/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6599 - val_loss: 0.4149\n",
            "Epoch 2253/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6558 - val_loss: 0.4245\n",
            "Epoch 2254/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6554 - val_loss: 0.4211\n",
            "Epoch 2255/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6571 - val_loss: 0.4181\n",
            "Epoch 2256/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6565 - val_loss: 0.4178\n",
            "Epoch 2257/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6538 - val_loss: 0.4195\n",
            "Epoch 2258/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6565 - val_loss: 0.4158\n",
            "Epoch 2259/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6553 - val_loss: 0.4414\n",
            "Epoch 2260/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6576 - val_loss: 0.4221\n",
            "Epoch 2261/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6580 - val_loss: 0.4232\n",
            "Epoch 2262/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6587 - val_loss: 0.4193\n",
            "Epoch 2263/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6567 - val_loss: 0.4182\n",
            "Epoch 2264/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6560 - val_loss: 0.4419\n",
            "Epoch 2265/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6578 - val_loss: 0.4202\n",
            "Epoch 2266/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6526 - val_loss: 0.4176\n",
            "Epoch 2267/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6536 - val_loss: 0.4148\n",
            "Epoch 2268/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6550 - val_loss: 0.4120\n",
            "Epoch 2269/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6567 - val_loss: 0.4153\n",
            "Epoch 2270/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6577 - val_loss: 0.4168\n",
            "Epoch 2271/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6569 - val_loss: 0.4134\n",
            "Epoch 2272/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6525 - val_loss: 0.4213\n",
            "Epoch 2273/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6559 - val_loss: 0.4284\n",
            "Epoch 2274/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6546 - val_loss: 0.4195\n",
            "Epoch 2275/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6570 - val_loss: 0.4150\n",
            "Epoch 2276/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6551 - val_loss: 0.4244\n",
            "Epoch 2277/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6581 - val_loss: 0.4252\n",
            "Epoch 2278/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6574 - val_loss: 0.4194\n",
            "Epoch 2279/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6544 - val_loss: 0.4204\n",
            "Epoch 2280/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6542 - val_loss: 0.4140\n",
            "Epoch 2281/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6561 - val_loss: 0.4185\n",
            "Epoch 2282/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6551 - val_loss: 0.4159\n",
            "Epoch 2283/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6557 - val_loss: 0.4170\n",
            "Epoch 2284/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6544 - val_loss: 0.4154\n",
            "Epoch 2285/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6560 - val_loss: 0.4141\n",
            "Epoch 2286/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6606 - val_loss: 0.4164\n",
            "Epoch 2287/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6562 - val_loss: 0.4217\n",
            "Epoch 2288/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6550 - val_loss: 0.4185\n",
            "Epoch 2289/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6550 - val_loss: 0.4264\n",
            "Epoch 2290/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6569 - val_loss: 0.4344\n",
            "Epoch 2291/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6563 - val_loss: 0.4223\n",
            "Epoch 2292/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6522 - val_loss: 0.4760\n",
            "Epoch 2293/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6554 - val_loss: 0.5594\n",
            "Epoch 2294/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6591 - val_loss: 0.4220\n",
            "Epoch 2295/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6579 - val_loss: 0.4207\n",
            "Epoch 2296/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6597 - val_loss: 0.4186\n",
            "Epoch 2297/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6572 - val_loss: 0.5383\n",
            "Epoch 2298/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6545 - val_loss: 0.4205\n",
            "Epoch 2299/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6547 - val_loss: 0.4440\n",
            "Epoch 2300/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6558 - val_loss: 0.4350\n",
            "Epoch 2301/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6535 - val_loss: 0.4373\n",
            "Epoch 2302/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6581 - val_loss: 0.4220\n",
            "Epoch 2303/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6587 - val_loss: 0.4433\n",
            "Epoch 2304/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6569 - val_loss: 0.4501\n",
            "Epoch 2305/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6524 - val_loss: 0.4593\n",
            "Epoch 2306/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6565 - val_loss: 0.4352\n",
            "Epoch 2307/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6580 - val_loss: 0.4209\n",
            "Epoch 2308/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6551 - val_loss: 0.4425\n",
            "Epoch 2309/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6558 - val_loss: 0.4236\n",
            "Epoch 2310/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6574 - val_loss: 0.4309\n",
            "Epoch 2311/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6592 - val_loss: 0.4356\n",
            "Epoch 2312/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6560 - val_loss: 0.4251\n",
            "Epoch 2313/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6585 - val_loss: 0.4172\n",
            "Epoch 2314/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6552 - val_loss: 0.4282\n",
            "Epoch 2315/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6562 - val_loss: 0.4180\n",
            "Epoch 2316/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6543 - val_loss: 0.4356\n",
            "Epoch 2317/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6565 - val_loss: 0.4508\n",
            "Epoch 2318/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6518 - val_loss: 0.4231\n",
            "Epoch 2319/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6555 - val_loss: 0.4208\n",
            "Epoch 2320/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6539 - val_loss: 0.4163\n",
            "Epoch 2321/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6586 - val_loss: 0.4199\n",
            "Epoch 2322/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6546 - val_loss: 0.4247\n",
            "Epoch 2323/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6541 - val_loss: 0.4258\n",
            "Epoch 2324/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6524 - val_loss: 0.4187\n",
            "Epoch 2325/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6552 - val_loss: 0.4174\n",
            "Epoch 2326/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6580 - val_loss: 0.6088\n",
            "Epoch 2327/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6587 - val_loss: 0.7996\n",
            "Epoch 2328/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6615 - val_loss: 0.4194\n",
            "Epoch 2329/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6594 - val_loss: 0.4166\n",
            "Epoch 2330/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6559 - val_loss: 0.4297\n",
            "Epoch 2331/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6535 - val_loss: 0.4452\n",
            "Epoch 2332/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6539 - val_loss: 0.4212\n",
            "Epoch 2333/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6511 - val_loss: 0.4220\n",
            "Epoch 2334/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6559 - val_loss: 0.4414\n",
            "Epoch 2335/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6600 - val_loss: 0.4331\n",
            "Epoch 2336/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6552 - val_loss: 0.4227\n",
            "Epoch 2337/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6563 - val_loss: 0.4284\n",
            "Epoch 2338/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6517 - val_loss: 0.4172\n",
            "Epoch 2339/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6536 - val_loss: 0.4271\n",
            "Epoch 2340/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6533 - val_loss: 0.4195\n",
            "Epoch 2341/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6553 - val_loss: 0.4444\n",
            "Epoch 2342/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6523 - val_loss: 0.4191\n",
            "Epoch 2343/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6529 - val_loss: 0.4182\n",
            "Epoch 2344/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6562 - val_loss: 0.4191\n",
            "Epoch 2345/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6529 - val_loss: 0.4173\n",
            "Epoch 2346/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6546 - val_loss: 0.4183\n",
            "Epoch 2347/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6525 - val_loss: 0.4163\n",
            "Epoch 2348/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6547 - val_loss: 0.4189\n",
            "Epoch 2349/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6539 - val_loss: 0.4249\n",
            "Epoch 2350/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6528 - val_loss: 0.4233\n",
            "Epoch 2351/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6543 - val_loss: 0.4225\n",
            "Epoch 2352/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6548 - val_loss: 0.4361\n",
            "Epoch 2353/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6605 - val_loss: 0.4183\n",
            "Epoch 2354/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6782 - val_loss: 0.4521\n",
            "Epoch 2355/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6560 - val_loss: 0.4696\n",
            "Epoch 2356/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6555 - val_loss: 0.4223\n",
            "Epoch 2357/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6576 - val_loss: 0.4174\n",
            "Epoch 2358/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6520 - val_loss: 0.4165\n",
            "Epoch 2359/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6553 - val_loss: 0.4186\n",
            "Epoch 2360/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6604 - val_loss: 0.4181\n",
            "Epoch 2361/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6554 - val_loss: 0.4180\n",
            "Epoch 2362/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6538 - val_loss: 0.4158\n",
            "Epoch 2363/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6558 - val_loss: 0.4166\n",
            "Epoch 2364/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6540 - val_loss: 0.4191\n",
            "Epoch 2365/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6517 - val_loss: 0.4331\n",
            "Epoch 2366/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6593 - val_loss: 0.4271\n",
            "Epoch 2367/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6547 - val_loss: 0.4173\n",
            "Epoch 2368/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6552 - val_loss: 0.4168\n",
            "Epoch 2369/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6550 - val_loss: 0.4178\n",
            "Epoch 2370/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6518 - val_loss: 0.4164\n",
            "Epoch 2371/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6511 - val_loss: 0.4182\n",
            "Epoch 2372/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6546 - val_loss: 0.4243\n",
            "Epoch 2373/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6533 - val_loss: 0.4234\n",
            "Epoch 2374/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6563 - val_loss: 0.4174\n",
            "Epoch 2375/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6518 - val_loss: 0.4197\n",
            "Epoch 2376/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6544 - val_loss: 0.4176\n",
            "Epoch 2377/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6539 - val_loss: 0.4189\n",
            "Epoch 2378/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6541 - val_loss: 0.4181\n",
            "Epoch 2379/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6511 - val_loss: 0.4392\n",
            "Epoch 2380/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6513 - val_loss: 0.4164\n",
            "Epoch 2381/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6545 - val_loss: 0.5761\n",
            "Epoch 2382/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6634 - val_loss: 0.4418\n",
            "Epoch 2383/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6772 - val_loss: 0.4387\n",
            "Epoch 2384/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6572 - val_loss: 0.4194\n",
            "Epoch 2385/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6563 - val_loss: 0.4261\n",
            "Epoch 2386/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6557 - val_loss: 0.4183\n",
            "Epoch 2387/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6570 - val_loss: 0.4180\n",
            "Epoch 2388/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6556 - val_loss: 0.4163\n",
            "Epoch 2389/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6519 - val_loss: 0.4242\n",
            "Epoch 2390/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6577 - val_loss: 0.4148\n",
            "Epoch 2391/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6547 - val_loss: 0.4160\n",
            "Epoch 2392/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6522 - val_loss: 0.4223\n",
            "Epoch 2393/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6545 - val_loss: 0.4165\n",
            "Epoch 2394/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6533 - val_loss: 0.4220\n",
            "Epoch 2395/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6573 - val_loss: 0.4137\n",
            "Epoch 2396/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6573 - val_loss: 0.4282\n",
            "Epoch 2397/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6562 - val_loss: 0.4161\n",
            "Epoch 2398/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6566 - val_loss: 0.4156\n",
            "Epoch 2399/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6547 - val_loss: 0.4215\n",
            "Epoch 2400/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6565 - val_loss: 0.4219\n",
            "Epoch 2401/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6550 - val_loss: 0.4173\n",
            "Epoch 2402/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6506 - val_loss: 0.4178\n",
            "Epoch 2403/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6547 - val_loss: 0.4164\n",
            "Epoch 2404/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6547 - val_loss: 0.4145\n",
            "Epoch 2405/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6517 - val_loss: 0.4171\n",
            "Epoch 2406/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6510 - val_loss: 0.4218\n",
            "Epoch 2407/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6517 - val_loss: 0.4196\n",
            "Epoch 2408/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6558 - val_loss: 0.4168\n",
            "Epoch 2409/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6525 - val_loss: 0.4188\n",
            "Epoch 2410/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6540 - val_loss: 0.4217\n",
            "Epoch 2411/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6548 - val_loss: 0.4237\n",
            "Epoch 2412/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6539 - val_loss: 0.4189\n",
            "Epoch 2413/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6519 - val_loss: 0.4215\n",
            "Epoch 2414/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6547 - val_loss: 0.4192\n",
            "Epoch 2415/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6555 - val_loss: 0.4173\n",
            "Epoch 2416/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6582 - val_loss: 0.4192\n",
            "Epoch 2417/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6537 - val_loss: 0.4182\n",
            "Epoch 2418/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6577 - val_loss: 0.4366\n",
            "Epoch 2419/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6662 - val_loss: 0.6833\n",
            "Epoch 2420/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7012 - val_loss: 0.4341\n",
            "Epoch 2421/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6692 - val_loss: 0.4192\n",
            "Epoch 2422/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6583 - val_loss: 0.4184\n",
            "Epoch 2423/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6523 - val_loss: 0.4210\n",
            "Epoch 2424/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6564 - val_loss: 0.4210\n",
            "Epoch 2425/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6506 - val_loss: 0.4233\n",
            "Epoch 2426/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6579 - val_loss: 0.4175\n",
            "Epoch 2427/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6556 - val_loss: 0.4211\n",
            "Epoch 2428/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6525 - val_loss: 0.4157\n",
            "Epoch 2429/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6536 - val_loss: 0.4179\n",
            "Epoch 2430/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6564 - val_loss: 0.4174\n",
            "Epoch 2431/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6514 - val_loss: 0.4171\n",
            "Epoch 2432/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6538 - val_loss: 0.4149\n",
            "Epoch 2433/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6551 - val_loss: 0.4209\n",
            "Epoch 2434/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6544 - val_loss: 0.4163\n",
            "Epoch 2435/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6568 - val_loss: 0.4218\n",
            "Epoch 2436/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6562 - val_loss: 0.4200\n",
            "Epoch 2437/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6501 - val_loss: 0.4151\n",
            "Epoch 2438/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6535 - val_loss: 0.4180\n",
            "Epoch 2439/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6549 - val_loss: 0.4159\n",
            "Epoch 2440/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6519 - val_loss: 0.4236\n",
            "Epoch 2441/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6499 - val_loss: 0.4171\n",
            "Epoch 2442/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6535 - val_loss: 0.4252\n",
            "Epoch 2443/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6648 - val_loss: 0.4283\n",
            "Epoch 2444/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6904 - val_loss: 0.4158\n",
            "Epoch 2445/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6540 - val_loss: 0.4285\n",
            "Epoch 2446/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6562 - val_loss: 0.4223\n",
            "Epoch 2447/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6510 - val_loss: 0.4731\n",
            "Epoch 2448/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6534 - val_loss: 0.4964\n",
            "Epoch 2449/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6570 - val_loss: 0.4244\n",
            "Epoch 2450/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6589 - val_loss: 0.4169\n",
            "Epoch 2451/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6517 - val_loss: 0.4179\n",
            "Epoch 2452/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6501 - val_loss: 0.4215\n",
            "Epoch 2453/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6518 - val_loss: 0.4449\n",
            "Epoch 2454/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6532 - val_loss: 0.4187\n",
            "Epoch 2455/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6567 - val_loss: 0.4181\n",
            "Epoch 2456/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6521 - val_loss: 0.4250\n",
            "Epoch 2457/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6512 - val_loss: 0.4238\n",
            "Epoch 2458/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6541 - val_loss: 0.4343\n",
            "Epoch 2459/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6568 - val_loss: 0.4205\n",
            "Epoch 2460/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6529 - val_loss: 0.4234\n",
            "Epoch 2461/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6526 - val_loss: 0.4248\n",
            "Epoch 2462/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6497 - val_loss: 0.4235\n",
            "Epoch 2463/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6516 - val_loss: 0.4252\n",
            "Epoch 2464/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6536 - val_loss: 0.4157\n",
            "Epoch 2465/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6537 - val_loss: 0.4171\n",
            "Epoch 2466/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6508 - val_loss: 0.4207\n",
            "Epoch 2467/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6518 - val_loss: 0.4236\n",
            "Epoch 2468/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6524 - val_loss: 0.4280\n",
            "Epoch 2469/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6518 - val_loss: 0.4216\n",
            "Epoch 2470/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6529 - val_loss: 0.4217\n",
            "Epoch 2471/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6516 - val_loss: 0.4175\n",
            "Epoch 2472/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6527 - val_loss: 0.4216\n",
            "Epoch 2473/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6559 - val_loss: 0.4763\n",
            "Epoch 2474/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6527 - val_loss: 0.7560\n",
            "Epoch 2475/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6623 - val_loss: 0.4464\n",
            "Epoch 2476/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6722 - val_loss: 0.4239\n",
            "Epoch 2477/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6527 - val_loss: 0.4278\n",
            "Epoch 2478/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6505 - val_loss: 0.4185\n",
            "Epoch 2479/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6532 - val_loss: 0.4276\n",
            "Epoch 2480/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6500 - val_loss: 0.4267\n",
            "Epoch 2481/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6510 - val_loss: 0.4355\n",
            "Epoch 2482/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6507 - val_loss: 0.4276\n",
            "Epoch 2483/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6528 - val_loss: 0.4250\n",
            "Epoch 2484/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6508 - val_loss: 0.4189\n",
            "Epoch 2485/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6536 - val_loss: 0.4183\n",
            "Epoch 2486/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6494 - val_loss: 0.4262\n",
            "Epoch 2487/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6521 - val_loss: 0.4317\n",
            "Epoch 2488/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6505 - val_loss: 0.4238\n",
            "Epoch 2489/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6516 - val_loss: 0.4455\n",
            "Epoch 2490/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6524 - val_loss: 0.4298\n",
            "Epoch 2491/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6535 - val_loss: 0.4291\n",
            "Epoch 2492/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6535 - val_loss: 0.4605\n",
            "Epoch 2493/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6517 - val_loss: 0.4164\n",
            "Epoch 2494/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6528 - val_loss: 0.4194\n",
            "Epoch 2495/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6532 - val_loss: 0.4174\n",
            "Epoch 2496/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6517 - val_loss: 0.4268\n",
            "Epoch 2497/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6550 - val_loss: 0.4185\n",
            "Epoch 2498/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6511 - val_loss: 0.4218\n",
            "Epoch 2499/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6518 - val_loss: 0.4210\n",
            "Epoch 2500/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6522 - val_loss: 0.4183\n",
            "Epoch 2501/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6521 - val_loss: 0.4266\n",
            "Epoch 2502/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6508 - val_loss: 0.4237\n",
            "Epoch 2503/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6519 - val_loss: 0.4233\n",
            "Epoch 2504/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6517 - val_loss: 0.4359\n",
            "Epoch 2505/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6541 - val_loss: 0.4224\n",
            "Epoch 2506/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6507 - val_loss: 0.4290\n",
            "Epoch 2507/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6510 - val_loss: 0.4752\n",
            "Epoch 2508/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6520 - val_loss: 0.4347\n",
            "Epoch 2509/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6556 - val_loss: 0.4203\n",
            "Epoch 2510/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6484 - val_loss: 0.4289\n",
            "Epoch 2511/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6522 - val_loss: 0.4353\n",
            "Epoch 2512/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6528 - val_loss: 0.4382\n",
            "Epoch 2513/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6544 - val_loss: 0.4696\n",
            "Epoch 2514/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6545 - val_loss: 0.4671\n",
            "Epoch 2515/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6516 - val_loss: 0.5754\n",
            "Epoch 2516/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6539 - val_loss: 0.4353\n",
            "Epoch 2517/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6594 - val_loss: 0.4169\n",
            "Epoch 2518/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6504 - val_loss: 0.4241\n",
            "Epoch 2519/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6468 - val_loss: 0.4146\n",
            "Epoch 2520/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6536 - val_loss: 0.4166\n",
            "Epoch 2521/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6500 - val_loss: 0.4163\n",
            "Epoch 2522/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6535 - val_loss: 0.4211\n",
            "Epoch 2523/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6490 - val_loss: 0.4171\n",
            "Epoch 2524/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6525 - val_loss: 0.4145\n",
            "Epoch 2525/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6530 - val_loss: 0.4143\n",
            "Epoch 2526/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6510 - val_loss: 0.4203\n",
            "Epoch 2527/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6516 - val_loss: 0.4171\n",
            "Epoch 2528/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6512 - val_loss: 0.4147\n",
            "Epoch 2529/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6512 - val_loss: 0.4203\n",
            "Epoch 2530/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6509 - val_loss: 0.4137\n",
            "Epoch 2531/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6474 - val_loss: 0.4183\n",
            "Epoch 2532/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6538 - val_loss: 0.4161\n",
            "Epoch 2533/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6523 - val_loss: 0.4143\n",
            "Epoch 2534/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6517 - val_loss: 0.4193\n",
            "Epoch 2535/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6503 - val_loss: 0.4223\n",
            "Epoch 2536/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6523 - val_loss: 0.4202\n",
            "Epoch 2537/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6494 - val_loss: 0.4209\n",
            "Epoch 2538/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6498 - val_loss: 0.4213\n",
            "Epoch 2539/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6510 - val_loss: 0.4152\n",
            "Epoch 2540/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6534 - val_loss: 0.4289\n",
            "Epoch 2541/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6529 - val_loss: 0.4151\n",
            "Epoch 2542/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6504 - val_loss: 0.4274\n",
            "Epoch 2543/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6518 - val_loss: 0.4299\n",
            "Epoch 2544/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6504 - val_loss: 0.4491\n",
            "Epoch 2545/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6536 - val_loss: 0.4214\n",
            "Epoch 2546/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6484 - val_loss: 0.4573\n",
            "Epoch 2547/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6470 - val_loss: 0.4342\n",
            "Epoch 2548/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6523 - val_loss: 0.4366\n",
            "Epoch 2549/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6501 - val_loss: 0.4313\n",
            "Epoch 2550/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6481 - val_loss: 0.4265\n",
            "Epoch 2551/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6527 - val_loss: 0.4190\n",
            "Epoch 2552/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6499 - val_loss: 0.4219\n",
            "Epoch 2553/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6551 - val_loss: 0.4322\n",
            "Epoch 2554/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6605 - val_loss: 0.4143\n",
            "Epoch 2555/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6514 - val_loss: 0.4108\n",
            "Epoch 2556/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6512 - val_loss: 0.4168\n",
            "Epoch 2557/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6492 - val_loss: 0.4168\n",
            "Epoch 2558/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6488 - val_loss: 0.4112\n",
            "Epoch 2559/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6482 - val_loss: 0.4188\n",
            "Epoch 2560/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6486 - val_loss: 0.4248\n",
            "Epoch 2561/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6472 - val_loss: 0.4167\n",
            "Epoch 2562/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6463 - val_loss: 0.4258\n",
            "Epoch 2563/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6515 - val_loss: 1.2504\n",
            "Epoch 2564/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6739 - val_loss: 0.4735\n",
            "Epoch 2565/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6750 - val_loss: 0.4147\n",
            "Epoch 2566/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6491 - val_loss: 0.4187\n",
            "Epoch 2567/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6475 - val_loss: 0.4166\n",
            "Epoch 2568/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6494 - val_loss: 0.4138\n",
            "Epoch 2569/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6522 - val_loss: 0.4117\n",
            "Epoch 2570/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6472 - val_loss: 0.4134\n",
            "Epoch 2571/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6474 - val_loss: 0.4151\n",
            "Epoch 2572/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6471 - val_loss: 0.4116\n",
            "Epoch 2573/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6471 - val_loss: 0.4144\n",
            "Epoch 2574/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6451 - val_loss: 0.4183\n",
            "Epoch 2575/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6501 - val_loss: 0.4133\n",
            "Epoch 2576/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6493 - val_loss: 0.4151\n",
            "Epoch 2577/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6536 - val_loss: 0.4207\n",
            "Epoch 2578/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6465 - val_loss: 0.4257\n",
            "Epoch 2579/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6531 - val_loss: 0.4093\n",
            "Epoch 2580/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6473 - val_loss: 0.4159\n",
            "Epoch 2581/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6485 - val_loss: 0.4161\n",
            "Epoch 2582/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6512 - val_loss: 0.4140\n",
            "Epoch 2583/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6486 - val_loss: 0.4285\n",
            "Epoch 2584/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6493 - val_loss: 0.4198\n",
            "Epoch 2585/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6496 - val_loss: 0.4263\n",
            "Epoch 2586/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6509 - val_loss: 0.4229\n",
            "Epoch 2587/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6548 - val_loss: 0.4129\n",
            "Epoch 2588/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6488 - val_loss: 0.4121\n",
            "Epoch 2589/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6487 - val_loss: 0.4240\n",
            "Epoch 2590/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6476 - val_loss: 0.4231\n",
            "Epoch 2591/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6482 - val_loss: 0.4223\n",
            "Epoch 2592/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6473 - val_loss: 0.4219\n",
            "Epoch 2593/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6469 - val_loss: 0.4268\n",
            "Epoch 2594/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6481 - val_loss: 0.4119\n",
            "Epoch 2595/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6481 - val_loss: 0.4157\n",
            "Epoch 2596/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6532 - val_loss: 0.4200\n",
            "Epoch 2597/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6472 - val_loss: 0.4297\n",
            "Epoch 2598/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6488 - val_loss: 0.4191\n",
            "Epoch 2599/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6470 - val_loss: 0.4262\n",
            "Epoch 2600/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6464 - val_loss: 0.5141\n",
            "Epoch 2601/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6504 - val_loss: 0.6167\n",
            "Epoch 2602/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6677 - val_loss: 0.4875\n",
            "Epoch 2603/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6493 - val_loss: 0.4587\n",
            "Epoch 2604/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6550 - val_loss: 0.4884\n",
            "Epoch 2605/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6506 - val_loss: 0.4197\n",
            "Epoch 2606/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6494 - val_loss: 0.4392\n",
            "Epoch 2607/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6459 - val_loss: 0.4347\n",
            "Epoch 2608/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6508 - val_loss: 0.4407\n",
            "Epoch 2609/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6474 - val_loss: 0.4252\n",
            "Epoch 2610/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6486 - val_loss: 0.4227\n",
            "Epoch 2611/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6451 - val_loss: 0.4269\n",
            "Epoch 2612/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6466 - val_loss: 0.4251\n",
            "Epoch 2613/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6460 - val_loss: 0.4320\n",
            "Epoch 2614/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6497 - val_loss: 0.4276\n",
            "Epoch 2615/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6474 - val_loss: 0.4541\n",
            "Epoch 2616/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6467 - val_loss: 0.4423\n",
            "Epoch 2617/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6448 - val_loss: 0.4155\n",
            "Epoch 2618/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6433 - val_loss: 0.4197\n",
            "Epoch 2619/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6495 - val_loss: 0.4314\n",
            "Epoch 2620/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6448 - val_loss: 0.4446\n",
            "Epoch 2621/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6517 - val_loss: 0.4592\n",
            "Epoch 2622/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6452 - val_loss: 0.4448\n",
            "Epoch 2623/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6454 - val_loss: 0.4343\n",
            "Epoch 2624/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6455 - val_loss: 0.4307\n",
            "Epoch 2625/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6458 - val_loss: 0.4495\n",
            "Epoch 2626/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6485 - val_loss: 0.4377\n",
            "Epoch 2627/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6495 - val_loss: 0.4453\n",
            "Epoch 2628/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6440 - val_loss: 0.4565\n",
            "Epoch 2629/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6485 - val_loss: 0.4349\n",
            "Epoch 2630/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6439 - val_loss: 0.4457\n",
            "Epoch 2631/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6490 - val_loss: 0.4376\n",
            "Epoch 2632/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6483 - val_loss: 0.4334\n",
            "Epoch 2633/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6514 - val_loss: 0.4192\n",
            "Epoch 2634/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6470 - val_loss: 0.4433\n",
            "Epoch 2635/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6456 - val_loss: 0.4443\n",
            "Epoch 2636/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6468 - val_loss: 0.4559\n",
            "Epoch 2637/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6443 - val_loss: 0.4440\n",
            "Epoch 2638/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6444 - val_loss: 0.4540\n",
            "Epoch 2639/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6472 - val_loss: 0.4205\n",
            "Epoch 2640/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6442 - val_loss: 0.4390\n",
            "Epoch 2641/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6496 - val_loss: 0.4468\n",
            "Epoch 2642/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6460 - val_loss: 0.4585\n",
            "Epoch 2643/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6475 - val_loss: 0.4504\n",
            "Epoch 2644/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6442 - val_loss: 0.4546\n",
            "Epoch 2645/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6441 - val_loss: 0.4486\n",
            "Epoch 2646/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6447 - val_loss: 0.4353\n",
            "Epoch 2647/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6447 - val_loss: 0.4554\n",
            "Epoch 2648/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6455 - val_loss: 0.4341\n",
            "Epoch 2649/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6430 - val_loss: 0.4458\n",
            "Epoch 2650/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6456 - val_loss: 0.4524\n",
            "Epoch 2651/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6475 - val_loss: 0.5180\n",
            "Epoch 2652/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6801 - val_loss: 0.4453\n",
            "Epoch 2653/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6519 - val_loss: 0.4091\n",
            "Epoch 2654/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6449 - val_loss: 0.4257\n",
            "Epoch 2655/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6437 - val_loss: 0.4278\n",
            "Epoch 2656/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6469 - val_loss: 0.4255\n",
            "Epoch 2657/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6447 - val_loss: 0.4220\n",
            "Epoch 2658/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6447 - val_loss: 0.4562\n",
            "Epoch 2659/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6432 - val_loss: 0.4416\n",
            "Epoch 2660/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6425 - val_loss: 0.4492\n",
            "Epoch 2661/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6468 - val_loss: 0.4361\n",
            "Epoch 2662/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6422 - val_loss: 0.4531\n",
            "Epoch 2663/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6436 - val_loss: 0.4481\n",
            "Epoch 2664/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6413 - val_loss: 0.4568\n",
            "Epoch 2665/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6432 - val_loss: 0.4431\n",
            "Epoch 2666/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6457 - val_loss: 0.4818\n",
            "Epoch 2667/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6461 - val_loss: 0.4387\n",
            "Epoch 2668/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6437 - val_loss: 0.4460\n",
            "Epoch 2669/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6423 - val_loss: 0.4365\n",
            "Epoch 2670/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6466 - val_loss: 0.4332\n",
            "Epoch 2671/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6415 - val_loss: 0.4441\n",
            "Epoch 2672/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6450 - val_loss: 0.4517\n",
            "Epoch 2673/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6464 - val_loss: 0.4226\n",
            "Epoch 2674/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6445 - val_loss: 0.4210\n",
            "Epoch 2675/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6476 - val_loss: 0.4279\n",
            "Epoch 2676/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6431 - val_loss: 0.4326\n",
            "Epoch 2677/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6455 - val_loss: 0.4333\n",
            "Epoch 2678/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6500 - val_loss: 0.4135\n",
            "Epoch 2679/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6449 - val_loss: 0.4379\n",
            "Epoch 2680/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6435 - val_loss: 0.4347\n",
            "Epoch 2681/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6415 - val_loss: 0.4406\n",
            "Epoch 2682/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6435 - val_loss: 0.4324\n",
            "Epoch 2683/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6413 - val_loss: 0.4450\n",
            "Epoch 2684/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6422 - val_loss: 0.4313\n",
            "Epoch 2685/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6435 - val_loss: 0.4468\n",
            "Epoch 2686/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6452 - val_loss: 0.4309\n",
            "Epoch 2687/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6412 - val_loss: 0.4387\n",
            "Epoch 2688/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6427 - val_loss: 0.4333\n",
            "Epoch 2689/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6410 - val_loss: 0.4400\n",
            "Epoch 2690/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6415 - val_loss: 0.4502\n",
            "Epoch 2691/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6458 - val_loss: 0.4578\n",
            "Epoch 2692/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6429 - val_loss: 0.4292\n",
            "Epoch 2693/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6397 - val_loss: 0.4288\n",
            "Epoch 2694/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6420 - val_loss: 0.4291\n",
            "Epoch 2695/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6439 - val_loss: 0.4304\n",
            "Epoch 2696/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6435 - val_loss: 0.4362\n",
            "Epoch 2697/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6444 - val_loss: 0.4129\n",
            "Epoch 2698/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6449 - val_loss: 0.4342\n",
            "Epoch 2699/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6407 - val_loss: 0.4268\n",
            "Epoch 2700/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6443 - val_loss: 0.4440\n",
            "Epoch 2701/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6410 - val_loss: 0.4482\n",
            "Epoch 2702/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6545 - val_loss: 0.4132\n",
            "Epoch 2703/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6423 - val_loss: 0.4239\n",
            "Epoch 2704/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6409 - val_loss: 0.4230\n",
            "Epoch 2705/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6444 - val_loss: 0.4322\n",
            "Epoch 2706/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6414 - val_loss: 0.4586\n",
            "Epoch 2707/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6380 - val_loss: 0.4391\n",
            "Epoch 2708/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6469 - val_loss: 0.4338\n",
            "Epoch 2709/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6434 - val_loss: 0.4316\n",
            "Epoch 2710/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6440 - val_loss: 0.4472\n",
            "Epoch 2711/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6444 - val_loss: 0.4501\n",
            "Epoch 2712/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6416 - val_loss: 0.4441\n",
            "Epoch 2713/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6415 - val_loss: 0.4543\n",
            "Epoch 2714/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6429 - val_loss: 0.4359\n",
            "Epoch 2715/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6463 - val_loss: 0.4420\n",
            "Epoch 2716/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6438 - val_loss: 0.4309\n",
            "Epoch 2717/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6456 - val_loss: 0.4547\n",
            "Epoch 2718/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6404 - val_loss: 0.4489\n",
            "Epoch 2719/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6392 - val_loss: 0.4502\n",
            "Epoch 2720/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6374 - val_loss: 0.4511\n",
            "Epoch 2721/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6464 - val_loss: 0.4115\n",
            "Epoch 2722/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6362 - val_loss: 0.4343\n",
            "Epoch 2723/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6422 - val_loss: 0.4468\n",
            "Epoch 2724/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6514 - val_loss: 0.5754\n",
            "Epoch 2725/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6517 - val_loss: 0.6834\n",
            "Epoch 2726/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6520 - val_loss: 0.5459\n",
            "Epoch 2727/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6401 - val_loss: 0.5284\n",
            "Epoch 2728/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6474 - val_loss: 0.5713\n",
            "Epoch 2729/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6440 - val_loss: 0.5035\n",
            "Epoch 2730/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6426 - val_loss: 0.5699\n",
            "Epoch 2731/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6410 - val_loss: 0.5946\n",
            "Epoch 2732/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6421 - val_loss: 0.5769\n",
            "Epoch 2733/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6414 - val_loss: 0.5507\n",
            "Epoch 2734/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6418 - val_loss: 0.5612\n",
            "Epoch 2735/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6407 - val_loss: 0.6048\n",
            "Epoch 2736/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6465 - val_loss: 0.5879\n",
            "Epoch 2737/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6484 - val_loss: 0.6202\n",
            "Epoch 2738/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6452 - val_loss: 0.5528\n",
            "Epoch 2739/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6471 - val_loss: 0.5498\n",
            "Epoch 2740/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6400 - val_loss: 0.5438\n",
            "Epoch 2741/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6453 - val_loss: 0.5764\n",
            "Epoch 2742/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6424 - val_loss: 0.6296\n",
            "Epoch 2743/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6397 - val_loss: 0.5927\n",
            "Epoch 2744/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6420 - val_loss: 0.6455\n",
            "Epoch 2745/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6405 - val_loss: 0.5705\n",
            "Epoch 2746/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6377 - val_loss: 0.5119\n",
            "Epoch 2747/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6397 - val_loss: 0.5704\n",
            "Epoch 2748/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6373 - val_loss: 0.5818\n",
            "Epoch 2749/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6375 - val_loss: 0.5931\n",
            "Epoch 2750/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6384 - val_loss: 0.6152\n",
            "Epoch 2751/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6371 - val_loss: 0.6050\n",
            "Epoch 2752/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6362 - val_loss: 0.6477\n",
            "Epoch 2753/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6390 - val_loss: 0.6052\n",
            "Epoch 2754/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6501 - val_loss: 0.6302\n",
            "Epoch 2755/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6618 - val_loss: 0.6793\n",
            "Epoch 2756/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6559 - val_loss: 0.5387\n",
            "Epoch 2757/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6535 - val_loss: 0.5409\n",
            "Epoch 2758/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6486 - val_loss: 0.5527\n",
            "Epoch 2759/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6536 - val_loss: 0.6415\n",
            "Epoch 2760/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6524 - val_loss: 0.6048\n",
            "Epoch 2761/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6479 - val_loss: 0.6462\n",
            "Epoch 2762/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6461 - val_loss: 0.7224\n",
            "Epoch 2763/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6463 - val_loss: 0.6732\n",
            "Epoch 2764/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6472 - val_loss: 0.6670\n",
            "Epoch 2765/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6493 - val_loss: 0.6195\n",
            "Epoch 2766/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6459 - val_loss: 0.4265\n",
            "Epoch 2767/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6565 - val_loss: 0.8753\n",
            "Epoch 2768/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6423 - val_loss: 0.5372\n",
            "Epoch 2769/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6881 - val_loss: 0.9445\n",
            "Epoch 2770/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6760 - val_loss: 0.6510\n",
            "Epoch 2771/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6423 - val_loss: 0.6194\n",
            "Epoch 2772/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6398 - val_loss: 0.6376\n",
            "Epoch 2773/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6414 - val_loss: 0.6340\n",
            "Epoch 2774/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6830 - val_loss: 0.6690\n",
            "Epoch 2775/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7100 - val_loss: 0.7343\n",
            "Epoch 2776/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6601 - val_loss: 0.7513\n",
            "Epoch 2777/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6571 - val_loss: 0.7745\n",
            "Epoch 2778/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6560 - val_loss: 0.7607\n",
            "Epoch 2779/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6573 - val_loss: 0.8039\n",
            "Epoch 2780/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6555 - val_loss: 0.7670\n",
            "Epoch 2781/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6556 - val_loss: 0.8144\n",
            "Epoch 2782/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6529 - val_loss: 0.7868\n",
            "Epoch 2783/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6521 - val_loss: 0.8163\n",
            "Epoch 2784/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6546 - val_loss: 0.7898\n",
            "Epoch 2785/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6540 - val_loss: 0.8022\n",
            "Epoch 2786/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6519 - val_loss: 0.8157\n",
            "Epoch 2787/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6500 - val_loss: 0.8280\n",
            "Epoch 2788/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6486 - val_loss: 0.8330\n",
            "Epoch 2789/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6507 - val_loss: 0.8180\n",
            "Epoch 2790/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6494 - val_loss: 0.8074\n",
            "Epoch 2791/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6511 - val_loss: 0.9459\n",
            "Epoch 2792/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6519 - val_loss: 0.8632\n",
            "Epoch 2793/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6514 - val_loss: 0.8437\n",
            "Epoch 2794/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6514 - val_loss: 0.7908\n",
            "Epoch 2795/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6508 - val_loss: 0.8002\n",
            "Epoch 2796/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6506 - val_loss: 0.7879\n",
            "Epoch 2797/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6500 - val_loss: 0.7068\n",
            "Epoch 2798/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6486 - val_loss: 0.7332\n",
            "Epoch 2799/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6510 - val_loss: 0.7501\n",
            "Epoch 2800/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6504 - val_loss: 0.7523\n",
            "Epoch 2801/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6511 - val_loss: 0.7660\n",
            "Epoch 2802/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6504 - val_loss: 0.7965\n",
            "Epoch 2803/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6480 - val_loss: 0.8269\n",
            "Epoch 2804/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6483 - val_loss: 0.8218\n",
            "Epoch 2805/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6488 - val_loss: 0.8070\n",
            "Epoch 2806/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6487 - val_loss: 0.8248\n",
            "Epoch 2807/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6476 - val_loss: 0.9825\n",
            "Epoch 2808/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6487 - val_loss: 0.7759\n",
            "Epoch 2809/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6491 - val_loss: 0.8230\n",
            "Epoch 2810/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6502 - val_loss: 0.7904\n",
            "Epoch 2811/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6464 - val_loss: 0.8318\n",
            "Epoch 2812/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6469 - val_loss: 0.7971\n",
            "Epoch 2813/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6447 - val_loss: 0.8207\n",
            "Epoch 2814/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6465 - val_loss: 0.8368\n",
            "Epoch 2815/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6478 - val_loss: 0.9522\n",
            "Epoch 2816/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6458 - val_loss: 0.5484\n",
            "Epoch 2817/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6500 - val_loss: 0.8333\n",
            "Epoch 2818/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6520 - val_loss: 0.6951\n",
            "Epoch 2819/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6514 - val_loss: 0.5768\n",
            "Epoch 2820/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6520 - val_loss: 0.5950\n",
            "Epoch 2821/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6486 - val_loss: 0.6003\n",
            "Epoch 2822/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6473 - val_loss: 0.6175\n",
            "Epoch 2823/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6486 - val_loss: 0.6258\n",
            "Epoch 2824/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6450 - val_loss: 0.6036\n",
            "Epoch 2825/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6463 - val_loss: 0.5977\n",
            "Epoch 2826/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6447 - val_loss: 0.6214\n",
            "Epoch 2827/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6468 - val_loss: 0.6481\n",
            "Epoch 2828/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6435 - val_loss: 0.6680\n",
            "Epoch 2829/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6484 - val_loss: 0.6523\n",
            "Epoch 2830/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6455 - val_loss: 0.6712\n",
            "Epoch 2831/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6519 - val_loss: 0.6926\n",
            "Epoch 2832/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6505 - val_loss: 0.7637\n",
            "Epoch 2833/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6495 - val_loss: 0.6578\n",
            "Epoch 2834/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6474 - val_loss: 0.6231\n",
            "Epoch 2835/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6447 - val_loss: 0.5944\n",
            "Epoch 2836/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6437 - val_loss: 0.6596\n",
            "Epoch 2837/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6457 - val_loss: 0.6729\n",
            "Epoch 2838/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6444 - val_loss: 0.6955\n",
            "Epoch 2839/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6418 - val_loss: 0.7013\n",
            "Epoch 2840/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6452 - val_loss: 0.7129\n",
            "Epoch 2841/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6419 - val_loss: 0.7287\n",
            "Epoch 2842/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6413 - val_loss: 0.7255\n",
            "Epoch 2843/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6446 - val_loss: 0.7153\n",
            "Epoch 2844/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6424 - val_loss: 0.6662\n",
            "Epoch 2845/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6429 - val_loss: 0.7856\n",
            "Epoch 2846/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6498 - val_loss: 0.7553\n",
            "Epoch 2847/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6457 - val_loss: 0.7115\n",
            "Epoch 2848/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6421 - val_loss: 0.7850\n",
            "Epoch 2849/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6410 - val_loss: 0.7753\n",
            "Epoch 2850/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6421 - val_loss: 0.7864\n",
            "Epoch 2851/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6414 - val_loss: 0.7626\n",
            "Epoch 2852/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6424 - val_loss: 0.7914\n",
            "Epoch 2853/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6402 - val_loss: 0.6440\n",
            "Epoch 2854/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6427 - val_loss: 0.6957\n",
            "Epoch 2855/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6407 - val_loss: 0.7127\n",
            "Epoch 2856/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6389 - val_loss: 0.7512\n",
            "Epoch 2857/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6407 - val_loss: 0.7834\n",
            "Epoch 2858/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6437 - val_loss: 1.0002\n",
            "Epoch 2859/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6477 - val_loss: 0.7619\n",
            "Epoch 2860/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7079 - val_loss: 1.3704\n",
            "Epoch 2861/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7674 - val_loss: 1.0337\n",
            "Epoch 2862/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6999 - val_loss: 0.7842\n",
            "Epoch 2863/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6639 - val_loss: 0.7623\n",
            "Epoch 2864/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6552 - val_loss: 0.7223\n",
            "Epoch 2865/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6478 - val_loss: 0.7393\n",
            "Epoch 2866/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6458 - val_loss: 0.7016\n",
            "Epoch 2867/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6432 - val_loss: 0.7537\n",
            "Epoch 2868/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6425 - val_loss: 0.7242\n",
            "Epoch 2869/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6390 - val_loss: 0.7193\n",
            "Epoch 2870/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6400 - val_loss: 0.7307\n",
            "Epoch 2871/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6376 - val_loss: 0.7341\n",
            "Epoch 2872/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6406 - val_loss: 0.7194\n",
            "Epoch 2873/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6383 - val_loss: 0.7540\n",
            "Epoch 2874/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6372 - val_loss: 0.7281\n",
            "Epoch 2875/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6374 - val_loss: 0.7453\n",
            "Epoch 2876/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6397 - val_loss: 0.7591\n",
            "Epoch 2877/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6362 - val_loss: 0.7223\n",
            "Epoch 2878/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6368 - val_loss: 0.7512\n",
            "Epoch 2879/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6369 - val_loss: 0.7721\n",
            "Epoch 2880/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6360 - val_loss: 0.7391\n",
            "Epoch 2881/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6366 - val_loss: 0.7473\n",
            "Epoch 2882/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6389 - val_loss: 0.7457\n",
            "Epoch 2883/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6367 - val_loss: 0.7414\n",
            "Epoch 2884/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6379 - val_loss: 0.7668\n",
            "Epoch 2885/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6402 - val_loss: 0.7588\n",
            "Epoch 2886/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6380 - val_loss: 0.7169\n",
            "Epoch 2887/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6373 - val_loss: 0.7722\n",
            "Epoch 2888/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6348 - val_loss: 0.7243\n",
            "Epoch 2889/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6356 - val_loss: 0.6993\n",
            "Epoch 2890/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6351 - val_loss: 0.6528\n",
            "Epoch 2891/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6331 - val_loss: 0.6725\n",
            "Epoch 2892/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6347 - val_loss: 0.7029\n",
            "Epoch 2893/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6353 - val_loss: 0.6868\n",
            "Epoch 2894/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6338 - val_loss: 0.6603\n",
            "Epoch 2895/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6332 - val_loss: 0.6376\n",
            "Epoch 2896/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6322 - val_loss: 0.6428\n",
            "Epoch 2897/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6386 - val_loss: 0.6715\n",
            "Epoch 2898/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6403 - val_loss: 0.6834\n",
            "Epoch 2899/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6397 - val_loss: 0.6745\n",
            "Epoch 2900/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6357 - val_loss: 0.6871\n",
            "Epoch 2901/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6352 - val_loss: 0.6990\n",
            "Epoch 2902/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6356 - val_loss: 0.7048\n",
            "Epoch 2903/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6353 - val_loss: 0.7062\n",
            "Epoch 2904/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6390 - val_loss: 0.8044\n",
            "Epoch 2905/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6416 - val_loss: 0.7728\n",
            "Epoch 2906/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6431 - val_loss: 0.6763\n",
            "Epoch 2907/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6463 - val_loss: 0.5691\n",
            "Epoch 2908/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6411 - val_loss: 0.5692\n",
            "Epoch 2909/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6698 - val_loss: 0.5834\n",
            "Epoch 2910/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6742 - val_loss: 0.6250\n",
            "Epoch 2911/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6675 - val_loss: 0.6021\n",
            "Epoch 2912/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6644 - val_loss: 0.5931\n",
            "Epoch 2913/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6620 - val_loss: 0.5823\n",
            "Epoch 2914/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6588 - val_loss: 0.5785\n",
            "Epoch 2915/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6564 - val_loss: 0.6255\n",
            "Epoch 2916/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6530 - val_loss: 0.5915\n",
            "Epoch 2917/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6548 - val_loss: 0.5714\n",
            "Epoch 2918/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6520 - val_loss: 0.5865\n",
            "Epoch 2919/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6515 - val_loss: 0.6093\n",
            "Epoch 2920/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6528 - val_loss: 0.5996\n",
            "Epoch 2921/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6503 - val_loss: 0.5358\n",
            "Epoch 2922/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6508 - val_loss: 0.5461\n",
            "Epoch 2923/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6509 - val_loss: 0.5650\n",
            "Epoch 2924/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6489 - val_loss: 0.5782\n",
            "Epoch 2925/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6472 - val_loss: 0.5667\n",
            "Epoch 2926/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6488 - val_loss: 0.5458\n",
            "Epoch 2927/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6460 - val_loss: 0.5821\n",
            "Epoch 2928/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6464 - val_loss: 0.5898\n",
            "Epoch 2929/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6469 - val_loss: 0.5748\n",
            "Epoch 2930/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6498 - val_loss: 0.6077\n",
            "Epoch 2931/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6462 - val_loss: 0.5749\n",
            "Epoch 2932/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6450 - val_loss: 0.6223\n",
            "Epoch 2933/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6491 - val_loss: 0.6062\n",
            "Epoch 2934/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6468 - val_loss: 0.6068\n",
            "Epoch 2935/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6467 - val_loss: 0.5419\n",
            "Epoch 2936/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6425 - val_loss: 0.5916\n",
            "Epoch 2937/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6434 - val_loss: 0.6204\n",
            "Epoch 2938/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6447 - val_loss: 0.6130\n",
            "Epoch 2939/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6453 - val_loss: 0.5942\n",
            "Epoch 2940/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6399 - val_loss: 0.5967\n",
            "Epoch 2941/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6421 - val_loss: 0.6370\n",
            "Epoch 2942/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6399 - val_loss: 0.5992\n",
            "Epoch 2943/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6406 - val_loss: 0.5796\n",
            "Epoch 2944/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6402 - val_loss: 0.6602\n",
            "Epoch 2945/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6412 - val_loss: 0.6096\n",
            "Epoch 2946/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6376 - val_loss: 0.5998\n",
            "Epoch 2947/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6613 - val_loss: 0.5785\n",
            "Epoch 2948/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6483 - val_loss: 0.5771\n",
            "Epoch 2949/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6493 - val_loss: 0.5775\n",
            "Epoch 2950/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6447 - val_loss: 0.5781\n",
            "Epoch 2951/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6463 - val_loss: 0.5680\n",
            "Epoch 2952/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6471 - val_loss: 0.5884\n",
            "Epoch 2953/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6412 - val_loss: 0.5814\n",
            "Epoch 2954/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6414 - val_loss: 0.5879\n",
            "Epoch 2955/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6416 - val_loss: 0.6139\n",
            "Epoch 2956/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6400 - val_loss: 0.5685\n",
            "Epoch 2957/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6407 - val_loss: 0.5750\n",
            "Epoch 2958/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6405 - val_loss: 0.5828\n",
            "Epoch 2959/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6383 - val_loss: 0.6044\n",
            "Epoch 2960/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6419 - val_loss: 0.5257\n",
            "Epoch 2961/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6372 - val_loss: 0.5724\n",
            "Epoch 2962/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6373 - val_loss: 0.5629\n",
            "Epoch 2963/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6407 - val_loss: 0.5849\n",
            "Epoch 2964/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6402 - val_loss: 0.5980\n",
            "Epoch 2965/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6352 - val_loss: 0.6119\n",
            "Epoch 2966/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6368 - val_loss: 0.6373\n",
            "Epoch 2967/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6411 - val_loss: 0.5412\n",
            "Epoch 2968/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6415 - val_loss: 0.5755\n",
            "Epoch 2969/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6371 - val_loss: 0.5528\n",
            "Epoch 2970/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6367 - val_loss: 0.5768\n",
            "Epoch 2971/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6384 - val_loss: 0.6381\n",
            "Epoch 2972/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6394 - val_loss: 0.6178\n",
            "Epoch 2973/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6346 - val_loss: 0.6107\n",
            "Epoch 2974/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6358 - val_loss: 0.6315\n",
            "Epoch 2975/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6403 - val_loss: 0.6492\n",
            "Epoch 2976/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6377 - val_loss: 0.6114\n",
            "Epoch 2977/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6409 - val_loss: 0.6430\n",
            "Epoch 2978/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6421 - val_loss: 0.4610\n",
            "Epoch 2979/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6375 - val_loss: 0.5492\n",
            "Epoch 2980/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6426 - val_loss: 0.5916\n",
            "Epoch 2981/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6445 - val_loss: 0.6113\n",
            "Epoch 2982/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6427 - val_loss: 0.5742\n",
            "Epoch 2983/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6395 - val_loss: 0.6025\n",
            "Epoch 2984/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6414 - val_loss: 0.5279\n",
            "Epoch 2985/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6634 - val_loss: 0.4579\n",
            "Epoch 2986/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6929 - val_loss: 0.6643\n",
            "Epoch 2987/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6484 - val_loss: 0.5143\n",
            "Epoch 2988/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6457 - val_loss: 0.5702\n",
            "Epoch 2989/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6372 - val_loss: 0.5746\n",
            "Epoch 2990/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6468 - val_loss: 0.5284\n",
            "Epoch 2991/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6422 - val_loss: 0.5630\n",
            "Epoch 2992/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6402 - val_loss: 0.5654\n",
            "Epoch 2993/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6421 - val_loss: 0.5737\n",
            "Epoch 2994/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6352 - val_loss: 0.5562\n",
            "Epoch 2995/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6563 - val_loss: 0.5879\n",
            "Epoch 2996/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6874 - val_loss: 0.5733\n",
            "Epoch 2997/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6743 - val_loss: 0.6082\n",
            "Epoch 2998/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6708 - val_loss: 0.5741\n",
            "Epoch 2999/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6694 - val_loss: 0.6356\n",
            "Epoch 3000/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6683 - val_loss: 0.5981\n",
            "Epoch 3001/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6684 - val_loss: 0.5837\n",
            "Epoch 3002/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6691 - val_loss: 0.6075\n",
            "Epoch 3003/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6676 - val_loss: 0.6129\n",
            "Epoch 3004/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6641 - val_loss: 0.5713\n",
            "Epoch 3005/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6649 - val_loss: 0.5723\n",
            "Epoch 3006/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6617 - val_loss: 0.5787\n",
            "Epoch 3007/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6622 - val_loss: 0.5999\n",
            "Epoch 3008/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6668 - val_loss: 0.5796\n",
            "Epoch 3009/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6639 - val_loss: 0.6130\n",
            "Epoch 3010/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6631 - val_loss: 0.6554\n",
            "Epoch 3011/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6610 - val_loss: 0.6108\n",
            "Epoch 3012/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6666 - val_loss: 0.5916\n",
            "Epoch 3013/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6628 - val_loss: 0.6102\n",
            "Epoch 3014/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6620 - val_loss: 0.6102\n",
            "Epoch 3015/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6643 - val_loss: 0.6145\n",
            "Epoch 3016/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6638 - val_loss: 0.6285\n",
            "Epoch 3017/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6608 - val_loss: 0.6370\n",
            "Epoch 3018/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6608 - val_loss: 0.6103\n",
            "Epoch 3019/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6600 - val_loss: 0.5837\n",
            "Epoch 3020/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6614 - val_loss: 0.6528\n",
            "Epoch 3021/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6613 - val_loss: 0.5846\n",
            "Epoch 3022/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6577 - val_loss: 0.6050\n",
            "Epoch 3023/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6593 - val_loss: 0.6050\n",
            "Epoch 3024/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6590 - val_loss: 0.6248\n",
            "Epoch 3025/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6627 - val_loss: 0.6613\n",
            "Epoch 3026/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6575 - val_loss: 0.4521\n",
            "Epoch 3027/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6567 - val_loss: 0.6535\n",
            "Epoch 3028/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6579 - val_loss: 0.4450\n",
            "Epoch 3029/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6588 - val_loss: 0.6553\n",
            "Epoch 3030/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6567 - val_loss: 0.4212\n",
            "Epoch 3031/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6591 - val_loss: 0.5811\n",
            "Epoch 3032/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6617 - val_loss: 0.4461\n",
            "Epoch 3033/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6595 - val_loss: 0.4579\n",
            "Epoch 3034/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6580 - val_loss: 0.5661\n",
            "Epoch 3035/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6535 - val_loss: 0.5454\n",
            "Epoch 3036/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6542 - val_loss: 0.5222\n",
            "Epoch 3037/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6592 - val_loss: 0.5823\n",
            "Epoch 3038/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6554 - val_loss: 0.5571\n",
            "Epoch 3039/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6551 - val_loss: 0.5716\n",
            "Epoch 3040/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6568 - val_loss: 0.5742\n",
            "Epoch 3041/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6528 - val_loss: 0.4822\n",
            "Epoch 3042/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6544 - val_loss: 0.4547\n",
            "Epoch 3043/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6521 - val_loss: 0.4525\n",
            "Epoch 3044/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6503 - val_loss: 0.4766\n",
            "Epoch 3045/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6515 - val_loss: 0.4793\n",
            "Epoch 3046/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6538 - val_loss: 0.4928\n",
            "Epoch 3047/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6530 - val_loss: 0.4859\n",
            "Epoch 3048/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6525 - val_loss: 0.4692\n",
            "Epoch 3049/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6516 - val_loss: 0.4895\n",
            "Epoch 3050/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6497 - val_loss: 0.5049\n",
            "Epoch 3051/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6528 - val_loss: 0.5160\n",
            "Epoch 3052/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6504 - val_loss: 0.5153\n",
            "Epoch 3053/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6513 - val_loss: 0.4894\n",
            "Epoch 3054/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6520 - val_loss: 0.4876\n",
            "Epoch 3055/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6512 - val_loss: 0.4923\n",
            "Epoch 3056/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6501 - val_loss: 0.5321\n",
            "Epoch 3057/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6521 - val_loss: 0.5075\n",
            "Epoch 3058/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6478 - val_loss: 0.5133\n",
            "Epoch 3059/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6568 - val_loss: 0.5087\n",
            "Epoch 3060/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6501 - val_loss: 0.5714\n",
            "Epoch 3061/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6513 - val_loss: 0.4920\n",
            "Epoch 3062/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6491 - val_loss: 0.5032\n",
            "Epoch 3063/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6542 - val_loss: 0.5424\n",
            "Epoch 3064/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6514 - val_loss: 0.4953\n",
            "Epoch 3065/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6488 - val_loss: 0.5297\n",
            "Epoch 3066/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6533 - val_loss: 0.5139\n",
            "Epoch 3067/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6531 - val_loss: 0.5001\n",
            "Epoch 3068/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6504 - val_loss: 0.4805\n",
            "Epoch 3069/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6487 - val_loss: 0.5169\n",
            "Epoch 3070/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6483 - val_loss: 0.4437\n",
            "Epoch 3071/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6492 - val_loss: 0.4964\n",
            "Epoch 3072/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6488 - val_loss: 0.4815\n",
            "Epoch 3073/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6539 - val_loss: 0.4898\n",
            "Epoch 3074/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6495 - val_loss: 0.4822\n",
            "Epoch 3075/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6464 - val_loss: 0.4963\n",
            "Epoch 3076/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6483 - val_loss: 0.5127\n",
            "Epoch 3077/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6476 - val_loss: 0.5042\n",
            "Epoch 3078/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6481 - val_loss: 0.5212\n",
            "Epoch 3079/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6536 - val_loss: 0.6281\n",
            "Epoch 3080/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6557 - val_loss: 0.9732\n",
            "Epoch 3081/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7418 - val_loss: 1.0847\n",
            "Epoch 3082/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7219 - val_loss: 0.7983\n",
            "Epoch 3083/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6740 - val_loss: 0.6814\n",
            "Epoch 3084/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6652 - val_loss: 0.6744\n",
            "Epoch 3085/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6585 - val_loss: 0.6697\n",
            "Epoch 3086/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6549 - val_loss: 0.6738\n",
            "Epoch 3087/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6543 - val_loss: 0.5698\n",
            "Epoch 3088/10000\n",
            " 1/33 [..............................] - ETA: 0s - loss: 0.3134Restoring model weights from the end of the best epoch: 2088.\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6539 - val_loss: 0.5912\n",
            "Epoch 3088: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_3596556/3424320398.py:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.4091355800628662\n",
            "Train loss: 0.651863694190979\n",
            "Test loss: 0.8902407288551331\n",
            "dO18 RMSE: 0.944438885031286\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0          26.130            0.19025\n",
            "1          26.984            0.40123\n",
            "2          24.656            0.17728\n",
            "3          26.356            0.03953\n",
            "4          23.962            0.30992\n",
            "5          24.848            0.15842\n",
            "6          25.080            0.05125\n",
            "7          26.546            2.14378\n",
            "8          25.682            0.94472\n",
            "9          24.028            0.45852\n",
            "10         23.944            0.15813\n",
            "11         23.752            0.52437\n",
            "12         26.018            0.96417\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       26.645658           0.757585\n",
            "1       26.645658           0.757585\n",
            "2       24.900812           1.047868\n",
            "3       25.676737           0.283033\n",
            "4       24.921232           1.892284\n",
            "5       23.815529           8.431017\n",
            "6       23.815529           8.431017\n",
            "7       24.921242           1.892245\n",
            "8       24.900808           1.047811\n",
            "9       23.815540           8.431225\n",
            "10      25.209883           0.728118\n",
            "11      25.000227           2.128335\n",
            "12      25.209883           0.728117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-03 20:34:44.027000: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [13,13]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/usr/local/google/home/ruru/Downloads/model_builds/random_all_boosted_residuals_transformer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Grouped, random, ablating other models except kriging\n",
        "\n",
        "We can generate isoscapes for this model easily."
      ],
      "metadata": {
        "id": "opmE3xc0vcY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_random_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_train_random_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_test_random_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_validation_random_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_passthrough = [\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "columns_to_scale = []\n",
        "columns_to_standardize = ['lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature', 'Mean Annual Precipitation',]\n",
        "\n",
        "data = load_and_scale(grouped_random_fileset, columns_to_passthrough, columns_to_scale, columns_to_standardize)\n",
        "model = train_and_evaluate(data, \"random_ablated_boosted\", training_batch_size=3)\n",
        "model.save(get_model_save_location(\"random_ablated_boosted.tf\"), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location('random_ablated_boosted_transformer.pkl'))"
      ],
      "metadata": {
        "id": "KCebsA7XvfRH",
        "outputId": "a379a648-a6b6-4722-fec5-aa69f59bd3c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('lat_standardizer', StandardScaler(), ['lat']),\n",
            "                                ('long_standardizer', StandardScaler(),\n",
            "                                 ['long']),\n",
            "                                ('VPD_standardizer', StandardScaler(), ['VPD']),\n",
            "                                ('RH_standardizer', StandardScaler(), ['RH']),\n",
            "                                ('PET_standardizer', StandardScaler(), ['PET']),\n",
            "                                ('DEM_standardizer', StandardScaler(), ['DEM']),\n",
            "                                ('PA_standardizer', StandardScaler(), ['PA']),\n",
            "                                ('Mean Annual Temperature_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation'])])\n",
            "         lat      long       VPD        RH       PET       DEM        PA  \\\n",
            "0   1.135752 -1.306548 -1.589808  1.355422 -1.150947 -0.357389  0.357389   \n",
            "1   1.158744 -1.277215 -1.280299  1.113753 -0.913730 -0.453502  0.454766   \n",
            "2   0.022203  0.653942  0.976401 -0.619928  0.737860 -0.837953  0.845024   \n",
            "3   1.135686 -1.306569 -1.589808  1.355422 -1.150947 -0.357389  0.357389   \n",
            "4   0.023136  1.211747 -0.433010  0.373747  0.286614 -0.273291  0.272244   \n",
            "..       ...       ...       ...       ...       ...       ...       ...   \n",
            "94  0.453174  0.335493  0.226872 -0.117257  0.407898  0.099147 -0.104125   \n",
            "95  0.454036  0.335285  0.226872 -0.117257  0.407898  0.099147 -0.104125   \n",
            "96  0.047599 -1.107278 -1.092892  1.073086 -1.404214 -0.513573  0.515664   \n",
            "98  0.452887  0.335285  0.226872 -0.117257  0.407898  0.099147 -0.104125   \n",
            "99  0.456910  0.334661  0.226872 -0.117257  0.407898 -0.417460  0.418242   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  \\\n",
            "0                 -0.482400                   1.523642   \n",
            "1                 -0.201347                   1.334959   \n",
            "2                  1.133634                  -0.068727   \n",
            "3                 -0.482400                   1.523642   \n",
            "4                 -0.334850                  -1.143649   \n",
            "..                      ...                        ...   \n",
            "94                 0.501277                  -0.125904   \n",
            "95                 0.501277                  -0.125904   \n",
            "96                 0.374806                   1.423583   \n",
            "98                 0.501277                  -0.125904   \n",
            "99                 0.501277                  -0.125904   \n",
            "\n",
            "    ordinary_kriging_linear_d18O_predicted_mean  \\\n",
            "0                                     24.994657   \n",
            "1                                     25.048990   \n",
            "2                                     26.133854   \n",
            "3                                     24.994657   \n",
            "4                                     26.632920   \n",
            "..                                          ...   \n",
            "94                                    25.512818   \n",
            "95                                    25.512818   \n",
            "96                                    24.949970   \n",
            "98                                    25.512818   \n",
            "99                                    25.512818   \n",
            "\n",
            "    ordinary_kriging_linear_d18O_predicted_variance  \n",
            "0                                          0.708097  \n",
            "1                                          0.688112  \n",
            "2                                          0.711751  \n",
            "3                                          0.708097  \n",
            "4                                          0.715229  \n",
            "..                                              ...  \n",
            "94                                         0.658146  \n",
            "95                                         0.658146  \n",
            "96                                         0.693620  \n",
            "98                                         0.658146  \n",
            "99                                         0.658146  \n",
            "\n",
            "[99 rows x 11 columns]\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       23.405260           0.148275\n",
            "1       25.725845           0.213718\n",
            "2       26.748000           0.532670\n",
            "3       25.936000           0.066530\n",
            "4       26.862000           0.489970\n",
            "..            ...                ...\n",
            "94      26.134000           0.194530\n",
            "95      25.576000           1.794680\n",
            "96      25.196000           0.410630\n",
            "98      25.330000           0.286467\n",
            "99      25.882000           0.233670\n",
            "\n",
            "[99 rows x 2 columns]\n",
            "==================\n",
            "random_ablated_boosted\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 11)]         0           []                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_12 (S  (None, 9)           0           ['input_5[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 20)           200         ['tf.__operators__.getitem_12[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_13 (S  (None,)             0           ['input_5[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 20)           420         ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_14 (S  (None,)             0           ['input_5[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " tf.expand_dims_8 (TFOpLambda)  (None, 1)            0           ['tf.__operators__.getitem_13[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " pred_mean_residual (Dense)     (None, 1)            21          ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " tf.expand_dims_9 (TFOpLambda)  (None, 1)            0           ['tf.__operators__.getitem_14[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " pred_var_output (Dense)        (None, 1)            21          ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TFOpLa  (None, 1)           0           ['tf.expand_dims_8[0][0]',       \n",
            " mbda)                                                            'pred_mean_residual[0][0]']     \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1)            0           ['tf.expand_dims_9[0][0]',       \n",
            "                                                                  'pred_var_output[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 2)            0           ['tf.__operators__.add_4[0][0]', \n",
            "                                                                  'lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 662\n",
            "Trainable params: 662\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Restoring model weights from the end of the best epoch: 188.\n",
            "Epoch 1188: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_3813950/3563715079.py:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.5435933470726013\n",
            "Train loss: 0.7792006731033325\n",
            "Test loss: 0.9979507923126221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-03 21:13:03.290520: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [13,11]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dO18 RMSE: 0.984656242426217\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0          26.130            0.19025\n",
            "1          26.984            0.40123\n",
            "2          24.656            0.17728\n",
            "3          26.356            0.03953\n",
            "4          23.962            0.30992\n",
            "5          24.848            0.15842\n",
            "6          25.080            0.05125\n",
            "7          26.546            2.14378\n",
            "8          25.682            0.94472\n",
            "9          24.028            0.45852\n",
            "10         23.944            0.15813\n",
            "11         23.752            0.52437\n",
            "12         26.018            0.96417\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       26.627794           0.897476\n",
            "1       26.627794           0.897476\n",
            "2       24.982353           1.202779\n",
            "3       25.066158           1.204025\n",
            "4       24.916113           2.477588\n",
            "5       23.823303           8.910612\n",
            "6       23.823303           8.910612\n",
            "7       24.916119           2.477689\n",
            "8       24.982349           1.202778\n",
            "9       23.823301           8.910519\n",
            "10      25.127268           0.874924\n",
            "11      24.992229           2.330199\n",
            "12      25.127268           0.874924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /usr/local/google/home/ruru/Downloads/model_builds/random_ablated_boosted.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /usr/local/google/home/ruru/Downloads/model_builds/random_ablated_boosted.tf/assets\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/usr/local/google/home/ruru/Downloads/model_builds/random_ablated_boosted_transformer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Grouped, fixed, all columns\n",
        "\n",
        "We can't (easily) generate isoscapes with these because the isoscapes for 'predkrig_br_lat_ISORG', 'Iso_Oxi_Stack_mean_TERZER',\n",
        "                   'isoscape_fullmodel_d18O_prec_REGRESSION' are not easily retrievable... but I'm curious how much better the model is if these columns are included."
      ],
      "metadata": {
        "id": "wPDSSm__DR53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_fixed_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_train_fixed_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_test_fixed_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_validation_fixed_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_passthrough = [\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "columns_to_scale = []\n",
        "columns_to_standardize = [\n",
        "    'lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature','Mean Annual Precipitation',\n",
        "    'Iso_Oxi_Stack_mean_TERZER',\n",
        "    'isoscape_fullmodel_d18O_prec_REGRESSION']\n",
        "\n",
        "data = load_and_scale(grouped_fixed_fileset, columns_to_passthrough, columns_to_scale, columns_to_standardize)\n",
        "model = train_and_evaluate(data, \"fixed_all_boosted\", training_batch_size=3)\n",
        "model.save(get_model_save_location(\"fixed_all_boosted.keras\"), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location('fixed_all_boosted_transformer.pkl'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V9_5iUkDVoCV",
        "outputId": "c281d15b-f002-4cbe-ea84-60e055f85bec"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('lat_standardizer', StandardScaler(), ['lat']),\n",
            "                                ('long_standardizer', StandardScaler(),\n",
            "                                 ['long']),\n",
            "                                ('VPD_standardizer', StandardScaler(), ['VPD']),\n",
            "                                ('RH_standardizer', StandardScaler(), ['RH']),\n",
            "                                ('PET_standardizer', StandardScaler(), ['PET']),\n",
            "                                ('DEM_standardizer', StandardScaler(), ['DEM']),\n",
            "                                ('PA_standardizer'...\n",
            "                                ('Mean Annual Temperature_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation']),\n",
            "                                ('Iso_Oxi_Stack_mean_TERZER_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Iso_Oxi_Stack_mean_TERZER']),\n",
            "                                ('isoscape_fullmodel_d18O_prec_REGRESSION_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['isoscape_fullmodel_d18O_prec_REGRESSION'])])\n",
            "         lat      long       VPD        RH       PET       DEM        PA  \\\n",
            "0  -1.283309  0.188728  1.866980 -1.778377  1.642663 -1.970213  1.972683   \n",
            "1  -1.279714  1.477609 -0.716901  0.689136 -0.474220 -0.193463  0.192112   \n",
            "2  -1.279714  1.477609 -0.716901  0.689136 -0.474220 -0.193463  0.192112   \n",
            "3  -1.279714  1.477609 -0.716901  0.689136 -0.474220 -0.193463  0.192112   \n",
            "4  -1.279714  1.477609 -0.716901  0.689136 -0.474220 -0.193463  0.192112   \n",
            "..       ...       ...       ...       ...       ...       ...       ...   \n",
            "64 -0.621769  1.445891 -1.837055  1.852120 -0.424018  0.978435 -0.977885   \n",
            "65 -1.279714  1.477609 -0.716901  0.689136 -0.474220 -0.193463  0.192112   \n",
            "66 -1.279714  1.477609 -0.716901  0.689136 -0.474220 -0.193463  0.192112   \n",
            "67 -1.279714  1.477609 -0.716901  0.689136 -0.474220 -0.193463  0.192112   \n",
            "68 -1.279714  1.477609 -0.716901  0.689136 -0.474220 -0.193463  0.192112   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  \\\n",
            "0                  1.591838                   0.680206   \n",
            "1                 -1.007340                  -1.379910   \n",
            "2                 -1.007340                  -1.379910   \n",
            "3                 -1.007340                  -1.379910   \n",
            "4                 -1.007340                  -1.379910   \n",
            "..                      ...                        ...   \n",
            "64                -1.890298                  -1.692215   \n",
            "65                -1.007340                  -1.379910   \n",
            "66                -1.007340                  -1.379910   \n",
            "67                -1.007340                  -1.379910   \n",
            "68                -1.007340                  -1.379910   \n",
            "\n",
            "    Iso_Oxi_Stack_mean_TERZER  isoscape_fullmodel_d18O_prec_REGRESSION  \\\n",
            "0                    0.263059                                 0.019732   \n",
            "1                    1.496809                                 1.344746   \n",
            "2                    1.496809                                 1.344746   \n",
            "3                    1.496809                                 1.344746   \n",
            "4                    1.496809                                 1.344746   \n",
            "..                        ...                                      ...   \n",
            "64                   1.371394                                 1.635463   \n",
            "65                   1.496809                                 1.344746   \n",
            "66                   1.496809                                 1.344746   \n",
            "67                   1.496809                                 1.344746   \n",
            "68                   1.496809                                 1.344746   \n",
            "\n",
            "    ordinary_kriging_linear_d18O_predicted_mean  \\\n",
            "0                                     25.920155   \n",
            "1                                     25.920155   \n",
            "2                                     25.920155   \n",
            "3                                     25.920155   \n",
            "4                                     25.920155   \n",
            "..                                          ...   \n",
            "64                                    25.920155   \n",
            "65                                    25.920155   \n",
            "66                                    25.920155   \n",
            "67                                    25.920155   \n",
            "68                                    25.920155   \n",
            "\n",
            "    ordinary_kriging_linear_d18O_predicted_variance  \n",
            "0                                          1.261054  \n",
            "1                                          1.261054  \n",
            "2                                          1.261054  \n",
            "3                                          1.261054  \n",
            "4                                          1.261054  \n",
            "..                                              ...  \n",
            "64                                         1.261054  \n",
            "65                                         1.261054  \n",
            "66                                         1.261054  \n",
            "67                                         1.261054  \n",
            "68                                         1.261054  \n",
            "\n",
            "[68 rows x 13 columns]\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0          26.748            0.53267\n",
            "1          26.862            0.48997\n",
            "2          26.290            0.82200\n",
            "3          26.884            0.36018\n",
            "4          26.536            0.64383\n",
            "..            ...                ...\n",
            "64         25.160            0.21505\n",
            "65         26.542            0.97147\n",
            "66         27.096            0.16743\n",
            "67         26.130            0.19025\n",
            "68         26.984            0.40123\n",
            "\n",
            "[68 rows x 2 columns]\n",
            "==================\n",
            "fixed_all_boosted\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 13)]         0           []                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_6 (Sl  (None, 11)          0           ['input_3[0][0]']                \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 20)           240         ['tf.__operators__.getitem_6[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_7 (Sl  (None,)             0           ['input_3[0][0]']                \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 20)           420         ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_8 (Sl  (None,)             0           ['input_3[0][0]']                \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " tf.expand_dims_4 (TFOpLambda)  (None, 1)            0           ['tf.__operators__.getitem_7[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " pred_mean_residual (Dense)     (None, 1)            21          ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " tf.expand_dims_5 (TFOpLambda)  (None, 1)            0           ['tf.__operators__.getitem_8[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " pred_var_output (Dense)        (None, 1)            21          ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 1)           0           ['tf.expand_dims_4[0][0]',       \n",
            " mbda)                                                            'pred_mean_residual[0][0]']     \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1)            0           ['tf.expand_dims_5[0][0]',       \n",
            "                                                                  'pred_var_output[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 2)            0           ['tf.__operators__.add_2[0][0]', \n",
            "                                                                  'lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 702\n",
            "Trainable params: 702\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Restoring model weights from the end of the best epoch: 7.\n",
            "Epoch 1007: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_3813950/3563715079.py:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 1.0689153671264648\n",
            "Train loss: 0.8149141669273376\n",
            "Test loss: 1.2395533323287964\n",
            "dO18 RMSE: 1.2603639060315739\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       24.042000           0.345970\n",
            "1       25.240000           0.035950\n",
            "2       25.782000           0.372220\n",
            "3       25.076000           0.230280\n",
            "4       25.966000           0.172480\n",
            "5       27.434000           0.501030\n",
            "6       28.156000           0.999030\n",
            "7       26.836000           0.120880\n",
            "8       28.180000           0.778250\n",
            "9       26.834000           0.094930\n",
            "10      26.644000           0.488430\n",
            "11      26.772000           0.373370\n",
            "12      27.684280           1.216389\n",
            "13      27.403235           0.892280\n",
            "14      24.777670           0.736571\n",
            "15      25.551850           0.312355\n",
            "16      25.115885           0.033519\n",
            "17      25.987815           5.280825\n",
            "18      24.132031           0.389042\n",
            "19      24.898000           0.236070\n",
            "20      23.944000           0.158130\n",
            "21      26.018000           0.964170\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       24.945362           1.525688\n",
            "1       24.945362           1.525688\n",
            "2       24.945362           1.525688\n",
            "3       24.945362           1.525688\n",
            "4       24.945362           1.525688\n",
            "5       28.760380           0.804838\n",
            "6       28.807522           0.901218\n",
            "7       28.807522           0.901218\n",
            "8       28.807522           0.901218\n",
            "9       28.807522           0.901218\n",
            "10      28.807522           0.901218\n",
            "11      28.807522           0.901218\n",
            "12      28.760380           0.804838\n",
            "13      28.807522           0.901218\n",
            "14      23.622986           1.474437\n",
            "15      24.128969           1.402159\n",
            "16      24.136972           1.386745\n",
            "17      24.116814           1.431584\n",
            "18      24.137281           1.397189\n",
            "19      24.945362           1.525688\n",
            "20      24.945362           1.525688\n",
            "21      24.945362           1.525688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-03 21:06:57.938724: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [22,13]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/usr/local/google/home/ruru/Downloads/model_builds/fixed_all_boosted_transformer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Grouped fixed, ablating other columns besides kriging"
      ],
      "metadata": {
        "id": "h1uuaygyDvXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_fixed_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_train_fixed_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_test_fixed_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_validation_fixed_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_passthrough = [\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "columns_to_scale = []\n",
        "columns_to_standardize =  ['lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature','Mean Annual Precipitation']\n",
        "\n",
        "data = load_and_scale(grouped_fixed_fileset, columns_to_passthrough, columns_to_scale, columns_to_standardize)\n",
        "model = train_and_evaluate(data, 'fixed_ablated', training_batch_size=3)\n",
        "model.save(get_model_save_location('fixed_ablated_boosted.tf'), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location('fixed_ablated_boosted._transformer.pkl'))"
      ],
      "metadata": {
        "id": "WQ60MVzlD5DJ",
        "outputId": "8aa60e54-ec21-4d5e-f1a1-485dc9a6d21d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('lat_standardizer', StandardScaler(), ['lat']),\n",
            "                                ('long_standardizer', StandardScaler(),\n",
            "                                 ['long']),\n",
            "                                ('VPD_standardizer', StandardScaler(), ['VPD']),\n",
            "                                ('RH_standardizer', StandardScaler(), ['RH']),\n",
            "                                ('PET_standardizer', StandardScaler(), ['PET']),\n",
            "                                ('DEM_standardizer', StandardScaler(), ['DEM']),\n",
            "                                ('PA_standardizer', StandardScaler(), ['PA']),\n",
            "                                ('Mean Annual Temperature_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation'])])\n",
            "==================\n",
            "fixed_ablated\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 11)]         0           []                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_18 (S  (None, 9)           0           ['input_7[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 20)           200         ['tf.__operators__.getitem_18[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_19 (S  (None,)             0           ['input_7[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 20)           420         ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_20 (S  (None,)             0           ['input_7[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " tf.expand_dims_12 (TFOpLambda)  (None, 1)           0           ['tf.__operators__.getitem_19[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " pred_mean_residual (Dense)     (None, 1)            21          ['dense_13[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_13 (TFOpLambda)  (None, 1)           0           ['tf.__operators__.getitem_20[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " pred_var_output (Dense)        (None, 1)            21          ['dense_13[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TFOpLa  (None, 1)           0           ['tf.expand_dims_12[0][0]',      \n",
            " mbda)                                                            'pred_mean_residual[0][0]']     \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1)            0           ['tf.expand_dims_13[0][0]',      \n",
            "                                                                  'pred_var_output[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 2)            0           ['tf.__operators__.add_6[0][0]', \n",
            "                                                                  'lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 662\n",
            "Trainable params: 662\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Restoring model weights from the end of the best epoch: 42.\n",
            "Epoch 1042: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_3813950/3563715079.py:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.989641547203064\n",
            "Train loss: 0.7396669387817383\n",
            "Test loss: 1.236862063407898\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb2e872a8e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-03 21:24:01.037850: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [22,11]\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb2e872a8e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dO18 RMSE: 1.1600789416900688\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       24.042000           0.345970\n",
            "1       25.240000           0.035950\n",
            "2       25.782000           0.372220\n",
            "3       25.076000           0.230280\n",
            "4       25.966000           0.172480\n",
            "5       27.434000           0.501030\n",
            "6       28.156000           0.999030\n",
            "7       26.836000           0.120880\n",
            "8       28.180000           0.778250\n",
            "9       26.834000           0.094930\n",
            "10      26.644000           0.488430\n",
            "11      26.772000           0.373370\n",
            "12      27.684280           1.216389\n",
            "13      27.403235           0.892280\n",
            "14      24.777670           0.736571\n",
            "15      25.551850           0.312355\n",
            "16      25.115885           0.033519\n",
            "17      25.987815           5.280825\n",
            "18      24.132031           0.389042\n",
            "19      24.898000           0.236070\n",
            "20      23.944000           0.158130\n",
            "21      26.018000           0.964170\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       26.019182           0.635364\n",
            "1       26.019182           0.635364\n",
            "2       26.019182           0.635364\n",
            "3       26.019182           0.635364\n",
            "4       26.019182           0.635364\n",
            "5       26.288429           3.217464\n",
            "6       26.367653           3.075664\n",
            "7       26.367653           3.075664\n",
            "8       26.367653           3.075664\n",
            "9       26.367653           3.075664\n",
            "10      26.367653           3.075664\n",
            "11      26.367653           3.075664\n",
            "12      26.288429           3.217464\n",
            "13      26.367653           3.075664\n",
            "14      26.222849           2.363685\n",
            "15      26.077663           2.608125\n",
            "16      26.077822           2.561823\n",
            "17      26.076906           2.690293\n",
            "18      26.078264           2.588497\n",
            "19      26.019182           0.635364\n",
            "20      26.019182           0.635364\n",
            "21      26.019182           0.635364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /usr/local/google/home/ruru/Downloads/model_builds/fixed_ablated_boosted.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /usr/local/google/home/ruru/Downloads/model_builds/fixed_ablated_boosted.tf/assets\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/usr/local/google/home/ruru/Downloads/model_builds/fixed_ablated_boosted._transformer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}