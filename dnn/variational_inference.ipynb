{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnc-br/ddf-isoscapes/blob/boosting/dnn/variational_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational model\n",
        "\n",
        "Find the mean/variance of O18 ratios (as well as N15 and C13 in the future) at a particular lat/lon across Brazil. At the bottom of the colab, train and evaluate 4 different versions of the model with different data partitioning strategies."
      ],
      "metadata": {
        "id": "-0IfT3kGwgK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "henIPlAPCb4i",
        "outputId": "529a530e-2cb6-466b-9b44-5f80e721aecc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-03 00:04:18.521988: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-08-03 00:04:18.560588: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-08-03 00:04:18.562216: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-03 00:04:19.307507: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import os\n",
        "from typing import List, Tuple, Dict\n",
        "from dataclasses import dataclass\n",
        "from joblib import dump\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.python.ops import math_ops\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "#@title Debugging\n",
        "# See https://zohaib.me/debugging-in-google-collab-notebook/ for tips,\n",
        "# as well as docs for pdb and ipdb.\n",
        "DEBUG = False #@param {type:\"boolean\"}\n",
        "USE_LOCAL_DRIVE = True #@param {type:\"boolean\"}\n",
        "LOCAL_DIR = \"/usr/local/google/home/ruru/Downloads/amazon_sample_data-20230712T203059Z-001\" #@param\n",
        "GDRIVE_DIR = \"MyDrive/amazon_rainforest_files/\" #@param\n",
        "FP_ROOT = LOCAL_DIR\n",
        "\n",
        "MODEL_SAVE_LOCATION = \"/usr/local/google/home/ruru/Downloads/model_builds\" #@param\n",
        "\n",
        "def get_model_save_location(filename) -> str:\n",
        "  root = '' if USE_LOCAL_DRIVE else '/content/drive'\n",
        "  return os.path.join(root, MODEL_SAVE_LOCATION, filename)\n",
        "\n",
        "# Access data stored on Google Drive if not reading data locally.\n",
        "if not USE_LOCAL_DRIVE:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  global FP_ROOT\n",
        "  FP_ROOT = os.path.join('/content/drive', GDRIVE_DIR)\n",
        "\n",
        "if DEBUG:\n",
        "    %pip install -Uqq ipdb\n",
        "    import ipdb\n",
        "    %pdb on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQrEv57zCb4q"
      },
      "source": [
        "# Data preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path: str, columns_to_keep: List[str]):\n",
        "  df = pd.read_csv(path, encoding=\"ISO-8859-1\", sep=',')\n",
        "  df = df[df['d18O_cel_variance'].notna()]\n",
        "  df = df[df['d18O_cel_variance'] < 40]\n",
        "  X = df\n",
        "\n",
        "  X['krig_mean_residual'] = X['ordinary_kriging_linear_d18O_predicted_mean'] - X['d18O_cel_mean']\n",
        "  X['krig_variance_residual'] = X['ordinary_kriging_linear_d18O_predicted_variance'] - X['d18O_cel_variance']\n",
        "\n",
        "  columns_to_keep = columns_to_keep + ['krig_mean_residual', 'krig_variance_residual']\n",
        "  X = X.drop(df.columns.difference(columns_to_keep), axis=1)\n",
        "\n",
        "  Y = df[[\"d18O_cel_mean\", \"d18O_cel_variance\"]]\n",
        "  print(X)\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "6XMee1aHfcik"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardization"
      ],
      "metadata": {
        "id": "DtkKhMOtb6cS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class FeaturesToLabels:\n",
        "  def __init__(self, X: pd.DataFrame, Y: pd.DataFrame):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "\n",
        "  def as_tuple(self):\n",
        "    return (self.X, self.Y)\n",
        "\n",
        "\n",
        "def create_feature_scaler(X: pd.DataFrame, columns_to_normalize, columns_to_standardize) -> ColumnTransformer:\n",
        "  feature_scaler = ColumnTransformer(\n",
        "      [('krig_mean_residual_scaler', StandardScaler(), ['krig_mean_residual']),\n",
        "       ('krig_variance_residual_scaler', StandardScaler(), ['krig_variance_residual'])] +\n",
        "      [(column+'_normalizer', Normalizer(), [column]) for column in columns_to_normalize] +\n",
        "      [(column+'_standardizer', StandardScaler(), [column]) for column in columns_to_standardize],\n",
        "      remainder='passthrough')\n",
        "  feature_scaler.fit(X)\n",
        "  print(feature_scaler)\n",
        "  return feature_scaler\n",
        "\n",
        "def create_label_scaler(Y: pd.DataFrame) -> ColumnTransformer:\n",
        "  label_scaler = ColumnTransformer([\n",
        "      ('mean_std_scaler', StandardScaler(), ['d18O_cel_mean']),\n",
        "      ('var_minmax_scaler', MinMaxScaler(), ['d18O_cel_variance'])],\n",
        "      remainder='passthrough')\n",
        "  label_scaler.fit(Y)\n",
        "  return label_scaler\n",
        "\n",
        "def scale(X: pd.DataFrame, Y: pd.DataFrame, feature_scaler, label_scaler):\n",
        "  # transform() outputs numpy arrays :(  need to convert back to DataFrame.\n",
        "  X_standardized = pd.DataFrame(feature_scaler.transform(X),\n",
        "                        index=X.index, columns=X.columns)\n",
        "  return FeaturesToLabels(X_standardized, Y)"
      ],
      "metadata": {
        "id": "XSDwdvMkb7w8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just a class organization, holds each scaled dataset and the scaler used.\n",
        "# Useful for unscaling predictions.\n",
        "@dataclass\n",
        "class ScaledPartitions():\n",
        "  def __init__(self,\n",
        "               feature_scaler: ColumnTransformer,\n",
        "               label_scaler: ColumnTransformer,\n",
        "               train: FeaturesToLabels, val: FeaturesToLabels,\n",
        "               test: FeaturesToLabels):\n",
        "    self.feature_scaler = feature_scaler\n",
        "    self.label_scaler = label_scaler\n",
        "    self.train = train\n",
        "    self.val = val\n",
        "    self.test = test\n",
        "\n",
        "\n",
        "def load_and_scale(config: Dict,\n",
        "                   columns_to_normalize: List[str],\n",
        "                   columns_to_standardize: List[str]) -> ScaledPartitions:\n",
        "  columns_to_keep = columns_to_normalize + columns_to_standardize\n",
        "  X_train, Y_train = load_dataset(config['TRAIN'], columns_to_keep)\n",
        "  X_val, Y_val = load_dataset(config['VALIDATION'], columns_to_keep)\n",
        "  X_test, Y_test = load_dataset(config['TEST'], columns_to_keep)\n",
        "\n",
        "  feature_scaler = create_feature_scaler(X_train, columns_to_normalize, columns_to_standardize)\n",
        "  label_scaler = create_label_scaler(Y_train)\n",
        "  train = scale(X_train, Y_train, feature_scaler, label_scaler)\n",
        "  val = scale(X_val, Y_val, feature_scaler, label_scaler)\n",
        "  test = scale(X_test, Y_test, feature_scaler, label_scaler)\n",
        "  return ScaledPartitions(feature_scaler, label_scaler, train, val, test)\n"
      ],
      "metadata": {
        "id": "_kf2e_fKon2P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition\n",
        "\n"
      ],
      "metadata": {
        "id": "usGznR593LZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KL Loss function:"
      ],
      "metadata": {
        "id": "khK7C8WvU8ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_normal_distribution(\n",
        "    mean: tf.Tensor,\n",
        "    stdev: tf.Tensor,\n",
        "    n: int) -> tf.Tensor:\n",
        "    '''\n",
        "    Given a batch of normal distributions described by a mean and stdev in\n",
        "    a tf.Tensor, sample n elements from each distribution and return the mean\n",
        "    and standard deviation per sample.\n",
        "    '''\n",
        "    batch_size = tf.shape(mean)[0]\n",
        "\n",
        "    # Output tensor is (n, batch_size, 1)\n",
        "    sample_values = tfp.distributions.Normal(\n",
        "        loc=mean,\n",
        "        scale=stdev).sample(\n",
        "            sample_shape=n)\n",
        "    # Reshaped tensor will be (batch_size, n)\n",
        "    sample_values = tf.transpose(sample_values)\n",
        "    # Get the mean per sample in the batch.\n",
        "    sample_mean = tf.transpose(tf.math.reduce_mean(sample_values, 2))\n",
        "    sample_stdev = tf.transpose(tf.math.reduce_std(sample_values, 2))\n",
        "\n",
        "    return sample_mean, sample_stdev\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# log(σ2/σ1) + ( σ1^2+(μ1−μ2)^2 ) / 2* σ^2   − 1/2\n",
        "def kl_divergence_helper(real, predicted, sample):\n",
        "    '''\n",
        "    real: tf.Tensor of the real mean and standard deviation of sample to compare\n",
        "    predicted: tf.Tensor of the predicted mean and standard deviation to compare\n",
        "    sample: Whether or not to sample the predicted distribution to get a new\n",
        "            mean and standard deviation.\n",
        "    '''\n",
        "    if real.shape != predicted.shape:\n",
        "      raise ValueError(\n",
        "          f\"real.shape {real.shape} != predicted.shape {predicted.shape}\")\n",
        "\n",
        "    real_value = tf.gather(real, [0], axis=1)\n",
        "    real_std = tf.math.sqrt(tf.gather(real, [1], axis=1))\n",
        "\n",
        "\n",
        "    predicted_value = tf.gather(predicted, [0], axis=1)\n",
        "    predicted_std = tf.math.sqrt(tf.gather(predicted, [1], axis=1))\n",
        "    # If true, sample from the distribution defined by the predicted mean and\n",
        "    # standard deviation to use for mean and stdev used in KL divergence loss.\n",
        "    if sample:\n",
        "      predicted_value, predicted_std = sample_normal_distribution(\n",
        "          mean=predicted_value, stdev=predicted_std, n=15)\n",
        "\n",
        "    kl_loss = -0.5 + tf.math.log(predicted_std/real_std) + \\\n",
        "     (tf.square(real_std) + tf.square(real_value - predicted_value))/ \\\n",
        "     (2*tf.square(predicted_std))\n",
        "\n",
        "    if tf.math.is_nan(tf.math.reduce_mean(kl_loss)):\n",
        "       tf.print(predicted)\n",
        "       sess = tf.compat.v1.Session()\n",
        "       sess.close()\n",
        "\n",
        "    return tf.math.reduce_mean(kl_loss)\n",
        "\n",
        "def kl_divergence(real, predicted):\n",
        "  return kl_divergence_helper(real, predicted, True)"
      ],
      "metadata": {
        "id": "urGjYNNnemX6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the loss function:"
      ],
      "metadata": {
        "id": "fJzBFWQVeqNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "\n",
        "class TensorsDifferShapeTest(unittest.TestCase):\n",
        "   def test(self):\n",
        "      test_real = tf.convert_to_tensor(np.array([[1, 0.02]]))\n",
        "      test_pred = tf.convert_to_tensor(np.array([[0.98]]))\n",
        "      with self.assertRaises(ValueError):\n",
        "         kl_divergence(test_real, test_pred, False)\n",
        "         assert(False) # Triggers if no exception is caught in the previous line.\n",
        "\n",
        "TensorsDifferShapeTest().test()\n",
        "\n",
        "test_real = tf.convert_to_tensor(np.array([[1, 0.02]]))\n",
        "test_pred = tf.convert_to_tensor(np.array([[0.98, 0.021]]))\n",
        "\n",
        "# https://screenshot.googleplex.com/5WM9dinAbhR26ZS\n",
        "assert float(kl_divergence(test_real, test_pred)) == pytest.approx(0.0101094, 1e-5)\n",
        "\n",
        "test_neg_real = tf.convert_to_tensor(np.array([[32.32, 0.0344]]))\n",
        "test_neg_pred = tf.convert_to_tensor(np.array([[32.01, -0.322]]))\n",
        "\n",
        "# Negative variance causes NaN\n",
        "assert tf.math.is_nan(kl_divergence(test_neg_real, test_neg_pred))\n",
        "\n",
        "# Calculated manually by computing the result of this equation in wolfram alpha:\n",
        "# log(σ2/σ1) + ( σ1^2+(μ1−μ2)^2 ) / 2* σ^2   − 1/2\n",
        "test_real_2d = tf.convert_to_tensor(np.array(\n",
        "    [[1.00, 0.020],\n",
        "     [1.01, 0.042]]))\n",
        "test_pred_2d = tf.convert_to_tensor(np.array(\n",
        "    [[0.98, 0.021],\n",
        "     [0.99, 0.012]]))\n",
        "\n",
        "# Should reduce to the average loss of all rows.\n",
        "assert float(kl_divergence(test_real_2d, test_pred_2d)) == pytest.approx(\n",
        "    sum([0.0101094, 0.6402851])/2, 1e-5)"
      ],
      "metadata": {
        "id": "48TaPd70erSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model definition"
      ],
      "metadata": {
        "id": "8rI6qPRh7oO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "def get_early_stopping_callback():\n",
        "  return EarlyStopping(monitor='val_loss', patience=100, min_delta=0.001,\n",
        "                       verbose=1, restore_best_weights=True, start_from_epoch=0)\n",
        "\n",
        "tf.keras.utils.set_random_seed(18731)\n",
        "\n",
        "# I was experimenting with models that took longer to train, and used this\n",
        "# checkpointing callback to periodically save the model. It's optional.\n",
        "def get_checkpoint_callback(model_file):\n",
        "  return ModelCheckpoint(\n",
        "      get_model_save_location(model_file),\n",
        "      monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
        "\n",
        "def train_or_update_variational_model(\n",
        "        sp: ScaledPartitions,\n",
        "        hidden_layers: List[int],\n",
        "        epochs: int,\n",
        "        batch_size: int,\n",
        "        lr: float,\n",
        "        model_file=None,\n",
        "        use_checkpoint=False):\n",
        "  callbacks_list = [get_early_stopping_callback(),\n",
        "                    get_checkpoint_callback(model_file)]\n",
        "  if not use_checkpoint:\n",
        "\n",
        "    # Find the kriging columns and make sure they are at the end of the dataframe.\n",
        "    krig_mean_index = sp.train.X.columns.get_loc(\"krig_mean_residual\")\n",
        "    krig_var_index = sp.train.X.columns.get_loc(\"krig_variance_residual\")\n",
        "    if (krig_mean_index != sp.train.X.shape[1]-2 and krig_var_index != sp.train.X.shape[1]-1):\n",
        "      raise ValueError(\"krig_mean_residual and krig_variance_residual must be\"\n",
        "      \"located in the last two columns of dataframe\")\n",
        "\n",
        "    inputs = keras.Input(shape=(sp.train.X.shape[1],))\n",
        "    krig_mean = tf.expand_dims(inputs[:,-2], 1)\n",
        "    krig_var = tf.expand_dims(inputs[:, -1], 1)\n",
        "\n",
        "    # X contains everything else.\n",
        "    x = inputs[:,0:-2]\n",
        "\n",
        "    for layer_size in hidden_layers:\n",
        "      x = keras.layers.Dense(layer_size, activation='relu')(x)\n",
        "\n",
        "    # Invert the normalization on our outputs, and add kriging predictions as\n",
        "    # constants so the network only predicts the residuals.\n",
        "    mean_residual_output = keras.layers.Dense(1, name='mean_output')(x)\n",
        "    mean_residual_scaler = sp.feature_scaler.named_transformers_['krig_mean_residual_scaler']\n",
        "\n",
        "    # Scale the residuals... just to immediately unscale them? This seems redundant.\n",
        "    # It's probably better to just leave the residuals alone and just fit the\n",
        "    # transformer to them.\n",
        "    unscaled_mean_residual = mean_residual_output * mean_residual_scaler.scale_ + mean_residual_scaler.mean_\n",
        "    unscaled_krig_mean = krig_mean * mean_residual_scaler.scale_ + mean_residual_scaler.mean_\n",
        "    untransformed_mean = unscaled_krig_mean + unscaled_mean_residual\n",
        "\n",
        "    var_residual_output = keras.layers.Dense(1, name='var_output')(x)\n",
        "    var_residual_scaler = sp.feature_scaler.named_transformers_['krig_variance_residual_scaler']\n",
        "    unscaled_var_residual = var_residual_output * var_residual_scaler.scale_  + var_residual_scaler.mean_\n",
        "    unscaled_krig_var = krig_var * var_residual_scaler.scale_  + var_residual_scaler.mean_\n",
        "    untransformed_var = keras.layers.Lambda(lambda t: tf.math.log(1 + tf.exp(t[0]+t[1])))([unscaled_var_residual, unscaled_krig_var])\n",
        "\n",
        "    # Output mean,  tuples.\n",
        "    outputs = keras.layers.concatenate([untransformed_mean, untransformed_var])\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Later epochs seem to benefit from lower learning rate... but it takes\n",
        "    # a while to get there.\n",
        "    decay = keras.optimizers.schedules.ExponentialDecay(\n",
        "       lr, decay_steps=100, decay_rate=0.5, staircase=True)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss=kl_divergence)\n",
        "    model.summary()\n",
        "  else:\n",
        "    model = keras.models.load_model(\n",
        "        get_model_save_location(model_file),\n",
        "        custom_objects={\"kl_divergence\": kl_divergence})\n",
        "  history = model.fit(sp.train.X, sp.train.Y, verbose=1, epochs=epochs, batch_size=batch_size,\n",
        "                      validation_data=sp.val.as_tuple(),\n",
        "                      shuffle=True, callbacks=callbacks_list)\n",
        "  return history, model"
      ],
      "metadata": {
        "id": "HCkGSPUo3KqY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def render_plot_loss(history, name):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title(name + ' model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.yscale(\"log\")\n",
        "  plt.ylim((0, 10))\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['loss', 'val_loss'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def destandardize(sd: ScaledPartitions, df: pd.DataFrame):\n",
        "  means = pd.DataFrame(\n",
        "      sd.label_scaler.named_transformers_['var_std_scaler'].inverse_transform(df[['d18O_cel_mean']]),\n",
        "      index=df.index, columns=['d18O_cel_mean'])\n",
        "  vars = df['d18O_cel_variance']\n",
        "  return means.join(vars)\n",
        "\n",
        "def train_and_evaluate(sp: ScaledPartitions, run_id: str, training_batch_size=5):\n",
        "  print(\"==================\")\n",
        "  print(run_id)\n",
        "  history, model = train_or_update_variational_model(\n",
        "      sp, hidden_layers=[20, 20], epochs=10000, batch_size=training_batch_size,\n",
        "      lr=0.0002, model_file=run_id+\".h5\", use_checkpoint=True)\n",
        "  render_plot_loss(history, run_id+\" kl_loss\")\n",
        "  model.save(get_model_save_location(run_id+\".h5\"), save_format=\"h5\")\n",
        "\n",
        "  best_epoch_index = history.history['val_loss'].index(min(history.history['val_loss']))\n",
        "  print('Val loss:', history.history['val_loss'][best_epoch_index])\n",
        "  print('Train loss:', history.history['loss'][best_epoch_index])\n",
        "  print('Test loss:', model.evaluate(x=sp.test.X, y=sp.test.Y, verbose=0))\n",
        "\n",
        "  predictions = model.predict_on_batch(sp.test.X)\n",
        "  predictions = pd.DataFrame(predictions, columns=['d18O_cel_mean', 'd18O_cel_variance'])\n",
        "  rmse = np.sqrt(mean_squared_error(sp.test.Y['d18O_cel_mean'], predictions['d18O_cel_mean']))\n",
        "  print(\"dO18 RMSE: \"+ str(rmse))\n",
        "  print(\"EXPECTED:\")\n",
        "  print(sp.test.Y.to_string())\n",
        "  print()\n",
        "  print(\"PREDICTED:\")\n",
        "  print(predictions.to_string())\n",
        "  return model"
      ],
      "metadata": {
        "id": "DALuUm8UOgNu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Grouped, random (jupyter crashed halfway so I reloaded a checkpoint)\n",
        "\n",
        "We can't (easily) generate isoscapes with these because the isoscapes for 'predkrig_br_lat_ISORG', 'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION' are not easily retrievable... but I'm curious how much better the model is if these columns are included."
      ],
      "metadata": {
        "id": "n8pNR1CrvF2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_random_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_train_random_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_test_random_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_validation_random_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_normalize = []\n",
        "columns_to_standardize = [\n",
        "    'lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature', 'Mean Annual Precipitation',\n",
        "    'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION',]\n",
        "\n",
        "data = load_and_scale(grouped_random_fileset, columns_to_normalize, columns_to_standardize)\n",
        "model = train_and_evaluate(data, \"random_all_boosted\", training_batch_size=3)\n",
        "model.save(get_model_save_location(\"random_all_boosted.keras\"), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location('random_all_boosted_transformer.pkl'))\n"
      ],
      "metadata": {
        "id": "rPONfgkjvJWz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "044f3a53-fbd7-4bbb-ee95-47eadb5e0bc7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         lat       long      VPD       RH       PET  DEM          PA  \\\n",
            "0  -0.121000 -67.013000  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "1  -0.041000 -66.872000  0.62083  0.82559  92.27500   93  1001.84741   \n",
            "2  -3.995545 -57.589273  0.85167  0.77358  99.99167   61  1005.67358   \n",
            "3  -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "4  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "..       ...        ...      ...      ...       ...  ...         ...   \n",
            "94 -2.496000 -59.120000  0.77500  0.78866  98.45000  139   996.36792   \n",
            "95 -2.493000 -59.121000  0.77500  0.78866  98.45000  139   996.36792   \n",
            "96 -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "98 -2.497000 -59.121000  0.77500  0.78866  98.45000  139   996.36792   \n",
            "99 -2.483000 -59.124000  0.77500  0.78866  98.45000   96  1001.48932   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  \\\n",
            "0                  26.20833                       2830   \n",
            "1                  26.37500                       2764   \n",
            "2                  27.16667                       2273   \n",
            "3                  26.20833                       2830   \n",
            "4                  26.29583                       1897   \n",
            "..                      ...                        ...   \n",
            "94                 26.79167                       2253   \n",
            "95                 26.79167                       2253   \n",
            "96                 26.71667                       2795   \n",
            "98                 26.79167                       2253   \n",
            "99                 26.79167                       2253   \n",
            "\n",
            "    Iso_Oxi_Stack_mean_TERZER  isoscape_fullmodel_d18O_prec_REGRESSION  \\\n",
            "0                    -3.96045                                 -4.69293   \n",
            "1                    -3.92301                                 -4.67525   \n",
            "2                    -3.51406                                 -3.73041   \n",
            "3                    -3.96045                                 -4.69293   \n",
            "4                    -3.27639                                 -3.48101   \n",
            "..                        ...                                      ...   \n",
            "94                   -3.70363                                 -3.84010   \n",
            "95                   -3.70363                                 -3.84010   \n",
            "96                   -4.16807                                 -4.86485   \n",
            "98                   -3.70363                                 -3.84010   \n",
            "99                   -3.69263                                 -3.84010   \n",
            "\n",
            "    krig_mean_residual  krig_variance_residual  \n",
            "0             1.589397                0.559822  \n",
            "1            -0.676855                0.474394  \n",
            "2            -0.614146                0.179081  \n",
            "3            -0.941343                0.641567  \n",
            "4            -0.229080                0.225259  \n",
            "..                 ...                     ...  \n",
            "94           -0.621182                0.463616  \n",
            "95           -0.063182               -1.136534  \n",
            "96           -0.246030                0.282990  \n",
            "98            0.182818                0.371679  \n",
            "99           -0.369182                0.424476  \n",
            "\n",
            "[98 rows x 13 columns]\n",
            "         lat       long      VPD       RH       PET  DEM          PA  \\\n",
            "0  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "1  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "2  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "3  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "4  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "5  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "6  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "7  -6.009707 -61.868657  0.77083  0.79509  93.97500   71  1004.47662   \n",
            "8  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "9  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "10 -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "11 -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  \\\n",
            "0                  26.00000                       1840   \n",
            "1                  26.00000                       1840   \n",
            "2                  26.00000                       1840   \n",
            "3                  26.00000                       1840   \n",
            "4                  26.00000                       1840   \n",
            "5                  26.00000                       1840   \n",
            "6                  26.00000                       1840   \n",
            "7                  27.20000                       1996   \n",
            "8                  26.00000                       1840   \n",
            "9                  26.00000                       1840   \n",
            "10                 26.29583                       1897   \n",
            "11                 26.29583                       1897   \n",
            "\n",
            "    Iso_Oxi_Stack_mean_TERZER  isoscape_fullmodel_d18O_prec_REGRESSION  \\\n",
            "0                    -3.30055                                 -3.42629   \n",
            "1                    -3.30055                                 -3.42629   \n",
            "2                    -3.30055                                 -3.42629   \n",
            "3                    -3.30055                                 -3.42629   \n",
            "4                    -3.30055                                 -3.42629   \n",
            "5                    -3.30055                                 -3.42629   \n",
            "6                    -3.30055                                 -3.42629   \n",
            "7                    -4.05694                                 -4.46622   \n",
            "8                    -3.30055                                 -3.42629   \n",
            "9                    -3.30055                                 -3.42629   \n",
            "10                   -3.27639                                 -3.48101   \n",
            "11                   -3.27639                                 -3.48101   \n",
            "\n",
            "    krig_mean_residual  krig_variance_residual  \n",
            "0             1.836516               -1.130660  \n",
            "1             0.408516                0.319060  \n",
            "2             0.856516               -1.771210  \n",
            "3             0.678516               -0.693590  \n",
            "4            -0.191484                0.665760  \n",
            "5             0.956016                0.807252  \n",
            "6             0.728516                0.008160  \n",
            "7             0.343073                0.488285  \n",
            "8             1.610516                0.241840  \n",
            "9             1.468516                0.636760  \n",
            "10            0.090920               -0.256241  \n",
            "11           -0.463080                0.547799  \n",
            "         lat       long      VPD       RH       PET  DEM          PA  \\\n",
            "0  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "1  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "2  -0.121000 -67.013000  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "3  -0.041000 -66.872000  0.62083  0.82559  92.27500   93  1001.84741   \n",
            "4  -3.907000 -66.055000  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "5  -4.304000 -70.291000  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "6  -4.304000 -70.291000  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "7  -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "8  -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "9  -4.303698 -70.291068  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "10 -6.009707 -61.868657  0.77083  0.79509  93.97500   71  1004.47662   \n",
            "11 -3.758534 -66.073754  0.64667  0.82247  90.44167   84  1002.92230   \n",
            "12 -6.009707 -61.868657  0.77083  0.79509  93.97500   71  1004.47662   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  \\\n",
            "0                  26.29583                       1897   \n",
            "1                  26.29583                       1897   \n",
            "2                  26.20833                       2830   \n",
            "3                  26.37500                       2764   \n",
            "4                  26.71667                       2795   \n",
            "5                  26.64583                       2708   \n",
            "6                  26.64583                       2708   \n",
            "7                  26.71667                       2795   \n",
            "8                  26.20833                       2830   \n",
            "9                  26.64583                       2708   \n",
            "10                 27.20000                       1996   \n",
            "11                 26.71667                       2856   \n",
            "12                 27.20000                       1996   \n",
            "\n",
            "    Iso_Oxi_Stack_mean_TERZER  isoscape_fullmodel_d18O_prec_REGRESSION  \\\n",
            "0                    -3.27639                                -3.481010   \n",
            "1                    -3.27639                                -3.481010   \n",
            "2                    -3.96045                                -4.692930   \n",
            "3                    -3.92301                                -4.675250   \n",
            "4                    -4.16807                                -4.864850   \n",
            "5                    -4.36128                                -5.472800   \n",
            "6                    -4.36128                                -5.472800   \n",
            "7                    -4.16807                                -4.864850   \n",
            "8                    -3.96045                                -4.692930   \n",
            "9                    -4.36128                                -5.472804   \n",
            "10                   -4.05694                                -4.466220   \n",
            "11                   -4.13790                                -4.845370   \n",
            "12                   -4.05694                                -4.466220   \n",
            "\n",
            "    krig_mean_residual  krig_variance_residual  \n",
            "0             0.502920                0.524979  \n",
            "1            -0.351080                0.313999  \n",
            "2             0.338657                0.530817  \n",
            "3            -1.307010                0.648582  \n",
            "4             0.987970                0.383700  \n",
            "5            -0.901265                0.554377  \n",
            "6            -1.133265                0.661547  \n",
            "7            -1.596030               -1.450160  \n",
            "8            -0.687343               -0.236623  \n",
            "9            -0.081265                0.254277  \n",
            "10            1.297073                0.566225  \n",
            "11            1.209347                0.203298  \n",
            "12           -0.776927               -0.239815  \n",
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('krig_mean_residual_scaler', StandardScaler(),\n",
            "                                 ['krig_mean_residual']),\n",
            "                                ('krig_variance_residual_scaler',\n",
            "                                 StandardScaler(), ['krig_variance_residual']),\n",
            "                                ('lat_standardizer', StandardScaler(), ['lat']),\n",
            "                                ('long_standardizer', StandardScaler(),\n",
            "                                 ['long']),\n",
            "                                ('VPD_standardizer', StandardScaler(), ['VPD']),\n",
            "                                ('RH_s...\n",
            "                                ('Mean Annual Temperature_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation']),\n",
            "                                ('Iso_Oxi_Stack_mean_TERZER_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Iso_Oxi_Stack_mean_TERZER']),\n",
            "                                ('isoscape_fullmodel_d18O_prec_REGRESSION_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['isoscape_fullmodel_d18O_prec_REGRESSION'])])\n",
            "==================\n",
            "random_all_boosted\n",
            "Epoch 1/10000\n",
            "33/33 [==============================] - 1s 9ms/step - loss: 0.1133 - val_loss: 0.5140\n",
            "Epoch 2/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0875 - val_loss: 0.3243\n",
            "Epoch 3/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1017 - val_loss: 0.3809\n",
            "Epoch 4/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1090 - val_loss: 0.4005\n",
            "Epoch 5/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1135 - val_loss: 0.2392\n",
            "Epoch 6/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0869 - val_loss: 0.3911\n",
            "Epoch 7/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1088 - val_loss: 0.2687\n",
            "Epoch 8/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1186 - val_loss: 0.1792\n",
            "Epoch 9/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1438 - val_loss: 0.2612\n",
            "Epoch 10/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0989 - val_loss: 0.3163\n",
            "Epoch 11/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1166 - val_loss: 0.3181\n",
            "Epoch 12/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0806 - val_loss: 0.3977\n",
            "Epoch 13/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0781 - val_loss: 0.4546\n",
            "Epoch 14/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0880 - val_loss: 0.2896\n",
            "Epoch 15/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1155 - val_loss: 0.6039\n",
            "Epoch 16/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1041 - val_loss: 0.2879\n",
            "Epoch 17/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1256 - val_loss: 0.2499\n",
            "Epoch 18/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1188 - val_loss: 0.3065\n",
            "Epoch 19/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1030 - val_loss: 0.2734\n",
            "Epoch 20/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0893 - val_loss: 0.3136\n",
            "Epoch 21/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0961 - val_loss: 0.4389\n",
            "Epoch 22/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0909 - val_loss: 0.3061\n",
            "Epoch 23/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1446 - val_loss: 0.3551\n",
            "Epoch 24/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1147 - val_loss: 0.4645\n",
            "Epoch 25/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1309 - val_loss: 0.3594\n",
            "Epoch 26/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1072 - val_loss: 0.5728\n",
            "Epoch 27/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0940 - val_loss: 0.3998\n",
            "Epoch 28/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1185 - val_loss: 0.3006\n",
            "Epoch 29/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1092 - val_loss: 0.4399\n",
            "Epoch 30/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1191 - val_loss: 0.1912\n",
            "Epoch 31/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1224 - val_loss: 0.3591\n",
            "Epoch 32/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1450 - val_loss: 0.2290\n",
            "Epoch 33/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0873 - val_loss: 0.3481\n",
            "Epoch 34/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1177 - val_loss: 0.3566\n",
            "Epoch 35/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1151 - val_loss: 0.3941\n",
            "Epoch 36/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0978 - val_loss: 0.4499\n",
            "Epoch 37/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0934 - val_loss: 0.1997\n",
            "Epoch 38/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0938 - val_loss: 0.4160\n",
            "Epoch 39/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1173 - val_loss: 0.3931\n",
            "Epoch 40/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0981 - val_loss: 0.5938\n",
            "Epoch 41/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1067 - val_loss: 0.2804\n",
            "Epoch 42/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1376 - val_loss: 0.3445\n",
            "Epoch 43/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0989 - val_loss: 0.3429\n",
            "Epoch 44/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1163 - val_loss: 0.3407\n",
            "Epoch 45/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1183 - val_loss: 0.5838\n",
            "Epoch 46/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1256 - val_loss: 0.3812\n",
            "Epoch 47/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1129 - val_loss: 0.3642\n",
            "Epoch 48/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0944 - val_loss: 0.2442\n",
            "Epoch 49/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1222 - val_loss: 0.2352\n",
            "Epoch 50/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1309 - val_loss: 0.2219\n",
            "Epoch 51/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1244 - val_loss: 0.2066\n",
            "Epoch 52/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0961 - val_loss: 0.5075\n",
            "Epoch 53/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0957 - val_loss: 0.7297\n",
            "Epoch 54/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1322 - val_loss: 0.2000\n",
            "Epoch 55/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1453 - val_loss: 0.1705\n",
            "Epoch 56/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1321 - val_loss: 0.5178\n",
            "Epoch 57/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1269 - val_loss: 0.4840\n",
            "Epoch 58/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1003 - val_loss: 0.3265\n",
            "Epoch 59/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1034 - val_loss: 0.3916\n",
            "Epoch 60/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1188 - val_loss: 0.2261\n",
            "Epoch 61/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0717 - val_loss: 0.2672\n",
            "Epoch 62/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1184 - val_loss: 0.2737\n",
            "Epoch 63/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1018 - val_loss: 0.3215\n",
            "Epoch 64/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0929 - val_loss: 0.3669\n",
            "Epoch 65/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1069 - val_loss: 0.1769\n",
            "Epoch 66/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1254 - val_loss: 0.3949\n",
            "Epoch 67/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1097 - val_loss: 0.3560\n",
            "Epoch 68/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1043 - val_loss: 0.3823\n",
            "Epoch 69/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1169 - val_loss: 0.4204\n",
            "Epoch 70/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0889 - val_loss: 0.4298\n",
            "Epoch 71/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1017 - val_loss: 0.3395\n",
            "Epoch 72/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1208 - val_loss: 0.3385\n",
            "Epoch 73/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0900 - val_loss: 0.4620\n",
            "Epoch 74/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1190 - val_loss: 0.4548\n",
            "Epoch 75/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1050 - val_loss: 0.3788\n",
            "Epoch 76/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1369 - val_loss: 0.4677\n",
            "Epoch 77/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1083 - val_loss: 0.3250\n",
            "Epoch 78/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1005 - val_loss: 0.2431\n",
            "Epoch 79/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0782 - val_loss: 0.3608\n",
            "Epoch 80/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1433 - val_loss: 0.2273\n",
            "Epoch 81/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1567 - val_loss: 0.1722\n",
            "Epoch 82/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1207 - val_loss: 0.2733\n",
            "Epoch 83/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1089 - val_loss: 0.3371\n",
            "Epoch 84/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1240 - val_loss: 0.3573\n",
            "Epoch 85/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0978 - val_loss: 0.2832\n",
            "Epoch 86/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1092 - val_loss: 0.2880\n",
            "Epoch 87/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1189 - val_loss: 0.3046\n",
            "Epoch 88/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1156 - val_loss: 0.4329\n",
            "Epoch 89/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0956 - val_loss: 0.4468\n",
            "Epoch 90/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0975 - val_loss: 0.3735\n",
            "Epoch 91/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1019 - val_loss: 0.3009\n",
            "Epoch 92/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1336 - val_loss: 0.3449\n",
            "Epoch 93/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1073 - val_loss: 0.2975\n",
            "Epoch 94/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1025 - val_loss: 0.4624\n",
            "Epoch 95/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0911 - val_loss: 0.2367\n",
            "Epoch 96/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1125 - val_loss: 0.3103\n",
            "Epoch 97/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1168 - val_loss: 0.3842\n",
            "Epoch 98/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1138 - val_loss: 0.1823\n",
            "Epoch 99/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1139 - val_loss: 0.3731\n",
            "Epoch 100/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0996 - val_loss: 0.3188\n",
            "Epoch 101/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1083 - val_loss: 0.4390\n",
            "Epoch 102/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0997 - val_loss: 0.3412\n",
            "Epoch 103/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1042 - val_loss: 0.2862\n",
            "Epoch 104/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1253 - val_loss: 0.2554\n",
            "Epoch 105/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0823 - val_loss: 0.2340\n",
            "Epoch 106/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1126 - val_loss: 0.2890\n",
            "Epoch 107/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1194 - val_loss: 0.2716\n",
            "Epoch 108/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1168 - val_loss: 0.2090\n",
            "Epoch 109/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0894 - val_loss: 0.4679\n",
            "Epoch 110/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0862 - val_loss: 0.2628\n",
            "Epoch 111/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1005 - val_loss: 0.3783\n",
            "Epoch 112/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0896 - val_loss: 0.3105\n",
            "Epoch 113/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1343 - val_loss: 0.4108\n",
            "Epoch 114/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1081 - val_loss: 0.2161\n",
            "Epoch 115/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1025 - val_loss: 0.4596\n",
            "Epoch 116/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1180 - val_loss: 0.3834\n",
            "Epoch 117/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0968 - val_loss: 0.2542\n",
            "Epoch 118/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0998 - val_loss: 0.2976\n",
            "Epoch 119/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0960 - val_loss: 0.3407\n",
            "Epoch 120/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1250 - val_loss: 0.3086\n",
            "Epoch 121/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1417 - val_loss: 0.2085\n",
            "Epoch 122/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1271 - val_loss: 0.4208\n",
            "Epoch 123/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1093 - val_loss: 0.2241\n",
            "Epoch 124/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1112 - val_loss: 0.4777\n",
            "Epoch 125/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1056 - val_loss: 0.2926\n",
            "Epoch 126/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1030 - val_loss: 0.3211\n",
            "Epoch 127/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1108 - val_loss: 0.1394\n",
            "Epoch 128/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1266 - val_loss: 0.5469\n",
            "Epoch 129/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1542 - val_loss: 0.2950\n",
            "Epoch 130/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1028 - val_loss: 0.1876\n",
            "Epoch 131/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1167 - val_loss: 0.5605\n",
            "Epoch 132/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1174 - val_loss: 0.5507\n",
            "Epoch 133/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1058 - val_loss: 0.3411\n",
            "Epoch 134/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0992 - val_loss: 0.4072\n",
            "Epoch 135/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1217 - val_loss: 0.2952\n",
            "Epoch 136/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0854 - val_loss: 0.2758\n",
            "Epoch 137/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1105 - val_loss: 0.3061\n",
            "Epoch 138/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1072 - val_loss: 0.3632\n",
            "Epoch 139/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1098 - val_loss: 0.2902\n",
            "Epoch 140/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1211 - val_loss: 0.3749\n",
            "Epoch 141/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0896 - val_loss: 0.4025\n",
            "Epoch 142/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1040 - val_loss: 0.5909\n",
            "Epoch 143/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1054 - val_loss: 0.2951\n",
            "Epoch 144/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1200 - val_loss: 0.4080\n",
            "Epoch 145/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1056 - val_loss: 0.2537\n",
            "Epoch 146/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0958 - val_loss: 0.3321\n",
            "Epoch 147/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1014 - val_loss: 0.3887\n",
            "Epoch 148/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0902 - val_loss: 0.4555\n",
            "Epoch 149/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0986 - val_loss: 0.2689\n",
            "Epoch 150/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0968 - val_loss: 0.2048\n",
            "Epoch 151/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1028 - val_loss: 0.3060\n",
            "Epoch 152/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0904 - val_loss: 0.4917\n",
            "Epoch 153/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1018 - val_loss: 0.4693\n",
            "Epoch 154/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0984 - val_loss: 0.3753\n",
            "Epoch 155/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1295 - val_loss: 0.3087\n",
            "Epoch 156/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1327 - val_loss: 0.3056\n",
            "Epoch 157/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1051 - val_loss: 0.4248\n",
            "Epoch 158/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0972 - val_loss: 0.5352\n",
            "Epoch 159/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0973 - val_loss: 0.1968\n",
            "Epoch 160/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1058 - val_loss: 0.3930\n",
            "Epoch 161/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1453 - val_loss: 0.2121\n",
            "Epoch 162/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1135 - val_loss: 0.2384\n",
            "Epoch 163/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1149 - val_loss: 0.3567\n",
            "Epoch 164/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1023 - val_loss: 0.2653\n",
            "Epoch 165/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1018 - val_loss: 0.2663\n",
            "Epoch 166/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1195 - val_loss: 0.3788\n",
            "Epoch 167/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1063 - val_loss: 0.2661\n",
            "Epoch 168/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1057 - val_loss: 0.7999\n",
            "Epoch 169/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0941 - val_loss: 0.2976\n",
            "Epoch 170/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0942 - val_loss: 0.2879\n",
            "Epoch 171/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0912 - val_loss: 0.1795\n",
            "Epoch 172/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1069 - val_loss: 0.2287\n",
            "Epoch 173/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0938 - val_loss: 0.2584\n",
            "Epoch 174/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1190 - val_loss: 0.2360\n",
            "Epoch 175/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0968 - val_loss: 0.2673\n",
            "Epoch 176/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0884 - val_loss: 0.3125\n",
            "Epoch 177/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0948 - val_loss: 0.1963\n",
            "Epoch 178/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1289 - val_loss: 0.2563\n",
            "Epoch 179/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0976 - val_loss: 0.2809\n",
            "Epoch 180/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1240 - val_loss: 0.1997\n",
            "Epoch 181/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0992 - val_loss: 0.3223\n",
            "Epoch 182/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1324 - val_loss: 0.2475\n",
            "Epoch 183/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1091 - val_loss: 0.2773\n",
            "Epoch 184/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1292 - val_loss: 0.3835\n",
            "Epoch 185/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1227 - val_loss: 0.2488\n",
            "Epoch 186/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0959 - val_loss: 0.6027\n",
            "Epoch 187/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1073 - val_loss: 0.4071\n",
            "Epoch 188/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1072 - val_loss: 0.1724\n",
            "Epoch 189/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0725 - val_loss: 0.3636\n",
            "Epoch 190/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1091 - val_loss: 0.2942\n",
            "Epoch 191/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1007 - val_loss: 0.7538\n",
            "Epoch 192/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1100 - val_loss: 0.3588\n",
            "Epoch 193/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0824 - val_loss: 0.4762\n",
            "Epoch 194/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0802 - val_loss: 0.3011\n",
            "Epoch 195/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0975 - val_loss: 0.2343\n",
            "Epoch 196/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1125 - val_loss: 0.2161\n",
            "Epoch 197/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1341 - val_loss: 0.3542\n",
            "Epoch 198/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0965 - val_loss: 0.5282\n",
            "Epoch 199/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0902 - val_loss: 0.4209\n",
            "Epoch 200/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1081 - val_loss: 0.3786\n",
            "Epoch 201/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1352 - val_loss: 0.2209\n",
            "Epoch 202/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1103 - val_loss: 0.5296\n",
            "Epoch 203/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1055 - val_loss: 0.2334\n",
            "Epoch 204/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1239 - val_loss: 0.5717\n",
            "Epoch 205/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1135 - val_loss: 0.3581\n",
            "Epoch 206/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1115 - val_loss: 0.2887\n",
            "Epoch 207/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1030 - val_loss: 0.1880\n",
            "Epoch 208/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1114 - val_loss: 0.2948\n",
            "Epoch 209/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0997 - val_loss: 0.2292\n",
            "Epoch 210/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0693 - val_loss: 0.3165\n",
            "Epoch 211/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0966 - val_loss: 0.3050\n",
            "Epoch 212/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1147 - val_loss: 0.3229\n",
            "Epoch 213/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1031 - val_loss: 0.3486\n",
            "Epoch 214/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1024 - val_loss: 0.4521\n",
            "Epoch 215/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0968 - val_loss: 0.2414\n",
            "Epoch 216/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1179 - val_loss: 0.3127\n",
            "Epoch 217/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1212 - val_loss: 0.4524\n",
            "Epoch 218/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1136 - val_loss: 0.2930\n",
            "Epoch 219/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1069 - val_loss: 0.3639\n",
            "Epoch 220/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0916 - val_loss: 0.2722\n",
            "Epoch 221/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1099 - val_loss: 0.2201\n",
            "Epoch 222/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1116 - val_loss: 0.2863\n",
            "Epoch 223/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1182 - val_loss: 0.4728\n",
            "Epoch 224/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0987 - val_loss: 0.2743\n",
            "Epoch 225/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1127 - val_loss: 0.4091\n",
            "Epoch 226/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0994 - val_loss: 0.4774\n",
            "Epoch 227/10000\n",
            " 1/33 [..............................] - ETA: 0s - loss: 0.0965Restoring model weights from the end of the best epoch: 127.\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1074 - val_loss: 0.3223\n",
            "Epoch 227: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_3309755/1138043673.py:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.13941197097301483\n",
            "Train loss: 0.11078975349664688\n",
            "Test loss: 0.09514506906270981\n",
            "dO18 RMSE: 0.1563712002538557\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0          26.130            0.19025\n",
            "1          26.984            0.40123\n",
            "2          24.656            0.17728\n",
            "3          26.356            0.03953\n",
            "4          23.962            0.30992\n",
            "5          24.848            0.15842\n",
            "6          25.080            0.05125\n",
            "7          26.546            2.14378\n",
            "8          25.682            0.94472\n",
            "9          24.028            0.45852\n",
            "10         23.944            0.15813\n",
            "11         23.752            0.52437\n",
            "12         26.018            0.96417\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       26.138597           0.253183\n",
            "1       26.962130           0.553033\n",
            "2       24.632973           0.233319\n",
            "3       26.469667           0.130157\n",
            "4       23.848272           0.488035\n",
            "5       24.892532           0.152681\n",
            "6       25.093409           0.060130\n",
            "7       26.585911           2.962878\n",
            "8       25.742226           1.177573\n",
            "9       24.055536           0.595948\n",
            "10      24.076668           0.192768\n",
            "11      23.324989           0.694686\n",
            "12      25.730148           1.108884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-03 00:04:58.215585: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [13,13]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/usr/local/google/home/ruru/Downloads/model_builds/random_all_boosted_transformer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Grouped, random, ablating other models except kriging\n",
        "\n",
        "We can generate isoscapes for this model easily."
      ],
      "metadata": {
        "id": "opmE3xc0vcY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_random_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_train_random_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_test_random_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_validation_random_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_normalize = []\n",
        "columns_to_standardize = [\n",
        "    'lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature', 'Mean Annual Precipitation']\n",
        "\n",
        "data = load_and_scale(grouped_random_fileset, columns_to_normalize, columns_to_standardize)\n",
        "model = train_and_evaluate(data, \"random_ablated_boosted\", training_batch_size=3)\n",
        "model.save(get_model_save_location(\"random_ablated_boosted.keras\"), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location('random_ablated_boosted_transformer.pkl'))"
      ],
      "metadata": {
        "id": "KCebsA7XvfRH",
        "outputId": "f6d2086a-56e8-4c6a-8126-01d31168e356",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         lat       long      VPD       RH       PET  DEM          PA  \\\n",
            "0  -0.121000 -67.013000  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "1  -0.041000 -66.872000  0.62083  0.82559  92.27500   93  1001.84741   \n",
            "2  -3.995545 -57.589273  0.85167  0.77358  99.99167   61  1005.67358   \n",
            "3  -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "4  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "..       ...        ...      ...      ...       ...  ...         ...   \n",
            "94 -2.496000 -59.120000  0.77500  0.78866  98.45000  139   996.36792   \n",
            "95 -2.493000 -59.121000  0.77500  0.78866  98.45000  139   996.36792   \n",
            "96 -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "98 -2.497000 -59.121000  0.77500  0.78866  98.45000  139   996.36792   \n",
            "99 -2.483000 -59.124000  0.77500  0.78866  98.45000   96  1001.48932   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  krig_mean_residual  \\\n",
            "0                  26.20833                       2830            1.589397   \n",
            "1                  26.37500                       2764           -0.676855   \n",
            "2                  27.16667                       2273           -0.614146   \n",
            "3                  26.20833                       2830           -0.941343   \n",
            "4                  26.29583                       1897           -0.229080   \n",
            "..                      ...                        ...                 ...   \n",
            "94                 26.79167                       2253           -0.621182   \n",
            "95                 26.79167                       2253           -0.063182   \n",
            "96                 26.71667                       2795           -0.246030   \n",
            "98                 26.79167                       2253            0.182818   \n",
            "99                 26.79167                       2253           -0.369182   \n",
            "\n",
            "    krig_variance_residual  \n",
            "0                 0.559822  \n",
            "1                 0.474394  \n",
            "2                 0.179081  \n",
            "3                 0.641567  \n",
            "4                 0.225259  \n",
            "..                     ...  \n",
            "94                0.463616  \n",
            "95               -1.136534  \n",
            "96                0.282990  \n",
            "98                0.371679  \n",
            "99                0.424476  \n",
            "\n",
            "[98 rows x 11 columns]\n",
            "         lat       long      VPD       RH       PET  DEM          PA  \\\n",
            "0  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "1  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "2  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "3  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "4  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "5  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "6  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "7  -6.009707 -61.868657  0.77083  0.79509  93.97500   71  1004.47662   \n",
            "8  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "9  -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "10 -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "11 -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  krig_mean_residual  \\\n",
            "0                  26.00000                       1840            1.836516   \n",
            "1                  26.00000                       1840            0.408516   \n",
            "2                  26.00000                       1840            0.856516   \n",
            "3                  26.00000                       1840            0.678516   \n",
            "4                  26.00000                       1840           -0.191484   \n",
            "5                  26.00000                       1840            0.956016   \n",
            "6                  26.00000                       1840            0.728516   \n",
            "7                  27.20000                       1996            0.343073   \n",
            "8                  26.00000                       1840            1.610516   \n",
            "9                  26.00000                       1840            1.468516   \n",
            "10                 26.29583                       1897            0.090920   \n",
            "11                 26.29583                       1897           -0.463080   \n",
            "\n",
            "    krig_variance_residual  \n",
            "0                -1.130660  \n",
            "1                 0.319060  \n",
            "2                -1.771210  \n",
            "3                -0.693590  \n",
            "4                 0.665760  \n",
            "5                 0.807252  \n",
            "6                 0.008160  \n",
            "7                 0.488285  \n",
            "8                 0.241840  \n",
            "9                 0.636760  \n",
            "10               -0.256241  \n",
            "11                0.547799  \n",
            "         lat       long      VPD       RH       PET  DEM          PA  \\\n",
            "0  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "1  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "2  -0.121000 -67.013000  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "3  -0.041000 -66.872000  0.62083  0.82559  92.27500   93  1001.84741   \n",
            "4  -3.907000 -66.055000  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "5  -4.304000 -70.291000  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "6  -4.304000 -70.291000  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "7  -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "8  -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "9  -4.303698 -70.291068  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "10 -6.009707 -61.868657  0.77083  0.79509  93.97500   71  1004.47662   \n",
            "11 -3.758534 -66.073754  0.64667  0.82247  90.44167   84  1002.92230   \n",
            "12 -6.009707 -61.868657  0.77083  0.79509  93.97500   71  1004.47662   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  krig_mean_residual  \\\n",
            "0                  26.29583                       1897            0.502920   \n",
            "1                  26.29583                       1897           -0.351080   \n",
            "2                  26.20833                       2830            0.338657   \n",
            "3                  26.37500                       2764           -1.307010   \n",
            "4                  26.71667                       2795            0.987970   \n",
            "5                  26.64583                       2708           -0.901265   \n",
            "6                  26.64583                       2708           -1.133265   \n",
            "7                  26.71667                       2795           -1.596030   \n",
            "8                  26.20833                       2830           -0.687343   \n",
            "9                  26.64583                       2708           -0.081265   \n",
            "10                 27.20000                       1996            1.297073   \n",
            "11                 26.71667                       2856            1.209347   \n",
            "12                 27.20000                       1996           -0.776927   \n",
            "\n",
            "    krig_variance_residual  \n",
            "0                 0.524979  \n",
            "1                 0.313999  \n",
            "2                 0.530817  \n",
            "3                 0.648582  \n",
            "4                 0.383700  \n",
            "5                 0.554377  \n",
            "6                 0.661547  \n",
            "7                -1.450160  \n",
            "8                -0.236623  \n",
            "9                 0.254277  \n",
            "10                0.566225  \n",
            "11                0.203298  \n",
            "12               -0.239815  \n",
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('krig_mean_residual_scaler', StandardScaler(),\n",
            "                                 ['krig_mean_residual']),\n",
            "                                ('krig_variance_residual_scaler',\n",
            "                                 StandardScaler(), ['krig_variance_residual']),\n",
            "                                ('lat_standardizer', StandardScaler(), ['lat']),\n",
            "                                ('long_standardizer', StandardScaler(),\n",
            "                                 ['long']),\n",
            "                                ('VPD_standardizer', StandardScaler(), ['VPD']),\n",
            "                                ('RH_standardizer', StandardScaler(), ['RH']),\n",
            "                                ('PET_standardizer', StandardScaler(), ['PET']),\n",
            "                                ('DEM_standardizer', StandardScaler(), ['DEM']),\n",
            "                                ('PA_standardizer', StandardScaler(), ['PA']),\n",
            "                                ('Mean Annual Temperature_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation'])])\n",
            "==================\n",
            "random_ablated_boosted\n",
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_9 (InputLayer)           [(None, 11)]         0           []                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_26 (S  (None, 9)           0           ['input_9[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 20)           200         ['tf.__operators__.getitem_26[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_24 (S  (None,)             0           ['input_9[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 20)           420         ['dense_16[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_25 (S  (None,)             0           ['input_9[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " tf.expand_dims_16 (TFOpLambda)  (None, 1)           0           ['tf.__operators__.getitem_24[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " mean_output (Dense)            (None, 1)            21          ['dense_17[0][0]']               \n",
            "                                                                                                  \n",
            " var_output (Dense)             (None, 1)            21          ['dense_17[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_17 (TFOpLambda)  (None, 1)           0           ['tf.__operators__.getitem_25[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " tf.math.multiply_29 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_16[0][0]']      \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.multiply_28 (TFOpLambd  (None, 1)           0           ['mean_output[0][0]']            \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.multiply_30 (TFOpLambd  (None, 1)           0           ['var_output[0][0]']             \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.multiply_31 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_17[0][0]']      \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_37 (TFOpL  (None, 1)           0           ['tf.math.multiply_29[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_36 (TFOpL  (None, 1)           0           ['tf.math.multiply_28[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_39 (TFOpL  (None, 1)           0           ['tf.math.multiply_30[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_40 (TFOpL  (None, 1)           0           ['tf.math.multiply_31[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_38 (TFOpL  (None, 1)           0           ['tf.__operators__.add_37[0][0]',\n",
            " ambda)                                                           'tf.__operators__.add_36[0][0]']\n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1)            0           ['tf.__operators__.add_39[0][0]',\n",
            "                                                                  'tf.__operators__.add_40[0][0]']\n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate)    (None, 2)            0           ['tf.__operators__.add_38[0][0]',\n",
            "                                                                  'lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 662\n",
            "Trainable params: 662\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10000\n",
            "33/33 [==============================] - 2s 8ms/step - loss: 876.1910 - val_loss: 1783.7260\n",
            "Epoch 2/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 775.2792 - val_loss: 1593.3956\n",
            "Epoch 3/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 787.5711 - val_loss: 1348.4293\n",
            "Epoch 4/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 753.3708 - val_loss: 1061.6893\n",
            "Epoch 5/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 726.1607 - val_loss: 1086.4319\n",
            "Epoch 6/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 596.2203 - val_loss: 1058.5521\n",
            "Epoch 7/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 584.0662 - val_loss: 933.7781\n",
            "Epoch 8/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 607.5704 - val_loss: 1089.6010\n",
            "Epoch 9/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 595.7203 - val_loss: 1067.7512\n",
            "Epoch 10/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 556.4136 - val_loss: 949.9811\n",
            "Epoch 11/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 515.3386 - val_loss: 935.7282\n",
            "Epoch 12/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 483.9031 - val_loss: 1223.9066\n",
            "Epoch 13/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 476.3257 - val_loss: 1060.3206\n",
            "Epoch 14/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 456.2846 - val_loss: 932.3486\n",
            "Epoch 15/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 412.5867 - val_loss: 972.9795\n",
            "Epoch 16/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 405.8217 - val_loss: 856.1697\n",
            "Epoch 17/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 450.1138 - val_loss: 689.4458\n",
            "Epoch 18/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 416.6409 - val_loss: 763.5087\n",
            "Epoch 19/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 363.0158 - val_loss: 802.8605\n",
            "Epoch 20/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 343.2135 - val_loss: 786.7651\n",
            "Epoch 21/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 389.3468 - val_loss: 704.3079\n",
            "Epoch 22/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 307.7185 - val_loss: 719.0624\n",
            "Epoch 23/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 413.2747 - val_loss: 600.3361\n",
            "Epoch 24/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 332.2120 - val_loss: 612.4280\n",
            "Epoch 25/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 316.5274 - val_loss: 636.0450\n",
            "Epoch 26/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 311.5707 - val_loss: 651.7062\n",
            "Epoch 27/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 285.4819 - val_loss: 593.9037\n",
            "Epoch 28/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 270.8867 - val_loss: 549.7046\n",
            "Epoch 29/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 280.7670 - val_loss: 541.7933\n",
            "Epoch 30/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 275.4360 - val_loss: 501.2739\n",
            "Epoch 31/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 264.2063 - val_loss: 506.9755\n",
            "Epoch 32/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 256.2530 - val_loss: 425.1771\n",
            "Epoch 33/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 232.3929 - val_loss: 543.6181\n",
            "Epoch 34/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 212.8737 - val_loss: 531.6376\n",
            "Epoch 35/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 213.3448 - val_loss: 385.0136\n",
            "Epoch 36/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 191.1282 - val_loss: 396.4934\n",
            "Epoch 37/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 203.8815 - val_loss: 285.9956\n",
            "Epoch 38/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 188.6477 - val_loss: 304.9342\n",
            "Epoch 39/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 179.7606 - val_loss: 355.9146\n",
            "Epoch 40/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 168.0391 - val_loss: 298.8242\n",
            "Epoch 41/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 160.7645 - val_loss: 308.8693\n",
            "Epoch 42/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 171.0696 - val_loss: 289.5768\n",
            "Epoch 43/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 148.4712 - val_loss: 245.8187\n",
            "Epoch 44/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 144.4514 - val_loss: 215.0555\n",
            "Epoch 45/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 139.8707 - val_loss: 225.8043\n",
            "Epoch 46/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 150.6414 - val_loss: 201.0068\n",
            "Epoch 47/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 140.4242 - val_loss: 182.6332\n",
            "Epoch 48/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 133.0846 - val_loss: 192.4707\n",
            "Epoch 49/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 132.1238 - val_loss: 173.4875\n",
            "Epoch 50/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 120.5350 - val_loss: 164.8075\n",
            "Epoch 51/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 107.3069 - val_loss: 151.8790\n",
            "Epoch 52/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 110.1923 - val_loss: 134.1891\n",
            "Epoch 53/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 105.8361 - val_loss: 157.6797\n",
            "Epoch 54/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 103.5210 - val_loss: 158.4090\n",
            "Epoch 55/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 101.4955 - val_loss: 122.1870\n",
            "Epoch 56/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 97.8777 - val_loss: 188.9384\n",
            "Epoch 57/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 90.1341 - val_loss: 165.4756\n",
            "Epoch 58/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 82.7351 - val_loss: 133.3849\n",
            "Epoch 59/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 92.8885 - val_loss: 111.1246\n",
            "Epoch 60/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 88.8859 - val_loss: 96.9135\n",
            "Epoch 61/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 83.1667 - val_loss: 112.0292\n",
            "Epoch 62/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 83.2448 - val_loss: 132.4448\n",
            "Epoch 63/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 80.1321 - val_loss: 92.7777\n",
            "Epoch 64/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 78.1017 - val_loss: 110.8270\n",
            "Epoch 65/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 70.7757 - val_loss: 96.7620\n",
            "Epoch 66/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 73.1767 - val_loss: 104.0614\n",
            "Epoch 67/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 68.4675 - val_loss: 117.8992\n",
            "Epoch 68/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 65.6977 - val_loss: 109.7056\n",
            "Epoch 69/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 75.5956 - val_loss: 97.7166\n",
            "Epoch 70/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 60.3995 - val_loss: 86.2307\n",
            "Epoch 71/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 59.7380 - val_loss: 73.5914\n",
            "Epoch 72/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 58.3215 - val_loss: 74.5489\n",
            "Epoch 73/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 57.2623 - val_loss: 77.5327\n",
            "Epoch 74/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 56.1149 - val_loss: 77.2558\n",
            "Epoch 75/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 55.6344 - val_loss: 83.4410\n",
            "Epoch 76/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 52.1014 - val_loss: 83.9856\n",
            "Epoch 77/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 54.4207 - val_loss: 82.3433\n",
            "Epoch 78/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 48.4785 - val_loss: 66.3043\n",
            "Epoch 79/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 45.0534 - val_loss: 79.1159\n",
            "Epoch 80/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 51.2217 - val_loss: 51.3341\n",
            "Epoch 81/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 49.5729 - val_loss: 57.0136\n",
            "Epoch 82/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 49.3246 - val_loss: 58.8614\n",
            "Epoch 83/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 43.8589 - val_loss: 55.2468\n",
            "Epoch 84/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 44.0491 - val_loss: 58.3900\n",
            "Epoch 85/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 40.7886 - val_loss: 51.1649\n",
            "Epoch 86/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 40.8695 - val_loss: 60.7347\n",
            "Epoch 87/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 41.3776 - val_loss: 57.7384\n",
            "Epoch 88/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 37.9168 - val_loss: 54.0058\n",
            "Epoch 89/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 39.6615 - val_loss: 57.5684\n",
            "Epoch 90/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 35.9718 - val_loss: 62.6595\n",
            "Epoch 91/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 37.5424 - val_loss: 40.2930\n",
            "Epoch 92/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 34.8305 - val_loss: 41.8036\n",
            "Epoch 93/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 34.3102 - val_loss: 44.9214\n",
            "Epoch 94/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 32.7364 - val_loss: 49.0992\n",
            "Epoch 95/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 29.8240 - val_loss: 39.1772\n",
            "Epoch 96/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 33.4136 - val_loss: 38.3410\n",
            "Epoch 97/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 29.0881 - val_loss: 46.9504\n",
            "Epoch 98/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 29.8576 - val_loss: 34.9856\n",
            "Epoch 99/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 29.5968 - val_loss: 39.7848\n",
            "Epoch 100/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 26.9824 - val_loss: 39.1000\n",
            "Epoch 101/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 29.3326 - val_loss: 37.5627\n",
            "Epoch 102/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 25.6540 - val_loss: 31.9810\n",
            "Epoch 103/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 25.4548 - val_loss: 29.7693\n",
            "Epoch 104/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 26.9053 - val_loss: 29.9741\n",
            "Epoch 105/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 22.9987 - val_loss: 32.4490\n",
            "Epoch 106/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 24.9253 - val_loss: 31.8674\n",
            "Epoch 107/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 25.5478 - val_loss: 28.0037\n",
            "Epoch 108/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 21.5693 - val_loss: 21.1617\n",
            "Epoch 109/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 21.1032 - val_loss: 34.4066\n",
            "Epoch 110/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 19.9267 - val_loss: 26.7894\n",
            "Epoch 111/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 21.0476 - val_loss: 22.9707\n",
            "Epoch 112/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 19.1860 - val_loss: 27.7172\n",
            "Epoch 113/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 18.7812 - val_loss: 25.9493\n",
            "Epoch 114/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 18.3415 - val_loss: 22.7892\n",
            "Epoch 115/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 19.0388 - val_loss: 28.3464\n",
            "Epoch 116/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 19.2188 - val_loss: 20.9453\n",
            "Epoch 117/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 16.3792 - val_loss: 24.8654\n",
            "Epoch 118/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 18.2770 - val_loss: 23.1272\n",
            "Epoch 119/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 16.6417 - val_loss: 19.0072\n",
            "Epoch 120/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 16.3657 - val_loss: 22.4943\n",
            "Epoch 121/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 16.2841 - val_loss: 19.6810\n",
            "Epoch 122/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 14.7876 - val_loss: 21.3011\n",
            "Epoch 123/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 14.4635 - val_loss: 16.8764\n",
            "Epoch 124/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 14.6580 - val_loss: 17.8938\n",
            "Epoch 125/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 14.0325 - val_loss: 14.3762\n",
            "Epoch 126/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 13.7473 - val_loss: 16.3655\n",
            "Epoch 127/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 15.1801 - val_loss: 18.1632\n",
            "Epoch 128/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 13.1311 - val_loss: 16.7696\n",
            "Epoch 129/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 12.6511 - val_loss: 18.9306\n",
            "Epoch 130/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 10.9284 - val_loss: 15.3725\n",
            "Epoch 131/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 11.4836 - val_loss: 16.3232\n",
            "Epoch 132/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 11.5592 - val_loss: 16.9427\n",
            "Epoch 133/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 10.3758 - val_loss: 13.5544\n",
            "Epoch 134/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 10.8247 - val_loss: 14.6139\n",
            "Epoch 135/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 9.6334 - val_loss: 13.6826\n",
            "Epoch 136/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 9.8814 - val_loss: 14.1772\n",
            "Epoch 137/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 10.0753 - val_loss: 12.9531\n",
            "Epoch 138/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 8.5901 - val_loss: 13.9313\n",
            "Epoch 139/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 8.6110 - val_loss: 11.7363\n",
            "Epoch 140/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 9.4440 - val_loss: 12.9664\n",
            "Epoch 141/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 8.1790 - val_loss: 14.9902\n",
            "Epoch 142/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 8.7013 - val_loss: 12.0721\n",
            "Epoch 143/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 7.8841 - val_loss: 10.5786\n",
            "Epoch 144/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 8.6295 - val_loss: 10.5156\n",
            "Epoch 145/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 7.1385 - val_loss: 10.6807\n",
            "Epoch 146/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 8.1334 - val_loss: 8.5857\n",
            "Epoch 147/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 7.5163 - val_loss: 8.2680\n",
            "Epoch 148/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 7.1617 - val_loss: 9.7576\n",
            "Epoch 149/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 7.3449 - val_loss: 8.0870\n",
            "Epoch 150/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 6.7574 - val_loss: 9.2281\n",
            "Epoch 151/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 6.6272 - val_loss: 9.4428\n",
            "Epoch 152/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 6.0087 - val_loss: 6.0399\n",
            "Epoch 153/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 6.6186 - val_loss: 7.8617\n",
            "Epoch 154/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.8593 - val_loss: 6.3153\n",
            "Epoch 155/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 6.4045 - val_loss: 7.7379\n",
            "Epoch 156/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.8570 - val_loss: 7.1393\n",
            "Epoch 157/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.7833 - val_loss: 6.4033\n",
            "Epoch 158/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.6442 - val_loss: 6.5941\n",
            "Epoch 159/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 5.0378 - val_loss: 5.9048\n",
            "Epoch 160/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.1533 - val_loss: 5.8253\n",
            "Epoch 161/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.1222 - val_loss: 6.1942\n",
            "Epoch 162/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 5.0604 - val_loss: 6.1353\n",
            "Epoch 163/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 4.7530 - val_loss: 5.6632\n",
            "Epoch 164/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.6597 - val_loss: 7.0393\n",
            "Epoch 165/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.3810 - val_loss: 6.6424\n",
            "Epoch 166/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.6044 - val_loss: 6.3327\n",
            "Epoch 167/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.6999 - val_loss: 5.2376\n",
            "Epoch 168/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.4689 - val_loss: 6.6209\n",
            "Epoch 169/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 4.0633 - val_loss: 5.2390\n",
            "Epoch 170/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.9906 - val_loss: 4.8085\n",
            "Epoch 171/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.7097 - val_loss: 4.7129\n",
            "Epoch 172/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.7007 - val_loss: 4.2699\n",
            "Epoch 173/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.9652 - val_loss: 4.4624\n",
            "Epoch 174/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 3.9840 - val_loss: 4.2011\n",
            "Epoch 175/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 3.5593 - val_loss: 3.9078\n",
            "Epoch 176/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.7494 - val_loss: 4.3573\n",
            "Epoch 177/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 3.7923 - val_loss: 3.5378\n",
            "Epoch 178/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.8763 - val_loss: 3.6169\n",
            "Epoch 179/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.3488 - val_loss: 3.4255\n",
            "Epoch 180/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.4319 - val_loss: 3.6398\n",
            "Epoch 181/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 3.5174 - val_loss: 3.2126\n",
            "Epoch 182/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.3913 - val_loss: 3.4956\n",
            "Epoch 183/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 3.4692 - val_loss: 3.0760\n",
            "Epoch 184/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 3.1034 - val_loss: 2.9755\n",
            "Epoch 185/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.1853 - val_loss: 3.2076\n",
            "Epoch 186/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.0506 - val_loss: 3.7548\n",
            "Epoch 187/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.3384 - val_loss: 3.4976\n",
            "Epoch 188/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 3.1338 - val_loss: 3.5578\n",
            "Epoch 189/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 2.9599 - val_loss: 2.9693\n",
            "Epoch 190/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.8286 - val_loss: 2.7160\n",
            "Epoch 191/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.8106 - val_loss: 3.5130\n",
            "Epoch 192/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.9687 - val_loss: 2.7199\n",
            "Epoch 193/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 2.6042 - val_loss: 2.3867\n",
            "Epoch 194/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 2.7610 - val_loss: 2.2140\n",
            "Epoch 195/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.6487 - val_loss: 2.9734\n",
            "Epoch 196/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.7941 - val_loss: 2.4549\n",
            "Epoch 197/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.8288 - val_loss: 2.4833\n",
            "Epoch 198/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.8251 - val_loss: 2.6414\n",
            "Epoch 199/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5292 - val_loss: 2.7745\n",
            "Epoch 200/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5197 - val_loss: 2.2781\n",
            "Epoch 201/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.9671 - val_loss: 2.5267\n",
            "Epoch 202/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5929 - val_loss: 2.2610\n",
            "Epoch 203/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 2.7136 - val_loss: 2.1995\n",
            "Epoch 204/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 2.5782 - val_loss: 2.0259\n",
            "Epoch 205/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5712 - val_loss: 2.2196\n",
            "Epoch 206/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5286 - val_loss: 2.2787\n",
            "Epoch 207/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3362 - val_loss: 2.1293\n",
            "Epoch 208/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5316 - val_loss: 2.1642\n",
            "Epoch 209/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 2.4242 - val_loss: 1.9542\n",
            "Epoch 210/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3367 - val_loss: 2.2663\n",
            "Epoch 211/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.5543 - val_loss: 2.0151\n",
            "Epoch 212/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 2.3735 - val_loss: 1.8838\n",
            "Epoch 213/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3313 - val_loss: 1.9051\n",
            "Epoch 214/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2661 - val_loss: 2.3358\n",
            "Epoch 215/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 2.5914 - val_loss: 1.8127\n",
            "Epoch 216/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4050 - val_loss: 2.1368\n",
            "Epoch 217/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3421 - val_loss: 1.9244\n",
            "Epoch 218/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2510 - val_loss: 1.7243\n",
            "Epoch 219/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3283 - val_loss: 1.9216\n",
            "Epoch 220/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2943 - val_loss: 1.7554\n",
            "Epoch 221/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 2.2526 - val_loss: 1.6412\n",
            "Epoch 222/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2671 - val_loss: 2.0941\n",
            "Epoch 223/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2588 - val_loss: 1.8925\n",
            "Epoch 224/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2442 - val_loss: 2.0425\n",
            "Epoch 225/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2647 - val_loss: 1.8559\n",
            "Epoch 226/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 2.3539 - val_loss: 1.6237\n",
            "Epoch 227/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.3135 - val_loss: 1.9008\n",
            "Epoch 228/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.4575 - val_loss: 1.9282\n",
            "Epoch 229/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2312 - val_loss: 1.8098\n",
            "Epoch 230/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2625 - val_loss: 1.8725\n",
            "Epoch 231/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.2112 - val_loss: 1.6527\n",
            "Epoch 232/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0632 - val_loss: 1.7651\n",
            "Epoch 233/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1474 - val_loss: 1.6694\n",
            "Epoch 234/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1239 - val_loss: 1.7439\n",
            "Epoch 235/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1657 - val_loss: 1.8435\n",
            "Epoch 236/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 2.0476 - val_loss: 1.5871\n",
            "Epoch 237/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0339 - val_loss: 1.8642\n",
            "Epoch 238/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0768 - val_loss: 1.6508\n",
            "Epoch 239/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1116 - val_loss: 1.6200\n",
            "Epoch 240/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1477 - val_loss: 1.6204\n",
            "Epoch 241/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1299 - val_loss: 1.7209\n",
            "Epoch 242/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 2.0949 - val_loss: 1.4353\n",
            "Epoch 243/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1298 - val_loss: 1.7607\n",
            "Epoch 244/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1897 - val_loss: 1.6430\n",
            "Epoch 245/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0991 - val_loss: 1.8305\n",
            "Epoch 246/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0043 - val_loss: 1.5586\n",
            "Epoch 247/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9725 - val_loss: 1.6773\n",
            "Epoch 248/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9819 - val_loss: 1.7594\n",
            "Epoch 249/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0181 - val_loss: 1.6646\n",
            "Epoch 250/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.1202 - val_loss: 1.5604\n",
            "Epoch 251/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9531 - val_loss: 1.5360\n",
            "Epoch 252/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9099 - val_loss: 1.6858\n",
            "Epoch 253/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9716 - val_loss: 1.5319\n",
            "Epoch 254/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9931 - val_loss: 1.5089\n",
            "Epoch 255/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0477 - val_loss: 1.5462\n",
            "Epoch 256/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9970 - val_loss: 1.6412\n",
            "Epoch 257/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0151 - val_loss: 1.5499\n",
            "Epoch 258/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9017 - val_loss: 1.7474\n",
            "Epoch 259/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9297 - val_loss: 1.5713\n",
            "Epoch 260/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 2.0228 - val_loss: 1.5740\n",
            "Epoch 261/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9513 - val_loss: 1.6016\n",
            "Epoch 262/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9791 - val_loss: 1.5054\n",
            "Epoch 263/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8950 - val_loss: 1.4529\n",
            "Epoch 264/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9159 - val_loss: 1.5187\n",
            "Epoch 265/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9776 - val_loss: 1.4697\n",
            "Epoch 266/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9034 - val_loss: 1.5844\n",
            "Epoch 267/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8834 - val_loss: 1.5005\n",
            "Epoch 268/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9855 - val_loss: 1.5272\n",
            "Epoch 269/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9122 - val_loss: 1.4724\n",
            "Epoch 270/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8639 - val_loss: 1.4522\n",
            "Epoch 271/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9127 - val_loss: 1.4937\n",
            "Epoch 272/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8865 - val_loss: 1.4375\n",
            "Epoch 273/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7779 - val_loss: 1.6505\n",
            "Epoch 274/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8044 - val_loss: 1.6103\n",
            "Epoch 275/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8911 - val_loss: 1.6062\n",
            "Epoch 276/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8923 - val_loss: 1.5052\n",
            "Epoch 277/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8826 - val_loss: 1.5235\n",
            "Epoch 278/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8539 - val_loss: 1.6633\n",
            "Epoch 279/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8459 - val_loss: 1.5063\n",
            "Epoch 280/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8274 - val_loss: 1.4917\n",
            "Epoch 281/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8714 - val_loss: 1.5180\n",
            "Epoch 282/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9156 - val_loss: 1.8071\n",
            "Epoch 283/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8526 - val_loss: 1.5612\n",
            "Epoch 284/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8316 - val_loss: 1.5213\n",
            "Epoch 285/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.9033 - val_loss: 1.4571\n",
            "Epoch 286/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.7725 - val_loss: 1.4117\n",
            "Epoch 287/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8760 - val_loss: 1.4553\n",
            "Epoch 288/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8948 - val_loss: 1.4662\n",
            "Epoch 289/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8141 - val_loss: 1.5588\n",
            "Epoch 290/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8142 - val_loss: 1.7116\n",
            "Epoch 291/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8401 - val_loss: 1.5876\n",
            "Epoch 292/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.8115 - val_loss: 1.4072\n",
            "Epoch 293/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7759 - val_loss: 1.5425\n",
            "Epoch 294/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7987 - val_loss: 1.4117\n",
            "Epoch 295/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8408 - val_loss: 1.5439\n",
            "Epoch 296/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8166 - val_loss: 1.6542\n",
            "Epoch 297/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7526 - val_loss: 1.5078\n",
            "Epoch 298/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8045 - val_loss: 1.4876\n",
            "Epoch 299/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7895 - val_loss: 1.4918\n",
            "Epoch 300/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7611 - val_loss: 1.4297\n",
            "Epoch 301/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7898 - val_loss: 1.4918\n",
            "Epoch 302/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7478 - val_loss: 1.4099\n",
            "Epoch 303/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7566 - val_loss: 1.5033\n",
            "Epoch 304/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6901 - val_loss: 1.4360\n",
            "Epoch 305/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7494 - val_loss: 1.5312\n",
            "Epoch 306/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7869 - val_loss: 1.4948\n",
            "Epoch 307/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7567 - val_loss: 1.4114\n",
            "Epoch 308/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8225 - val_loss: 1.4070\n",
            "Epoch 309/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7563 - val_loss: 1.5058\n",
            "Epoch 310/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6908 - val_loss: 1.5927\n",
            "Epoch 311/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8361 - val_loss: 1.5111\n",
            "Epoch 312/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8167 - val_loss: 1.4525\n",
            "Epoch 313/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8595 - val_loss: 1.4308\n",
            "Epoch 314/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7096 - val_loss: 1.4427\n",
            "Epoch 315/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.8000 - val_loss: 1.4702\n",
            "Epoch 316/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6863 - val_loss: 1.4326\n",
            "Epoch 317/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6839 - val_loss: 1.4670\n",
            "Epoch 318/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6768 - val_loss: 1.4354\n",
            "Epoch 319/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.6707 - val_loss: 1.3646\n",
            "Epoch 320/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7152 - val_loss: 1.5195\n",
            "Epoch 321/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7235 - val_loss: 1.5098\n",
            "Epoch 322/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6845 - val_loss: 1.4123\n",
            "Epoch 323/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7140 - val_loss: 1.3781\n",
            "Epoch 324/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6856 - val_loss: 1.5110\n",
            "Epoch 325/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.6947 - val_loss: 1.3143\n",
            "Epoch 326/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.7005 - val_loss: 1.4618\n",
            "Epoch 327/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6230 - val_loss: 1.4589\n",
            "Epoch 328/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6692 - val_loss: 1.3780\n",
            "Epoch 329/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6765 - val_loss: 1.5488\n",
            "Epoch 330/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6521 - val_loss: 1.4093\n",
            "Epoch 331/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6433 - val_loss: 1.5005\n",
            "Epoch 332/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6288 - val_loss: 1.3300\n",
            "Epoch 333/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6391 - val_loss: 1.5500\n",
            "Epoch 334/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6383 - val_loss: 1.5000\n",
            "Epoch 335/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6162 - val_loss: 1.3936\n",
            "Epoch 336/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6854 - val_loss: 1.3659\n",
            "Epoch 337/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6521 - val_loss: 1.4915\n",
            "Epoch 338/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6176 - val_loss: 1.4500\n",
            "Epoch 339/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6500 - val_loss: 1.4172\n",
            "Epoch 340/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6170 - val_loss: 1.3869\n",
            "Epoch 341/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5528 - val_loss: 1.4379\n",
            "Epoch 342/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5806 - val_loss: 1.3892\n",
            "Epoch 343/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5505 - val_loss: 1.4716\n",
            "Epoch 344/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6367 - val_loss: 1.4993\n",
            "Epoch 345/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6098 - val_loss: 1.4602\n",
            "Epoch 346/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6349 - val_loss: 1.5208\n",
            "Epoch 347/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6610 - val_loss: 1.4399\n",
            "Epoch 348/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6032 - val_loss: 1.4505\n",
            "Epoch 349/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5756 - val_loss: 1.3696\n",
            "Epoch 350/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5843 - val_loss: 1.4164\n",
            "Epoch 351/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6214 - val_loss: 1.4569\n",
            "Epoch 352/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5867 - val_loss: 1.3621\n",
            "Epoch 353/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5590 - val_loss: 1.4157\n",
            "Epoch 354/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.6164 - val_loss: 1.3493\n",
            "Epoch 355/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5480 - val_loss: 1.4622\n",
            "Epoch 356/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5165 - val_loss: 1.3870\n",
            "Epoch 357/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5582 - val_loss: 1.3686\n",
            "Epoch 358/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5611 - val_loss: 1.3976\n",
            "Epoch 359/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.5479 - val_loss: 1.2930\n",
            "Epoch 360/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5609 - val_loss: 1.4471\n",
            "Epoch 361/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5633 - val_loss: 1.4047\n",
            "Epoch 362/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5651 - val_loss: 1.3219\n",
            "Epoch 363/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5350 - val_loss: 1.4336\n",
            "Epoch 364/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5686 - val_loss: 1.3712\n",
            "Epoch 365/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.5356 - val_loss: 1.2799\n",
            "Epoch 366/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5230 - val_loss: 1.3907\n",
            "Epoch 367/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5124 - val_loss: 1.3764\n",
            "Epoch 368/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5484 - val_loss: 1.3741\n",
            "Epoch 369/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5656 - val_loss: 1.3070\n",
            "Epoch 370/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5020 - val_loss: 1.3413\n",
            "Epoch 371/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5084 - val_loss: 1.3805\n",
            "Epoch 372/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5288 - val_loss: 1.3812\n",
            "Epoch 373/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5122 - val_loss: 1.3277\n",
            "Epoch 374/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5377 - val_loss: 1.4405\n",
            "Epoch 375/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5396 - val_loss: 1.4246\n",
            "Epoch 376/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5429 - val_loss: 1.3909\n",
            "Epoch 377/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5030 - val_loss: 1.4724\n",
            "Epoch 378/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4556 - val_loss: 1.4140\n",
            "Epoch 379/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5047 - val_loss: 1.3594\n",
            "Epoch 380/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.5152 - val_loss: 1.3111\n",
            "Epoch 381/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4904 - val_loss: 1.2917\n",
            "Epoch 382/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4997 - val_loss: 1.4150\n",
            "Epoch 383/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4946 - val_loss: 1.3821\n",
            "Epoch 384/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4813 - val_loss: 1.4451\n",
            "Epoch 385/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4738 - val_loss: 1.4211\n",
            "Epoch 386/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4588 - val_loss: 1.3359\n",
            "Epoch 387/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.4960 - val_loss: 1.2577\n",
            "Epoch 388/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4494 - val_loss: 1.3623\n",
            "Epoch 389/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4619 - val_loss: 1.2749\n",
            "Epoch 390/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4540 - val_loss: 1.3353\n",
            "Epoch 391/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4892 - val_loss: 1.4419\n",
            "Epoch 392/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4980 - val_loss: 1.3443\n",
            "Epoch 393/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4575 - val_loss: 1.3092\n",
            "Epoch 394/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4599 - val_loss: 1.3269\n",
            "Epoch 395/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.4250 - val_loss: 1.1668\n",
            "Epoch 396/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4184 - val_loss: 1.4256\n",
            "Epoch 397/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4095 - val_loss: 1.1991\n",
            "Epoch 398/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4851 - val_loss: 1.2250\n",
            "Epoch 399/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3817 - val_loss: 1.3882\n",
            "Epoch 400/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4596 - val_loss: 1.2619\n",
            "Epoch 401/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3773 - val_loss: 1.3382\n",
            "Epoch 402/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4161 - val_loss: 1.2351\n",
            "Epoch 403/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4067 - val_loss: 1.2890\n",
            "Epoch 404/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4069 - val_loss: 1.3991\n",
            "Epoch 405/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4128 - val_loss: 1.3391\n",
            "Epoch 406/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4091 - val_loss: 1.2555\n",
            "Epoch 407/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4143 - val_loss: 1.2201\n",
            "Epoch 408/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4262 - val_loss: 1.2471\n",
            "Epoch 409/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.4107 - val_loss: 1.1866\n",
            "Epoch 410/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3746 - val_loss: 1.2395\n",
            "Epoch 411/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3627 - val_loss: 1.4045\n",
            "Epoch 412/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3860 - val_loss: 1.2503\n",
            "Epoch 413/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3576 - val_loss: 1.3211\n",
            "Epoch 414/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3581 - val_loss: 1.2461\n",
            "Epoch 415/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3153 - val_loss: 1.2648\n",
            "Epoch 416/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3853 - val_loss: 1.1687\n",
            "Epoch 417/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3347 - val_loss: 1.2876\n",
            "Epoch 418/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3657 - val_loss: 1.2354\n",
            "Epoch 419/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3339 - val_loss: 1.1805\n",
            "Epoch 420/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3468 - val_loss: 1.2189\n",
            "Epoch 421/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3128 - val_loss: 1.2300\n",
            "Epoch 422/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2942 - val_loss: 1.2637\n",
            "Epoch 423/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.3384 - val_loss: 1.1530\n",
            "Epoch 424/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3079 - val_loss: 1.1949\n",
            "Epoch 425/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3061 - val_loss: 1.1859\n",
            "Epoch 426/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2923 - val_loss: 1.3475\n",
            "Epoch 427/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2968 - val_loss: 1.1992\n",
            "Epoch 428/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2369 - val_loss: 1.3516\n",
            "Epoch 429/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.3085 - val_loss: 1.1971\n",
            "Epoch 430/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2731 - val_loss: 1.2993\n",
            "Epoch 431/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.2433 - val_loss: 1.0930\n",
            "Epoch 432/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2497 - val_loss: 1.1870\n",
            "Epoch 433/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2331 - val_loss: 1.1364\n",
            "Epoch 434/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2086 - val_loss: 1.2880\n",
            "Epoch 435/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2615 - val_loss: 1.2271\n",
            "Epoch 436/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2265 - val_loss: 1.1389\n",
            "Epoch 437/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2242 - val_loss: 1.2545\n",
            "Epoch 438/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2018 - val_loss: 1.1409\n",
            "Epoch 439/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.1897 - val_loss: 1.1427\n",
            "Epoch 440/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.2103 - val_loss: 1.0949\n",
            "Epoch 441/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.1552 - val_loss: 1.1625\n",
            "Epoch 442/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.1473 - val_loss: 1.2023\n",
            "Epoch 443/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.1509 - val_loss: 1.0726\n",
            "Epoch 444/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.1364 - val_loss: 1.1078\n",
            "Epoch 445/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.1146 - val_loss: 1.1548\n",
            "Epoch 446/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.1084 - val_loss: 1.1511\n",
            "Epoch 447/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.1256 - val_loss: 1.1460\n",
            "Epoch 448/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.1159 - val_loss: 1.1800\n",
            "Epoch 449/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.0937 - val_loss: 1.2261\n",
            "Epoch 450/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.0445 - val_loss: 1.0209\n",
            "Epoch 451/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.0604 - val_loss: 1.1468\n",
            "Epoch 452/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.0261 - val_loss: 0.9547\n",
            "Epoch 453/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.0714 - val_loss: 1.1879\n",
            "Epoch 454/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.0269 - val_loss: 0.9971\n",
            "Epoch 455/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 1.0645 - val_loss: 1.0098\n",
            "Epoch 456/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.9787 - val_loss: 1.0548\n",
            "Epoch 457/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.9651 - val_loss: 1.0443\n",
            "Epoch 458/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.9677 - val_loss: 0.9561\n",
            "Epoch 459/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.9400 - val_loss: 1.0103\n",
            "Epoch 460/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.9373 - val_loss: 1.1041\n",
            "Epoch 461/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.8858 - val_loss: 1.2382\n",
            "Epoch 462/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.8954 - val_loss: 1.0166\n",
            "Epoch 463/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.9211 - val_loss: 1.1027\n",
            "Epoch 464/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.8659 - val_loss: 1.1972\n",
            "Epoch 465/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.8432 - val_loss: 1.0671\n",
            "Epoch 466/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.8244 - val_loss: 0.9318\n",
            "Epoch 467/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.8241 - val_loss: 1.0246\n",
            "Epoch 468/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7761 - val_loss: 0.9744\n",
            "Epoch 469/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.8087 - val_loss: 1.0081\n",
            "Epoch 470/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7733 - val_loss: 1.0000\n",
            "Epoch 471/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7382 - val_loss: 1.1332\n",
            "Epoch 472/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7090 - val_loss: 1.2036\n",
            "Epoch 473/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7731 - val_loss: 1.0722\n",
            "Epoch 474/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6939 - val_loss: 1.1788\n",
            "Epoch 475/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7319 - val_loss: 1.1891\n",
            "Epoch 476/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6415 - val_loss: 1.2056\n",
            "Epoch 477/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.7351 - val_loss: 1.1516\n",
            "Epoch 478/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6651 - val_loss: 1.3238\n",
            "Epoch 479/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6724 - val_loss: 0.8119\n",
            "Epoch 480/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6304 - val_loss: 1.3049\n",
            "Epoch 481/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6616 - val_loss: 1.3879\n",
            "Epoch 482/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6560 - val_loss: 1.5548\n",
            "Epoch 483/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6663 - val_loss: 1.1370\n",
            "Epoch 484/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6316 - val_loss: 1.7769\n",
            "Epoch 485/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6969 - val_loss: 1.6264\n",
            "Epoch 486/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6286 - val_loss: 1.2810\n",
            "Epoch 487/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5717 - val_loss: 1.6677\n",
            "Epoch 488/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6205 - val_loss: 1.7430\n",
            "Epoch 489/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5700 - val_loss: 1.1913\n",
            "Epoch 490/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6829 - val_loss: 2.1805\n",
            "Epoch 491/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5838 - val_loss: 1.2522\n",
            "Epoch 492/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6701 - val_loss: 1.7008\n",
            "Epoch 493/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5398 - val_loss: 1.1542\n",
            "Epoch 494/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6098 - val_loss: 1.3201\n",
            "Epoch 495/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6000 - val_loss: 1.1414\n",
            "Epoch 496/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5858 - val_loss: 1.0446\n",
            "Epoch 497/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5914 - val_loss: 1.0860\n",
            "Epoch 498/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5223 - val_loss: 1.6022\n",
            "Epoch 499/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6604 - val_loss: 1.1308\n",
            "Epoch 500/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 1.3408\n",
            "Epoch 501/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5012 - val_loss: 1.0928\n",
            "Epoch 502/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5944 - val_loss: 1.5279\n",
            "Epoch 503/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5402 - val_loss: 1.2555\n",
            "Epoch 504/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6070 - val_loss: 1.2671\n",
            "Epoch 505/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5272 - val_loss: 1.6465\n",
            "Epoch 506/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5278 - val_loss: 1.8952\n",
            "Epoch 507/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5571 - val_loss: 1.6200\n",
            "Epoch 508/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5267 - val_loss: 1.1529\n",
            "Epoch 509/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5838 - val_loss: 1.3016\n",
            "Epoch 510/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6071 - val_loss: 2.4035\n",
            "Epoch 511/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5555 - val_loss: 1.3622\n",
            "Epoch 512/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5223 - val_loss: 1.2640\n",
            "Epoch 513/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5518 - val_loss: 1.0019\n",
            "Epoch 514/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5401 - val_loss: 1.2403\n",
            "Epoch 515/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4980 - val_loss: 1.8569\n",
            "Epoch 516/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5056 - val_loss: 1.0863\n",
            "Epoch 517/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4835 - val_loss: 1.2553\n",
            "Epoch 518/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5131 - val_loss: 0.9969\n",
            "Epoch 519/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5051 - val_loss: 1.4704\n",
            "Epoch 520/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4773 - val_loss: 1.5066\n",
            "Epoch 521/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5060 - val_loss: 1.2543\n",
            "Epoch 522/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4652 - val_loss: 1.0523\n",
            "Epoch 523/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4560 - val_loss: 1.3923\n",
            "Epoch 524/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4701 - val_loss: 1.4462\n",
            "Epoch 525/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5081 - val_loss: 0.9259\n",
            "Epoch 526/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5586 - val_loss: 1.5999\n",
            "Epoch 527/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4948 - val_loss: 1.1849\n",
            "Epoch 528/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4958 - val_loss: 1.2702\n",
            "Epoch 529/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4737 - val_loss: 1.2845\n",
            "Epoch 530/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4913 - val_loss: 1.3182\n",
            "Epoch 531/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4433 - val_loss: 1.3236\n",
            "Epoch 532/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5571 - val_loss: 1.1627\n",
            "Epoch 533/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5023 - val_loss: 1.2676\n",
            "Epoch 534/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4357 - val_loss: 0.9555\n",
            "Epoch 535/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 0.8199\n",
            "Epoch 536/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4459 - val_loss: 1.0322\n",
            "Epoch 537/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4715 - val_loss: 1.2884\n",
            "Epoch 538/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4148 - val_loss: 1.0831\n",
            "Epoch 539/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4428 - val_loss: 1.6402\n",
            "Epoch 540/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4390 - val_loss: 1.2600\n",
            "Epoch 541/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4813 - val_loss: 1.2083\n",
            "Epoch 542/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4659 - val_loss: 0.9199\n",
            "Epoch 543/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4671 - val_loss: 1.0345\n",
            "Epoch 544/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4369 - val_loss: 0.9281\n",
            "Epoch 545/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4280 - val_loss: 1.0839\n",
            "Epoch 546/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4604 - val_loss: 1.3494\n",
            "Epoch 547/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4379 - val_loss: 0.9182\n",
            "Epoch 548/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4137 - val_loss: 1.4903\n",
            "Epoch 549/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4121 - val_loss: 0.8257\n",
            "Epoch 550/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4395 - val_loss: 1.1135\n",
            "Epoch 551/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3918 - val_loss: 1.4519\n",
            "Epoch 552/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4262 - val_loss: 0.8771\n",
            "Epoch 553/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5002 - val_loss: 1.1169\n",
            "Epoch 554/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4346 - val_loss: 0.8068\n",
            "Epoch 555/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4703 - val_loss: 0.6733\n",
            "Epoch 556/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4018 - val_loss: 0.8838\n",
            "Epoch 557/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3887 - val_loss: 1.0931\n",
            "Epoch 558/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4336 - val_loss: 1.3171\n",
            "Epoch 559/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4207 - val_loss: 1.0744\n",
            "Epoch 560/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4959 - val_loss: 1.1502\n",
            "Epoch 561/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4199 - val_loss: 0.9983\n",
            "Epoch 562/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4441 - val_loss: 0.8422\n",
            "Epoch 563/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4505 - val_loss: 0.9096\n",
            "Epoch 564/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4138 - val_loss: 0.8284\n",
            "Epoch 565/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4744 - val_loss: 1.2604\n",
            "Epoch 566/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4172 - val_loss: 0.9525\n",
            "Epoch 567/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4090 - val_loss: 1.1183\n",
            "Epoch 568/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3974 - val_loss: 0.9852\n",
            "Epoch 569/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4137 - val_loss: 1.1064\n",
            "Epoch 570/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3916 - val_loss: 1.1039\n",
            "Epoch 571/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3908 - val_loss: 1.1253\n",
            "Epoch 572/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4214 - val_loss: 0.8295\n",
            "Epoch 573/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4013 - val_loss: 1.0833\n",
            "Epoch 574/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4329 - val_loss: 0.9191\n",
            "Epoch 575/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4014 - val_loss: 0.8752\n",
            "Epoch 576/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4116 - val_loss: 0.8952\n",
            "Epoch 577/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3972 - val_loss: 0.7361\n",
            "Epoch 578/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3652 - val_loss: 0.8651\n",
            "Epoch 579/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3990 - val_loss: 1.0260\n",
            "Epoch 580/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3858 - val_loss: 1.0516\n",
            "Epoch 581/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.4057 - val_loss: 0.6306\n",
            "Epoch 582/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4320 - val_loss: 0.8833\n",
            "Epoch 583/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3831 - val_loss: 0.7885\n",
            "Epoch 584/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4347 - val_loss: 0.9481\n",
            "Epoch 585/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3894 - val_loss: 0.7498\n",
            "Epoch 586/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3717 - val_loss: 1.1034\n",
            "Epoch 587/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4039 - val_loss: 1.2126\n",
            "Epoch 588/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3932 - val_loss: 1.0652\n",
            "Epoch 589/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3829 - val_loss: 0.7691\n",
            "Epoch 590/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3505 - val_loss: 0.6311\n",
            "Epoch 591/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4022 - val_loss: 1.0067\n",
            "Epoch 592/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3646 - val_loss: 0.9551\n",
            "Epoch 593/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3700 - val_loss: 1.0171\n",
            "Epoch 594/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3891 - val_loss: 0.9802\n",
            "Epoch 595/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3624 - val_loss: 1.0755\n",
            "Epoch 596/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3692 - val_loss: 0.7763\n",
            "Epoch 597/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3514 - val_loss: 0.7631\n",
            "Epoch 598/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3433 - val_loss: 0.6370\n",
            "Epoch 599/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3234 - val_loss: 0.7913\n",
            "Epoch 600/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3504 - val_loss: 0.9220\n",
            "Epoch 601/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3413 - val_loss: 0.6795\n",
            "Epoch 602/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3910 - val_loss: 0.8752\n",
            "Epoch 603/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4052 - val_loss: 0.8272\n",
            "Epoch 604/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3796 - val_loss: 0.9374\n",
            "Epoch 605/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3302 - val_loss: 0.8174\n",
            "Epoch 606/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3474 - val_loss: 0.7448\n",
            "Epoch 607/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3125 - val_loss: 0.7867\n",
            "Epoch 608/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3272 - val_loss: 0.9198\n",
            "Epoch 609/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3325 - val_loss: 1.7532\n",
            "Epoch 610/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3634 - val_loss: 0.6913\n",
            "Epoch 611/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3717 - val_loss: 0.9265\n",
            "Epoch 612/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3289 - val_loss: 0.7109\n",
            "Epoch 613/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3510 - val_loss: 1.2301\n",
            "Epoch 614/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3916 - val_loss: 0.8467\n",
            "Epoch 615/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3530 - val_loss: 0.7152\n",
            "Epoch 616/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3886 - val_loss: 0.7555\n",
            "Epoch 617/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3461 - val_loss: 0.8079\n",
            "Epoch 618/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3188 - val_loss: 0.7632\n",
            "Epoch 619/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3064 - val_loss: 1.0509\n",
            "Epoch 620/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3801 - val_loss: 0.8013\n",
            "Epoch 621/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2995 - val_loss: 0.7724\n",
            "Epoch 622/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3252 - val_loss: 0.5313\n",
            "Epoch 623/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3202 - val_loss: 0.8799\n",
            "Epoch 624/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3685 - val_loss: 0.6226\n",
            "Epoch 625/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3388 - val_loss: 0.7474\n",
            "Epoch 626/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.4036 - val_loss: 0.6898\n",
            "Epoch 627/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3178 - val_loss: 0.9056\n",
            "Epoch 628/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3316 - val_loss: 0.5491\n",
            "Epoch 629/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3259 - val_loss: 0.7423\n",
            "Epoch 630/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3253 - val_loss: 0.7239\n",
            "Epoch 631/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3326 - val_loss: 0.6318\n",
            "Epoch 632/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3011 - val_loss: 0.6277\n",
            "Epoch 633/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3445 - val_loss: 0.7422\n",
            "Epoch 634/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2771 - val_loss: 0.8110\n",
            "Epoch 635/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2882 - val_loss: 0.6773\n",
            "Epoch 636/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2740 - val_loss: 0.6539\n",
            "Epoch 637/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3081 - val_loss: 0.6730\n",
            "Epoch 638/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3003 - val_loss: 0.5924\n",
            "Epoch 639/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3186 - val_loss: 0.7338\n",
            "Epoch 640/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2891 - val_loss: 0.5893\n",
            "Epoch 641/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2965 - val_loss: 0.5574\n",
            "Epoch 642/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3045 - val_loss: 0.5805\n",
            "Epoch 643/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3057 - val_loss: 0.4645\n",
            "Epoch 644/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2733 - val_loss: 0.4485\n",
            "Epoch 645/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3020 - val_loss: 0.4899\n",
            "Epoch 646/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3132 - val_loss: 0.5117\n",
            "Epoch 647/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3043 - val_loss: 0.7090\n",
            "Epoch 648/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3046 - val_loss: 0.5103\n",
            "Epoch 649/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2978 - val_loss: 0.5479\n",
            "Epoch 650/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3097 - val_loss: 0.5897\n",
            "Epoch 651/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2603 - val_loss: 0.5799\n",
            "Epoch 652/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2875 - val_loss: 0.8183\n",
            "Epoch 653/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3037 - val_loss: 0.4844\n",
            "Epoch 654/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2627 - val_loss: 0.5497\n",
            "Epoch 655/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2851 - val_loss: 0.5122\n",
            "Epoch 656/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2577 - val_loss: 0.5307\n",
            "Epoch 657/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3136 - val_loss: 0.6263\n",
            "Epoch 658/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2748 - val_loss: 0.8428\n",
            "Epoch 659/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2901 - val_loss: 0.9366\n",
            "Epoch 660/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3148 - val_loss: 0.4809\n",
            "Epoch 661/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2816 - val_loss: 0.9995\n",
            "Epoch 662/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2789 - val_loss: 0.7249\n",
            "Epoch 663/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2734 - val_loss: 0.7921\n",
            "Epoch 664/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2630 - val_loss: 0.7015\n",
            "Epoch 665/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2515 - val_loss: 0.6202\n",
            "Epoch 666/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2879 - val_loss: 0.5595\n",
            "Epoch 667/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2955 - val_loss: 0.7565\n",
            "Epoch 668/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2521 - val_loss: 0.5924\n",
            "Epoch 669/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2707 - val_loss: 0.5566\n",
            "Epoch 670/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3134 - val_loss: 0.4432\n",
            "Epoch 671/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.2449 - val_loss: 0.3354\n",
            "Epoch 672/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2707 - val_loss: 0.6474\n",
            "Epoch 673/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2584 - val_loss: 0.4816\n",
            "Epoch 674/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2758 - val_loss: 0.6534\n",
            "Epoch 675/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2987 - val_loss: 0.3840\n",
            "Epoch 676/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2874 - val_loss: 0.5755\n",
            "Epoch 677/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2720 - val_loss: 0.7308\n",
            "Epoch 678/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2611 - val_loss: 0.4665\n",
            "Epoch 679/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2544 - val_loss: 0.6251\n",
            "Epoch 680/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2510 - val_loss: 0.5863\n",
            "Epoch 681/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2944 - val_loss: 0.4521\n",
            "Epoch 682/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2477 - val_loss: 0.7311\n",
            "Epoch 683/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2539 - val_loss: 0.4409\n",
            "Epoch 684/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2536 - val_loss: 0.4347\n",
            "Epoch 685/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2655 - val_loss: 0.5595\n",
            "Epoch 686/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2764 - val_loss: 0.5867\n",
            "Epoch 687/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2550 - val_loss: 0.3896\n",
            "Epoch 688/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2953 - val_loss: 0.4919\n",
            "Epoch 689/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2531 - val_loss: 0.8501\n",
            "Epoch 690/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2300 - val_loss: 0.6390\n",
            "Epoch 691/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2854 - val_loss: 0.6582\n",
            "Epoch 692/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2398 - val_loss: 0.5795\n",
            "Epoch 693/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2395 - val_loss: 0.6274\n",
            "Epoch 694/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2352 - val_loss: 0.4425\n",
            "Epoch 695/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2001 - val_loss: 0.3626\n",
            "Epoch 696/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2552 - val_loss: 0.5952\n",
            "Epoch 697/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3021 - val_loss: 0.7668\n",
            "Epoch 698/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2512 - val_loss: 0.4189\n",
            "Epoch 699/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2597 - val_loss: 0.6707\n",
            "Epoch 700/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2668 - val_loss: 0.5878\n",
            "Epoch 701/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2971 - val_loss: 0.6549\n",
            "Epoch 702/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2483 - val_loss: 0.3502\n",
            "Epoch 703/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2481 - val_loss: 0.7468\n",
            "Epoch 704/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2150 - val_loss: 0.5909\n",
            "Epoch 705/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2499 - val_loss: 0.6212\n",
            "Epoch 706/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2539 - val_loss: 0.7465\n",
            "Epoch 707/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2552 - val_loss: 0.4575\n",
            "Epoch 708/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2166 - val_loss: 0.3849\n",
            "Epoch 709/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2233 - val_loss: 0.4342\n",
            "Epoch 710/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2505 - val_loss: 0.4987\n",
            "Epoch 711/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2202 - val_loss: 0.4399\n",
            "Epoch 712/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2127 - val_loss: 0.4285\n",
            "Epoch 713/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2738 - val_loss: 0.4298\n",
            "Epoch 714/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2309 - val_loss: 0.4613\n",
            "Epoch 715/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2402 - val_loss: 0.5936\n",
            "Epoch 716/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2678 - val_loss: 0.5735\n",
            "Epoch 717/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2488 - val_loss: 0.5056\n",
            "Epoch 718/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2486 - val_loss: 0.4298\n",
            "Epoch 719/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2359 - val_loss: 0.4231\n",
            "Epoch 720/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2183 - val_loss: 0.4908\n",
            "Epoch 721/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2441 - val_loss: 0.5388\n",
            "Epoch 722/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2204 - val_loss: 0.3447\n",
            "Epoch 723/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2611 - val_loss: 0.5290\n",
            "Epoch 724/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2537 - val_loss: 0.5132\n",
            "Epoch 725/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2308 - val_loss: 0.5120\n",
            "Epoch 726/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2044 - val_loss: 0.5704\n",
            "Epoch 727/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1849 - val_loss: 0.6269\n",
            "Epoch 728/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2173 - val_loss: 0.5957\n",
            "Epoch 729/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2357 - val_loss: 0.5697\n",
            "Epoch 730/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2606 - val_loss: 0.4921\n",
            "Epoch 731/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2143 - val_loss: 0.7308\n",
            "Epoch 732/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2396 - val_loss: 0.4336\n",
            "Epoch 733/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2331 - val_loss: 0.3326\n",
            "Epoch 734/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2212 - val_loss: 0.7172\n",
            "Epoch 735/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2058 - val_loss: 0.5905\n",
            "Epoch 736/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2242 - val_loss: 0.5293\n",
            "Epoch 737/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2120 - val_loss: 0.5755\n",
            "Epoch 738/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.2272 - val_loss: 0.3001\n",
            "Epoch 739/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2540 - val_loss: 0.4633\n",
            "Epoch 740/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2111 - val_loss: 0.6123\n",
            "Epoch 741/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2453 - val_loss: 0.4909\n",
            "Epoch 742/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2337 - val_loss: 0.5186\n",
            "Epoch 743/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2110 - val_loss: 0.4805\n",
            "Epoch 744/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2625 - val_loss: 0.3787\n",
            "Epoch 745/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2109 - val_loss: 0.5627\n",
            "Epoch 746/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2148 - val_loss: 0.4346\n",
            "Epoch 747/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2301 - val_loss: 0.4904\n",
            "Epoch 748/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2305 - val_loss: 0.5251\n",
            "Epoch 749/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1767 - val_loss: 0.6590\n",
            "Epoch 750/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1975 - val_loss: 0.5950\n",
            "Epoch 751/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2030 - val_loss: 0.4690\n",
            "Epoch 752/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1909 - val_loss: 0.4002\n",
            "Epoch 753/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2285 - val_loss: 0.5894\n",
            "Epoch 754/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2329 - val_loss: 0.3434\n",
            "Epoch 755/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2500 - val_loss: 0.3845\n",
            "Epoch 756/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2073 - val_loss: 0.5965\n",
            "Epoch 757/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2143 - val_loss: 0.3569\n",
            "Epoch 758/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2124 - val_loss: 0.4091\n",
            "Epoch 759/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2207 - val_loss: 0.3019\n",
            "Epoch 760/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2062 - val_loss: 0.4647\n",
            "Epoch 761/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2014 - val_loss: 0.4439\n",
            "Epoch 762/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1901 - val_loss: 0.5370\n",
            "Epoch 763/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2033 - val_loss: 0.5363\n",
            "Epoch 764/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1941 - val_loss: 0.3385\n",
            "Epoch 765/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2127 - val_loss: 0.5680\n",
            "Epoch 766/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2106 - val_loss: 0.3942\n",
            "Epoch 767/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2593 - val_loss: 0.3102\n",
            "Epoch 768/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2123 - val_loss: 0.5820\n",
            "Epoch 769/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1863 - val_loss: 0.3870\n",
            "Epoch 770/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2352 - val_loss: 0.4759\n",
            "Epoch 771/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2035 - val_loss: 0.5525\n",
            "Epoch 772/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2057 - val_loss: 0.5167\n",
            "Epoch 773/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1820 - val_loss: 0.3013\n",
            "Epoch 774/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1916 - val_loss: 0.6041\n",
            "Epoch 775/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2078 - val_loss: 0.4894\n",
            "Epoch 776/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1995 - val_loss: 0.4826\n",
            "Epoch 777/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1892 - val_loss: 0.3795\n",
            "Epoch 778/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1842 - val_loss: 0.4141\n",
            "Epoch 779/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1743 - val_loss: 0.3518\n",
            "Epoch 780/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2087 - val_loss: 0.4508\n",
            "Epoch 781/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2014 - val_loss: 0.3487\n",
            "Epoch 782/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2406 - val_loss: 0.5784\n",
            "Epoch 783/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1973 - val_loss: 0.4020\n",
            "Epoch 784/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2309 - val_loss: 0.4500\n",
            "Epoch 785/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.2170 - val_loss: 0.2885\n",
            "Epoch 786/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1931 - val_loss: 0.4619\n",
            "Epoch 787/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2230 - val_loss: 0.4387\n",
            "Epoch 788/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2129 - val_loss: 0.3850\n",
            "Epoch 789/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1881 - val_loss: 0.3901\n",
            "Epoch 790/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2400 - val_loss: 0.4962\n",
            "Epoch 791/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1833 - val_loss: 0.4205\n",
            "Epoch 792/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1897 - val_loss: 0.4292\n",
            "Epoch 793/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2208 - val_loss: 0.3998\n",
            "Epoch 794/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1901 - val_loss: 0.5734\n",
            "Epoch 795/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2286 - val_loss: 0.4318\n",
            "Epoch 796/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1855 - val_loss: 0.4324\n",
            "Epoch 797/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2192 - val_loss: 0.3138\n",
            "Epoch 798/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1830 - val_loss: 0.3816\n",
            "Epoch 799/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2144 - val_loss: 0.6083\n",
            "Epoch 800/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1994 - val_loss: 0.3723\n",
            "Epoch 801/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1951 - val_loss: 0.3665\n",
            "Epoch 802/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2073 - val_loss: 0.3183\n",
            "Epoch 803/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1828 - val_loss: 0.5410\n",
            "Epoch 804/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1993 - val_loss: 0.4953\n",
            "Epoch 805/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1653 - val_loss: 0.6750\n",
            "Epoch 806/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1956 - val_loss: 0.5648\n",
            "Epoch 807/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2150 - val_loss: 0.5030\n",
            "Epoch 808/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2156 - val_loss: 0.6143\n",
            "Epoch 809/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2123 - val_loss: 0.6369\n",
            "Epoch 810/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1949 - val_loss: 0.3895\n",
            "Epoch 811/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2048 - val_loss: 0.4026\n",
            "Epoch 812/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1662 - val_loss: 0.3313\n",
            "Epoch 813/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1810 - val_loss: 0.3958\n",
            "Epoch 814/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2400 - val_loss: 0.4027\n",
            "Epoch 815/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1848 - val_loss: 0.5605\n",
            "Epoch 816/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1789 - val_loss: 0.3977\n",
            "Epoch 817/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2071 - val_loss: 0.4720\n",
            "Epoch 818/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1941 - val_loss: 0.3497\n",
            "Epoch 819/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1813 - val_loss: 0.2712\n",
            "Epoch 820/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1960 - val_loss: 0.4553\n",
            "Epoch 821/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2493 - val_loss: 0.5826\n",
            "Epoch 822/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2120 - val_loss: 0.4183\n",
            "Epoch 823/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1915 - val_loss: 0.4417\n",
            "Epoch 824/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1829 - val_loss: 0.3230\n",
            "Epoch 825/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1848 - val_loss: 0.5947\n",
            "Epoch 826/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1828 - val_loss: 0.5565\n",
            "Epoch 827/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.2172 - val_loss: 0.2675\n",
            "Epoch 828/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1914 - val_loss: 0.3425\n",
            "Epoch 829/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1921 - val_loss: 0.2918\n",
            "Epoch 830/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2169 - val_loss: 0.3453\n",
            "Epoch 831/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2014 - val_loss: 0.3554\n",
            "Epoch 832/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2006 - val_loss: 0.4100\n",
            "Epoch 833/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1782 - val_loss: 0.7163\n",
            "Epoch 834/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1784 - val_loss: 0.4472\n",
            "Epoch 835/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1665 - val_loss: 0.2943\n",
            "Epoch 836/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1817 - val_loss: 0.4445\n",
            "Epoch 837/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2159 - val_loss: 0.3715\n",
            "Epoch 838/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1658 - val_loss: 0.2840\n",
            "Epoch 839/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1747 - val_loss: 0.8822\n",
            "Epoch 840/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1879 - val_loss: 0.3228\n",
            "Epoch 841/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1782 - val_loss: 0.4529\n",
            "Epoch 842/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1698 - val_loss: 0.3737\n",
            "Epoch 843/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1712 - val_loss: 0.5522\n",
            "Epoch 844/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1856 - val_loss: 0.3875\n",
            "Epoch 845/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1524 - val_loss: 0.3413\n",
            "Epoch 846/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1873 - val_loss: 0.3792\n",
            "Epoch 847/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1916 - val_loss: 0.5903\n",
            "Epoch 848/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1742 - val_loss: 0.7203\n",
            "Epoch 849/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1861 - val_loss: 0.4977\n",
            "Epoch 850/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1783 - val_loss: 0.3533\n",
            "Epoch 851/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1882 - val_loss: 0.4644\n",
            "Epoch 852/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1876 - val_loss: 0.4786\n",
            "Epoch 853/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2165 - val_loss: 0.5205\n",
            "Epoch 854/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1871 - val_loss: 0.4387\n",
            "Epoch 855/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1613 - val_loss: 0.4390\n",
            "Epoch 856/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2282 - val_loss: 0.3869\n",
            "Epoch 857/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2035 - val_loss: 0.3561\n",
            "Epoch 858/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1653 - val_loss: 0.3127\n",
            "Epoch 859/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1851 - val_loss: 0.3176\n",
            "Epoch 860/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2302 - val_loss: 0.5570\n",
            "Epoch 861/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1831 - val_loss: 0.6335\n",
            "Epoch 862/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1873 - val_loss: 0.6258\n",
            "Epoch 863/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1805 - val_loss: 0.4690\n",
            "Epoch 864/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1762 - val_loss: 0.3944\n",
            "Epoch 865/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1704 - val_loss: 0.4190\n",
            "Epoch 866/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1982 - val_loss: 0.3221\n",
            "Epoch 867/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1999 - val_loss: 0.4628\n",
            "Epoch 868/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1538 - val_loss: 0.4514\n",
            "Epoch 869/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1848 - val_loss: 0.6217\n",
            "Epoch 870/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1875 - val_loss: 0.4103\n",
            "Epoch 871/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1985 - val_loss: 0.3844\n",
            "Epoch 872/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1895 - val_loss: 0.8036\n",
            "Epoch 873/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1858 - val_loss: 0.3628\n",
            "Epoch 874/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1941 - val_loss: 0.4045\n",
            "Epoch 875/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1661 - val_loss: 0.5232\n",
            "Epoch 876/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1751 - val_loss: 0.5576\n",
            "Epoch 877/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1712 - val_loss: 0.2924\n",
            "Epoch 878/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1746 - val_loss: 0.4646\n",
            "Epoch 879/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1605 - val_loss: 0.5176\n",
            "Epoch 880/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1992 - val_loss: 0.4211\n",
            "Epoch 881/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2019 - val_loss: 0.4867\n",
            "Epoch 882/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1670 - val_loss: 0.4797\n",
            "Epoch 883/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2024 - val_loss: 0.4006\n",
            "Epoch 884/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1813 - val_loss: 0.4503\n",
            "Epoch 885/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1496 - val_loss: 0.4256\n",
            "Epoch 886/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1663 - val_loss: 0.4014\n",
            "Epoch 887/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1693 - val_loss: 0.4485\n",
            "Epoch 888/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1909 - val_loss: 0.3095\n",
            "Epoch 889/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1949 - val_loss: 0.3990\n",
            "Epoch 890/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1544 - val_loss: 0.4494\n",
            "Epoch 891/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1629 - val_loss: 0.4033\n",
            "Epoch 892/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1904 - val_loss: 0.2560\n",
            "Epoch 893/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2042 - val_loss: 0.5497\n",
            "Epoch 894/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2081 - val_loss: 0.5151\n",
            "Epoch 895/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1645 - val_loss: 0.2879\n",
            "Epoch 896/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1907 - val_loss: 0.3101\n",
            "Epoch 897/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1691 - val_loss: 0.4851\n",
            "Epoch 898/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2343 - val_loss: 0.6045\n",
            "Epoch 899/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1827 - val_loss: 0.6421\n",
            "Epoch 900/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1586 - val_loss: 0.5338\n",
            "Epoch 901/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1663 - val_loss: 0.4226\n",
            "Epoch 902/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1740 - val_loss: 0.2981\n",
            "Epoch 903/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1900 - val_loss: 0.6712\n",
            "Epoch 904/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1886 - val_loss: 0.2619\n",
            "Epoch 905/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1885 - val_loss: 0.4943\n",
            "Epoch 906/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1795 - val_loss: 0.4471\n",
            "Epoch 907/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1857 - val_loss: 0.4407\n",
            "Epoch 908/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1884 - val_loss: 0.4562\n",
            "Epoch 909/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1727 - val_loss: 0.3920\n",
            "Epoch 910/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1663 - val_loss: 0.3988\n",
            "Epoch 911/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1611 - val_loss: 0.3961\n",
            "Epoch 912/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1602 - val_loss: 0.4068\n",
            "Epoch 913/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1859 - val_loss: 0.4750\n",
            "Epoch 914/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2126 - val_loss: 0.7631\n",
            "Epoch 915/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1705 - val_loss: 0.4270\n",
            "Epoch 916/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1765 - val_loss: 0.4582\n",
            "Epoch 917/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1951 - val_loss: 0.7055\n",
            "Epoch 918/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1940 - val_loss: 0.2796\n",
            "Epoch 919/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1703 - val_loss: 0.5733\n",
            "Epoch 920/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1602 - val_loss: 0.4135\n",
            "Epoch 921/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1894 - val_loss: 0.6573\n",
            "Epoch 922/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1367 - val_loss: 0.5916\n",
            "Epoch 923/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1782 - val_loss: 0.6016\n",
            "Epoch 924/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1639 - val_loss: 0.3628\n",
            "Epoch 925/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1715 - val_loss: 0.2956\n",
            "Epoch 926/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1678 - val_loss: 0.4127\n",
            "Epoch 927/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1433 - val_loss: 0.3846\n",
            "Epoch 928/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1916 - val_loss: 0.4172\n",
            "Epoch 929/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1825 - val_loss: 0.5323\n",
            "Epoch 930/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1584 - val_loss: 0.5229\n",
            "Epoch 931/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1934 - val_loss: 0.6310\n",
            "Epoch 932/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1894 - val_loss: 0.3181\n",
            "Epoch 933/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1608 - val_loss: 0.3889\n",
            "Epoch 934/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1872 - val_loss: 0.7872\n",
            "Epoch 935/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1750 - val_loss: 0.4112\n",
            "Epoch 936/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1893 - val_loss: 0.4817\n",
            "Epoch 937/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1695 - val_loss: 0.3490\n",
            "Epoch 938/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1635 - val_loss: 0.2267\n",
            "Epoch 939/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1616 - val_loss: 0.3354\n",
            "Epoch 940/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1492 - val_loss: 0.3520\n",
            "Epoch 941/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1412 - val_loss: 0.4616\n",
            "Epoch 942/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2158 - val_loss: 0.4183\n",
            "Epoch 943/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1594 - val_loss: 0.3491\n",
            "Epoch 944/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1670 - val_loss: 0.3063\n",
            "Epoch 945/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1856 - val_loss: 0.2597\n",
            "Epoch 946/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1955 - val_loss: 0.3892\n",
            "Epoch 947/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1593 - val_loss: 0.4141\n",
            "Epoch 948/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1505 - val_loss: 0.4003\n",
            "Epoch 949/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1420 - val_loss: 0.3031\n",
            "Epoch 950/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1408 - val_loss: 0.6280\n",
            "Epoch 951/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1466 - val_loss: 0.4602\n",
            "Epoch 952/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1457 - val_loss: 0.3798\n",
            "Epoch 953/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1690 - val_loss: 0.4229\n",
            "Epoch 954/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1720 - val_loss: 0.4476\n",
            "Epoch 955/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1611 - val_loss: 0.4723\n",
            "Epoch 956/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1643 - val_loss: 0.5259\n",
            "Epoch 957/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1480 - val_loss: 0.2986\n",
            "Epoch 958/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1779 - val_loss: 0.5868\n",
            "Epoch 959/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.2064 - val_loss: 0.2189\n",
            "Epoch 960/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1802 - val_loss: 0.2889\n",
            "Epoch 961/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1648 - val_loss: 0.3547\n",
            "Epoch 962/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1696 - val_loss: 0.3923\n",
            "Epoch 963/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1637 - val_loss: 0.4473\n",
            "Epoch 964/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1665 - val_loss: 0.3664\n",
            "Epoch 965/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1488 - val_loss: 0.4817\n",
            "Epoch 966/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1854 - val_loss: 0.5352\n",
            "Epoch 967/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1766 - val_loss: 0.7645\n",
            "Epoch 968/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1607 - val_loss: 0.4395\n",
            "Epoch 969/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1604 - val_loss: 0.3532\n",
            "Epoch 970/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1651 - val_loss: 0.8047\n",
            "Epoch 971/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1784 - val_loss: 0.5155\n",
            "Epoch 972/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1532 - val_loss: 0.4643\n",
            "Epoch 973/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1553 - val_loss: 0.4963\n",
            "Epoch 974/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1655 - val_loss: 0.4079\n",
            "Epoch 975/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1751 - val_loss: 0.5603\n",
            "Epoch 976/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1779 - val_loss: 0.4026\n",
            "Epoch 977/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1530 - val_loss: 0.1698\n",
            "Epoch 978/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1588 - val_loss: 0.3893\n",
            "Epoch 979/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1705 - val_loss: 0.3519\n",
            "Epoch 980/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1568 - val_loss: 0.6394\n",
            "Epoch 981/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1356 - val_loss: 0.2780\n",
            "Epoch 982/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1225 - val_loss: 0.5699\n",
            "Epoch 983/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1631 - val_loss: 0.3794\n",
            "Epoch 984/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1549 - val_loss: 0.3854\n",
            "Epoch 985/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1597 - val_loss: 0.5738\n",
            "Epoch 986/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1736 - val_loss: 0.3933\n",
            "Epoch 987/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1689 - val_loss: 0.3961\n",
            "Epoch 988/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1509 - val_loss: 0.3211\n",
            "Epoch 989/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1881 - val_loss: 0.4197\n",
            "Epoch 990/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1355 - val_loss: 0.4752\n",
            "Epoch 991/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1761 - val_loss: 0.2777\n",
            "Epoch 992/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1665 - val_loss: 0.4282\n",
            "Epoch 993/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1482 - val_loss: 0.3575\n",
            "Epoch 994/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1618 - val_loss: 0.3648\n",
            "Epoch 995/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1667 - val_loss: 0.4333\n",
            "Epoch 996/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1478 - val_loss: 0.3946\n",
            "Epoch 997/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1839 - val_loss: 0.3950\n",
            "Epoch 998/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1782 - val_loss: 0.4240\n",
            "Epoch 999/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1579 - val_loss: 0.5290\n",
            "Epoch 1000/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1637 - val_loss: 0.4394\n",
            "Epoch 1001/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1563 - val_loss: 0.7140\n",
            "Epoch 1002/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1362 - val_loss: 0.5705\n",
            "Epoch 1003/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1569 - val_loss: 0.5524\n",
            "Epoch 1004/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1872 - val_loss: 0.4499\n",
            "Epoch 1005/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1434 - val_loss: 0.2589\n",
            "Epoch 1006/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1726 - val_loss: 0.5706\n",
            "Epoch 1007/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1589 - val_loss: 0.4935\n",
            "Epoch 1008/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1335 - val_loss: 0.6260\n",
            "Epoch 1009/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1988 - val_loss: 0.4539\n",
            "Epoch 1010/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1754 - val_loss: 0.7314\n",
            "Epoch 1011/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1504 - val_loss: 0.4270\n",
            "Epoch 1012/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1561 - val_loss: 0.5475\n",
            "Epoch 1013/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1412 - val_loss: 0.4391\n",
            "Epoch 1014/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1813 - val_loss: 0.4675\n",
            "Epoch 1015/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1533 - val_loss: 0.4840\n",
            "Epoch 1016/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1823 - val_loss: 0.4498\n",
            "Epoch 1017/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1643 - val_loss: 0.4907\n",
            "Epoch 1018/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1630 - val_loss: 0.4643\n",
            "Epoch 1019/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1821 - val_loss: 0.2458\n",
            "Epoch 1020/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1531 - val_loss: 0.3666\n",
            "Epoch 1021/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1747 - val_loss: 0.4335\n",
            "Epoch 1022/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1307 - val_loss: 0.4183\n",
            "Epoch 1023/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1780 - val_loss: 0.5006\n",
            "Epoch 1024/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1452 - val_loss: 0.4558\n",
            "Epoch 1025/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1971 - val_loss: 0.5273\n",
            "Epoch 1026/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1535 - val_loss: 0.3474\n",
            "Epoch 1027/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2031 - val_loss: 0.3600\n",
            "Epoch 1028/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1604 - val_loss: 0.5237\n",
            "Epoch 1029/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1569 - val_loss: 0.3612\n",
            "Epoch 1030/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1522 - val_loss: 0.7357\n",
            "Epoch 1031/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1576 - val_loss: 0.1996\n",
            "Epoch 1032/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1433 - val_loss: 0.7378\n",
            "Epoch 1033/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1506 - val_loss: 0.5218\n",
            "Epoch 1034/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1334 - val_loss: 0.5395\n",
            "Epoch 1035/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1568 - val_loss: 0.2858\n",
            "Epoch 1036/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1463 - val_loss: 0.3977\n",
            "Epoch 1037/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1412 - val_loss: 0.3114\n",
            "Epoch 1038/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1591 - val_loss: 0.4870\n",
            "Epoch 1039/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1678 - val_loss: 0.3947\n",
            "Epoch 1040/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1521 - val_loss: 0.4649\n",
            "Epoch 1041/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1723 - val_loss: 0.4074\n",
            "Epoch 1042/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1563 - val_loss: 0.4858\n",
            "Epoch 1043/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1555 - val_loss: 0.3867\n",
            "Epoch 1044/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1766 - val_loss: 0.3608\n",
            "Epoch 1045/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1474 - val_loss: 0.3922\n",
            "Epoch 1046/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1717 - val_loss: 0.9187\n",
            "Epoch 1047/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1290 - val_loss: 0.2353\n",
            "Epoch 1048/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1580 - val_loss: 0.2679\n",
            "Epoch 1049/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1540 - val_loss: 0.4806\n",
            "Epoch 1050/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1728 - val_loss: 0.3967\n",
            "Epoch 1051/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1591 - val_loss: 0.4724\n",
            "Epoch 1052/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1571 - val_loss: 0.5124\n",
            "Epoch 1053/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1563 - val_loss: 0.3297\n",
            "Epoch 1054/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1561 - val_loss: 0.5645\n",
            "Epoch 1055/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1378 - val_loss: 0.3983\n",
            "Epoch 1056/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1529 - val_loss: 0.5478\n",
            "Epoch 1057/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1777 - val_loss: 0.4118\n",
            "Epoch 1058/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1543 - val_loss: 0.4768\n",
            "Epoch 1059/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1543 - val_loss: 0.3127\n",
            "Epoch 1060/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1467 - val_loss: 0.5026\n",
            "Epoch 1061/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1409 - val_loss: 0.2851\n",
            "Epoch 1062/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1398 - val_loss: 0.3078\n",
            "Epoch 1063/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1414 - val_loss: 0.5871\n",
            "Epoch 1064/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1387 - val_loss: 0.4384\n",
            "Epoch 1065/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1602 - val_loss: 0.5408\n",
            "Epoch 1066/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1382 - val_loss: 0.4585\n",
            "Epoch 1067/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1385 - val_loss: 0.7800\n",
            "Epoch 1068/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1571 - val_loss: 0.3301\n",
            "Epoch 1069/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1349 - val_loss: 0.3308\n",
            "Epoch 1070/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1495 - val_loss: 0.5378\n",
            "Epoch 1071/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1527 - val_loss: 0.4691\n",
            "Epoch 1072/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1630 - val_loss: 0.3948\n",
            "Epoch 1073/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1326 - val_loss: 0.4005\n",
            "Epoch 1074/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1605 - val_loss: 0.2950\n",
            "Epoch 1075/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1419 - val_loss: 0.4786\n",
            "Epoch 1076/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1393 - val_loss: 0.6746\n",
            "Epoch 1077/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1694 - val_loss: 0.7841\n",
            "Epoch 1078/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1558 - val_loss: 0.6691\n",
            "Epoch 1079/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1645 - val_loss: 0.7123\n",
            "Epoch 1080/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1448 - val_loss: 0.4105\n",
            "Epoch 1081/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1399 - val_loss: 0.5768\n",
            "Epoch 1082/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1588 - val_loss: 0.3753\n",
            "Epoch 1083/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1634 - val_loss: 0.6380\n",
            "Epoch 1084/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1716 - val_loss: 0.2932\n",
            "Epoch 1085/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1385 - val_loss: 0.4443\n",
            "Epoch 1086/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1531 - val_loss: 0.5399\n",
            "Epoch 1087/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1416 - val_loss: 0.4113\n",
            "Epoch 1088/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1455 - val_loss: 0.5814\n",
            "Epoch 1089/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1487 - val_loss: 0.7705\n",
            "Epoch 1090/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1279 - val_loss: 0.4621\n",
            "Epoch 1091/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1345 - val_loss: 0.5645\n",
            "Epoch 1092/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1784 - val_loss: 0.3749\n",
            "Epoch 1093/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1559 - val_loss: 0.2827\n",
            "Epoch 1094/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1718 - val_loss: 0.6035\n",
            "Epoch 1095/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1692 - val_loss: 0.4693\n",
            "Epoch 1096/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1312 - val_loss: 0.3136\n",
            "Epoch 1097/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1386 - val_loss: 0.2531\n",
            "Epoch 1098/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1632 - val_loss: 0.4571\n",
            "Epoch 1099/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1235 - val_loss: 0.4894\n",
            "Epoch 1100/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1495 - val_loss: 0.4096\n",
            "Epoch 1101/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1352 - val_loss: 0.3702\n",
            "Epoch 1102/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1555 - val_loss: 0.3625\n",
            "Epoch 1103/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1443 - val_loss: 0.3719\n",
            "Epoch 1104/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1763 - val_loss: 0.3456\n",
            "Epoch 1105/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1337 - val_loss: 0.3213\n",
            "Epoch 1106/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1446 - val_loss: 0.4103\n",
            "Epoch 1107/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1438 - val_loss: 0.2809\n",
            "Epoch 1108/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1577 - val_loss: 0.7351\n",
            "Epoch 1109/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1404 - val_loss: 0.2829\n",
            "Epoch 1110/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1573 - val_loss: 0.5309\n",
            "Epoch 1111/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1560 - val_loss: 0.3211\n",
            "Epoch 1112/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1525 - val_loss: 0.3889\n",
            "Epoch 1113/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1559 - val_loss: 0.3826\n",
            "Epoch 1114/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1293 - val_loss: 0.5597\n",
            "Epoch 1115/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1765 - val_loss: 0.3699\n",
            "Epoch 1116/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1524 - val_loss: 0.6323\n",
            "Epoch 1117/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1475 - val_loss: 0.4849\n",
            "Epoch 1118/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1276 - val_loss: 0.2957\n",
            "Epoch 1119/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1543 - val_loss: 0.5026\n",
            "Epoch 1120/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1363 - val_loss: 0.3296\n",
            "Epoch 1121/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1494 - val_loss: 0.7147\n",
            "Epoch 1122/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1540 - val_loss: 0.3596\n",
            "Epoch 1123/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1488 - val_loss: 0.5262\n",
            "Epoch 1124/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1568 - val_loss: 0.5526\n",
            "Epoch 1125/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1366 - val_loss: 0.3845\n",
            "Epoch 1126/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1698 - val_loss: 0.3419\n",
            "Epoch 1127/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1404 - val_loss: 0.2649\n",
            "Epoch 1128/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1817 - val_loss: 0.3302\n",
            "Epoch 1129/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1725 - val_loss: 0.3332\n",
            "Epoch 1130/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1625 - val_loss: 0.6677\n",
            "Epoch 1131/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1403 - val_loss: 0.5274\n",
            "Epoch 1132/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1370 - val_loss: 0.3615\n",
            "Epoch 1133/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1260 - val_loss: 0.3855\n",
            "Epoch 1134/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1622 - val_loss: 0.3811\n",
            "Epoch 1135/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1607 - val_loss: 0.4002\n",
            "Epoch 1136/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1584 - val_loss: 0.4996\n",
            "Epoch 1137/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1659 - val_loss: 0.4690\n",
            "Epoch 1138/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1242 - val_loss: 0.3594\n",
            "Epoch 1139/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1232 - val_loss: 0.5493\n",
            "Epoch 1140/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1463 - val_loss: 0.5086\n",
            "Epoch 1141/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1444 - val_loss: 0.2568\n",
            "Epoch 1142/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1366 - val_loss: 0.6404\n",
            "Epoch 1143/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1533 - val_loss: 0.5905\n",
            "Epoch 1144/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1436 - val_loss: 0.6788\n",
            "Epoch 1145/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1353 - val_loss: 0.3424\n",
            "Epoch 1146/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1494 - val_loss: 0.4354\n",
            "Epoch 1147/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1324 - val_loss: 0.5523\n",
            "Epoch 1148/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1374 - val_loss: 0.6819\n",
            "Epoch 1149/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1667 - val_loss: 0.7515\n",
            "Epoch 1150/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1285 - val_loss: 0.2598\n",
            "Epoch 1151/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1194 - val_loss: 0.5119\n",
            "Epoch 1152/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1805 - val_loss: 0.5050\n",
            "Epoch 1153/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1329 - val_loss: 0.3342\n",
            "Epoch 1154/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1564 - val_loss: 0.2850\n",
            "Epoch 1155/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1584 - val_loss: 0.4125\n",
            "Epoch 1156/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1703 - val_loss: 0.3846\n",
            "Epoch 1157/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1107 - val_loss: 0.3143\n",
            "Epoch 1158/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1248 - val_loss: 0.4766\n",
            "Epoch 1159/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1380 - val_loss: 0.3810\n",
            "Epoch 1160/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1662 - val_loss: 0.6028\n",
            "Epoch 1161/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1447 - val_loss: 0.7356\n",
            "Epoch 1162/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1616 - val_loss: 0.2608\n",
            "Epoch 1163/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1594 - val_loss: 0.3691\n",
            "Epoch 1164/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1603 - val_loss: 0.5563\n",
            "Epoch 1165/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1381 - val_loss: 0.3573\n",
            "Epoch 1166/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1405 - val_loss: 0.4245\n",
            "Epoch 1167/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1674 - val_loss: 0.6220\n",
            "Epoch 1168/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1573 - val_loss: 0.4355\n",
            "Epoch 1169/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1254 - val_loss: 0.4870\n",
            "Epoch 1170/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1812 - val_loss: 0.5955\n",
            "Epoch 1171/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1333 - val_loss: 0.2406\n",
            "Epoch 1172/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1391 - val_loss: 0.6759\n",
            "Epoch 1173/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1319 - val_loss: 0.4171\n",
            "Epoch 1174/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1444 - val_loss: 0.8696\n",
            "Epoch 1175/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1432 - val_loss: 0.5555\n",
            "Epoch 1176/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1466 - val_loss: 0.5375\n",
            "Epoch 1177/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1254 - val_loss: 0.3466\n",
            "Epoch 1178/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1274 - val_loss: 0.5986\n",
            "Epoch 1179/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1445 - val_loss: 0.6018\n",
            "Epoch 1180/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1250 - val_loss: 0.7615\n",
            "Epoch 1181/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1477 - val_loss: 0.6884\n",
            "Epoch 1182/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1405 - val_loss: 0.5104\n",
            "Epoch 1183/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1444 - val_loss: 0.3697\n",
            "Epoch 1184/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1491 - val_loss: 0.5113\n",
            "Epoch 1185/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1414 - val_loss: 0.2082\n",
            "Epoch 1186/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1430 - val_loss: 0.4551\n",
            "Epoch 1187/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1579 - val_loss: 0.5769\n",
            "Epoch 1188/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1702 - val_loss: 0.4070\n",
            "Epoch 1189/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1493 - val_loss: 0.5100\n",
            "Epoch 1190/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1252 - val_loss: 0.5112\n",
            "Epoch 1191/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1336 - val_loss: 0.2129\n",
            "Epoch 1192/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1417 - val_loss: 0.4742\n",
            "Epoch 1193/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1272 - val_loss: 0.6792\n",
            "Epoch 1194/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1354 - val_loss: 0.2428\n",
            "Epoch 1195/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1596 - val_loss: 0.3744\n",
            "Epoch 1196/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1564 - val_loss: 0.3849\n",
            "Epoch 1197/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1434 - val_loss: 0.7022\n",
            "Epoch 1198/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1144 - val_loss: 0.6719\n",
            "Epoch 1199/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1442 - val_loss: 0.4790\n",
            "Epoch 1200/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1845 - val_loss: 0.5990\n",
            "Epoch 1201/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1776 - val_loss: 0.6521\n",
            "Epoch 1202/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1317 - val_loss: 0.2184\n",
            "Epoch 1203/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1418 - val_loss: 0.3812\n",
            "Epoch 1204/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1384 - val_loss: 0.5045\n",
            "Epoch 1205/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1682 - val_loss: 0.4693\n",
            "Epoch 1206/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1536 - val_loss: 0.3971\n",
            "Epoch 1207/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1461 - val_loss: 0.4166\n",
            "Epoch 1208/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1424 - val_loss: 0.4043\n",
            "Epoch 1209/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1476 - val_loss: 0.4173\n",
            "Epoch 1210/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1423 - val_loss: 0.7008\n",
            "Epoch 1211/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1376 - val_loss: 0.6559\n",
            "Epoch 1212/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1234 - val_loss: 0.8767\n",
            "Epoch 1213/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1356 - val_loss: 0.4950\n",
            "Epoch 1214/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1855 - val_loss: 0.3927\n",
            "Epoch 1215/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1409 - val_loss: 0.6694\n",
            "Epoch 1216/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1344 - val_loss: 0.5326\n",
            "Epoch 1217/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1435 - val_loss: 0.4421\n",
            "Epoch 1218/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1301 - val_loss: 0.7458\n",
            "Epoch 1219/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1398 - val_loss: 0.6347\n",
            "Epoch 1220/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1196 - val_loss: 0.5103\n",
            "Epoch 1221/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1404 - val_loss: 0.3830\n",
            "Epoch 1222/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1348 - val_loss: 0.7213\n",
            "Epoch 1223/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1817 - val_loss: 0.5490\n",
            "Epoch 1224/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1449 - val_loss: 0.5987\n",
            "Epoch 1225/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1487 - val_loss: 0.3290\n",
            "Epoch 1226/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1427 - val_loss: 0.6649\n",
            "Epoch 1227/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1220 - val_loss: 0.5259\n",
            "Epoch 1228/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1373 - val_loss: 0.2629\n",
            "Epoch 1229/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1248 - val_loss: 0.5969\n",
            "Epoch 1230/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1179 - val_loss: 0.4832\n",
            "Epoch 1231/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1153 - val_loss: 0.6734\n",
            "Epoch 1232/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1646 - val_loss: 0.4233\n",
            "Epoch 1233/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1338 - val_loss: 0.8767\n",
            "Epoch 1234/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1367 - val_loss: 0.5897\n",
            "Epoch 1235/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1169 - val_loss: 0.3139\n",
            "Epoch 1236/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1617 - val_loss: 0.4497\n",
            "Epoch 1237/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1237 - val_loss: 0.7420\n",
            "Epoch 1238/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1415 - val_loss: 0.4094\n",
            "Epoch 1239/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1468 - val_loss: 0.2831\n",
            "Epoch 1240/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1382 - val_loss: 0.5085\n",
            "Epoch 1241/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1381 - val_loss: 0.4297\n",
            "Epoch 1242/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1436 - val_loss: 0.9584\n",
            "Epoch 1243/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1341 - val_loss: 0.5707\n",
            "Epoch 1244/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1517 - val_loss: 0.4398\n",
            "Epoch 1245/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1576 - val_loss: 0.6712\n",
            "Epoch 1246/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1675 - val_loss: 0.5798\n",
            "Epoch 1247/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1737 - val_loss: 0.6974\n",
            "Epoch 1248/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1352 - val_loss: 0.5173\n",
            "Epoch 1249/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1246 - val_loss: 0.4511\n",
            "Epoch 1250/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1338 - val_loss: 0.7054\n",
            "Epoch 1251/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1282 - val_loss: 0.7303\n",
            "Epoch 1252/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1534 - val_loss: 0.2387\n",
            "Epoch 1253/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1357 - val_loss: 0.3336\n",
            "Epoch 1254/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1327 - val_loss: 0.5541\n",
            "Epoch 1255/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1287 - val_loss: 0.4550\n",
            "Epoch 1256/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1346 - val_loss: 0.6223\n",
            "Epoch 1257/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1270 - val_loss: 0.4164\n",
            "Epoch 1258/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1323 - val_loss: 0.4412\n",
            "Epoch 1259/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1261 - val_loss: 0.4810\n",
            "Epoch 1260/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1748 - val_loss: 0.3536\n",
            "Epoch 1261/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1236 - val_loss: 0.3995\n",
            "Epoch 1262/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1651 - val_loss: 0.3397\n",
            "Epoch 1263/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1546 - val_loss: 0.4420\n",
            "Epoch 1264/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1332 - val_loss: 0.5977\n",
            "Epoch 1265/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1293 - val_loss: 0.4205\n",
            "Epoch 1266/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1348 - val_loss: 0.4913\n",
            "Epoch 1267/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1372 - val_loss: 0.3504\n",
            "Epoch 1268/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1063 - val_loss: 0.4619\n",
            "Epoch 1269/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1331 - val_loss: 0.3258\n",
            "Epoch 1270/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1293 - val_loss: 0.5055\n",
            "Epoch 1271/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1267 - val_loss: 0.5146\n",
            "Epoch 1272/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1377 - val_loss: 0.7611\n",
            "Epoch 1273/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1381 - val_loss: 0.7320\n",
            "Epoch 1274/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1427 - val_loss: 0.7720\n",
            "Epoch 1275/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1179 - val_loss: 0.3191\n",
            "Epoch 1276/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1413 - val_loss: 0.5095\n",
            "Epoch 1277/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1494 - val_loss: 0.6309\n",
            "Epoch 1278/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1530 - val_loss: 0.3689\n",
            "Epoch 1279/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1263 - val_loss: 0.4574\n",
            "Epoch 1280/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1293 - val_loss: 0.3464\n",
            "Epoch 1281/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1472 - val_loss: 0.4819\n",
            "Epoch 1282/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1559 - val_loss: 0.6229\n",
            "Epoch 1283/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1479 - val_loss: 0.7412\n",
            "Epoch 1284/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1614 - val_loss: 0.4774\n",
            "Epoch 1285/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1500 - val_loss: 0.7645\n",
            "Epoch 1286/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1530 - val_loss: 0.5730\n",
            "Epoch 1287/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1056 - val_loss: 0.3499\n",
            "Epoch 1288/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1404 - val_loss: 0.4016\n",
            "Epoch 1289/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1233 - val_loss: 0.4757\n",
            "Epoch 1290/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1277 - val_loss: 0.4707\n",
            "Epoch 1291/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1175 - val_loss: 0.3589\n",
            "Epoch 1292/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1338 - val_loss: 0.4064\n",
            "Epoch 1293/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1476 - val_loss: 0.4349\n",
            "Epoch 1294/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1542 - val_loss: 0.5150\n",
            "Epoch 1295/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1325 - val_loss: 0.5346\n",
            "Epoch 1296/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0923 - val_loss: 0.4532\n",
            "Epoch 1297/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1386 - val_loss: 0.5253\n",
            "Epoch 1298/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1564 - val_loss: 0.6660\n",
            "Epoch 1299/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1280 - val_loss: 0.5676\n",
            "Epoch 1300/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1170 - val_loss: 0.6589\n",
            "Epoch 1301/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1143 - val_loss: 0.7486\n",
            "Epoch 1302/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1194 - val_loss: 0.5231\n",
            "Epoch 1303/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1271 - val_loss: 0.5246\n",
            "Epoch 1304/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1373 - val_loss: 0.3700\n",
            "Epoch 1305/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1496 - val_loss: 0.6141\n",
            "Epoch 1306/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1488 - val_loss: 0.8451\n",
            "Epoch 1307/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1310 - val_loss: 0.5540\n",
            "Epoch 1308/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1633 - val_loss: 0.3106\n",
            "Epoch 1309/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1675 - val_loss: 0.3970\n",
            "Epoch 1310/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1510 - val_loss: 0.3935\n",
            "Epoch 1311/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1553 - val_loss: 0.5950\n",
            "Epoch 1312/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1313 - val_loss: 0.5399\n",
            "Epoch 1313/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1126 - val_loss: 0.6788\n",
            "Epoch 1314/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1272 - val_loss: 0.4297\n",
            "Epoch 1315/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1380 - val_loss: 0.6263\n",
            "Epoch 1316/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1107 - val_loss: 0.5000\n",
            "Epoch 1317/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1210 - val_loss: 0.3566\n",
            "Epoch 1318/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1407 - val_loss: 0.5205\n",
            "Epoch 1319/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1549 - val_loss: 0.6879\n",
            "Epoch 1320/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1589 - val_loss: 0.6231\n",
            "Epoch 1321/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1418 - val_loss: 0.7396\n",
            "Epoch 1322/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1660 - val_loss: 0.5413\n",
            "Epoch 1323/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1164 - val_loss: 0.4775\n",
            "Epoch 1324/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1654 - val_loss: 0.3651\n",
            "Epoch 1325/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1545 - val_loss: 0.5419\n",
            "Epoch 1326/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1308 - val_loss: 0.4042\n",
            "Epoch 1327/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1109 - val_loss: 0.4805\n",
            "Epoch 1328/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1268 - val_loss: 0.5243\n",
            "Epoch 1329/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1344 - val_loss: 0.6147\n",
            "Epoch 1330/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1283 - val_loss: 0.6278\n",
            "Epoch 1331/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1496 - val_loss: 1.0714\n",
            "Epoch 1332/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1387 - val_loss: 0.2769\n",
            "Epoch 1333/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1198 - val_loss: 0.4969\n",
            "Epoch 1334/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1275 - val_loss: 0.5955\n",
            "Epoch 1335/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1294 - val_loss: 0.7700\n",
            "Epoch 1336/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1249 - val_loss: 0.4147\n",
            "Epoch 1337/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1137 - val_loss: 0.3675\n",
            "Epoch 1338/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1318 - val_loss: 0.5322\n",
            "Epoch 1339/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1343 - val_loss: 0.8123\n",
            "Epoch 1340/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1558 - val_loss: 0.3601\n",
            "Epoch 1341/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1305 - val_loss: 0.7936\n",
            "Epoch 1342/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1384 - val_loss: 0.6141\n",
            "Epoch 1343/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1045 - val_loss: 0.7266\n",
            "Epoch 1344/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1432 - val_loss: 0.5491\n",
            "Epoch 1345/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1402 - val_loss: 0.5055\n",
            "Epoch 1346/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1095 - val_loss: 0.4237\n",
            "Epoch 1347/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1412 - val_loss: 0.4471\n",
            "Epoch 1348/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1901 - val_loss: 0.7280\n",
            "Epoch 1349/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1214 - val_loss: 0.5192\n",
            "Epoch 1350/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1462 - val_loss: 0.5479\n",
            "Epoch 1351/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1212 - val_loss: 0.5595\n",
            "Epoch 1352/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1262 - val_loss: 0.7503\n",
            "Epoch 1353/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1570 - val_loss: 0.4480\n",
            "Epoch 1354/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2072 - val_loss: 0.4350\n",
            "Epoch 1355/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1413 - val_loss: 0.5857\n",
            "Epoch 1356/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1749 - val_loss: 0.4117\n",
            "Epoch 1357/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1415 - val_loss: 0.2870\n",
            "Epoch 1358/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1436 - val_loss: 0.2281\n",
            "Epoch 1359/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1196 - val_loss: 0.5867\n",
            "Epoch 1360/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1360 - val_loss: 0.7811\n",
            "Epoch 1361/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1273 - val_loss: 0.7034\n",
            "Epoch 1362/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1327 - val_loss: 0.3824\n",
            "Epoch 1363/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1363 - val_loss: 0.4181\n",
            "Epoch 1364/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1398 - val_loss: 0.4619\n",
            "Epoch 1365/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1395 - val_loss: 0.6021\n",
            "Epoch 1366/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1336 - val_loss: 0.3153\n",
            "Epoch 1367/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1547 - val_loss: 0.5178\n",
            "Epoch 1368/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1198 - val_loss: 0.8159\n",
            "Epoch 1369/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1336 - val_loss: 0.4295\n",
            "Epoch 1370/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1297 - val_loss: 0.5725\n",
            "Epoch 1371/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1461 - val_loss: 0.7136\n",
            "Epoch 1372/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1333 - val_loss: 0.4827\n",
            "Epoch 1373/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1616 - val_loss: 0.4057\n",
            "Epoch 1374/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1489 - val_loss: 0.4138\n",
            "Epoch 1375/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1215 - val_loss: 0.5825\n",
            "Epoch 1376/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1155 - val_loss: 0.6192\n",
            "Epoch 1377/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1279 - val_loss: 0.5632\n",
            "Epoch 1378/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1176 - val_loss: 0.7714\n",
            "Epoch 1379/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1253 - val_loss: 0.4351\n",
            "Epoch 1380/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1505 - val_loss: 0.5217\n",
            "Epoch 1381/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1180 - val_loss: 0.5942\n",
            "Epoch 1382/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1510 - val_loss: 0.5098\n",
            "Epoch 1383/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1240 - val_loss: 0.5847\n",
            "Epoch 1384/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1506 - val_loss: 0.8090\n",
            "Epoch 1385/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1358 - val_loss: 0.3346\n",
            "Epoch 1386/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1260 - val_loss: 0.8685\n",
            "Epoch 1387/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1590 - val_loss: 0.8020\n",
            "Epoch 1388/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1502 - val_loss: 0.6465\n",
            "Epoch 1389/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1328 - val_loss: 0.4587\n",
            "Epoch 1390/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1422 - val_loss: 0.4804\n",
            "Epoch 1391/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1281 - val_loss: 1.0177\n",
            "Epoch 1392/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1721 - val_loss: 0.5466\n",
            "Epoch 1393/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1256 - val_loss: 0.4218\n",
            "Epoch 1394/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1270 - val_loss: 0.4231\n",
            "Epoch 1395/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1340 - val_loss: 0.7248\n",
            "Epoch 1396/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1288 - val_loss: 0.6236\n",
            "Epoch 1397/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1483 - val_loss: 0.4427\n",
            "Epoch 1398/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1387 - val_loss: 0.5203\n",
            "Epoch 1399/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1372 - val_loss: 0.4314\n",
            "Epoch 1400/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1281 - val_loss: 0.3913\n",
            "Epoch 1401/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1275 - val_loss: 0.4494\n",
            "Epoch 1402/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1452 - val_loss: 0.6309\n",
            "Epoch 1403/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1545 - val_loss: 0.4523\n",
            "Epoch 1404/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1467 - val_loss: 0.6610\n",
            "Epoch 1405/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1328 - val_loss: 0.4512\n",
            "Epoch 1406/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1099 - val_loss: 0.4906\n",
            "Epoch 1407/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1085 - val_loss: 0.4950\n",
            "Epoch 1408/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1236 - val_loss: 0.4800\n",
            "Epoch 1409/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1315 - val_loss: 0.5716\n",
            "Epoch 1410/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1158 - val_loss: 0.5869\n",
            "Epoch 1411/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1262 - val_loss: 0.6968\n",
            "Epoch 1412/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1329 - val_loss: 0.4678\n",
            "Epoch 1413/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1115 - val_loss: 0.6263\n",
            "Epoch 1414/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1381 - val_loss: 0.5564\n",
            "Epoch 1415/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1223 - val_loss: 0.8222\n",
            "Epoch 1416/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1286 - val_loss: 0.3827\n",
            "Epoch 1417/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1158 - val_loss: 0.4493\n",
            "Epoch 1418/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1284 - val_loss: 0.7529\n",
            "Epoch 1419/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1254 - val_loss: 0.4083\n",
            "Epoch 1420/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1187 - val_loss: 0.5406\n",
            "Epoch 1421/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1328 - val_loss: 0.6043\n",
            "Epoch 1422/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1139 - val_loss: 0.3566\n",
            "Epoch 1423/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1343 - val_loss: 0.5195\n",
            "Epoch 1424/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1262 - val_loss: 0.5627\n",
            "Epoch 1425/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1119 - val_loss: 0.4196\n",
            "Epoch 1426/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1271 - val_loss: 0.4225\n",
            "Epoch 1427/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1498 - val_loss: 0.5785\n",
            "Epoch 1428/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1366 - val_loss: 0.4849\n",
            "Epoch 1429/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1628 - val_loss: 0.6357\n",
            "Epoch 1430/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1416 - val_loss: 1.0485\n",
            "Epoch 1431/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1232 - val_loss: 0.5126\n",
            "Epoch 1432/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1314 - val_loss: 0.6671\n",
            "Epoch 1433/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1182 - val_loss: 0.8128\n",
            "Epoch 1434/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1216 - val_loss: 0.7602\n",
            "Epoch 1435/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1534 - val_loss: 0.5720\n",
            "Epoch 1436/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1440 - val_loss: 0.6548\n",
            "Epoch 1437/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2049 - val_loss: 0.4015\n",
            "Epoch 1438/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1550 - val_loss: 0.3844\n",
            "Epoch 1439/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1471 - val_loss: 0.5708\n",
            "Epoch 1440/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1409 - val_loss: 0.4318\n",
            "Epoch 1441/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1317 - val_loss: 0.5760\n",
            "Epoch 1442/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1249 - val_loss: 0.5534\n",
            "Epoch 1443/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1191 - val_loss: 0.7996\n",
            "Epoch 1444/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1098 - val_loss: 0.4969\n",
            "Epoch 1445/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1377 - val_loss: 0.8356\n",
            "Epoch 1446/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1324 - val_loss: 0.4185\n",
            "Epoch 1447/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1153 - val_loss: 0.8526\n",
            "Epoch 1448/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1237 - val_loss: 0.4720\n",
            "Epoch 1449/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1099 - val_loss: 0.7916\n",
            "Epoch 1450/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1251 - val_loss: 0.3031\n",
            "Epoch 1451/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1410 - val_loss: 0.7142\n",
            "Epoch 1452/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1437 - val_loss: 0.5093\n",
            "Epoch 1453/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1230 - val_loss: 0.5812\n",
            "Epoch 1454/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1267 - val_loss: 0.9054\n",
            "Epoch 1455/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1324 - val_loss: 0.5329\n",
            "Epoch 1456/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1156 - val_loss: 0.3849\n",
            "Epoch 1457/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1294 - val_loss: 0.4709\n",
            "Epoch 1458/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1324 - val_loss: 0.5321\n",
            "Epoch 1459/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1274 - val_loss: 0.4197\n",
            "Epoch 1460/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1332 - val_loss: 0.7875\n",
            "Epoch 1461/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1304 - val_loss: 0.5150\n",
            "Epoch 1462/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1550 - val_loss: 0.5035\n",
            "Epoch 1463/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1582 - val_loss: 0.6079\n",
            "Epoch 1464/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1321 - val_loss: 0.6446\n",
            "Epoch 1465/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1188 - val_loss: 0.7142\n",
            "Epoch 1466/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1153 - val_loss: 0.6000\n",
            "Epoch 1467/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0999 - val_loss: 0.4775\n",
            "Epoch 1468/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1242 - val_loss: 0.3235\n",
            "Epoch 1469/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1156 - val_loss: 0.6636\n",
            "Epoch 1470/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1329 - val_loss: 0.8108\n",
            "Epoch 1471/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1034 - val_loss: 1.1049\n",
            "Epoch 1472/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1415 - val_loss: 0.6879\n",
            "Epoch 1473/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1166 - val_loss: 0.6951\n",
            "Epoch 1474/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1525 - val_loss: 0.6092\n",
            "Epoch 1475/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1334 - val_loss: 0.7777\n",
            "Epoch 1476/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1390 - val_loss: 0.5406\n",
            "Epoch 1477/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1349 - val_loss: 0.5183\n",
            "Epoch 1478/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1103 - val_loss: 0.5671\n",
            "Epoch 1479/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1378 - val_loss: 0.6277\n",
            "Epoch 1480/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1267 - val_loss: 0.4076\n",
            "Epoch 1481/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1315 - val_loss: 0.4869\n",
            "Epoch 1482/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1049 - val_loss: 0.4995\n",
            "Epoch 1483/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1170 - val_loss: 0.4272\n",
            "Epoch 1484/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1269 - val_loss: 0.5040\n",
            "Epoch 1485/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1357 - val_loss: 0.3342\n",
            "Epoch 1486/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1109 - val_loss: 0.4790\n",
            "Epoch 1487/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1339 - val_loss: 0.4660\n",
            "Epoch 1488/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1247 - val_loss: 0.3334\n",
            "Epoch 1489/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1311 - val_loss: 0.6299\n",
            "Epoch 1490/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1318 - val_loss: 0.6058\n",
            "Epoch 1491/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1294 - val_loss: 0.4505\n",
            "Epoch 1492/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1448 - val_loss: 0.4077\n",
            "Epoch 1493/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1324 - val_loss: 0.7649\n",
            "Epoch 1494/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1286 - val_loss: 0.4343\n",
            "Epoch 1495/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1372 - val_loss: 0.5937\n",
            "Epoch 1496/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1338 - val_loss: 0.6268\n",
            "Epoch 1497/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1175 - val_loss: 0.6207\n",
            "Epoch 1498/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1506 - val_loss: 0.5674\n",
            "Epoch 1499/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1110 - val_loss: 0.6486\n",
            "Epoch 1500/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1124 - val_loss: 0.5776\n",
            "Epoch 1501/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1405 - val_loss: 0.4024\n",
            "Epoch 1502/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1156 - val_loss: 0.5086\n",
            "Epoch 1503/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1242 - val_loss: 0.8270\n",
            "Epoch 1504/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1097 - val_loss: 0.6044\n",
            "Epoch 1505/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1352 - val_loss: 0.3362\n",
            "Epoch 1506/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1280 - val_loss: 0.5944\n",
            "Epoch 1507/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1422 - val_loss: 0.4159\n",
            "Epoch 1508/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1486 - val_loss: 0.4385\n",
            "Epoch 1509/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1161 - val_loss: 0.8753\n",
            "Epoch 1510/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1414 - val_loss: 0.3746\n",
            "Epoch 1511/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1539 - val_loss: 0.4022\n",
            "Epoch 1512/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1284 - val_loss: 0.6632\n",
            "Epoch 1513/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1337 - val_loss: 0.7547\n",
            "Epoch 1514/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1211 - val_loss: 0.7666\n",
            "Epoch 1515/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1513 - val_loss: 0.3286\n",
            "Epoch 1516/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1266 - val_loss: 0.9443\n",
            "Epoch 1517/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1322 - val_loss: 0.7353\n",
            "Epoch 1518/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1028 - val_loss: 0.4280\n",
            "Epoch 1519/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1022 - val_loss: 0.4686\n",
            "Epoch 1520/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1601 - val_loss: 0.4875\n",
            "Epoch 1521/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1248 - val_loss: 0.3835\n",
            "Epoch 1522/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1441 - val_loss: 0.5434\n",
            "Epoch 1523/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1055 - val_loss: 0.4106\n",
            "Epoch 1524/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1350 - val_loss: 0.9650\n",
            "Epoch 1525/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1143 - val_loss: 0.6026\n",
            "Epoch 1526/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1305 - val_loss: 0.6699\n",
            "Epoch 1527/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1630 - val_loss: 0.4855\n",
            "Epoch 1528/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1522 - val_loss: 0.3259\n",
            "Epoch 1529/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1292 - val_loss: 0.6149\n",
            "Epoch 1530/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1195 - val_loss: 0.4196\n",
            "Epoch 1531/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1383 - val_loss: 0.6413\n",
            "Epoch 1532/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1140 - val_loss: 0.4740\n",
            "Epoch 1533/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1357 - val_loss: 0.6576\n",
            "Epoch 1534/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1157 - val_loss: 0.5358\n",
            "Epoch 1535/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1269 - val_loss: 0.4234\n",
            "Epoch 1536/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1033 - val_loss: 0.5201\n",
            "Epoch 1537/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1449 - val_loss: 0.7381\n",
            "Epoch 1538/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1287 - val_loss: 0.5157\n",
            "Epoch 1539/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1068 - val_loss: 0.4766\n",
            "Epoch 1540/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1068 - val_loss: 0.4278\n",
            "Epoch 1541/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1299 - val_loss: 0.5119\n",
            "Epoch 1542/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1379 - val_loss: 0.7562\n",
            "Epoch 1543/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1085 - val_loss: 0.3807\n",
            "Epoch 1544/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1346 - val_loss: 0.6912\n",
            "Epoch 1545/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1427 - val_loss: 0.5656\n",
            "Epoch 1546/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0991 - val_loss: 0.8112\n",
            "Epoch 1547/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1274 - val_loss: 0.6816\n",
            "Epoch 1548/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1244 - val_loss: 0.8285\n",
            "Epoch 1549/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1351 - val_loss: 0.3639\n",
            "Epoch 1550/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1194 - val_loss: 0.7252\n",
            "Epoch 1551/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1415 - val_loss: 0.6152\n",
            "Epoch 1552/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1280 - val_loss: 0.3996\n",
            "Epoch 1553/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1370 - val_loss: 0.6337\n",
            "Epoch 1554/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1304 - val_loss: 0.7058\n",
            "Epoch 1555/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1492 - val_loss: 0.6361\n",
            "Epoch 1556/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1314 - val_loss: 0.9584\n",
            "Epoch 1557/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1261 - val_loss: 0.9014\n",
            "Epoch 1558/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1254 - val_loss: 0.5146\n",
            "Epoch 1559/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1131 - val_loss: 1.0312\n",
            "Epoch 1560/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1287 - val_loss: 0.3673\n",
            "Epoch 1561/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1305 - val_loss: 0.9012\n",
            "Epoch 1562/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1539 - val_loss: 0.5185\n",
            "Epoch 1563/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1242 - val_loss: 0.4867\n",
            "Epoch 1564/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1251 - val_loss: 0.9510\n",
            "Epoch 1565/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1156 - val_loss: 0.6649\n",
            "Epoch 1566/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1116 - val_loss: 0.6706\n",
            "Epoch 1567/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1777 - val_loss: 0.5108\n",
            "Epoch 1568/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1292 - val_loss: 0.4054\n",
            "Epoch 1569/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1301 - val_loss: 0.7884\n",
            "Epoch 1570/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1257 - val_loss: 0.5831\n",
            "Epoch 1571/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1024 - val_loss: 0.3878\n",
            "Epoch 1572/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1132 - val_loss: 0.7126\n",
            "Epoch 1573/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1177 - val_loss: 0.5844\n",
            "Epoch 1574/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1215 - val_loss: 0.4549\n",
            "Epoch 1575/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1144 - val_loss: 0.6711\n",
            "Epoch 1576/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1377 - val_loss: 0.6195\n",
            "Epoch 1577/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1204 - val_loss: 0.6105\n",
            "Epoch 1578/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1344 - val_loss: 0.6495\n",
            "Epoch 1579/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1311 - val_loss: 0.5728\n",
            "Epoch 1580/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1078 - val_loss: 0.7522\n",
            "Epoch 1581/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1071 - val_loss: 0.5500\n",
            "Epoch 1582/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1207 - val_loss: 0.7686\n",
            "Epoch 1583/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1097 - val_loss: 0.5555\n",
            "Epoch 1584/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1296 - val_loss: 0.6964\n",
            "Epoch 1585/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1262 - val_loss: 0.8209\n",
            "Epoch 1586/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1406 - val_loss: 0.6144\n",
            "Epoch 1587/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1202 - val_loss: 0.6214\n",
            "Epoch 1588/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1228 - val_loss: 0.6333\n",
            "Epoch 1589/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1371 - val_loss: 0.7867\n",
            "Epoch 1590/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1333 - val_loss: 0.8861\n",
            "Epoch 1591/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1362 - val_loss: 0.7169\n",
            "Epoch 1592/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1206 - val_loss: 0.5839\n",
            "Epoch 1593/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1354 - val_loss: 0.5381\n",
            "Epoch 1594/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1148 - val_loss: 0.8430\n",
            "Epoch 1595/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1180 - val_loss: 0.5641\n",
            "Epoch 1596/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1478 - val_loss: 0.6084\n",
            "Epoch 1597/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1116 - val_loss: 0.3604\n",
            "Epoch 1598/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1391 - val_loss: 0.6845\n",
            "Epoch 1599/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1166 - val_loss: 0.8830\n",
            "Epoch 1600/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1198 - val_loss: 0.4560\n",
            "Epoch 1601/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1248 - val_loss: 0.7597\n",
            "Epoch 1602/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1239 - val_loss: 0.4773\n",
            "Epoch 1603/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1077 - val_loss: 0.7185\n",
            "Epoch 1604/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1294 - val_loss: 0.6583\n",
            "Epoch 1605/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1334 - val_loss: 0.6508\n",
            "Epoch 1606/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1026 - val_loss: 0.9406\n",
            "Epoch 1607/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1315 - val_loss: 0.8835\n",
            "Epoch 1608/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1386 - val_loss: 0.7233\n",
            "Epoch 1609/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1254 - val_loss: 0.8349\n",
            "Epoch 1610/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1038 - val_loss: 0.7776\n",
            "Epoch 1611/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1420 - val_loss: 0.6832\n",
            "Epoch 1612/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1210 - val_loss: 1.0938\n",
            "Epoch 1613/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1168 - val_loss: 0.5155\n",
            "Epoch 1614/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1369 - val_loss: 0.6031\n",
            "Epoch 1615/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1269 - val_loss: 0.4620\n",
            "Epoch 1616/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1181 - val_loss: 0.7843\n",
            "Epoch 1617/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1334 - val_loss: 0.8075\n",
            "Epoch 1618/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1061 - val_loss: 0.6446\n",
            "Epoch 1619/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1210 - val_loss: 0.7032\n",
            "Epoch 1620/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0988 - val_loss: 0.9910\n",
            "Epoch 1621/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1176 - val_loss: 0.3518\n",
            "Epoch 1622/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1169 - val_loss: 0.6586\n",
            "Epoch 1623/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1430 - val_loss: 0.4647\n",
            "Epoch 1624/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1259 - val_loss: 0.4397\n",
            "Epoch 1625/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1353 - val_loss: 0.7732\n",
            "Epoch 1626/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1076 - val_loss: 0.5201\n",
            "Epoch 1627/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1178 - val_loss: 0.6222\n",
            "Epoch 1628/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1174 - val_loss: 0.5404\n",
            "Epoch 1629/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1355 - val_loss: 0.6962\n",
            "Epoch 1630/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1140 - val_loss: 0.6335\n",
            "Epoch 1631/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1092 - val_loss: 0.4383\n",
            "Epoch 1632/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1277 - val_loss: 0.5291\n",
            "Epoch 1633/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1516 - val_loss: 0.7779\n",
            "Epoch 1634/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1464 - val_loss: 0.7803\n",
            "Epoch 1635/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1453 - val_loss: 0.6987\n",
            "Epoch 1636/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1090 - val_loss: 0.6715\n",
            "Epoch 1637/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1272 - val_loss: 1.7668\n",
            "Epoch 1638/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1241 - val_loss: 0.4662\n",
            "Epoch 1639/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0987 - val_loss: 1.1206\n",
            "Epoch 1640/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1232 - val_loss: 0.7587\n",
            "Epoch 1641/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1078 - val_loss: 0.8380\n",
            "Epoch 1642/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1573 - val_loss: 0.4312\n",
            "Epoch 1643/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1329 - val_loss: 0.6996\n",
            "Epoch 1644/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1253 - val_loss: 0.9637\n",
            "Epoch 1645/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1241 - val_loss: 0.7859\n",
            "Epoch 1646/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1055 - val_loss: 0.5608\n",
            "Epoch 1647/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1246 - val_loss: 0.5913\n",
            "Epoch 1648/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1119 - val_loss: 0.5418\n",
            "Epoch 1649/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1300 - val_loss: 0.5766\n",
            "Epoch 1650/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1144 - val_loss: 0.5388\n",
            "Epoch 1651/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1118 - val_loss: 0.7465\n",
            "Epoch 1652/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1146 - val_loss: 0.5952\n",
            "Epoch 1653/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1198 - val_loss: 0.7906\n",
            "Epoch 1654/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1087 - val_loss: 0.7421\n",
            "Epoch 1655/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1101 - val_loss: 0.4826\n",
            "Epoch 1656/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1338 - val_loss: 0.7853\n",
            "Epoch 1657/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1196 - val_loss: 0.3942\n",
            "Epoch 1658/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1163 - val_loss: 0.5469\n",
            "Epoch 1659/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1436 - val_loss: 1.1632\n",
            "Epoch 1660/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1259 - val_loss: 0.6420\n",
            "Epoch 1661/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1353 - val_loss: 0.6403\n",
            "Epoch 1662/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1161 - val_loss: 0.7198\n",
            "Epoch 1663/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1082 - val_loss: 0.6257\n",
            "Epoch 1664/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0939 - val_loss: 1.3411\n",
            "Epoch 1665/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1396 - val_loss: 0.8427\n",
            "Epoch 1666/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1298 - val_loss: 0.3871\n",
            "Epoch 1667/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1073 - val_loss: 0.8409\n",
            "Epoch 1668/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1082 - val_loss: 0.5374\n",
            "Epoch 1669/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1195 - val_loss: 0.9451\n",
            "Epoch 1670/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1355 - val_loss: 0.6469\n",
            "Epoch 1671/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1203 - val_loss: 0.6927\n",
            "Epoch 1672/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1266 - val_loss: 0.4696\n",
            "Epoch 1673/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1106 - val_loss: 0.7191\n",
            "Epoch 1674/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1677 - val_loss: 1.0370\n",
            "Epoch 1675/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1166 - val_loss: 0.6412\n",
            "Epoch 1676/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1563 - val_loss: 0.6411\n",
            "Epoch 1677/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1204 - val_loss: 0.5419\n",
            "Epoch 1678/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1449 - val_loss: 0.7364\n",
            "Epoch 1679/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1275 - val_loss: 0.6670\n",
            "Epoch 1680/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1278 - val_loss: 0.5404\n",
            "Epoch 1681/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1166 - val_loss: 0.6529\n",
            "Epoch 1682/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1197 - val_loss: 0.5280\n",
            "Epoch 1683/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1443 - val_loss: 0.5774\n",
            "Epoch 1684/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1377 - val_loss: 0.5459\n",
            "Epoch 1685/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1052 - val_loss: 1.2081\n",
            "Epoch 1686/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1168 - val_loss: 0.9423\n",
            "Epoch 1687/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1571 - val_loss: 0.8822\n",
            "Epoch 1688/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1333 - val_loss: 0.7214\n",
            "Epoch 1689/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1380 - val_loss: 0.5109\n",
            "Epoch 1690/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1166 - val_loss: 0.7275\n",
            "Epoch 1691/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1389 - val_loss: 0.5441\n",
            "Epoch 1692/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1112 - val_loss: 0.5684\n",
            "Epoch 1693/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1176 - val_loss: 0.7587\n",
            "Epoch 1694/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1055 - val_loss: 1.0544\n",
            "Epoch 1695/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1216 - val_loss: 0.5127\n",
            "Epoch 1696/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1209 - val_loss: 0.7811\n",
            "Epoch 1697/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1205 - val_loss: 0.6090\n",
            "Epoch 1698/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1074 - val_loss: 0.7172\n",
            "Epoch 1699/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1169 - val_loss: 0.7161\n",
            "Epoch 1700/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1236 - val_loss: 0.5155\n",
            "Epoch 1701/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1213 - val_loss: 0.7371\n",
            "Epoch 1702/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1337 - val_loss: 0.5743\n",
            "Epoch 1703/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1200 - val_loss: 0.8488\n",
            "Epoch 1704/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1544 - val_loss: 0.7186\n",
            "Epoch 1705/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1554 - val_loss: 0.5556\n",
            "Epoch 1706/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1249 - val_loss: 0.8332\n",
            "Epoch 1707/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1268 - val_loss: 0.7512\n",
            "Epoch 1708/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1305 - val_loss: 0.5902\n",
            "Epoch 1709/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1220 - val_loss: 0.7206\n",
            "Epoch 1710/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1230 - val_loss: 1.0668\n",
            "Epoch 1711/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1216 - val_loss: 0.6167\n",
            "Epoch 1712/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0909 - val_loss: 1.0524\n",
            "Epoch 1713/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1431 - val_loss: 0.8791\n",
            "Epoch 1714/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1375 - val_loss: 0.5697\n",
            "Epoch 1715/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1302 - val_loss: 0.6869\n",
            "Epoch 1716/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1419 - val_loss: 0.8710\n",
            "Epoch 1717/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1184 - val_loss: 0.6894\n",
            "Epoch 1718/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1064 - val_loss: 1.0230\n",
            "Epoch 1719/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0996 - val_loss: 0.7392\n",
            "Epoch 1720/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1001 - val_loss: 0.5113\n",
            "Epoch 1721/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1179 - val_loss: 0.6761\n",
            "Epoch 1722/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1221 - val_loss: 0.8280\n",
            "Epoch 1723/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1055 - val_loss: 1.0508\n",
            "Epoch 1724/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1198 - val_loss: 0.7712\n",
            "Epoch 1725/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1049 - val_loss: 0.5776\n",
            "Epoch 1726/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1363 - val_loss: 0.9244\n",
            "Epoch 1727/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1110 - val_loss: 0.9487\n",
            "Epoch 1728/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1156 - val_loss: 0.5986\n",
            "Epoch 1729/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1370 - val_loss: 0.7346\n",
            "Epoch 1730/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1327 - val_loss: 0.5041\n",
            "Epoch 1731/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1076 - val_loss: 0.5692\n",
            "Epoch 1732/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1076 - val_loss: 0.6053\n",
            "Epoch 1733/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1088 - val_loss: 0.6166\n",
            "Epoch 1734/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1238 - val_loss: 0.5927\n",
            "Epoch 1735/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1512 - val_loss: 0.4452\n",
            "Epoch 1736/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1286 - val_loss: 0.7180\n",
            "Epoch 1737/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1202 - val_loss: 0.5932\n",
            "Epoch 1738/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1116 - val_loss: 0.6699\n",
            "Epoch 1739/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1185 - val_loss: 1.0431\n",
            "Epoch 1740/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1072 - val_loss: 0.8097\n",
            "Epoch 1741/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0865 - val_loss: 0.5242\n",
            "Epoch 1742/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0986 - val_loss: 0.8963\n",
            "Epoch 1743/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1254 - val_loss: 0.6141\n",
            "Epoch 1744/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0941 - val_loss: 0.8630\n",
            "Epoch 1745/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1015 - val_loss: 0.6652\n",
            "Epoch 1746/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1007 - val_loss: 1.0851\n",
            "Epoch 1747/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1542 - val_loss: 0.8448\n",
            "Epoch 1748/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1302 - val_loss: 0.6782\n",
            "Epoch 1749/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1227 - val_loss: 0.9866\n",
            "Epoch 1750/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1040 - val_loss: 0.8798\n",
            "Epoch 1751/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1506 - val_loss: 0.7311\n",
            "Epoch 1752/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1106 - val_loss: 0.6763\n",
            "Epoch 1753/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1271 - val_loss: 0.8626\n",
            "Epoch 1754/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1305 - val_loss: 0.3556\n",
            "Epoch 1755/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1291 - val_loss: 0.9938\n",
            "Epoch 1756/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1122 - val_loss: 0.6659\n",
            "Epoch 1757/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1432 - val_loss: 0.5578\n",
            "Epoch 1758/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1228 - val_loss: 0.9117\n",
            "Epoch 1759/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1275 - val_loss: 0.7403\n",
            "Epoch 1760/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1079 - val_loss: 0.8603\n",
            "Epoch 1761/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1144 - val_loss: 1.2925\n",
            "Epoch 1762/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1290 - val_loss: 0.8027\n",
            "Epoch 1763/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1298 - val_loss: 0.7936\n",
            "Epoch 1764/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1009 - val_loss: 0.3896\n",
            "Epoch 1765/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1367 - val_loss: 0.5857\n",
            "Epoch 1766/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1271 - val_loss: 0.6086\n",
            "Epoch 1767/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1153 - val_loss: 1.2954\n",
            "Epoch 1768/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1324 - val_loss: 0.5222\n",
            "Epoch 1769/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1096 - val_loss: 0.7547\n",
            "Epoch 1770/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1219 - val_loss: 0.5578\n",
            "Epoch 1771/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1421 - val_loss: 1.0099\n",
            "Epoch 1772/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1042 - val_loss: 1.2485\n",
            "Epoch 1773/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1300 - val_loss: 1.1668\n",
            "Epoch 1774/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1359 - val_loss: 0.8994\n",
            "Epoch 1775/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1154 - val_loss: 0.5852\n",
            "Epoch 1776/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1228 - val_loss: 1.2393\n",
            "Epoch 1777/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1202 - val_loss: 0.8794\n",
            "Epoch 1778/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1168 - val_loss: 0.9798\n",
            "Epoch 1779/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1267 - val_loss: 0.9749\n",
            "Epoch 1780/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1390 - val_loss: 0.9755\n",
            "Epoch 1781/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1088 - val_loss: 0.5699\n",
            "Epoch 1782/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1335 - val_loss: 1.0836\n",
            "Epoch 1783/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1281 - val_loss: 0.4443\n",
            "Epoch 1784/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1132 - val_loss: 0.7791\n",
            "Epoch 1785/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1084 - val_loss: 0.8154\n",
            "Epoch 1786/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1403 - val_loss: 1.7621\n",
            "Epoch 1787/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1145 - val_loss: 0.4370\n",
            "Epoch 1788/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1605 - val_loss: 0.5876\n",
            "Epoch 1789/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1238 - val_loss: 0.4547\n",
            "Epoch 1790/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1050 - val_loss: 0.5271\n",
            "Epoch 1791/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1539 - val_loss: 0.6766\n",
            "Epoch 1792/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1178 - val_loss: 0.5209\n",
            "Epoch 1793/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1297 - val_loss: 0.8187\n",
            "Epoch 1794/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1370 - val_loss: 1.0459\n",
            "Epoch 1795/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1288 - val_loss: 0.4594\n",
            "Epoch 1796/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1227 - val_loss: 1.0895\n",
            "Epoch 1797/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1183 - val_loss: 0.5256\n",
            "Epoch 1798/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1140 - val_loss: 0.7313\n",
            "Epoch 1799/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1021 - val_loss: 0.6275\n",
            "Epoch 1800/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1403 - val_loss: 0.5267\n",
            "Epoch 1801/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1339 - val_loss: 0.5707\n",
            "Epoch 1802/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1288 - val_loss: 1.0020\n",
            "Epoch 1803/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1324 - val_loss: 1.2894\n",
            "Epoch 1804/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1307 - val_loss: 0.5434\n",
            "Epoch 1805/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1278 - val_loss: 0.7791\n",
            "Epoch 1806/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1273 - val_loss: 0.4782\n",
            "Epoch 1807/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1008 - val_loss: 0.6809\n",
            "Epoch 1808/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1219 - val_loss: 0.7458\n",
            "Epoch 1809/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1203 - val_loss: 0.8779\n",
            "Epoch 1810/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1049 - val_loss: 0.6544\n",
            "Epoch 1811/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1148 - val_loss: 0.6800\n",
            "Epoch 1812/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1050 - val_loss: 0.6289\n",
            "Epoch 1813/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1116 - val_loss: 0.7769\n",
            "Epoch 1814/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1188 - val_loss: 0.5726\n",
            "Epoch 1815/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1193 - val_loss: 0.6392\n",
            "Epoch 1816/10000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1321 - val_loss: 1.0529\n",
            "Epoch 1817/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1193 - val_loss: 0.7141\n",
            "Epoch 1818/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1144 - val_loss: 0.5909\n",
            "Epoch 1819/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1392 - val_loss: 0.6328\n",
            "Epoch 1820/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1129 - val_loss: 0.6816\n",
            "Epoch 1821/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1178 - val_loss: 1.5037\n",
            "Epoch 1822/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1159 - val_loss: 0.8840\n",
            "Epoch 1823/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1291 - val_loss: 0.7886\n",
            "Epoch 1824/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1060 - val_loss: 0.9531\n",
            "Epoch 1825/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1147 - val_loss: 0.7497\n",
            "Epoch 1826/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1063 - val_loss: 1.0330\n",
            "Epoch 1827/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1316 - val_loss: 0.7615\n",
            "Epoch 1828/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1372 - val_loss: 0.9305\n",
            "Epoch 1829/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1455 - val_loss: 0.9290\n",
            "Epoch 1830/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1157 - val_loss: 0.5853\n",
            "Epoch 1831/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1219 - val_loss: 0.6198\n",
            "Epoch 1832/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1133 - val_loss: 0.6267\n",
            "Epoch 1833/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1510 - val_loss: 0.9437\n",
            "Epoch 1834/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1471 - val_loss: 0.5640\n",
            "Epoch 1835/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1284 - val_loss: 0.7589\n",
            "Epoch 1836/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1034 - val_loss: 0.7037\n",
            "Epoch 1837/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1490 - val_loss: 0.6179\n",
            "Epoch 1838/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1612 - val_loss: 0.7873\n",
            "Epoch 1839/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1363 - val_loss: 0.7505\n",
            "Epoch 1840/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1229 - val_loss: 0.5284\n",
            "Epoch 1841/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1046 - val_loss: 0.4754\n",
            "Epoch 1842/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1113 - val_loss: 0.4532\n",
            "Epoch 1843/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1229 - val_loss: 0.5602\n",
            "Epoch 1844/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1502 - val_loss: 0.6966\n",
            "Epoch 1845/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1328 - val_loss: 0.3605\n",
            "Epoch 1846/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1247 - val_loss: 0.5094\n",
            "Epoch 1847/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1212 - val_loss: 0.6645\n",
            "Epoch 1848/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1259 - val_loss: 1.0830\n",
            "Epoch 1849/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1166 - val_loss: 0.4561\n",
            "Epoch 1850/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1062 - val_loss: 0.6168\n",
            "Epoch 1851/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1025 - val_loss: 0.7553\n",
            "Epoch 1852/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1160 - val_loss: 0.5910\n",
            "Epoch 1853/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1170 - val_loss: 0.5356\n",
            "Epoch 1854/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1385 - val_loss: 0.5912\n",
            "Epoch 1855/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0961 - val_loss: 0.4012\n",
            "Epoch 1856/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1221 - val_loss: 0.5955\n",
            "Epoch 1857/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1077 - val_loss: 0.5112\n",
            "Epoch 1858/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1294 - val_loss: 0.5798\n",
            "Epoch 1859/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1228 - val_loss: 0.8652\n",
            "Epoch 1860/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1318 - val_loss: 0.5618\n",
            "Epoch 1861/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1100 - val_loss: 0.5259\n",
            "Epoch 1862/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1293 - val_loss: 0.9477\n",
            "Epoch 1863/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1017 - val_loss: 0.7046\n",
            "Epoch 1864/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1176 - val_loss: 0.7663\n",
            "Epoch 1865/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1262 - val_loss: 0.8530\n",
            "Epoch 1866/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1092 - val_loss: 0.8548\n",
            "Epoch 1867/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1047 - val_loss: 0.4519\n",
            "Epoch 1868/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1049 - val_loss: 0.7624\n",
            "Epoch 1869/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1200 - val_loss: 0.5502\n",
            "Epoch 1870/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1294 - val_loss: 0.4415\n",
            "Epoch 1871/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1322 - val_loss: 1.6235\n",
            "Epoch 1872/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1164 - val_loss: 0.5976\n",
            "Epoch 1873/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1237 - val_loss: 0.9792\n",
            "Epoch 1874/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1330 - val_loss: 0.8624\n",
            "Epoch 1875/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1083 - val_loss: 0.7428\n",
            "Epoch 1876/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1062 - val_loss: 0.6013\n",
            "Epoch 1877/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0872 - val_loss: 0.6987\n",
            "Epoch 1878/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1210 - val_loss: 0.4191\n",
            "Epoch 1879/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1229 - val_loss: 1.0554\n",
            "Epoch 1880/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1068 - val_loss: 0.7066\n",
            "Epoch 1881/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1068 - val_loss: 0.8333\n",
            "Epoch 1882/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1248 - val_loss: 0.5076\n",
            "Epoch 1883/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1171 - val_loss: 1.2501\n",
            "Epoch 1884/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1288 - val_loss: 0.5501\n",
            "Epoch 1885/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1240 - val_loss: 0.4633\n",
            "Epoch 1886/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1394 - val_loss: 0.7822\n",
            "Epoch 1887/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0945 - val_loss: 0.5920\n",
            "Epoch 1888/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1131 - val_loss: 0.4814\n",
            "Epoch 1889/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1125 - val_loss: 1.0024\n",
            "Epoch 1890/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1377 - val_loss: 0.8379\n",
            "Epoch 1891/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1332 - val_loss: 0.5857\n",
            "Epoch 1892/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1209 - val_loss: 0.7300\n",
            "Epoch 1893/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0990 - val_loss: 0.7316\n",
            "Epoch 1894/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1078 - val_loss: 0.5423\n",
            "Epoch 1895/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1312 - val_loss: 0.5109\n",
            "Epoch 1896/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1352 - val_loss: 0.2854\n",
            "Epoch 1897/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1155 - val_loss: 1.0967\n",
            "Epoch 1898/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1297 - val_loss: 0.7534\n",
            "Epoch 1899/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1073 - val_loss: 0.6910\n",
            "Epoch 1900/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1171 - val_loss: 0.6001\n",
            "Epoch 1901/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0939 - val_loss: 0.9003\n",
            "Epoch 1902/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1028 - val_loss: 0.7685\n",
            "Epoch 1903/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0983 - val_loss: 0.4655\n",
            "Epoch 1904/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1127 - val_loss: 0.5196\n",
            "Epoch 1905/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1220 - val_loss: 0.8828\n",
            "Epoch 1906/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1297 - val_loss: 0.4508\n",
            "Epoch 1907/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1227 - val_loss: 0.4759\n",
            "Epoch 1908/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1263 - val_loss: 0.4459\n",
            "Epoch 1909/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1218 - val_loss: 1.2203\n",
            "Epoch 1910/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1240 - val_loss: 0.6758\n",
            "Epoch 1911/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1054 - val_loss: 0.5455\n",
            "Epoch 1912/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1212 - val_loss: 0.4452\n",
            "Epoch 1913/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1430 - val_loss: 0.4228\n",
            "Epoch 1914/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1178 - val_loss: 0.7328\n",
            "Epoch 1915/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1101 - val_loss: 0.5390\n",
            "Epoch 1916/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1297 - val_loss: 0.5663\n",
            "Epoch 1917/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1315 - val_loss: 0.5971\n",
            "Epoch 1918/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1317 - val_loss: 0.6938\n",
            "Epoch 1919/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1478 - val_loss: 0.7195\n",
            "Epoch 1920/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1422 - val_loss: 0.4341\n",
            "Epoch 1921/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1151 - val_loss: 0.5916\n",
            "Epoch 1922/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1002 - val_loss: 0.5746\n",
            "Epoch 1923/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1410 - val_loss: 0.4892\n",
            "Epoch 1924/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1294 - val_loss: 0.6076\n",
            "Epoch 1925/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1328 - val_loss: 0.5592\n",
            "Epoch 1926/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1093 - val_loss: 0.6546\n",
            "Epoch 1927/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1172 - val_loss: 1.0819\n",
            "Epoch 1928/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1029 - val_loss: 0.5797\n",
            "Epoch 1929/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1128 - val_loss: 0.5635\n",
            "Epoch 1930/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1133 - val_loss: 0.8587\n",
            "Epoch 1931/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1486 - val_loss: 0.6722\n",
            "Epoch 1932/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1565 - val_loss: 0.6588\n",
            "Epoch 1933/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1123 - val_loss: 0.9212\n",
            "Epoch 1934/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1244 - val_loss: 0.8712\n",
            "Epoch 1935/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1365 - val_loss: 0.8022\n",
            "Epoch 1936/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1244 - val_loss: 0.9559\n",
            "Epoch 1937/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1083 - val_loss: 0.6813\n",
            "Epoch 1938/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1082 - val_loss: 0.6025\n",
            "Epoch 1939/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1205 - val_loss: 0.9418\n",
            "Epoch 1940/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1016 - val_loss: 1.1616\n",
            "Epoch 1941/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1223 - val_loss: 0.8359\n",
            "Epoch 1942/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1141 - val_loss: 1.0435\n",
            "Epoch 1943/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0955 - val_loss: 0.5303\n",
            "Epoch 1944/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1234 - val_loss: 0.5965\n",
            "Epoch 1945/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1461 - val_loss: 0.8930\n",
            "Epoch 1946/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1185 - val_loss: 0.4913\n",
            "Epoch 1947/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1012 - val_loss: 0.6995\n",
            "Epoch 1948/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1272 - val_loss: 0.7863\n",
            "Epoch 1949/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1239 - val_loss: 0.3646\n",
            "Epoch 1950/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1325 - val_loss: 0.5055\n",
            "Epoch 1951/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1188 - val_loss: 0.8777\n",
            "Epoch 1952/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1236 - val_loss: 0.4743\n",
            "Epoch 1953/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1025 - val_loss: 0.6658\n",
            "Epoch 1954/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1165 - val_loss: 0.6486\n",
            "Epoch 1955/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1288 - val_loss: 1.4407\n",
            "Epoch 1956/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1160 - val_loss: 0.6605\n",
            "Epoch 1957/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1246 - val_loss: 0.7354\n",
            "Epoch 1958/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1171 - val_loss: 0.4898\n",
            "Epoch 1959/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1056 - val_loss: 0.5730\n",
            "Epoch 1960/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1140 - val_loss: 0.5147\n",
            "Epoch 1961/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0920 - val_loss: 0.7574\n",
            "Epoch 1962/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1287 - val_loss: 0.4825\n",
            "Epoch 1963/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1039 - val_loss: 0.5678\n",
            "Epoch 1964/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0946 - val_loss: 0.7674\n",
            "Epoch 1965/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1278 - val_loss: 0.4385\n",
            "Epoch 1966/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0956 - val_loss: 1.2039\n",
            "Epoch 1967/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1398 - val_loss: 0.7050\n",
            "Epoch 1968/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1145 - val_loss: 0.9781\n",
            "Epoch 1969/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1079 - val_loss: 1.0747\n",
            "Epoch 1970/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1441 - val_loss: 0.9583\n",
            "Epoch 1971/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1217 - val_loss: 0.7009\n",
            "Epoch 1972/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0953 - val_loss: 0.9958\n",
            "Epoch 1973/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1338 - val_loss: 0.7318\n",
            "Epoch 1974/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1336 - val_loss: 0.8166\n",
            "Epoch 1975/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1116 - val_loss: 0.5159\n",
            "Epoch 1976/10000\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0989 - val_loss: 0.4758\n",
            "Epoch 1977/10000\n",
            " 1/33 [..............................] - ETA: 0s - loss: 0.0715Restoring model weights from the end of the best epoch: 977.\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0891 - val_loss: 0.7730\n",
            "Epoch 1977: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_2707984/581984845.py:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.16975589096546173\n",
            "Train loss: 0.15299665927886963\n",
            "Test loss: 0.31176942586898804\n",
            "dO18 RMSE: 0.3363789650415229\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0          26.130            0.19025\n",
            "1          26.984            0.40123\n",
            "2          24.656            0.17728\n",
            "3          26.356            0.03953\n",
            "4          23.962            0.30992\n",
            "5          24.848            0.15842\n",
            "6          25.080            0.05125\n",
            "7          26.546            2.14378\n",
            "8          25.682            0.94472\n",
            "9          24.028            0.45852\n",
            "10         23.944            0.15813\n",
            "11         23.752            0.52437\n",
            "12         26.018            0.96417\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       26.093887           0.274173\n",
            "1       27.036869           0.456736\n",
            "2       24.532656           0.252093\n",
            "3       26.261192           0.121680\n",
            "4       23.854376           0.422971\n",
            "5       24.554884           0.128348\n",
            "6       24.822388           0.100565\n",
            "7       27.004000           3.493802\n",
            "8       25.653076           0.842501\n",
            "9       23.896856           0.270837\n",
            "10      24.435169           0.289799\n",
            "11      23.434578           0.635378\n",
            "12      25.175983           0.629401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-02 23:57:53.130128: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [13,11]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/usr/local/google/home/ruru/Downloads/model_builds/random_ablated_boosted_transformer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Grouped, fixed, all columns\n",
        "\n",
        "We can't (easily) generate isoscapes with these because the isoscapes for 'predkrig_br_lat_ISORG', 'Iso_Oxi_Stack_mean_TERZER',\n",
        "                   'isoscape_fullmodel_d18O_prec_REGRESSION' are not easily retrievable... but I'm curious how much better the model is if these columns are included."
      ],
      "metadata": {
        "id": "wPDSSm__DR53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_fixed_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_train_fixed_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_test_fixed_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_validation_fixed_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_normalize = []\n",
        "columns_to_standardize = [\n",
        "    'lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature','Mean Annual Precipitation',\n",
        "    'Iso_Oxi_Stack_mean_TERZER',\n",
        "    'isoscape_fullmodel_d18O_prec_REGRESSION']\n",
        "\n",
        "data = load_and_scale(grouped_fixed_fileset, columns_to_normalize, columns_to_standardize)\n",
        "model = train_and_evaluate(data, \"fixed_all_boosted\", training_batch_size=3)\n",
        "model.save(get_model_save_location(\"fixed_all_boosted.keras\"), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location('fixed_all_boosted_transformer.pkl'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V9_5iUkDVoCV",
        "outputId": "5e8094ce-75f9-412b-adea-9d1aa011e567"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         lat       long      VPD       RH       PET  DEM          PA  \\\n",
            "0  -3.995545 -57.589273  0.85167  0.77358  99.99167   61  1005.67358   \n",
            "1  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "2  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "3  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "4  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "..       ...        ...      ...      ...       ...  ...         ...   \n",
            "64 -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "65 -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "66 -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "67 -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "68 -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  \\\n",
            "0                  27.16667                       2273   \n",
            "1                  26.29583                       1897   \n",
            "2                  26.29583                       1897   \n",
            "3                  26.29583                       1897   \n",
            "4                  26.29583                       1897   \n",
            "..                      ...                        ...   \n",
            "64                 26.00000                       1840   \n",
            "65                 26.29583                       1897   \n",
            "66                 26.29583                       1897   \n",
            "67                 26.29583                       1897   \n",
            "68                 26.29583                       1897   \n",
            "\n",
            "    Iso_Oxi_Stack_mean_TERZER  isoscape_fullmodel_d18O_prec_REGRESSION  \\\n",
            "0                    -3.51406                                 -3.73041   \n",
            "1                    -3.27639                                 -3.48101   \n",
            "2                    -3.27639                                 -3.48101   \n",
            "3                    -3.27639                                 -3.48101   \n",
            "4                    -3.27639                                 -3.48101   \n",
            "..                        ...                                      ...   \n",
            "64                   -3.30055                                 -3.42629   \n",
            "65                   -3.27639                                 -3.48101   \n",
            "66                   -3.27639                                 -3.48101   \n",
            "67                   -3.27639                                 -3.48101   \n",
            "68                   -3.27639                                 -3.48101   \n",
            "\n",
            "    krig_mean_residual  krig_variance_residual  \n",
            "0            -0.827845                0.728384  \n",
            "1            -0.941845                0.771084  \n",
            "2            -0.369845                0.439054  \n",
            "3            -0.963845                0.900874  \n",
            "4            -0.615845                0.617224  \n",
            "..                 ...                     ...  \n",
            "64            0.760155                1.046004  \n",
            "65           -0.621845                0.289584  \n",
            "66           -1.175845                1.093624  \n",
            "67           -0.209845                1.070804  \n",
            "68           -1.063845                0.859824  \n",
            "\n",
            "[68 rows x 13 columns]\n",
            "         lat       long      VPD       RH       PET  DEM          PA  \\\n",
            "0  -0.121000 -67.013000  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "1  -0.041000 -66.872000  0.62083  0.82559  92.27500   93  1001.84741   \n",
            "2  -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "3  -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "4  -4.303698 -70.291068  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "5  -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "6  -4.303698 -70.291068  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "7  -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "8  -4.303698 -70.291068  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "9  -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "10 -4.303698 -70.291068  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "11 -3.907000 -66.055000  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "12 -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "13 -4.303698 -70.291068  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "14 -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "15 -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "16 -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "17 -3.907000 -66.055000  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "18 -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "19 -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "20 -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "21 -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "22 -4.303698 -70.291068  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "24 -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "25 -0.121000 -67.013000  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "26 -0.041000 -66.872000  0.62083  0.82559  92.27500   93  1001.84741   \n",
            "27 -3.907000 -66.055000  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "28 -4.304000 -70.291000  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "29 -4.304000 -70.291000  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "30 -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "31 -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "32 -4.303698 -70.291068  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "33 -3.758534 -66.073754  0.64667  0.82247  90.44167   84  1002.92230   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  \\\n",
            "0                  26.20833                       2830   \n",
            "1                  26.37500                       2764   \n",
            "2                  26.20833                       2830   \n",
            "3                  26.71667                       2795   \n",
            "4                  26.64583                       2708   \n",
            "5                  26.20833                       2830   \n",
            "6                  26.64583                       2708   \n",
            "7                  26.20833                       2830   \n",
            "8                  26.64583                       2708   \n",
            "9                  26.20833                       2830   \n",
            "10                 26.64583                       2708   \n",
            "11                 26.71667                       2795   \n",
            "12                 26.20833                       2830   \n",
            "13                 26.64583                       2708   \n",
            "14                 26.20833                       2830   \n",
            "15                 26.20833                       2830   \n",
            "16                 26.20833                       2830   \n",
            "17                 26.71667                       2795   \n",
            "18                 26.71667                       2795   \n",
            "19                 26.71667                       2795   \n",
            "20                 26.71667                       2795   \n",
            "21                 26.71667                       2795   \n",
            "22                 26.64583                       2708   \n",
            "24                 26.71667                       2795   \n",
            "25                 26.20833                       2830   \n",
            "26                 26.37500                       2764   \n",
            "27                 26.71667                       2795   \n",
            "28                 26.64583                       2708   \n",
            "29                 26.64583                       2708   \n",
            "30                 26.71667                       2795   \n",
            "31                 26.20833                       2830   \n",
            "32                 26.64583                       2708   \n",
            "33                 26.71667                       2856   \n",
            "\n",
            "    Iso_Oxi_Stack_mean_TERZER  isoscape_fullmodel_d18O_prec_REGRESSION  \\\n",
            "0                    -3.96045                                -4.692930   \n",
            "1                    -3.92301                                -4.675250   \n",
            "2                    -3.96045                                -4.692930   \n",
            "3                    -4.16807                                -4.864850   \n",
            "4                    -4.36128                                -5.456932   \n",
            "5                    -3.96045                                -4.692930   \n",
            "6                    -4.36128                                -5.456932   \n",
            "7                    -3.96045                                -4.692930   \n",
            "8                    -4.36128                                -5.472804   \n",
            "9                    -3.96045                                -4.692930   \n",
            "10                   -4.36128                                -5.456932   \n",
            "11                   -4.16807                                -4.864850   \n",
            "12                   -3.96045                                -4.692930   \n",
            "13                   -4.36128                                -5.456932   \n",
            "14                   -3.96045                                -4.692930   \n",
            "15                   -3.96045                                -4.692930   \n",
            "16                   -3.96045                                -4.692930   \n",
            "17                   -4.16807                                -4.864850   \n",
            "18                   -4.16807                                -4.864850   \n",
            "19                   -4.16807                                -4.864850   \n",
            "20                   -4.16807                                -4.864850   \n",
            "21                   -4.16807                                -4.864850   \n",
            "22                   -4.36128                                -5.456932   \n",
            "24                   -4.16807                                -4.864850   \n",
            "25                   -3.96045                                -4.692930   \n",
            "26                   -3.92301                                -4.675250   \n",
            "27                   -4.16807                                -4.864850   \n",
            "28                   -4.36128                                -5.472800   \n",
            "29                   -4.36128                                -5.472800   \n",
            "30                   -4.16807                                -4.864850   \n",
            "31                   -3.96045                                -4.692930   \n",
            "32                   -4.36128                                -5.472804   \n",
            "33                   -4.13790                                -4.845370   \n",
            "\n",
            "    krig_mean_residual  krig_variance_residual  \n",
            "0             2.514895                1.112778  \n",
            "1             0.194310                1.047335  \n",
            "2            -0.015845                1.194524  \n",
            "3             2.484155                1.126274  \n",
            "4             0.506155                1.235124  \n",
            "5             0.752155                1.154734  \n",
            "6             1.338155               -2.801616  \n",
            "7             0.812155                1.090434  \n",
            "8             2.522155                1.141884  \n",
            "9             0.914155                0.988924  \n",
            "10            2.152155                1.212634  \n",
            "11            2.120155                0.983704  \n",
            "12            2.178155                1.142184  \n",
            "13            1.882155                0.997084  \n",
            "14            1.312155               -0.389416  \n",
            "15            0.224155                0.469474  \n",
            "16            0.396155                1.041474  \n",
            "17            0.126155                1.037024  \n",
            "18            2.150155                0.256504  \n",
            "19            0.008155                0.782084  \n",
            "20            0.042155                0.911134  \n",
            "21           -0.139845               -1.323696  \n",
            "22            2.592155                0.598884  \n",
            "24            0.724155                0.850424  \n",
            "25            1.264155                1.083774  \n",
            "26           -0.435845                1.221524  \n",
            "27            1.958155                0.951134  \n",
            "28            1.072155                1.102634  \n",
            "29            0.840155                1.209804  \n",
            "30           -0.625845               -0.882726  \n",
            "31            0.238155                0.316334  \n",
            "32            1.892155                0.802534  \n",
            "33            2.168155                0.736684  \n",
            "          lat       long      VPD       RH        PET  DEM          PA  \\\n",
            "0   -6.009707 -61.868657  0.77083  0.79509   93.97500   71  1004.47662   \n",
            "1   -6.009707 -61.868657  0.77083  0.79509   93.97500   71  1004.47662   \n",
            "2   -6.009707 -61.868657  0.77083  0.79509   93.97500   71  1004.47662   \n",
            "3   -6.009707 -61.868657  0.77083  0.79509   93.97500   71  1004.47662   \n",
            "4   -6.009707 -61.868657  0.77083  0.79509   93.97500   71  1004.47662   \n",
            "5  -13.081300 -52.377100  0.99167  0.71167  105.92500  391   966.77917   \n",
            "6  -13.079000 -52.386400  0.99167  0.71167  105.92500  381   967.93964   \n",
            "7  -13.079000 -52.386400  0.99167  0.71167  105.92500  381   967.93964   \n",
            "8  -13.079000 -52.386400  0.99167  0.71167  105.92500  381   967.93964   \n",
            "9  -13.079000 -52.386400  0.99167  0.71167  105.92500  381   967.93964   \n",
            "10 -13.079000 -52.386400  0.99167  0.71167  105.92500  381   967.93964   \n",
            "11 -13.079000 -52.386400  0.99167  0.71167  105.92500  381   967.93964   \n",
            "12 -13.081300 -52.377100  0.99167  0.71167  105.92500  391   966.77917   \n",
            "13 -13.079000 -52.386400  0.99167  0.71167  105.92500  381   967.93964   \n",
            "14  -9.318000 -60.978000  0.72583  0.79095   93.08334  151   994.94250   \n",
            "15  -9.312000 -62.982000  0.63333  0.81416   91.68333  129   997.55707   \n",
            "16  -9.311000 -62.974000  0.63333  0.81416   91.68333  124   998.15204   \n",
            "17  -9.317000 -62.981000  0.63333  0.81416   91.68333  138   996.48682   \n",
            "18  -9.299000 -62.977000  0.63333  0.81416   91.68333  127   997.79504   \n",
            "19  -6.009707 -61.868657  0.77083  0.79509   93.97500   71  1004.47662   \n",
            "20  -6.009707 -61.868657  0.77083  0.79509   93.97500   71  1004.47662   \n",
            "21  -6.009707 -61.868657  0.77083  0.79509   93.97500   71  1004.47662   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  \\\n",
            "0                  27.20000                       1996   \n",
            "1                  27.20000                       1996   \n",
            "2                  27.20000                       1996   \n",
            "3                  27.20000                       1996   \n",
            "4                  27.20000                       1996   \n",
            "5                  25.06667                       1593   \n",
            "6                  25.06667                       1593   \n",
            "7                  25.06667                       1593   \n",
            "8                  25.06667                       1593   \n",
            "9                  25.06667                       1593   \n",
            "10                 25.06667                       1593   \n",
            "11                 25.06667                       1593   \n",
            "12                 25.06667                       1593   \n",
            "13                 25.06667                       1593   \n",
            "14                 25.45833                       2190   \n",
            "15                 25.37500                       2344   \n",
            "16                 25.37500                       2344   \n",
            "17                 25.37500                       2344   \n",
            "18                 25.37500                       2344   \n",
            "19                 27.20000                       1996   \n",
            "20                 27.20000                       1996   \n",
            "21                 27.20000                       1996   \n",
            "\n",
            "    Iso_Oxi_Stack_mean_TERZER  isoscape_fullmodel_d18O_prec_REGRESSION  \\\n",
            "0                    -4.05694                                 -4.46622   \n",
            "1                    -4.05694                                 -4.46622   \n",
            "2                    -4.05694                                 -4.46622   \n",
            "3                    -4.05694                                 -4.46622   \n",
            "4                    -4.05694                                 -4.46622   \n",
            "5                    -3.54571                                 -4.11961   \n",
            "6                    -3.54571                                 -4.11961   \n",
            "7                    -3.54571                                 -4.11961   \n",
            "8                    -3.54571                                 -4.11961   \n",
            "9                    -3.54571                                 -4.11961   \n",
            "10                   -3.54571                                 -4.11961   \n",
            "11                   -3.54571                                 -4.11961   \n",
            "12                   -3.54571                                 -4.11961   \n",
            "13                   -3.54571                                 -4.11961   \n",
            "14                   -4.23224                                 -4.69205   \n",
            "15                   -4.45845                                 -4.92778   \n",
            "16                   -4.45328                                 -4.92778   \n",
            "17                   -4.47060                                 -4.92778   \n",
            "18                   -4.45845                                 -4.92778   \n",
            "19                   -4.05694                                 -4.46622   \n",
            "20                   -4.05694                                 -4.46622   \n",
            "21                   -4.05694                                 -4.46622   \n",
            "\n",
            "    krig_mean_residual  krig_variance_residual  \n",
            "0             1.878155                0.915084  \n",
            "1             0.680155                1.225104  \n",
            "2             0.138155                0.888834  \n",
            "3             0.844155                1.030774  \n",
            "4            -0.045845                1.088574  \n",
            "5            -1.513845                0.760024  \n",
            "6            -2.235845                0.262024  \n",
            "7            -0.915845                1.140174  \n",
            "8            -2.259845                0.482804  \n",
            "9            -0.913845                1.166124  \n",
            "10           -0.723845                0.772624  \n",
            "11           -0.851845                0.887684  \n",
            "12           -1.764125                0.044664  \n",
            "13           -1.483080                0.368773  \n",
            "14            1.142485                0.524482  \n",
            "15            0.368305                0.948699  \n",
            "16            0.804270                1.227534  \n",
            "17           -0.067660               -4.019772  \n",
            "18            1.788124                0.872011  \n",
            "19            1.022155                1.024984  \n",
            "20            1.976155                1.102924  \n",
            "21           -0.097845                0.296884  \n",
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('krig_mean_residual_scaler', StandardScaler(),\n",
            "                                 ['krig_mean_residual']),\n",
            "                                ('krig_variance_residual_scaler',\n",
            "                                 StandardScaler(), ['krig_variance_residual']),\n",
            "                                ('lat_standardizer', StandardScaler(), ['lat']),\n",
            "                                ('long_standardizer', StandardScaler(),\n",
            "                                 ['long']),\n",
            "                                ('VPD_standardizer', StandardScaler(), ['VPD']),\n",
            "                                ('RH_s...\n",
            "                                ('Mean Annual Temperature_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation']),\n",
            "                                ('Iso_Oxi_Stack_mean_TERZER_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Iso_Oxi_Stack_mean_TERZER']),\n",
            "                                ('isoscape_fullmodel_d18O_prec_REGRESSION_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['isoscape_fullmodel_d18O_prec_REGRESSION'])])\n",
            "==================\n",
            "fixed_all_boosted\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 13)]         0           []                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_11 (S  (None, 11)          0           ['input_4[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 20)           240         ['tf.__operators__.getitem_11[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_9 (Sl  (None,)             0           ['input_4[0][0]']                \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 20)           420         ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_10 (S  (None,)             0           ['input_4[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " tf.expand_dims_6 (TFOpLambda)  (None, 1)            0           ['tf.__operators__.getitem_9[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " mean_output (Dense)            (None, 1)            21          ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " var_output (Dense)             (None, 1)            21          ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " tf.expand_dims_7 (TFOpLambda)  (None, 1)            0           ['tf.__operators__.getitem_10[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " tf.math.multiply_9 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_6[0][0]']       \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.multiply_8 (TFOpLambda  (None, 1)           0           ['mean_output[0][0]']            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.multiply_10 (TFOpLambd  (None, 1)           0           ['var_output[0][0]']             \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.multiply_11 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_7[0][0]']       \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_12 (TFOpL  (None, 1)           0           ['tf.math.multiply_9[0][0]']     \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_11 (TFOpL  (None, 1)           0           ['tf.math.multiply_8[0][0]']     \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_14 (TFOpL  (None, 1)           0           ['tf.math.multiply_10[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_15 (TFOpL  (None, 1)           0           ['tf.math.multiply_11[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_13 (TFOpL  (None, 1)           0           ['tf.__operators__.add_12[0][0]',\n",
            " ambda)                                                           'tf.__operators__.add_11[0][0]']\n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1)            0           ['tf.__operators__.add_14[0][0]',\n",
            "                                                                  'tf.__operators__.add_15[0][0]']\n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 2)            0           ['tf.__operators__.add_13[0][0]',\n",
            "                                                                  'lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 702\n",
            "Trainable params: 702\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10000\n",
            "23/23 [==============================] - 1s 13ms/step - loss: 267.1715 - val_loss: 33667.0000\n",
            "Epoch 2/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 255.0089 - val_loss: 19647.1348\n",
            "Epoch 3/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 246.3942 - val_loss: 13599.8115\n",
            "Epoch 4/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 228.6533 - val_loss: 10684.8809\n",
            "Epoch 5/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 236.2567 - val_loss: 8903.9385\n",
            "Epoch 6/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 224.2275 - val_loss: 5743.9644\n",
            "Epoch 7/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 211.2395 - val_loss: 5521.4995\n",
            "Epoch 8/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 197.7376 - val_loss: 3403.0110\n",
            "Epoch 9/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 199.7637 - val_loss: 2843.4641\n",
            "Epoch 10/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 188.3810 - val_loss: 2389.9744\n",
            "Epoch 11/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 213.1520 - val_loss: 1720.5801\n",
            "Epoch 12/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 176.0988 - val_loss: 1513.7479\n",
            "Epoch 13/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 170.3294 - val_loss: 1000.9947\n",
            "Epoch 14/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 167.0677 - val_loss: 737.8063\n",
            "Epoch 15/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 166.1185 - val_loss: 678.0428\n",
            "Epoch 16/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 170.3707 - val_loss: 464.5603\n",
            "Epoch 17/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 149.2201 - val_loss: 395.2218\n",
            "Epoch 18/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 138.7873 - val_loss: 292.2588\n",
            "Epoch 19/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 126.9522 - val_loss: 213.8021\n",
            "Epoch 20/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 135.6225 - val_loss: 250.2432\n",
            "Epoch 21/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 124.8699 - val_loss: 237.5544\n",
            "Epoch 22/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 117.5034 - val_loss: 146.0159\n",
            "Epoch 23/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 122.3879 - val_loss: 164.3126\n",
            "Epoch 24/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 118.5034 - val_loss: 129.2957\n",
            "Epoch 25/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 121.4912 - val_loss: 135.2715\n",
            "Epoch 26/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 106.6630 - val_loss: 100.9936\n",
            "Epoch 27/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 109.2865 - val_loss: 89.5976\n",
            "Epoch 28/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 95.2404 - val_loss: 99.8617\n",
            "Epoch 29/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 92.1781 - val_loss: 74.3639\n",
            "Epoch 30/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 103.2250 - val_loss: 66.1570\n",
            "Epoch 31/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 83.8081 - val_loss: 62.0137\n",
            "Epoch 32/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 87.3180 - val_loss: 63.6705\n",
            "Epoch 33/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 87.4748 - val_loss: 60.9087\n",
            "Epoch 34/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 85.4242 - val_loss: 46.3889\n",
            "Epoch 35/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 81.3792 - val_loss: 43.0503\n",
            "Epoch 36/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 71.0386 - val_loss: 41.6075\n",
            "Epoch 37/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 70.2091 - val_loss: 37.9090\n",
            "Epoch 38/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 72.0995 - val_loss: 32.5873\n",
            "Epoch 39/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 72.7015 - val_loss: 31.1948\n",
            "Epoch 40/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 66.3615 - val_loss: 29.5006\n",
            "Epoch 41/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 62.3735 - val_loss: 26.2542\n",
            "Epoch 42/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 62.1837 - val_loss: 24.6833\n",
            "Epoch 43/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 69.3728 - val_loss: 25.0135\n",
            "Epoch 44/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 59.1243 - val_loss: 19.3003\n",
            "Epoch 45/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 54.5838 - val_loss: 19.8731\n",
            "Epoch 46/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 49.8524 - val_loss: 15.4544\n",
            "Epoch 47/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 53.9581 - val_loss: 18.2858\n",
            "Epoch 48/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 49.6586 - val_loss: 16.3493\n",
            "Epoch 49/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 47.9802 - val_loss: 14.1346\n",
            "Epoch 50/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 45.9175 - val_loss: 12.9550\n",
            "Epoch 51/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 43.5998 - val_loss: 12.9092\n",
            "Epoch 52/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 44.0092 - val_loss: 12.4654\n",
            "Epoch 53/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 42.7560 - val_loss: 9.8545\n",
            "Epoch 54/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 39.7359 - val_loss: 9.0100\n",
            "Epoch 55/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 39.0580 - val_loss: 7.9518\n",
            "Epoch 56/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 39.1798 - val_loss: 6.6552\n",
            "Epoch 57/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 39.1205 - val_loss: 6.4847\n",
            "Epoch 58/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 35.6707 - val_loss: 5.6637\n",
            "Epoch 59/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 33.3163 - val_loss: 5.3945\n",
            "Epoch 60/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 34.2636 - val_loss: 5.7155\n",
            "Epoch 61/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 31.2403 - val_loss: 4.9055\n",
            "Epoch 62/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 33.2487 - val_loss: 4.3331\n",
            "Epoch 63/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.5444 - val_loss: 3.7265\n",
            "Epoch 64/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 29.0807 - val_loss: 3.3213\n",
            "Epoch 65/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 28.2888 - val_loss: 2.8218\n",
            "Epoch 66/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.6987 - val_loss: 2.7521\n",
            "Epoch 67/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 24.4733 - val_loss: 2.5918\n",
            "Epoch 68/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.3910 - val_loss: 2.3219\n",
            "Epoch 69/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 24.3313 - val_loss: 2.2717\n",
            "Epoch 70/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 22.0720 - val_loss: 2.0162\n",
            "Epoch 71/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 22.7400 - val_loss: 1.8956\n",
            "Epoch 72/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 22.7362 - val_loss: 1.8131\n",
            "Epoch 73/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 21.0558 - val_loss: 1.8394\n",
            "Epoch 74/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 19.1436 - val_loss: 1.7492\n",
            "Epoch 75/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 18.4708 - val_loss: 1.8905\n",
            "Epoch 76/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 17.5694 - val_loss: 1.7436\n",
            "Epoch 77/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 16.8340 - val_loss: 1.8640\n",
            "Epoch 78/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 17.6269 - val_loss: 1.9103\n",
            "Epoch 79/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 14.7389 - val_loss: 2.0555\n",
            "Epoch 80/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 16.3861 - val_loss: 2.2364\n",
            "Epoch 81/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 14.5502 - val_loss: 2.2667\n",
            "Epoch 82/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 13.5784 - val_loss: 2.6641\n",
            "Epoch 83/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 14.6987 - val_loss: 2.6198\n",
            "Epoch 84/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 13.6779 - val_loss: 2.9104\n",
            "Epoch 85/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 12.6743 - val_loss: 3.3908\n",
            "Epoch 86/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 11.2333 - val_loss: 3.7957\n",
            "Epoch 87/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 12.7706 - val_loss: 3.5174\n",
            "Epoch 88/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 11.3827 - val_loss: 5.0299\n",
            "Epoch 89/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 10.7039 - val_loss: 4.7575\n",
            "Epoch 90/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 10.4382 - val_loss: 4.6640\n",
            "Epoch 91/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 10.2457 - val_loss: 5.3871\n",
            "Epoch 92/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 10.9478 - val_loss: 5.1933\n",
            "Epoch 93/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 9.6662 - val_loss: 6.3601\n",
            "Epoch 94/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.5705 - val_loss: 6.1216\n",
            "Epoch 95/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.1735 - val_loss: 5.8873\n",
            "Epoch 96/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.0741 - val_loss: 8.0647\n",
            "Epoch 97/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 7.5376 - val_loss: 7.3900\n",
            "Epoch 98/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 7.4502 - val_loss: 9.0856\n",
            "Epoch 99/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.2963 - val_loss: 7.8739\n",
            "Epoch 100/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 6.7995 - val_loss: 9.4970\n",
            "Epoch 101/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 6.9401 - val_loss: 8.9540\n",
            "Epoch 102/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 6.4986 - val_loss: 9.2020\n",
            "Epoch 103/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 5.8121 - val_loss: 9.9084\n",
            "Epoch 104/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 5.7958 - val_loss: 10.7702\n",
            "Epoch 105/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 5.5728 - val_loss: 10.8292\n",
            "Epoch 106/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 5.1395 - val_loss: 11.6375\n",
            "Epoch 107/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 4.9666 - val_loss: 13.9526\n",
            "Epoch 108/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 5.1456 - val_loss: 12.2748\n",
            "Epoch 109/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 4.5316 - val_loss: 14.7164\n",
            "Epoch 110/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 4.4789 - val_loss: 13.9750\n",
            "Epoch 111/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 4.4207 - val_loss: 15.5484\n",
            "Epoch 112/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 4.2743 - val_loss: 15.6546\n",
            "Epoch 113/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 5.0064 - val_loss: 14.9513\n",
            "Epoch 114/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 4.1427 - val_loss: 15.4839\n",
            "Epoch 115/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.9121 - val_loss: 17.1098\n",
            "Epoch 116/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.8237 - val_loss: 15.5770\n",
            "Epoch 117/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.4178 - val_loss: 15.7868\n",
            "Epoch 118/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.3412 - val_loss: 17.6069\n",
            "Epoch 119/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.4426 - val_loss: 20.1228\n",
            "Epoch 120/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 3.5247 - val_loss: 17.2471\n",
            "Epoch 121/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.3334 - val_loss: 19.1790\n",
            "Epoch 122/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.2312 - val_loss: 21.6949\n",
            "Epoch 123/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.8911 - val_loss: 22.4879\n",
            "Epoch 124/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.0410 - val_loss: 23.1743\n",
            "Epoch 125/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.6936 - val_loss: 22.0496\n",
            "Epoch 126/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.8259 - val_loss: 21.8498\n",
            "Epoch 127/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.7906 - val_loss: 21.3861\n",
            "Epoch 128/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.7286 - val_loss: 22.5274\n",
            "Epoch 129/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.3091 - val_loss: 21.2140\n",
            "Epoch 130/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.4886 - val_loss: 23.6818\n",
            "Epoch 131/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.3281 - val_loss: 27.5193\n",
            "Epoch 132/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.5259 - val_loss: 23.6281\n",
            "Epoch 133/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.3413 - val_loss: 27.9398\n",
            "Epoch 134/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.3953 - val_loss: 27.0578\n",
            "Epoch 135/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.1760 - val_loss: 32.2775\n",
            "Epoch 136/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.2493 - val_loss: 26.6817\n",
            "Epoch 137/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.0741 - val_loss: 28.0548\n",
            "Epoch 138/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.4332 - val_loss: 29.0036\n",
            "Epoch 139/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.0749 - val_loss: 30.2711\n",
            "Epoch 140/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.0150 - val_loss: 25.6631\n",
            "Epoch 141/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.1879 - val_loss: 33.3230\n",
            "Epoch 142/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.0180 - val_loss: 28.7585\n",
            "Epoch 143/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.0777 - val_loss: 29.3299\n",
            "Epoch 144/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.0478 - val_loss: 32.5276\n",
            "Epoch 145/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.0141 - val_loss: 36.6774\n",
            "Epoch 146/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.0462 - val_loss: 33.9837\n",
            "Epoch 147/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.9397 - val_loss: 33.6172\n",
            "Epoch 148/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.8959 - val_loss: 34.1255\n",
            "Epoch 149/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.9862 - val_loss: 38.0337\n",
            "Epoch 150/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.9907 - val_loss: 36.1281\n",
            "Epoch 151/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.8730 - val_loss: 32.1751\n",
            "Epoch 152/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.9006 - val_loss: 41.0430\n",
            "Epoch 153/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.8495 - val_loss: 34.3214\n",
            "Epoch 154/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.8755 - val_loss: 33.0909\n",
            "Epoch 155/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.8991 - val_loss: 33.4419\n",
            "Epoch 156/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.8907 - val_loss: 32.7876\n",
            "Epoch 157/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7829 - val_loss: 35.5388\n",
            "Epoch 158/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7859 - val_loss: 30.9117\n",
            "Epoch 159/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.9497 - val_loss: 38.1390\n",
            "Epoch 160/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7313 - val_loss: 38.9391\n",
            "Epoch 161/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.8061 - val_loss: 43.3129\n",
            "Epoch 162/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7445 - val_loss: 39.9583\n",
            "Epoch 163/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.7548 - val_loss: 31.5495\n",
            "Epoch 164/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.7423 - val_loss: 42.0667\n",
            "Epoch 165/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.8657 - val_loss: 33.2604\n",
            "Epoch 166/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6203 - val_loss: 37.9860\n",
            "Epoch 167/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6963 - val_loss: 35.8505\n",
            "Epoch 168/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7619 - val_loss: 39.7945\n",
            "Epoch 169/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6685 - val_loss: 35.9118\n",
            "Epoch 170/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.8621 - val_loss: 34.6119\n",
            "Epoch 171/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7814 - val_loss: 32.4184\n",
            "Epoch 172/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7739 - val_loss: 37.4516\n",
            "Epoch 173/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6690 - val_loss: 39.7587\n",
            "Epoch 174/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.8486 - val_loss: 38.3181\n",
            "Epoch 175/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7103 - val_loss: 38.3536\n",
            "Epoch 176/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6829 - val_loss: 41.7234\n",
            "Epoch 177/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6727 - val_loss: 46.1768\n",
            "Epoch 178/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7135 - val_loss: 43.5081\n",
            "Epoch 179/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6275 - val_loss: 42.9545\n",
            "Epoch 180/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7664 - val_loss: 45.3111\n",
            "Epoch 181/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6232 - val_loss: 38.4189\n",
            "Epoch 182/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6893 - val_loss: 43.0298\n",
            "Epoch 183/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7373 - val_loss: 40.2425\n",
            "Epoch 184/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6972 - val_loss: 44.4244\n",
            "Epoch 185/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6668 - val_loss: 42.6120\n",
            "Epoch 186/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6881 - val_loss: 44.4278\n",
            "Epoch 187/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6416 - val_loss: 40.5032\n",
            "Epoch 188/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.6123 - val_loss: 43.6624\n",
            "Epoch 189/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.6785 - val_loss: 42.5997\n",
            "Epoch 190/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6910 - val_loss: 40.4146\n",
            "Epoch 191/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7283 - val_loss: 43.7565\n",
            "Epoch 192/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6523 - val_loss: 40.6737\n",
            "Epoch 193/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6304 - val_loss: 43.7111\n",
            "Epoch 194/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6443 - val_loss: 38.9225\n",
            "Epoch 195/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7487 - val_loss: 38.5287\n",
            "Epoch 196/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7113 - val_loss: 40.7952\n",
            "Epoch 197/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6096 - val_loss: 43.8557\n",
            "Epoch 198/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6690 - val_loss: 38.2920\n",
            "Epoch 199/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6047 - val_loss: 45.7129\n",
            "Epoch 200/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7259 - val_loss: 41.7104\n",
            "Epoch 201/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6404 - val_loss: 42.5387\n",
            "Epoch 202/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6572 - val_loss: 35.4379\n",
            "Epoch 203/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6546 - val_loss: 44.1898\n",
            "Epoch 204/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6366 - val_loss: 41.8444\n",
            "Epoch 205/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7004 - val_loss: 42.7548\n",
            "Epoch 206/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6157 - val_loss: 41.6371\n",
            "Epoch 207/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6706 - val_loss: 45.1339\n",
            "Epoch 208/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5810 - val_loss: 43.1011\n",
            "Epoch 209/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6169 - val_loss: 41.6465\n",
            "Epoch 210/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5644 - val_loss: 43.5809\n",
            "Epoch 211/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6342 - val_loss: 43.3812\n",
            "Epoch 212/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6209 - val_loss: 49.0135\n",
            "Epoch 213/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5689 - val_loss: 43.0856\n",
            "Epoch 214/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6293 - val_loss: 41.6466\n",
            "Epoch 215/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6200 - val_loss: 42.5064\n",
            "Epoch 216/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6298 - val_loss: 46.4776\n",
            "Epoch 217/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6259 - val_loss: 41.8994\n",
            "Epoch 218/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5959 - val_loss: 38.5970\n",
            "Epoch 219/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.6125 - val_loss: 43.1324\n",
            "Epoch 220/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5868 - val_loss: 45.6973\n",
            "Epoch 221/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6188 - val_loss: 45.0834\n",
            "Epoch 222/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5607 - val_loss: 49.0359\n",
            "Epoch 223/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6302 - val_loss: 45.1777\n",
            "Epoch 224/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5385 - val_loss: 37.8864\n",
            "Epoch 225/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7150 - val_loss: 46.5492\n",
            "Epoch 226/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5522 - val_loss: 43.4723\n",
            "Epoch 227/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5796 - val_loss: 43.8978\n",
            "Epoch 228/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6116 - val_loss: 44.8570\n",
            "Epoch 229/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6102 - val_loss: 40.4257\n",
            "Epoch 230/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6066 - val_loss: 49.7709\n",
            "Epoch 231/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5535 - val_loss: 51.7456\n",
            "Epoch 232/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5646 - val_loss: 44.2587\n",
            "Epoch 233/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5616 - val_loss: 45.9006\n",
            "Epoch 234/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5413 - val_loss: 38.1641\n",
            "Epoch 235/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5966 - val_loss: 46.0683\n",
            "Epoch 236/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5500 - val_loss: 48.2477\n",
            "Epoch 237/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5012 - val_loss: 50.4432\n",
            "Epoch 238/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5762 - val_loss: 42.8037\n",
            "Epoch 239/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5404 - val_loss: 47.6736\n",
            "Epoch 240/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.6470 - val_loss: 50.4847\n",
            "Epoch 241/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5217 - val_loss: 53.5340\n",
            "Epoch 242/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5656 - val_loss: 50.1867\n",
            "Epoch 243/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5906 - val_loss: 41.1812\n",
            "Epoch 244/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5023 - val_loss: 46.6353\n",
            "Epoch 245/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5503 - val_loss: 45.4385\n",
            "Epoch 246/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5274 - val_loss: 46.4217\n",
            "Epoch 247/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4925 - val_loss: 41.1166\n",
            "Epoch 248/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5432 - val_loss: 46.2578\n",
            "Epoch 249/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5183 - val_loss: 48.7063\n",
            "Epoch 250/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5065 - val_loss: 45.6557\n",
            "Epoch 251/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5127 - val_loss: 47.6320\n",
            "Epoch 252/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5498 - val_loss: 46.7198\n",
            "Epoch 253/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6030 - val_loss: 47.6914\n",
            "Epoch 254/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5298 - val_loss: 58.4319\n",
            "Epoch 255/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5376 - val_loss: 50.9232\n",
            "Epoch 256/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4852 - val_loss: 54.0259\n",
            "Epoch 257/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4744 - val_loss: 49.6176\n",
            "Epoch 258/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4857 - val_loss: 42.7038\n",
            "Epoch 259/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4880 - val_loss: 42.2650\n",
            "Epoch 260/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4604 - val_loss: 53.0401\n",
            "Epoch 261/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5558 - val_loss: 49.0320\n",
            "Epoch 262/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5454 - val_loss: 47.6273\n",
            "Epoch 263/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5157 - val_loss: 44.0056\n",
            "Epoch 264/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4506 - val_loss: 44.0665\n",
            "Epoch 265/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5138 - val_loss: 47.1006\n",
            "Epoch 266/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4515 - val_loss: 50.3030\n",
            "Epoch 267/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4788 - val_loss: 49.0835\n",
            "Epoch 268/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5553 - val_loss: 50.5879\n",
            "Epoch 269/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5178 - val_loss: 43.4891\n",
            "Epoch 270/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4651 - val_loss: 46.0735\n",
            "Epoch 271/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4817 - val_loss: 46.0356\n",
            "Epoch 272/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4818 - val_loss: 50.6229\n",
            "Epoch 273/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5094 - val_loss: 49.4829\n",
            "Epoch 274/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4487 - val_loss: 47.6377\n",
            "Epoch 275/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5164 - val_loss: 49.7617\n",
            "Epoch 276/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4723 - val_loss: 41.3478\n",
            "Epoch 277/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4657 - val_loss: 44.3918\n",
            "Epoch 278/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4893 - val_loss: 49.7104\n",
            "Epoch 279/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4558 - val_loss: 49.5807\n",
            "Epoch 280/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4437 - val_loss: 49.6124\n",
            "Epoch 281/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4652 - val_loss: 50.0187\n",
            "Epoch 282/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4939 - val_loss: 48.3817\n",
            "Epoch 283/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4770 - val_loss: 47.2886\n",
            "Epoch 284/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4264 - val_loss: 45.7582\n",
            "Epoch 285/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5087 - val_loss: 51.9769\n",
            "Epoch 286/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4555 - val_loss: 45.7084\n",
            "Epoch 287/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4208 - val_loss: 52.3742\n",
            "Epoch 288/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4630 - val_loss: 45.5197\n",
            "Epoch 289/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4882 - val_loss: 49.8943\n",
            "Epoch 290/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4552 - val_loss: 47.0678\n",
            "Epoch 291/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4797 - val_loss: 48.0612\n",
            "Epoch 292/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5273 - val_loss: 45.7927\n",
            "Epoch 293/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4244 - val_loss: 51.9899\n",
            "Epoch 294/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4552 - val_loss: 51.4351\n",
            "Epoch 295/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4314 - val_loss: 47.9318\n",
            "Epoch 296/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4451 - val_loss: 45.6255\n",
            "Epoch 297/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4402 - val_loss: 43.4819\n",
            "Epoch 298/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3826 - val_loss: 47.6298\n",
            "Epoch 299/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4147 - val_loss: 50.4985\n",
            "Epoch 300/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3975 - val_loss: 48.8572\n",
            "Epoch 301/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4221 - val_loss: 46.3390\n",
            "Epoch 302/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4748 - val_loss: 45.8200\n",
            "Epoch 303/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4356 - val_loss: 51.8643\n",
            "Epoch 304/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4108 - val_loss: 48.2293\n",
            "Epoch 305/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4457 - val_loss: 43.6192\n",
            "Epoch 306/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3847 - val_loss: 51.4035\n",
            "Epoch 307/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4067 - val_loss: 52.6099\n",
            "Epoch 308/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3989 - val_loss: 44.3614\n",
            "Epoch 309/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3993 - val_loss: 48.5420\n",
            "Epoch 310/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4173 - val_loss: 47.5073\n",
            "Epoch 311/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3948 - val_loss: 47.4019\n",
            "Epoch 312/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4086 - val_loss: 48.9222\n",
            "Epoch 313/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3854 - val_loss: 44.1892\n",
            "Epoch 314/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3672 - val_loss: 54.7638\n",
            "Epoch 315/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4046 - val_loss: 49.2352\n",
            "Epoch 316/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4272 - val_loss: 53.9981\n",
            "Epoch 317/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3950 - val_loss: 49.3510\n",
            "Epoch 318/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4244 - val_loss: 53.1354\n",
            "Epoch 319/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3369 - val_loss: 46.7821\n",
            "Epoch 320/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3625 - val_loss: 51.4444\n",
            "Epoch 321/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3235 - val_loss: 50.0199\n",
            "Epoch 322/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3733 - val_loss: 49.3440\n",
            "Epoch 323/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3927 - val_loss: 47.4183\n",
            "Epoch 324/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3673 - val_loss: 51.0995\n",
            "Epoch 325/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3734 - val_loss: 51.1159\n",
            "Epoch 326/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3888 - val_loss: 51.4389\n",
            "Epoch 327/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3554 - val_loss: 52.9168\n",
            "Epoch 328/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4144 - val_loss: 48.8907\n",
            "Epoch 329/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3862 - val_loss: 51.9541\n",
            "Epoch 330/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3846 - val_loss: 51.5684\n",
            "Epoch 331/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3033 - val_loss: 47.0792\n",
            "Epoch 332/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3661 - val_loss: 45.0326\n",
            "Epoch 333/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3492 - val_loss: 44.5964\n",
            "Epoch 334/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3466 - val_loss: 54.9395\n",
            "Epoch 335/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3950 - val_loss: 53.9940\n",
            "Epoch 336/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3315 - val_loss: 52.4367\n",
            "Epoch 337/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3267 - val_loss: 44.2062\n",
            "Epoch 338/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3520 - val_loss: 53.6847\n",
            "Epoch 339/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3497 - val_loss: 58.5108\n",
            "Epoch 340/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3564 - val_loss: 57.3407\n",
            "Epoch 341/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3339 - val_loss: 56.2038\n",
            "Epoch 342/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4115 - val_loss: 48.6448\n",
            "Epoch 343/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3227 - val_loss: 51.1715\n",
            "Epoch 344/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3735 - val_loss: 48.6340\n",
            "Epoch 345/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3348 - val_loss: 53.9900\n",
            "Epoch 346/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3604 - val_loss: 61.0881\n",
            "Epoch 347/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3527 - val_loss: 51.0309\n",
            "Epoch 348/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3671 - val_loss: 59.5733\n",
            "Epoch 349/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2992 - val_loss: 50.6808\n",
            "Epoch 350/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3108 - val_loss: 50.5496\n",
            "Epoch 351/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3112 - val_loss: 57.9377\n",
            "Epoch 352/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3298 - val_loss: 50.5039\n",
            "Epoch 353/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2832 - val_loss: 42.8430\n",
            "Epoch 354/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3003 - val_loss: 54.6815\n",
            "Epoch 355/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3341 - val_loss: 51.9726\n",
            "Epoch 356/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3013 - val_loss: 54.1489\n",
            "Epoch 357/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3086 - val_loss: 52.8275\n",
            "Epoch 358/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2738 - val_loss: 63.7339\n",
            "Epoch 359/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2683 - val_loss: 52.3002\n",
            "Epoch 360/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2706 - val_loss: 52.9941\n",
            "Epoch 361/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2718 - val_loss: 50.2639\n",
            "Epoch 362/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2977 - val_loss: 55.4260\n",
            "Epoch 363/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3004 - val_loss: 50.4417\n",
            "Epoch 364/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2962 - val_loss: 60.1147\n",
            "Epoch 365/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2834 - val_loss: 54.2750\n",
            "Epoch 366/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2790 - val_loss: 53.2205\n",
            "Epoch 367/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2879 - val_loss: 63.1082\n",
            "Epoch 368/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3477 - val_loss: 64.3030\n",
            "Epoch 369/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3014 - val_loss: 50.5217\n",
            "Epoch 370/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2887 - val_loss: 56.6289\n",
            "Epoch 371/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2739 - val_loss: 54.9665\n",
            "Epoch 372/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2988 - val_loss: 75.6107\n",
            "Epoch 373/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2657 - val_loss: 63.3650\n",
            "Epoch 374/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2399 - val_loss: 67.6102\n",
            "Epoch 375/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3075 - val_loss: 60.4854\n",
            "Epoch 376/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2314 - val_loss: 69.3044\n",
            "Epoch 377/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2592 - val_loss: 60.4252\n",
            "Epoch 378/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2502 - val_loss: 58.2507\n",
            "Epoch 379/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1915 - val_loss: 55.5053\n",
            "Epoch 380/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3188 - val_loss: 54.4440\n",
            "Epoch 381/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1770 - val_loss: 71.8349\n",
            "Epoch 382/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2015 - val_loss: 56.6419\n",
            "Epoch 383/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2315 - val_loss: 70.4359\n",
            "Epoch 384/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2186 - val_loss: 57.2641\n",
            "Epoch 385/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2216 - val_loss: 58.9117\n",
            "Epoch 386/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2343 - val_loss: 65.5359\n",
            "Epoch 387/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2688 - val_loss: 69.2772\n",
            "Epoch 388/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2776 - val_loss: 61.5917\n",
            "Epoch 389/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2595 - val_loss: 58.6208\n",
            "Epoch 390/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1576 - val_loss: 67.0149\n",
            "Epoch 391/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2571 - val_loss: 66.2728\n",
            "Epoch 392/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2621 - val_loss: 80.0251\n",
            "Epoch 393/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2419 - val_loss: 68.2450\n",
            "Epoch 394/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1870 - val_loss: 66.5906\n",
            "Epoch 395/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1628 - val_loss: 68.8808\n",
            "Epoch 396/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2248 - val_loss: 74.6263\n",
            "Epoch 397/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2002 - val_loss: 72.1027\n",
            "Epoch 398/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2251 - val_loss: 75.7737\n",
            "Epoch 399/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1995 - val_loss: 69.6734\n",
            "Epoch 400/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1937 - val_loss: 71.1469\n",
            "Epoch 401/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1929 - val_loss: 64.7471\n",
            "Epoch 402/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1902 - val_loss: 67.7648\n",
            "Epoch 403/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1789 - val_loss: 73.6644\n",
            "Epoch 404/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1990 - val_loss: 73.9244\n",
            "Epoch 405/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1678 - val_loss: 65.8445\n",
            "Epoch 406/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2156 - val_loss: 74.5794\n",
            "Epoch 407/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2194 - val_loss: 69.7222\n",
            "Epoch 408/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1643 - val_loss: 77.9180\n",
            "Epoch 409/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2127 - val_loss: 85.8917\n",
            "Epoch 410/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1852 - val_loss: 71.1982\n",
            "Epoch 411/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2319 - val_loss: 78.7223\n",
            "Epoch 412/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1720 - val_loss: 78.5200\n",
            "Epoch 413/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1163 - val_loss: 73.0530\n",
            "Epoch 414/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1542 - val_loss: 78.3983\n",
            "Epoch 415/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2017 - val_loss: 80.2446\n",
            "Epoch 416/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1514 - val_loss: 96.8632\n",
            "Epoch 417/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1624 - val_loss: 79.3708\n",
            "Epoch 418/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1351 - val_loss: 83.2929\n",
            "Epoch 419/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1413 - val_loss: 79.8303\n",
            "Epoch 420/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1643 - val_loss: 75.0044\n",
            "Epoch 421/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1505 - val_loss: 80.9524\n",
            "Epoch 422/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1307 - val_loss: 98.9124\n",
            "Epoch 423/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1530 - val_loss: 95.4040\n",
            "Epoch 424/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1165 - val_loss: 86.4794\n",
            "Epoch 425/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0892 - val_loss: 81.2009\n",
            "Epoch 426/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1265 - val_loss: 90.0487\n",
            "Epoch 427/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0856 - val_loss: 105.1805\n",
            "Epoch 428/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0964 - val_loss: 93.8562\n",
            "Epoch 429/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0658 - val_loss: 81.9252\n",
            "Epoch 430/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0590 - val_loss: 98.5849\n",
            "Epoch 431/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0912 - val_loss: 92.0053\n",
            "Epoch 432/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1012 - val_loss: 105.8544\n",
            "Epoch 433/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0540 - val_loss: 96.4244\n",
            "Epoch 434/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1013 - val_loss: 102.7085\n",
            "Epoch 435/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0922 - val_loss: 103.7008\n",
            "Epoch 436/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1053 - val_loss: 122.5601\n",
            "Epoch 437/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1062 - val_loss: 99.4029\n",
            "Epoch 438/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0862 - val_loss: 115.2209\n",
            "Epoch 439/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.0486 - val_loss: 114.1385\n",
            "Epoch 440/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0970 - val_loss: 111.6700\n",
            "Epoch 441/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0277 - val_loss: 127.0926\n",
            "Epoch 442/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0329 - val_loss: 110.1198\n",
            "Epoch 443/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0442 - val_loss: 116.0658\n",
            "Epoch 444/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0165 - val_loss: 132.5212\n",
            "Epoch 445/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0192 - val_loss: 118.6287\n",
            "Epoch 446/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0227 - val_loss: 140.8646\n",
            "Epoch 447/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.0013 - val_loss: 136.0289\n",
            "Epoch 448/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9391 - val_loss: 125.4025\n",
            "Epoch 449/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0143 - val_loss: 155.1383\n",
            "Epoch 450/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0327 - val_loss: 153.9959\n",
            "Epoch 451/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9726 - val_loss: 161.4124\n",
            "Epoch 452/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9799 - val_loss: 170.3654\n",
            "Epoch 453/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9550 - val_loss: 164.4592\n",
            "Epoch 454/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0052 - val_loss: 173.3991\n",
            "Epoch 455/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9143 - val_loss: 188.6297\n",
            "Epoch 456/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9396 - val_loss: 179.3619\n",
            "Epoch 457/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9873 - val_loss: 199.7505\n",
            "Epoch 458/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9162 - val_loss: 212.4434\n",
            "Epoch 459/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9294 - val_loss: 226.6418\n",
            "Epoch 460/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9094 - val_loss: 262.1497\n",
            "Epoch 461/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.9365 - val_loss: 252.5927\n",
            "Epoch 462/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.8987 - val_loss: 264.6176\n",
            "Epoch 463/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8662 - val_loss: 333.3820\n",
            "Epoch 464/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8932 - val_loss: 352.5695\n",
            "Epoch 465/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8602 - val_loss: 322.2997\n",
            "Epoch 466/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8325 - val_loss: 419.4536\n",
            "Epoch 467/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8731 - val_loss: 498.2760\n",
            "Epoch 468/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8015 - val_loss: 554.8109\n",
            "Epoch 469/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8014 - val_loss: 635.0927\n",
            "Epoch 470/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.8432 - val_loss: 770.1377\n",
            "Epoch 471/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.7857 - val_loss: 901.9651\n",
            "Epoch 472/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.8034 - val_loss: 1211.2638\n",
            "Epoch 473/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7725 - val_loss: 1778.8926\n",
            "Epoch 474/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7447 - val_loss: 2370.4277\n",
            "Epoch 475/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8375 - val_loss: 2837.1941\n",
            "Epoch 476/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7536 - val_loss: 4034.3967\n",
            "Epoch 477/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7884 - val_loss: 6020.1660\n",
            "Epoch 478/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7406 - val_loss: 6591.5674\n",
            "Epoch 479/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7139 - val_loss: 12927.0068\n",
            "Epoch 480/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6971 - val_loss: 24724.6348\n",
            "Epoch 481/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7593 - val_loss: 25339.6797\n",
            "Epoch 482/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6863 - val_loss: 26369.3809\n",
            "Epoch 483/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7573 - val_loss: 35468.4531\n",
            "Epoch 484/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7377 - val_loss: 40329.9375\n",
            "Epoch 485/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6731 - val_loss: 52429.5312\n",
            "Epoch 486/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6869 - val_loss: 89970.0469\n",
            "Epoch 487/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6665 - val_loss: 88598.8984\n",
            "Epoch 488/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6173 - val_loss: 80927.3672\n",
            "Epoch 489/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7556 - val_loss: 113995.1172\n",
            "Epoch 490/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6622 - val_loss: 98672.4688\n",
            "Epoch 491/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7004 - val_loss: 81325.7188\n",
            "Epoch 492/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6431 - val_loss: 98391.4141\n",
            "Epoch 493/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5874 - val_loss: 107312.7266\n",
            "Epoch 494/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6354 - val_loss: 127610.9375\n",
            "Epoch 495/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6722 - val_loss: 141847.2500\n",
            "Epoch 496/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6495 - val_loss: 153897.4531\n",
            "Epoch 497/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.6677 - val_loss: 187802.5781\n",
            "Epoch 498/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6373 - val_loss: 223087.4688\n",
            "Epoch 499/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6154 - val_loss: 209886.0000\n",
            "Epoch 500/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6795 - val_loss: 218538.7031\n",
            "Epoch 501/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5998 - val_loss: 303813.7500\n",
            "Epoch 502/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6257 - val_loss: 256159.8125\n",
            "Epoch 503/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5669 - val_loss: 330346.1250\n",
            "Epoch 504/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5813 - val_loss: 306034.0938\n",
            "Epoch 505/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5914 - val_loss: 365995.0938\n",
            "Epoch 506/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5474 - val_loss: 411485.0938\n",
            "Epoch 507/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5903 - val_loss: 382353.7188\n",
            "Epoch 508/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5687 - val_loss: 501971.4375\n",
            "Epoch 509/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5509 - val_loss: 423782.3438\n",
            "Epoch 510/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6056 - val_loss: 562819.0000\n",
            "Epoch 511/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5743 - val_loss: 438239.7500\n",
            "Epoch 512/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6376 - val_loss: 452494.0625\n",
            "Epoch 513/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5905 - val_loss: 401344.8125\n",
            "Epoch 514/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5723 - val_loss: 394194.9375\n",
            "Epoch 515/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.6763 - val_loss: 367794.2188\n",
            "Epoch 516/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5504 - val_loss: 360925.7500\n",
            "Epoch 517/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5850 - val_loss: 406397.7188\n",
            "Epoch 518/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6010 - val_loss: 432458.7500\n",
            "Epoch 519/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5344 - val_loss: 601460.8125\n",
            "Epoch 520/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5624 - val_loss: 609473.0625\n",
            "Epoch 521/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.6109 - val_loss: 470376.2812\n",
            "Epoch 522/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5663 - val_loss: 673034.4375\n",
            "Epoch 523/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5431 - val_loss: 629515.8125\n",
            "Epoch 524/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5651 - val_loss: 694310.5000\n",
            "Epoch 525/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5587 - val_loss: 602954.8750\n",
            "Epoch 526/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5499 - val_loss: 749088.3750\n",
            "Epoch 527/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5931 - val_loss: 859507.5625\n",
            "Epoch 528/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5979 - val_loss: 914378.8750\n",
            "Epoch 529/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5780 - val_loss: 968570.7500\n",
            "Epoch 530/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5913 - val_loss: 845183.5000\n",
            "Epoch 531/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4854 - val_loss: 701064.0000\n",
            "Epoch 532/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5360 - val_loss: 787841.1250\n",
            "Epoch 533/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6462 - val_loss: 800989.5625\n",
            "Epoch 534/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5681 - val_loss: 1016977.5625\n",
            "Epoch 535/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5050 - val_loss: 1108733.3750\n",
            "Epoch 536/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5459 - val_loss: 866324.4375\n",
            "Epoch 537/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5227 - val_loss: 1026764.2500\n",
            "Epoch 538/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5200 - val_loss: 937643.9375\n",
            "Epoch 539/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5631 - val_loss: 1072497.5000\n",
            "Epoch 540/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5271 - val_loss: 1144963.7500\n",
            "Epoch 541/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5167 - val_loss: 1146214.7500\n",
            "Epoch 542/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4825 - val_loss: 1252157.1250\n",
            "Epoch 543/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4943 - val_loss: 1599130.3750\n",
            "Epoch 544/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4886 - val_loss: 1534999.7500\n",
            "Epoch 545/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4853 - val_loss: 1789242.0000\n",
            "Epoch 546/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5386 - val_loss: 1637686.0000\n",
            "Epoch 547/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4866 - val_loss: 2091795.1250\n",
            "Epoch 548/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5198 - val_loss: 1431580.3750\n",
            "Epoch 549/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4611 - val_loss: 1404228.1250\n",
            "Epoch 550/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4933 - val_loss: 1535122.0000\n",
            "Epoch 551/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5286 - val_loss: 2238281.0000\n",
            "Epoch 552/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4706 - val_loss: 2492290.0000\n",
            "Epoch 553/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4727 - val_loss: 2960697.0000\n",
            "Epoch 554/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4815 - val_loss: 2451841.5000\n",
            "Epoch 555/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4199 - val_loss: 2642164.0000\n",
            "Epoch 556/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4404 - val_loss: 3510440.0000\n",
            "Epoch 557/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5056 - val_loss: 3291885.2500\n",
            "Epoch 558/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5162 - val_loss: 2839656.0000\n",
            "Epoch 559/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4655 - val_loss: 2926791.7500\n",
            "Epoch 560/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5030 - val_loss: 2575460.7500\n",
            "Epoch 561/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4497 - val_loss: 2134209.2500\n",
            "Epoch 562/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4573 - val_loss: 2870915.7500\n",
            "Epoch 563/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4893 - val_loss: 3368351.7500\n",
            "Epoch 564/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4563 - val_loss: 4265123.0000\n",
            "Epoch 565/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4878 - val_loss: 3307833.7500\n",
            "Epoch 566/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4913 - val_loss: 2601893.0000\n",
            "Epoch 567/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4766 - val_loss: 4065455.5000\n",
            "Epoch 568/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4836 - val_loss: 5919582.5000\n",
            "Epoch 569/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4706 - val_loss: 3406462.2500\n",
            "Epoch 570/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4562 - val_loss: 4576620.5000\n",
            "Epoch 571/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4461 - val_loss: 2835008.5000\n",
            "Epoch 572/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4763 - val_loss: 3455569.5000\n",
            "Epoch 573/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4265 - val_loss: 3956620.7500\n",
            "Epoch 574/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4911 - val_loss: 5067371.0000\n",
            "Epoch 575/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4976 - val_loss: 3625845.2500\n",
            "Epoch 576/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4578 - val_loss: 4865944.0000\n",
            "Epoch 577/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4842 - val_loss: 4052410.0000\n",
            "Epoch 578/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4541 - val_loss: 3963311.2500\n",
            "Epoch 579/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4774 - val_loss: 3961259.2500\n",
            "Epoch 580/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4214 - val_loss: 4628544.5000\n",
            "Epoch 581/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4413 - val_loss: 4091727.0000\n",
            "Epoch 582/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4383 - val_loss: 6882885.0000\n",
            "Epoch 583/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4026 - val_loss: 6736471.0000\n",
            "Epoch 584/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4286 - val_loss: 7346927.5000\n",
            "Epoch 585/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4424 - val_loss: 7436676.5000\n",
            "Epoch 586/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4445 - val_loss: 6523370.5000\n",
            "Epoch 587/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4634 - val_loss: 6026519.0000\n",
            "Epoch 588/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4762 - val_loss: 6279381.0000\n",
            "Epoch 589/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3994 - val_loss: 6537099.5000\n",
            "Epoch 590/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4174 - val_loss: 6503404.0000\n",
            "Epoch 591/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4290 - val_loss: 7272938.5000\n",
            "Epoch 592/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4717 - val_loss: 7231526.0000\n",
            "Epoch 593/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4138 - val_loss: 8546191.0000\n",
            "Epoch 594/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4203 - val_loss: 8756888.0000\n",
            "Epoch 595/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4162 - val_loss: 7219612.0000\n",
            "Epoch 596/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4842 - val_loss: 10873037.0000\n",
            "Epoch 597/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4179 - val_loss: 10250403.0000\n",
            "Epoch 598/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4060 - val_loss: 10358137.0000\n",
            "Epoch 599/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4056 - val_loss: 8432726.0000\n",
            "Epoch 600/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3649 - val_loss: 10886726.0000\n",
            "Epoch 601/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3929 - val_loss: 13587880.0000\n",
            "Epoch 602/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4388 - val_loss: 10835946.0000\n",
            "Epoch 603/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3997 - val_loss: 11456254.0000\n",
            "Epoch 604/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3822 - val_loss: 14166731.0000\n",
            "Epoch 605/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4773 - val_loss: 10513189.0000\n",
            "Epoch 606/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3597 - val_loss: 14653613.0000\n",
            "Epoch 607/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4690 - val_loss: 17538696.0000\n",
            "Epoch 608/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4119 - val_loss: 15875720.0000\n",
            "Epoch 609/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3956 - val_loss: 18108340.0000\n",
            "Epoch 610/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3674 - val_loss: 20992454.0000\n",
            "Epoch 611/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4459 - val_loss: 14726686.0000\n",
            "Epoch 612/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4412 - val_loss: 15748931.0000\n",
            "Epoch 613/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3878 - val_loss: 17196484.0000\n",
            "Epoch 614/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3818 - val_loss: 18694910.0000\n",
            "Epoch 615/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3643 - val_loss: 12761171.0000\n",
            "Epoch 616/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3718 - val_loss: 17576796.0000\n",
            "Epoch 617/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3703 - val_loss: 17423442.0000\n",
            "Epoch 618/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3627 - val_loss: 18807620.0000\n",
            "Epoch 619/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4514 - val_loss: 20195790.0000\n",
            "Epoch 620/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3511 - val_loss: 25025804.0000\n",
            "Epoch 621/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3227 - val_loss: 23672954.0000\n",
            "Epoch 622/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3657 - val_loss: 26497308.0000\n",
            "Epoch 623/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4480 - val_loss: 30346302.0000\n",
            "Epoch 624/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3940 - val_loss: 24915740.0000\n",
            "Epoch 625/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4104 - val_loss: 22150808.0000\n",
            "Epoch 626/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3637 - val_loss: 21327382.0000\n",
            "Epoch 627/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3294 - val_loss: 23885034.0000\n",
            "Epoch 628/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3414 - val_loss: 31842048.0000\n",
            "Epoch 629/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3679 - val_loss: 26848516.0000\n",
            "Epoch 630/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4406 - val_loss: 38002944.0000\n",
            "Epoch 631/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3619 - val_loss: 25340446.0000\n",
            "Epoch 632/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3597 - val_loss: 27459418.0000\n",
            "Epoch 633/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3179 - val_loss: 29411980.0000\n",
            "Epoch 634/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3349 - val_loss: 30662664.0000\n",
            "Epoch 635/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3657 - val_loss: 31396524.0000\n",
            "Epoch 636/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3603 - val_loss: 33081162.0000\n",
            "Epoch 637/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3194 - val_loss: 27055170.0000\n",
            "Epoch 638/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3449 - val_loss: 39453136.0000\n",
            "Epoch 639/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3701 - val_loss: 26950698.0000\n",
            "Epoch 640/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3454 - val_loss: 41731848.0000\n",
            "Epoch 641/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2919 - val_loss: 35401092.0000\n",
            "Epoch 642/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4243 - val_loss: 34550080.0000\n",
            "Epoch 643/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3371 - val_loss: 34753540.0000\n",
            "Epoch 644/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3051 - val_loss: 38343212.0000\n",
            "Epoch 645/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3373 - val_loss: 51685324.0000\n",
            "Epoch 646/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3651 - val_loss: 50214984.0000\n",
            "Epoch 647/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3281 - val_loss: 52476004.0000\n",
            "Epoch 648/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3151 - val_loss: 38196936.0000\n",
            "Epoch 649/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3898 - val_loss: 38457064.0000\n",
            "Epoch 650/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3740 - val_loss: 44093552.0000\n",
            "Epoch 651/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3475 - val_loss: 45678704.0000\n",
            "Epoch 652/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3917 - val_loss: 54858740.0000\n",
            "Epoch 653/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3751 - val_loss: 39672676.0000\n",
            "Epoch 654/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3088 - val_loss: 36131076.0000\n",
            "Epoch 655/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3585 - val_loss: 39044692.0000\n",
            "Epoch 656/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3301 - val_loss: 38006544.0000\n",
            "Epoch 657/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3191 - val_loss: 40887596.0000\n",
            "Epoch 658/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3213 - val_loss: 47918948.0000\n",
            "Epoch 659/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3368 - val_loss: 40656160.0000\n",
            "Epoch 660/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3461 - val_loss: 48441016.0000\n",
            "Epoch 661/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3017 - val_loss: 54098180.0000\n",
            "Epoch 662/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3751 - val_loss: 58958464.0000\n",
            "Epoch 663/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3559 - val_loss: 47675264.0000\n",
            "Epoch 664/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3155 - val_loss: 53033192.0000\n",
            "Epoch 665/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3197 - val_loss: 64964120.0000\n",
            "Epoch 666/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3060 - val_loss: 58090160.0000\n",
            "Epoch 667/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3145 - val_loss: 53605452.0000\n",
            "Epoch 668/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2765 - val_loss: 72919056.0000\n",
            "Epoch 669/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2999 - val_loss: 100690096.0000\n",
            "Epoch 670/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3321 - val_loss: 92994208.0000\n",
            "Epoch 671/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3176 - val_loss: 73611920.0000\n",
            "Epoch 672/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3045 - val_loss: 100437456.0000\n",
            "Epoch 673/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3046 - val_loss: 82510520.0000\n",
            "Epoch 674/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2823 - val_loss: 68261528.0000\n",
            "Epoch 675/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2797 - val_loss: 78651512.0000\n",
            "Epoch 676/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2595 - val_loss: 104642560.0000\n",
            "Epoch 677/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2702 - val_loss: 91383680.0000\n",
            "Epoch 678/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3145 - val_loss: 136808176.0000\n",
            "Epoch 679/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2780 - val_loss: 80450680.0000\n",
            "Epoch 680/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3135 - val_loss: 91718672.0000\n",
            "Epoch 681/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3306 - val_loss: 93223344.0000\n",
            "Epoch 682/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2582 - val_loss: 81864576.0000\n",
            "Epoch 683/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3252 - val_loss: 79502704.0000\n",
            "Epoch 684/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2839 - val_loss: 174538160.0000\n",
            "Epoch 685/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2954 - val_loss: 100111408.0000\n",
            "Epoch 686/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2674 - val_loss: 108377072.0000\n",
            "Epoch 687/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2894 - val_loss: 116419248.0000\n",
            "Epoch 688/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3581 - val_loss: 76778376.0000\n",
            "Epoch 689/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2965 - val_loss: 79110104.0000\n",
            "Epoch 690/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3478 - val_loss: 75129992.0000\n",
            "Epoch 691/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3114 - val_loss: 82606968.0000\n",
            "Epoch 692/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3285 - val_loss: 79046584.0000\n",
            "Epoch 693/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2644 - val_loss: 53212488.0000\n",
            "Epoch 694/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2915 - val_loss: 69897688.0000\n",
            "Epoch 695/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2930 - val_loss: 72662360.0000\n",
            "Epoch 696/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3019 - val_loss: 92541000.0000\n",
            "Epoch 697/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2921 - val_loss: 81562960.0000\n",
            "Epoch 698/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3978 - val_loss: 98625264.0000\n",
            "Epoch 699/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2841 - val_loss: 51083732.0000\n",
            "Epoch 700/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3309 - val_loss: 71454776.0000\n",
            "Epoch 701/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2855 - val_loss: 68364112.0000\n",
            "Epoch 702/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2614 - val_loss: 73831824.0000\n",
            "Epoch 703/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2487 - val_loss: 71504880.0000\n",
            "Epoch 704/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2931 - val_loss: 88398272.0000\n",
            "Epoch 705/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2685 - val_loss: 99363600.0000\n",
            "Epoch 706/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3073 - val_loss: 103304744.0000\n",
            "Epoch 707/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2655 - val_loss: 124330232.0000\n",
            "Epoch 708/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2737 - val_loss: 96675936.0000\n",
            "Epoch 709/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2688 - val_loss: 82670680.0000\n",
            "Epoch 710/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2646 - val_loss: 108462312.0000\n",
            "Epoch 711/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3055 - val_loss: 154923776.0000\n",
            "Epoch 712/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2770 - val_loss: 90822128.0000\n",
            "Epoch 713/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3051 - val_loss: 100880944.0000\n",
            "Epoch 714/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2331 - val_loss: 92718536.0000\n",
            "Epoch 715/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2637 - val_loss: 85208424.0000\n",
            "Epoch 716/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2639 - val_loss: 121110712.0000\n",
            "Epoch 717/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2756 - val_loss: 134861984.0000\n",
            "Epoch 718/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2417 - val_loss: 96108112.0000\n",
            "Epoch 719/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2672 - val_loss: 98316832.0000\n",
            "Epoch 720/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2544 - val_loss: 102454896.0000\n",
            "Epoch 721/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2588 - val_loss: 111518464.0000\n",
            "Epoch 722/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2761 - val_loss: 135200896.0000\n",
            "Epoch 723/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2590 - val_loss: 140414128.0000\n",
            "Epoch 724/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2811 - val_loss: 126995192.0000\n",
            "Epoch 725/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2639 - val_loss: 98967152.0000\n",
            "Epoch 726/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2696 - val_loss: 117124024.0000\n",
            "Epoch 727/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2599 - val_loss: 109551904.0000\n",
            "Epoch 728/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2644 - val_loss: 158225568.0000\n",
            "Epoch 729/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3057 - val_loss: 100309104.0000\n",
            "Epoch 730/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2865 - val_loss: 117325544.0000\n",
            "Epoch 731/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3008 - val_loss: 120451152.0000\n",
            "Epoch 732/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2815 - val_loss: 73537128.0000\n",
            "Epoch 733/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2303 - val_loss: 109790288.0000\n",
            "Epoch 734/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2972 - val_loss: 104746928.0000\n",
            "Epoch 735/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2463 - val_loss: 96415384.0000\n",
            "Epoch 736/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2820 - val_loss: 118458728.0000\n",
            "Epoch 737/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2384 - val_loss: 141547792.0000\n",
            "Epoch 738/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2597 - val_loss: 130601208.0000\n",
            "Epoch 739/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2725 - val_loss: 143276256.0000\n",
            "Epoch 740/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2395 - val_loss: 124412496.0000\n",
            "Epoch 741/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3021 - val_loss: 103206344.0000\n",
            "Epoch 742/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2156 - val_loss: 134304640.0000\n",
            "Epoch 743/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2622 - val_loss: 96714824.0000\n",
            "Epoch 744/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2575 - val_loss: 132405496.0000\n",
            "Epoch 745/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2108 - val_loss: 136538896.0000\n",
            "Epoch 746/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2768 - val_loss: 146279024.0000\n",
            "Epoch 747/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2391 - val_loss: 148688896.0000\n",
            "Epoch 748/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2405 - val_loss: 88967064.0000\n",
            "Epoch 749/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2380 - val_loss: 96179608.0000\n",
            "Epoch 750/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2243 - val_loss: 128322360.0000\n",
            "Epoch 751/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2415 - val_loss: 151258384.0000\n",
            "Epoch 752/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2048 - val_loss: 173539680.0000\n",
            "Epoch 753/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2482 - val_loss: 135164048.0000\n",
            "Epoch 754/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2428 - val_loss: 131537176.0000\n",
            "Epoch 755/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2410 - val_loss: 156898880.0000\n",
            "Epoch 756/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2374 - val_loss: 113691728.0000\n",
            "Epoch 757/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2842 - val_loss: 133033088.0000\n",
            "Epoch 758/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2300 - val_loss: 133127912.0000\n",
            "Epoch 759/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2245 - val_loss: 152342720.0000\n",
            "Epoch 760/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2282 - val_loss: 177646784.0000\n",
            "Epoch 761/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2439 - val_loss: 144487824.0000\n",
            "Epoch 762/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2213 - val_loss: 193783872.0000\n",
            "Epoch 763/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2574 - val_loss: 169883392.0000\n",
            "Epoch 764/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2171 - val_loss: 154221056.0000\n",
            "Epoch 765/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2338 - val_loss: 141879552.0000\n",
            "Epoch 766/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2484 - val_loss: 239852880.0000\n",
            "Epoch 767/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2296 - val_loss: 127190376.0000\n",
            "Epoch 768/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2167 - val_loss: 173794320.0000\n",
            "Epoch 769/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2425 - val_loss: 176951616.0000\n",
            "Epoch 770/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2355 - val_loss: 125733504.0000\n",
            "Epoch 771/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2530 - val_loss: 172602944.0000\n",
            "Epoch 772/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2548 - val_loss: 123082504.0000\n",
            "Epoch 773/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2090 - val_loss: 118664944.0000\n",
            "Epoch 774/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1818 - val_loss: 131304280.0000\n",
            "Epoch 775/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2902 - val_loss: 127470440.0000\n",
            "Epoch 776/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2539 - val_loss: 137406000.0000\n",
            "Epoch 777/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2214 - val_loss: 123147768.0000\n",
            "Epoch 778/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2244 - val_loss: 120024272.0000\n",
            "Epoch 779/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2826 - val_loss: 136560544.0000\n",
            "Epoch 780/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2194 - val_loss: 152898928.0000\n",
            "Epoch 781/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2353 - val_loss: 120144248.0000\n",
            "Epoch 782/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1998 - val_loss: 173744960.0000\n",
            "Epoch 783/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2265 - val_loss: 192558480.0000\n",
            "Epoch 784/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2240 - val_loss: 155295712.0000\n",
            "Epoch 785/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2201 - val_loss: 146146496.0000\n",
            "Epoch 786/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1986 - val_loss: 128705472.0000\n",
            "Epoch 787/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2524 - val_loss: 182838832.0000\n",
            "Epoch 788/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2547 - val_loss: 157943280.0000\n",
            "Epoch 789/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2031 - val_loss: 124606528.0000\n",
            "Epoch 790/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2217 - val_loss: 145158944.0000\n",
            "Epoch 791/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2618 - val_loss: 139254368.0000\n",
            "Epoch 792/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2545 - val_loss: 163074800.0000\n",
            "Epoch 793/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2028 - val_loss: 125406856.0000\n",
            "Epoch 794/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2014 - val_loss: 215660592.0000\n",
            "Epoch 795/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2211 - val_loss: 257813152.0000\n",
            "Epoch 796/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2062 - val_loss: 134476032.0000\n",
            "Epoch 797/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1942 - val_loss: 180411696.0000\n",
            "Epoch 798/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2722 - val_loss: 190605616.0000\n",
            "Epoch 799/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2173 - val_loss: 113793200.0000\n",
            "Epoch 800/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2109 - val_loss: 251254768.0000\n",
            "Epoch 801/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2177 - val_loss: 231095104.0000\n",
            "Epoch 802/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2270 - val_loss: 139683712.0000\n",
            "Epoch 803/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2370 - val_loss: 137737648.0000\n",
            "Epoch 804/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2187 - val_loss: 181746928.0000\n",
            "Epoch 805/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1932 - val_loss: 214559984.0000\n",
            "Epoch 806/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1963 - val_loss: 146976912.0000\n",
            "Epoch 807/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2189 - val_loss: 138737008.0000\n",
            "Epoch 808/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2264 - val_loss: 174396544.0000\n",
            "Epoch 809/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1999 - val_loss: 158952544.0000\n",
            "Epoch 810/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2110 - val_loss: 145883632.0000\n",
            "Epoch 811/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2253 - val_loss: 153601584.0000\n",
            "Epoch 812/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2284 - val_loss: 162046480.0000\n",
            "Epoch 813/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2318 - val_loss: 146676288.0000\n",
            "Epoch 814/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2229 - val_loss: 175368592.0000\n",
            "Epoch 815/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2120 - val_loss: 148759008.0000\n",
            "Epoch 816/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1883 - val_loss: 130826112.0000\n",
            "Epoch 817/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2055 - val_loss: 153762464.0000\n",
            "Epoch 818/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2029 - val_loss: 195373504.0000\n",
            "Epoch 819/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2257 - val_loss: 181689088.0000\n",
            "Epoch 820/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2087 - val_loss: 206421952.0000\n",
            "Epoch 821/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2589 - val_loss: 141212112.0000\n",
            "Epoch 822/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2290 - val_loss: 158400560.0000\n",
            "Epoch 823/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2217 - val_loss: 127727664.0000\n",
            "Epoch 824/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2284 - val_loss: 171661200.0000\n",
            "Epoch 825/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2427 - val_loss: 100238496.0000\n",
            "Epoch 826/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2078 - val_loss: 109529216.0000\n",
            "Epoch 827/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2174 - val_loss: 152422560.0000\n",
            "Epoch 828/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2433 - val_loss: 120289216.0000\n",
            "Epoch 829/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2071 - val_loss: 113129840.0000\n",
            "Epoch 830/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2264 - val_loss: 161641808.0000\n",
            "Epoch 831/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2144 - val_loss: 140125088.0000\n",
            "Epoch 832/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2009 - val_loss: 286201664.0000\n",
            "Epoch 833/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2123 - val_loss: 144092320.0000\n",
            "Epoch 834/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2271 - val_loss: 142930320.0000\n",
            "Epoch 835/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2218 - val_loss: 176604320.0000\n",
            "Epoch 836/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2701 - val_loss: 143685344.0000\n",
            "Epoch 837/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2178 - val_loss: 151383056.0000\n",
            "Epoch 838/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1989 - val_loss: 171712928.0000\n",
            "Epoch 839/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2232 - val_loss: 206092144.0000\n",
            "Epoch 840/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1920 - val_loss: 167313024.0000\n",
            "Epoch 841/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2191 - val_loss: 216363280.0000\n",
            "Epoch 842/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1949 - val_loss: 183714480.0000\n",
            "Epoch 843/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2530 - val_loss: 173553632.0000\n",
            "Epoch 844/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2027 - val_loss: 118162256.0000\n",
            "Epoch 845/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1948 - val_loss: 152282496.0000\n",
            "Epoch 846/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2059 - val_loss: 134817792.0000\n",
            "Epoch 847/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2190 - val_loss: 106583592.0000\n",
            "Epoch 848/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2613 - val_loss: 137063216.0000\n",
            "Epoch 849/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2418 - val_loss: 130963360.0000\n",
            "Epoch 850/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2033 - val_loss: 132229464.0000\n",
            "Epoch 851/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2049 - val_loss: 146077088.0000\n",
            "Epoch 852/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1882 - val_loss: 120160048.0000\n",
            "Epoch 853/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2209 - val_loss: 140433872.0000\n",
            "Epoch 854/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2062 - val_loss: 95962936.0000\n",
            "Epoch 855/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1995 - val_loss: 86108128.0000\n",
            "Epoch 856/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2167 - val_loss: 132700128.0000\n",
            "Epoch 857/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2079 - val_loss: 144568624.0000\n",
            "Epoch 858/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2026 - val_loss: 194291552.0000\n",
            "Epoch 859/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2014 - val_loss: 176443232.0000\n",
            "Epoch 860/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1833 - val_loss: 194785680.0000\n",
            "Epoch 861/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1768 - val_loss: 192602016.0000\n",
            "Epoch 862/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2831 - val_loss: 103111504.0000\n",
            "Epoch 863/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2254 - val_loss: 129722448.0000\n",
            "Epoch 864/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2241 - val_loss: 97886152.0000\n",
            "Epoch 865/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2151 - val_loss: 149671664.0000\n",
            "Epoch 866/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2233 - val_loss: 158910560.0000\n",
            "Epoch 867/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1679 - val_loss: 148374320.0000\n",
            "Epoch 868/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2188 - val_loss: 172597680.0000\n",
            "Epoch 869/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2165 - val_loss: 162747728.0000\n",
            "Epoch 870/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2357 - val_loss: 141587104.0000\n",
            "Epoch 871/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2016 - val_loss: 134752368.0000\n",
            "Epoch 872/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2137 - val_loss: 117910664.0000\n",
            "Epoch 873/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2177 - val_loss: 127723528.0000\n",
            "Epoch 874/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1891 - val_loss: 144107632.0000\n",
            "Epoch 875/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2006 - val_loss: 176566640.0000\n",
            "Epoch 876/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1791 - val_loss: 156968864.0000\n",
            "Epoch 877/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1786 - val_loss: 184601872.0000\n",
            "Epoch 878/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2218 - val_loss: 171819232.0000\n",
            "Epoch 879/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2000 - val_loss: 171709792.0000\n",
            "Epoch 880/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1747 - val_loss: 120642640.0000\n",
            "Epoch 881/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1803 - val_loss: 182181024.0000\n",
            "Epoch 882/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1540 - val_loss: 160976832.0000\n",
            "Epoch 883/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2005 - val_loss: 151208560.0000\n",
            "Epoch 884/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2110 - val_loss: 151824096.0000\n",
            "Epoch 885/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1837 - val_loss: 152160544.0000\n",
            "Epoch 886/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2126 - val_loss: 156656928.0000\n",
            "Epoch 887/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2273 - val_loss: 179812608.0000\n",
            "Epoch 888/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2103 - val_loss: 168517584.0000\n",
            "Epoch 889/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1823 - val_loss: 132216664.0000\n",
            "Epoch 890/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1667 - val_loss: 134705632.0000\n",
            "Epoch 891/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2132 - val_loss: 166796944.0000\n",
            "Epoch 892/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1762 - val_loss: 120955288.0000\n",
            "Epoch 893/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2383 - val_loss: 132592640.0000\n",
            "Epoch 894/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1834 - val_loss: 135784608.0000\n",
            "Epoch 895/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1879 - val_loss: 97816760.0000\n",
            "Epoch 896/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1911 - val_loss: 106288448.0000\n",
            "Epoch 897/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1617 - val_loss: 142957264.0000\n",
            "Epoch 898/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1797 - val_loss: 130160512.0000\n",
            "Epoch 899/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1864 - val_loss: 126605856.0000\n",
            "Epoch 900/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1880 - val_loss: 153286624.0000\n",
            "Epoch 901/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2373 - val_loss: 101758384.0000\n",
            "Epoch 902/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1698 - val_loss: 102320096.0000\n",
            "Epoch 903/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2247 - val_loss: 75220744.0000\n",
            "Epoch 904/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1722 - val_loss: 113058840.0000\n",
            "Epoch 905/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1776 - val_loss: 130295016.0000\n",
            "Epoch 906/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1949 - val_loss: 143100368.0000\n",
            "Epoch 907/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2065 - val_loss: 143110688.0000\n",
            "Epoch 908/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2016 - val_loss: 150482528.0000\n",
            "Epoch 909/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1809 - val_loss: 158024848.0000\n",
            "Epoch 910/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1913 - val_loss: 134364000.0000\n",
            "Epoch 911/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2111 - val_loss: 118909952.0000\n",
            "Epoch 912/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2122 - val_loss: 129189880.0000\n",
            "Epoch 913/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1766 - val_loss: 109493808.0000\n",
            "Epoch 914/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1647 - val_loss: 130010968.0000\n",
            "Epoch 915/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1718 - val_loss: 116648152.0000\n",
            "Epoch 916/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1862 - val_loss: 134095048.0000\n",
            "Epoch 917/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1651 - val_loss: 161617696.0000\n",
            "Epoch 918/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1958 - val_loss: 159598016.0000\n",
            "Epoch 919/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1596 - val_loss: 143840336.0000\n",
            "Epoch 920/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1918 - val_loss: 127284720.0000\n",
            "Epoch 921/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2286 - val_loss: 112487808.0000\n",
            "Epoch 922/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1988 - val_loss: 96704160.0000\n",
            "Epoch 923/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2017 - val_loss: 113489200.0000\n",
            "Epoch 924/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2027 - val_loss: 111129760.0000\n",
            "Epoch 925/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1847 - val_loss: 130523264.0000\n",
            "Epoch 926/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1794 - val_loss: 126862208.0000\n",
            "Epoch 927/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2269 - val_loss: 99700584.0000\n",
            "Epoch 928/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1950 - val_loss: 96750648.0000\n",
            "Epoch 929/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2054 - val_loss: 112401360.0000\n",
            "Epoch 930/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1539 - val_loss: 121688264.0000\n",
            "Epoch 931/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2058 - val_loss: 141412144.0000\n",
            "Epoch 932/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1900 - val_loss: 117004008.0000\n",
            "Epoch 933/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1865 - val_loss: 114492816.0000\n",
            "Epoch 934/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1876 - val_loss: 124669456.0000\n",
            "Epoch 935/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1566 - val_loss: 150808832.0000\n",
            "Epoch 936/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1753 - val_loss: 167287392.0000\n",
            "Epoch 937/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1992 - val_loss: 203297920.0000\n",
            "Epoch 938/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1854 - val_loss: 122929680.0000\n",
            "Epoch 939/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1845 - val_loss: 154732256.0000\n",
            "Epoch 940/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1896 - val_loss: 102565240.0000\n",
            "Epoch 941/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1766 - val_loss: 117440568.0000\n",
            "Epoch 942/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2022 - val_loss: 111114768.0000\n",
            "Epoch 943/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1848 - val_loss: 117471384.0000\n",
            "Epoch 944/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1669 - val_loss: 99235120.0000\n",
            "Epoch 945/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1781 - val_loss: 143947120.0000\n",
            "Epoch 946/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1666 - val_loss: 108505320.0000\n",
            "Epoch 947/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1877 - val_loss: 118772592.0000\n",
            "Epoch 948/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2136 - val_loss: 112876128.0000\n",
            "Epoch 949/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1632 - val_loss: 130026904.0000\n",
            "Epoch 950/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1971 - val_loss: 111557248.0000\n",
            "Epoch 951/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2033 - val_loss: 98889616.0000\n",
            "Epoch 952/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1675 - val_loss: 89272824.0000\n",
            "Epoch 953/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1951 - val_loss: 121755688.0000\n",
            "Epoch 954/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1769 - val_loss: 120987312.0000\n",
            "Epoch 955/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2230 - val_loss: 123841840.0000\n",
            "Epoch 956/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2214 - val_loss: 69127552.0000\n",
            "Epoch 957/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1806 - val_loss: 116386688.0000\n",
            "Epoch 958/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1764 - val_loss: 91543896.0000\n",
            "Epoch 959/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1929 - val_loss: 98367792.0000\n",
            "Epoch 960/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2007 - val_loss: 189097040.0000\n",
            "Epoch 961/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2284 - val_loss: 83242296.0000\n",
            "Epoch 962/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1664 - val_loss: 127066272.0000\n",
            "Epoch 963/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1668 - val_loss: 102323328.0000\n",
            "Epoch 964/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2418 - val_loss: 142198352.0000\n",
            "Epoch 965/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1481 - val_loss: 109787008.0000\n",
            "Epoch 966/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1487 - val_loss: 102855304.0000\n",
            "Epoch 967/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1884 - val_loss: 111478840.0000\n",
            "Epoch 968/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1463 - val_loss: 125826312.0000\n",
            "Epoch 969/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2061 - val_loss: 93625984.0000\n",
            "Epoch 970/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1691 - val_loss: 97647632.0000\n",
            "Epoch 971/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1897 - val_loss: 91735360.0000\n",
            "Epoch 972/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1974 - val_loss: 83445024.0000\n",
            "Epoch 973/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1717 - val_loss: 94836248.0000\n",
            "Epoch 974/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1947 - val_loss: 81464088.0000\n",
            "Epoch 975/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1578 - val_loss: 95888824.0000\n",
            "Epoch 976/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2073 - val_loss: 116510104.0000\n",
            "Epoch 977/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1823 - val_loss: 76563240.0000\n",
            "Epoch 978/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2012 - val_loss: 80440912.0000\n",
            "Epoch 979/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2083 - val_loss: 73624216.0000\n",
            "Epoch 980/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1789 - val_loss: 87321808.0000\n",
            "Epoch 981/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1612 - val_loss: 130486040.0000\n",
            "Epoch 982/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1794 - val_loss: 108134400.0000\n",
            "Epoch 983/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1925 - val_loss: 110497568.0000\n",
            "Epoch 984/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1608 - val_loss: 95590136.0000\n",
            "Epoch 985/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1998 - val_loss: 121200208.0000\n",
            "Epoch 986/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1653 - val_loss: 108554272.0000\n",
            "Epoch 987/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1744 - val_loss: 98716512.0000\n",
            "Epoch 988/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1836 - val_loss: 92892576.0000\n",
            "Epoch 989/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1856 - val_loss: 93894128.0000\n",
            "Epoch 990/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1677 - val_loss: 108970368.0000\n",
            "Epoch 991/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2288 - val_loss: 137613408.0000\n",
            "Epoch 992/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1698 - val_loss: 92005904.0000\n",
            "Epoch 993/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2073 - val_loss: 93203304.0000\n",
            "Epoch 994/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2027 - val_loss: 91515704.0000\n",
            "Epoch 995/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1784 - val_loss: 71328592.0000\n",
            "Epoch 996/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1629 - val_loss: 67730160.0000\n",
            "Epoch 997/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1751 - val_loss: 93066784.0000\n",
            "Epoch 998/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1968 - val_loss: 109609368.0000\n",
            "Epoch 999/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1573 - val_loss: 81894200.0000\n",
            "Epoch 1000/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1627 - val_loss: 98128440.0000\n",
            "Epoch 1001/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1679 - val_loss: 91105576.0000\n",
            "Epoch 1002/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1478 - val_loss: 226367520.0000\n",
            "Epoch 1003/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1544 - val_loss: 126974928.0000\n",
            "Epoch 1004/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1915 - val_loss: 109605248.0000\n",
            "Epoch 1005/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1467 - val_loss: 119701080.0000\n",
            "Epoch 1006/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2001 - val_loss: 102354368.0000\n",
            "Epoch 1007/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1468 - val_loss: 125872464.0000\n",
            "Epoch 1008/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1999 - val_loss: 78030968.0000\n",
            "Epoch 1009/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2241 - val_loss: 91056936.0000\n",
            "Epoch 1010/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2051 - val_loss: 81082488.0000\n",
            "Epoch 1011/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1875 - val_loss: 86747840.0000\n",
            "Epoch 1012/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2297 - val_loss: 58882848.0000\n",
            "Epoch 1013/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1559 - val_loss: 89111224.0000\n",
            "Epoch 1014/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1634 - val_loss: 76233976.0000\n",
            "Epoch 1015/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1468 - val_loss: 107282192.0000\n",
            "Epoch 1016/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1751 - val_loss: 141804448.0000\n",
            "Epoch 1017/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1767 - val_loss: 126063808.0000\n",
            "Epoch 1018/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1780 - val_loss: 117501104.0000\n",
            "Epoch 1019/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1751 - val_loss: 95048824.0000\n",
            "Epoch 1020/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1531 - val_loss: 95341392.0000\n",
            "Epoch 1021/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1757 - val_loss: 110937792.0000\n",
            "Epoch 1022/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1753 - val_loss: 100221608.0000\n",
            "Epoch 1023/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2006 - val_loss: 84566304.0000\n",
            "Epoch 1024/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1915 - val_loss: 86646864.0000\n",
            "Epoch 1025/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1680 - val_loss: 100505464.0000\n",
            "Epoch 1026/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1741 - val_loss: 69449704.0000\n",
            "Epoch 1027/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2190 - val_loss: 104233192.0000\n",
            "Epoch 1028/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1839 - val_loss: 125257208.0000\n",
            "Epoch 1029/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1685 - val_loss: 117886648.0000\n",
            "Epoch 1030/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1527 - val_loss: 114975680.0000\n",
            "Epoch 1031/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1758 - val_loss: 119494328.0000\n",
            "Epoch 1032/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1656 - val_loss: 123581408.0000\n",
            "Epoch 1033/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1788 - val_loss: 67389392.0000\n",
            "Epoch 1034/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1885 - val_loss: 68154568.0000\n",
            "Epoch 1035/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1733 - val_loss: 59355672.0000\n",
            "Epoch 1036/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1801 - val_loss: 88489200.0000\n",
            "Epoch 1037/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1584 - val_loss: 113182320.0000\n",
            "Epoch 1038/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1759 - val_loss: 117665024.0000\n",
            "Epoch 1039/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1626 - val_loss: 121593192.0000\n",
            "Epoch 1040/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1504 - val_loss: 92793432.0000\n",
            "Epoch 1041/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2056 - val_loss: 94973240.0000\n",
            "Epoch 1042/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1668 - val_loss: 83298832.0000\n",
            "Epoch 1043/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2023 - val_loss: 83486976.0000\n",
            "Epoch 1044/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1693 - val_loss: 89268336.0000\n",
            "Epoch 1045/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1985 - val_loss: 86783408.0000\n",
            "Epoch 1046/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1430 - val_loss: 83556784.0000\n",
            "Epoch 1047/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1550 - val_loss: 95348280.0000\n",
            "Epoch 1048/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1604 - val_loss: 101992640.0000\n",
            "Epoch 1049/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2056 - val_loss: 106847080.0000\n",
            "Epoch 1050/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1899 - val_loss: 97478776.0000\n",
            "Epoch 1051/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1475 - val_loss: 99132680.0000\n",
            "Epoch 1052/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1482 - val_loss: 103803896.0000\n",
            "Epoch 1053/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1535 - val_loss: 102679960.0000\n",
            "Epoch 1054/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1613 - val_loss: 113931360.0000\n",
            "Epoch 1055/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1471 - val_loss: 96400560.0000\n",
            "Epoch 1056/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1775 - val_loss: 96970520.0000\n",
            "Epoch 1057/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1584 - val_loss: 123063360.0000\n",
            "Epoch 1058/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1980 - val_loss: 95352736.0000\n",
            "Epoch 1059/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1564 - val_loss: 81008696.0000\n",
            "Epoch 1060/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2420 - val_loss: 101974528.0000\n",
            "Epoch 1061/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1630 - val_loss: 84155144.0000\n",
            "Epoch 1062/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1274 - val_loss: 97700592.0000\n",
            "Epoch 1063/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1543 - val_loss: 99222728.0000\n",
            "Epoch 1064/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1619 - val_loss: 105016800.0000\n",
            "Epoch 1065/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1498 - val_loss: 123380360.0000\n",
            "Epoch 1066/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1341 - val_loss: 116547360.0000\n",
            "Epoch 1067/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1273 - val_loss: 139276816.0000\n",
            "Epoch 1068/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1722 - val_loss: 117260088.0000\n",
            "Epoch 1069/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1639 - val_loss: 89531440.0000\n",
            "Epoch 1070/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1517 - val_loss: 109117312.0000\n",
            "Epoch 1071/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1524 - val_loss: 135402864.0000\n",
            "Epoch 1072/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1688 - val_loss: 92823848.0000\n",
            "Epoch 1073/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1484 - val_loss: 117276888.0000\n",
            "Epoch 1074/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1710 - val_loss: 91521536.0000\n",
            "Epoch 1075/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1532 - val_loss: 126663048.0000\n",
            "Epoch 1076/10000\n",
            " 1/23 [>.............................] - ETA: 0s - loss: 0.0988Restoring model weights from the end of the best epoch: 76.\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1568 - val_loss: 115618632.0000\n",
            "Epoch 1076: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_2707984/581984845.py:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAHHCAYAAABTMjf2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABvYElEQVR4nO3dd3wT9eMG8CejabpLBy0d0LJbRlkFWbKKZchwICJqwYGjiogLVMSBgiLKT6mCA8QJCghfAZkyBBlljwKlUDZtodC9k/v9EXJNmqRN27RJk+f9evFqcne5++Satg+fKREEQQARERGRA5JauwBERERE1sIgRERERA6LQYiIiIgcFoMQEREROSwGISIiInJYDEJERETksBiEiIiIyGExCBEREZHDYhAiIiIih8UgRA1OYmIievXqBTc3N0gkEhw5cgTvvvsuJBJJvZflhx9+gEQiwYULFyx+7u3bt0MikWD79u3itgkTJiAsLKxa5+nfvz/at29v2cI1AOZ+Jsy9P2FhYZgwYUK1ylCT19i7CxcuQCKR4Icffqj2a439TBhTlz+XZH/k1i4AUXWUlpZizJgxUCqV+Pzzz+Hq6opmzZpZu1hUQ9euXcM333yD0aNHo1OnTtYuDhE5INYIUYNy7tw5XLx4Ea+++iomTZqERx99FI0aNcLbb7+NwsJCaxePqunatWt47733cOTIEWsXhYgcFGuEqEHJyMgAAHh7e+ttl8vlkMv5cSYiouphjRA1GBMmTEC/fv0AAGPGjIFEIkH//v0BGPYHWbJkCSQSCRYvXqx3jo8++ggSiQTr168Xt50+fRoPPvggfHx8oFQq0a1bN/zvf/8zuP7JkycxcOBAuLi4ICQkBLNmzYJara72+7h48SKef/55tGnTBi4uLvD19cWYMWPqvD/DwYMH0atXL7i4uCA8PBwLFy40OCYjIwNPPvkkAgICoFQqERUVhaVLlxocl5+fj1deeQWhoaFwdnZGmzZt8Omnn0IQBL3jNm/ejD59+sDb2xvu7u5o06YN3nzzTQCa/h7R0dEAgIkTJ0IikRj0Hdm3bx+GDBkCLy8vuLq6ol+/fti9e7dBeXbt2oXo6GgolUq0aNECixYtqs2twqZNm+Dq6opx48ahrKysVueq6Pz58xgzZgx8fHzg6uqKu+66C+vWrTM47ssvv0S7du3g6uqKRo0aoVu3bvj111/F/bm5uZgyZQrCwsLg7OyMxo0bY/DgwTh06FCl19f+rCQnJ+PRRx+Fl5cX/P39MWPGDAiCgMuXL2PUqFHw9PREYGAg5s2bZ3AOcz8nWVlZmDBhAry8vODt7Y24uDhkZWUZLZe5P4e18dVXX6Fdu3ZwdnZGUFAQ4uPjDcpz9uxZPPDAAwgMDIRSqURISAgefvhhZGdni8dU9rmmhof/haYG45lnnkFwcDA++ugjTJ48GdHR0QgICDB67MSJE7Fq1SpMnToVgwcPRmhoKI4fP4733nsPTz75JIYNGwZAE2569+6N4OBgTJs2DW5ubvj9998xevRorFy5Evfddx8AIC0tDQMGDEBZWZl43DfffAMXF5dqv4/ExET8999/ePjhhxESEoILFy7g66+/Rv/+/ZGUlARXV9ea3yQTbt++jWHDhuGhhx7CuHHj8Pvvv+O5556DQqHAE088AQAoLCxE//79kZKSghdeeAHh4eH4448/MGHCBGRlZeGll14CAAiCgJEjR2Lbtm148skn0alTJ2zcuBGvvfYarl69is8//xyA5t7ee++96NixI95//304OzsjJSVFDDIRERF4//338c4772DSpEno27cvAKBXr14AgH/++QdDhw5F165dMXPmTEilUixZsgQDBw7Ev//+i+7duwMAjh8/jnvuuQf+/v549913UVZWhpkzZ5r8bFRl7dq1ePDBBzF27FgsXrwYMpms5je+gvT0dPTq1QsFBQWYPHkyfH19sXTpUowcORIrVqwQP2/ffvstJk+ejAcffBAvvfQSioqKcOzYMezbtw+PPPIIAODZZ5/FihUr8MILLyAyMhKZmZnYtWsXTp06hS5dulRZlrFjxyIiIgJz5szBunXrMGvWLPj4+GDRokUYOHAgPv74Y/zyyy949dVXER0djbvvvhtA9T4no0aNwq5du/Dss88iIiICf/75J+Li4gzKYu7PYW28++67eO+99xATE4PnnnsOZ86cwddff43ExETs3r0bTk5OKCkpQWxsLIqLi/Hiiy8iMDAQV69exdq1a5GVlQUvL68qP9fUAAlEDci2bdsEAMIff/yht33mzJlCxY/z9evXBR8fH2Hw4MFCcXGx0LlzZ6Fp06ZCdna2eMygQYOEDh06CEVFReI2tVot9OrVS2jVqpW4bcqUKQIAYd++feK2jIwMwcvLSwAgpKammv0eCgoKDLbt2bNHACD8+OOPBu9127Zt4ra4uDihWbNmZl9LEAShX79+AgBh3rx54rbi4mKhU6dOQuPGjYWSkhJBEARh/vz5AgDh559/Fo8rKSkRevbsKbi7uws5OTmCIAjC6tWrBQDCrFmz9K7z4IMPChKJREhJSREEQRA+//xzAYBw48YNk2VLTEwUAAhLlizR265Wq4VWrVoJsbGxglqtFrcXFBQI4eHhwuDBg8Vto0ePFpRKpXDx4kVxW1JSkiCTyQw+E6buT7t27QRBEISVK1cKTk5OwtNPPy2oVCq945o1aybExcVVeb7KXqP9HP3777/ittzcXCE8PFwICwsTrzlq1CixTKZ4eXkJ8fHx1SqPIJT/rEyaNEncVlZWJoSEhAgSiUSYM2eOuP327duCi4uL3nuo7ufkk08+0btO3759Db7n5v4cGvuZMGbJkiV6P5cZGRmCQqEQ7rnnHr3v64IFCwQAwuLFiwVBEITDhw8b/f2iy5zPNTUsbBojuxUYGIiEhARs3rwZffv2xZEjR7B48WJ4enoCAG7duoV//vkHDz30EHJzc3Hz5k3cvHkTmZmZiI2NxdmzZ3H16lUAwPr163HXXXeJtRAA4O/vj/Hjx1e7XLq1SKWlpcjMzETLli3h7e1dZbNGTcnlcjzzzDPic4VCgWeeeQYZGRk4ePAgAM17DAwMxLhx48TjnJycMHnyZOTl5WHHjh3icTKZDJMnT9a7xiuvvAJBEPD3338DKO/HtWbNmmo3IR45cgRnz57FI488gszMTPF7k5+fj0GDBmHnzp1Qq9VQqVTYuHEjRo8ejaZNm4qvj4iIQGxsbLWu+dtvv2Hs2LF45plnsGjRIkillv/1uH79enTv3h19+vQRt7m7u2PSpEm4cOECkpKSAGju3ZUrV5CYmGjyXN7e3ti3bx+uXbtWo7I89dRT4mOZTIZu3bpBEAQ8+eSTetdo06YNzp8/r/cezP2cyOVyPPfcc3rXefHFF/XKUZ2fw5rasmULSkpKMGXKFL3v69NPPw1PT0+xadLLywsAsHHjRhQUFBg9V20+12SbGITIrj388MMYPnw49u/fj6effhqDBg0S96WkpEAQBMyYMQP+/v56/2bOnAmgvHP2xYsX0apVK4Pzt2nTptplKiwsxDvvvCP2r/Hz84O/vz+ysrL0+iFYUlBQENzc3PS2tW7dGgDEvkna91gxAERERIj7tV+DgoLg4eFR6XFjx45F79698dRTTyEgIAAPP/wwfv/9d7P+eJw9exYAEBcXZ/C9+e6771BcXIzs7GzcuHEDhYWFtf7epKam4tFHH8UDDzyAL7/8ss7mpLp48aLRclW8d2+88Qbc3d3RvXt3tGrVCvHx8QZNL5988glOnDiB0NBQdO/eHe+++65eYKmKbnAENCFAqVTCz8/PYPvt27f13oO5n5MmTZrA3d1d77iK7786P4c1pS1TxWsrFAo0b95c3B8eHo6pU6fiu+++g5+fH2JjY5GQkKD3c1mbzzXZJvYRIruWmZmJAwcOAACSkpKgVqvFX+DaX1yvvvqqydqDli1bWrxML774IpYsWYIpU6agZ8+e8PLygkQiwcMPP2xXv0xdXFywc+dObNu2DevWrcOGDRuwfPlyDBw4EJs2baq07432PsydO9fk/ELu7u4oLi62SFmbNGmCJk2aYP369Thw4AC6detmkfPWVEREBM6cOYO1a9diw4YNWLlyJb766iu88847eO+99wAADz30EPr27Ys///wTmzZtwty5c/Hxxx9j1apVGDp0aJXXMHb/TX1PhAqd4C3JWj+HpsybNw8TJkzAmjVrsGnTJkyePBmzZ8/G3r17ERISUqvPNdkmBiGya/Hx8cjNzcXs2bMxffp0zJ8/H1OnTgUANG/eHICmWj8mJqbS8zRr1kyspdB15syZapdpxYoViIuL0xuNU1RUZHI0jSVcu3YN+fn5erVCycnJACDOVN2sWTMcO3ZMLywCmtE82v3ar1u2bEFubq5erVDF4wBAKpVi0KBBGDRoED777DN89NFHeOutt7Bt2zbExMSYrHlp0aIFAMDT07PS742/vz9cXFxq/b1RKpVYu3YtBg4ciCFDhmDHjh1o166d2a83V7NmzYyWy9i9c3Nzw9ixYzF27FiUlJTg/vvvx4cffojp06dDqVQC0AS4559/Hs8//zwyMjLQpUsXfPjhh2YFodq8B3M/J1u3bkVeXp5erVDF91+dn8PalFl7be31AKCkpASpqakG1+3QoQM6dOiAt99+G//99x969+6NhQsXYtasWQCq/lxTw8KmMbJbK1aswPLlyzFnzhxMmzYNDz/8MN5++20xADRu3Bj9+/fHokWLcP36dYPX37hxQ3w8bNgw7N27F/v379fb/8svv1S7XDKZzOB/2F9++SVUKlW1z2WusrIyvSHlJSUlWLRoEfz9/dG1a1cAmveYlpaG5cuX673uyy+/hLu7uzh1wbBhw6BSqbBgwQK9a3z++eeQSCTiH+Fbt24ZlENbu6OtydEGs4ohsGvXrmjRogU+/fRT5OXlGZxH+72RyWSIjY3F6tWrcenSJXH/qVOnsHHjxqpvjA4vLy9s3LhRHIZ+7ty5ar3eHMOGDcP+/fuxZ88ecVt+fj6++eYbhIWFITIyEoCmJlOXQqFAZGQkBEFAaWkpVCqVQTNq48aNERQUZLFassreg7mfk7KyMnz99dficSqVCl9++aVBuc39OaypmJgYKBQKfPHFF3o/e99//z2ys7MxfPhwAEBOTo7BdAkdOnSAVCoV76s5n2tqWFgjRHYpIyMDzz33HAYMGIAXXngBALBgwQJs27YNEyZMwK5duyCVSpGQkIA+ffqgQ4cOePrpp9G8eXOkp6djz549uHLlCo4ePQoAeP311/HTTz9hyJAheOmll8Th89r/HVfHvffei59++gleXl6IjIzEnj17sGXLFvj6+lr8PmgFBQXh448/xoULF9C6dWssX74cR44cwTfffAMnJycAwKRJk7Bo0SJMmDABBw8eRFhYGFasWIHdu3dj/vz5Yu3PiBEjMGDAALz11lu4cOECoqKisGnTJqxZswZTpkwRa3Pef/997Ny5E8OHD0ezZs2QkZGBr776CiEhIWJn4RYtWsDb2xsLFy6Eh4cH3Nzc0KNHD4SHh+O7777D0KFD0a5dO0ycOBHBwcG4evUqtm3bBk9PT/z1118AgPfeew8bNmxA37598fzzz4t/lNu1a1ft742fn584R0xMTAx27dqF4OBgS30bMG3aNPz2228YOnQoJk+eDB8fHyxduhSpqalYuXKlWMNyzz33IDAwEL1790ZAQABOnTqFBQsWYPjw4fDw8EBWVhZCQkLw4IMPIioqCu7u7tiyZQsSExONzvtjSdX5nPTu3RvTpk3DhQsXEBkZiVWrVhntB2fuz2FN+fv7Y/r06XjvvfcwZMgQjBw5EmfOnMFXX32F6OhoPProowA0Uza88MILGDNmDFq3bo2ysjL89NNPkMlkeOCBBwCY97mmBsZ6A9aIqs/c4fP333+/4OHhIVy4cEHvuDVr1ggAhI8//ljcdu7cOeHxxx8XAgMDBScnJyE4OFi49957hRUrVui99tixY0K/fv0EpVIpBAcHCx988IHw/fffV3v4/O3bt4WJEycKfn5+gru7uxAbGyucPn3aYKi1JYfPt2vXTjhw4IDQs2dPQalUCs2aNRMWLFhgcGx6erpYNoVCIXTo0MFgaLsgaIZ8v/zyy0JQUJDg5OQktGrVSpg7d67eUPetW7cKo0aNEoKCggSFQiEEBQUJ48aNE5KTk/XOtWbNGiEyMlKQy+UGw6oPHz4s3H///YKvr6/g7OwsNGvWTHjooYeErVu36p1jx44dQteuXQWFQiE0b95cWLhwodEpFSq7P7pSUlKEJk2aCBEREeIwaUsMnxcEzeftwQcfFLy9vQWlUil0795dWLt2rd4xixYtEu6++27xfbdo0UJ47bXXxKkfiouLhddee02IiooSPDw8BDc3NyEqKkr46quvqiyT9r5UHP4dFxcnuLm5GRxv7P6Y+znJzMwUHnvsMcHT01Pw8vISHnvsMXGIesXjzfk5rOnwea0FCxYIbdu2FZycnISAgADhueeeE27fvi3uP3/+vPDEE08ILVq0EJRKpeDj4yMMGDBA2LJli3iMuZ9rajgkglCHveCIiIiIbBj7CBEREZHDcog+Qvfddx+2b9+OQYMGYcWKFdYuDtmpvLw8ox17dfn7+1tseO2tW7dQUlJicr9MJoO/v79FrkX60tLSKt3v4uIiTs5HRLbNIZrGtm/fjtzcXCxdupRBiOqMdi2jyqSmporD1Wurf//+4iy+xjRr1qzOF3J1VFVNuBgXF6e3eCwR2S6HqBHq378/tm/fbu1ikJ17/PHHqxw1EhgYaLHrzZs3T2/G34pqsiAsmWfz5s2V7g8KCqqnkhBRbdl8ENq5cyfmzp2LgwcP4vr16/jzzz8xevRovWMSEhIwd+5cpKWlISoqCl9++aXemlBE9aF58+Z6k7XVNe38P1T/OGkekf2w+c7S+fn5iIqKQkJCgtH9y5cvx9SpUzFz5kwcOnQIUVFRiI2NrfXaNERERGT/bL5GaOjQoZVOF//ZZ5/h6aefxsSJEwEACxcuxLp167B48WJMmzatWtcqLi7WmxlUrVbj1q1b8PX1rbNFGImIiMiyBEFAbm4ugoKCDBYIrsjmg1BlSkpKcPDgQUyfPl3cJpVKERMTozeFvblmz55dZWdXIiIiahguX76MkJCQSo9p0EHo5s2bUKlUCAgI0NseEBAgLgAIaNrzjx49ivz8fISEhOCPP/5Az549Dc43ffp0cUFOAMjOzkbTpk1x+fJleHp61t0bsZRrR4Cl9wLugcCLB2p0iq+3nUPC9hQ82DUE745sByStAdbEA6F3AY9yxB0REdm+nJwchIaG6i0MbUqDDkLm2rJli1nHOTs7w9nZ2WC7p6dnwwhCZaGAswRALlDD8vr6eEHq7IoymRKeLk7Amd815/QNqPE5iYiIrMGcbi0231m6Mn5+fpDJZEhPT9fbnp6ebtFhyg2GSyPN19ICoKxmqyC7O2uycX5xGbB5JpC6U7PDte4WBCUiIrKWBh2EFAoFunbtiq1bt4rb1Go1tm7darTpy+45ewK4k34Ls2p0Crc7QSivuAw4tLR8B4MQERHZIZtvGsvLy0NKSor4PDU1FUeOHIGPjw+aNm2KqVOnIi4uDt26dUP37t0xf/585Ofni6PIHIpUCrh4A4W3Nf88Aqp8SUXaGqGC4hKgrKh8B4MQERHZIZsPQgcOHMCAAQPE59rOzNop7MeOHYsbN27gnXfeQVpaGjp16oQNGzYYdKCuSyqVCqWlpfV2vUr5RADZl4G8LMCzqMrDK3KTqxHsIUOssAdF7qH65y2q/vmsxcnJyWJrehERkf1yiLXGqishIQEJCQlQqVRITk5Gdna20c7SgiAgLS0NWVlZ9V9IU3LTAFUJ4OYPOFV/iYVSlRrpOcXwk+RACZ1+Rl4hgKRhtaR6e3sjMDCQc0ARETmYnJwceHl5mfz7rYtBqBJV3cjr168jKysLjRs3hqurq238wb19QdNZ2iNI00xWTSVlKqTezEewJBPukkLNRrkS8Km/pSNqSxAEFBQUICMjA97e3mjSpIm1i0RERPWoOkHI5pvGbJVKpRJDkK+vDfWfUTgBggRQyAGlstovl6nUkMhLIZfIoNQGO++AGp3LmrQLjmZkZKBx48ZsJiMiIqMaVluHDdH2CXJ1dbVySSqQ3vmDL6hr9HKZ0VotG6jpqgHt98Zm+m8REZHNYRCqJZtoDtOl7ccjqGr0cqlUArlUAkE3/DSwvkFaNve9ISIim9Mw/8KRaZI7NULqmtUIAYBCLoVuxzGVwEBBRET2iUHI3kirrhHq378/pkyZYnK/k0wKZ5Q3JxWX1TxUERER2TIGIXsjqV0fIQDwUghwk5QPnZdIOLCQiIjsE4OQEQkJCYiMjER0dLS1i1J92v48tWgac5for1PGhjEiIrJXDEJGxMfHIykpCYmJidYuSvWJnaXNC0K3b9/G448/jkaNGsHV1RVDhw7FudSL4v6LV67h/rGPolGjRnBzc0O7du2wfv168bXjx4+Hv78/XFxc0KpVKyxZssTib4mIiKiucB4hCxIEAYWlNRutVVsuTjLNKClxhJd5QWjChAk4e/Ys/ve//8HT0xNvvPEG7r1vDJK2/gYnJyc88+ZclMAJO3fuhJubG5KSkuDu7g4AmDFjBpKSkvD333/Dz88PKSkpKCwsrKN3SEREZHkMQhZUWKpC5DsbrXLtpPdj4aqQA9oh42bUCGkD0O7du9GrVy8AwC+//ILQ0FCs3rAdY0YMxqWraRj1wEPo0KEDAKB58/IZpi9duoTOnTujW7duAICwsDDLvikiIqI6xqYxeyM2jVXdwfnUqVOQy+Xo0aOHuM3X1xdtWrfEqZRUAMCkJx7Hpx/PRu/evTFz5kwcO3ZMPPa5557DsmXL0KlTJ7z++uv477//LPteiIiI6hhrhCzIxUmGpPdjrXZtANXuI2SUToZ6+JFHMGjkQ9i9bQs2bdqE2bNnY968eXjxxRcxdOhQXLx4EevXr8fmzZsxaNAgxMfH49NPP635tYmIiOoRa4QsSCKRwFUht8q/8lmUzW8ai4iIQFlZGfbt2yduy8zMxJmzKYhspWkCuyl4oUlwKJ599lmsWrUKr7zyCr799lvxeH9/f8TFxeHnn3/G/Pnz8c0331jsfhIREdU11gjZm2o0jbVq1QqjRo3C008/jUWLFsHDwwPTpk1DcFATjIrthyKpK+a88yZGDB+G7p3a4/bt29i2bRsiIiIAAO+88w66du2Kdu3aobi4GGvXrhX3ERERNQSsEbI31WwaW7JkCbp27Yp7770XPXv2hCAIWL9qGZycnABIoFKpMO2VKYiIiMCQIUPQunVrfPXVVwAAhUKB6dOno2PHjrj77rshk8mwbNmyOnpjRERElicRBDOqDhxMQkICEhISoFKpkJycjOzsbHh6euodU1RUhNTUVISHh0OpVFqppEaoSoH0E5rHTTqVjyKrjvxMIPsSCqVuOFvWGADQMcTbYkWsLzb7PSIiojqVk5MDLy8vo3+/K2KNkBF2MaEiUIsO05psrLvEmErNvExERPaHQcje6AWhGoaXO69zksvETWUqLrxKRET2h0HI3kgkKB85VtNZrjVByFk3CLFGiIiI7BCDkD2SO2u+ltZwuYs7NUISiQRuCs3AwlLWCBERkR1iELJHCjfN19KCmr1eXar5KpFALtPULrFGiIiI7BGDkD2SOmm+qmvQNFaUDeTfuPNEAtmdUWdqBiEiIrJDDEL2qDbLbORc0zmPBFKpJgipOMsCERHZIQYheyTVBqEa1AjJFDpPyoMQa4SIiMgeMQjZo9rUCMl1gpCgEpvGVMxBRERkhxiE7JHkzrD3Gk2oqDMTdVmxWLmkWyMUFhaG+fPnm3c2iQSrV6+uQTmIiIjqHoOQEQkJCYiMjER0dLS1i1IzEjG9VP+1un2BVCU6NUKsEiIiIvvDIGREg15iA6hd05juazyDIbvTRyi/uAxlNQlWRERENoxByB5pg5CqGMjL0Nv1zTffICgoCOoKoWbUqFF44okncO78BYya+DICOt0Dd/8QDOjbC3v/3Q4AyC+u6UzV5Y4fP46BAwfCxcUFvr6+mDRpEvLy8sT927dvR/fu3eHm5gZvb2/07t0bFy9eBAAcPXoUAwYMgIeHBzw9PdG1a1ccOHCg1mUiIiLHxSBkSYIAlORb559u05VU59uac1WviGPGjEFmZia2bdsmbrt16xY2bNiA8ePHIy8/D8MG9sbWtStx+PBhDB0yBJOfGIfrVy+juKx2QSg/Px+xsbFo1KgREhMT8ccff2DLli144YUXAABlZWUYPXo0+vXrh2PHjmHPnj2YNGkSJHea58aPH4+QkBAkJibi4MGDmDZtGpycnGpVJiIicmxyaxfArpQWAB8FWefab14rn1FaajocNGrUCEOHDsWvv/6KQYMGAQBWrFgBPz8/DBgwANLbqYhqGQR4NwVcffHBBx/gj5WrsH3z34hs1aJWRfz1119RVFSEH3/8EW5umrIuWLAAI0aMwMcffwwnJydkZ2fj3nvvRYsWmmtFRESIr7906RJee+01tG3bFgDQqlWrWpWHiIiINUL2SCqrdPf48eOxcuVKFBcXAwB++eUXPPzww5BKpcjLzcWr73+OiC494e3tDXd3d5w9cxppV6/Uer2xU6dOISoqSgxBANC7d2+o1WqcOXMGPj4+mDBhAmJjYzFixAj83//9H65fvy4eO3XqVDz11FOIiYnBnDlzcO7cuVqVh4iIiDVCluTkqqmZsda1zTRixAgIgoB169YhOjoa//77Lz7//HNAVYZXZ8zC5n/34dM5s9GyXRRcXFxw//0PoLS0FKp6mFRxyZIlmDx5MjZs2IDly5fj7bffxubNm3HXXXfh3XffxSOPPIJ169bh77//xsyZM7Fs2TLcd999dV4uIiKyTwxCliSRlDdPWZurL1CQqXksCJqy3aFUKnH//ffjl19+QUpKCtq0aYMuXboAeenYfeAoJowZgftGjwSUnsjLy8OlSxfRsXuvWgehiIgI/PDDD8jPzxdrhXbv3g2pVIo2bdqIx3Xu3BmdO3fG9OnT0bNnT/z666+46667AACtW7dG69at8fLLL2PcuHFYsmQJgxAREdUYm8bsladuXyXDADN+/HisW7cOixcvxvjx4+9slaJVeChW/f0Pjhw9iqNHj+KRRx4RR5jVdgX68ePHQ6lUIi4uDidOnMC2bdvw4osv4rHHHkNAQABSU1Mxffp07NmzBxcvXsSmTZtw9uxZREREoLCwEC+88AK2b9+OixcvYvfu3UhMTNTrQ0RERFRdrBGyVxKdfkJqNSDTz7wDBw6Ej48Pzpw5g0ceeeTOayT4bOYreGLqu+g1IBZ+fn544403kJ2TozmNICAjt6jGRXJ1dcXGjRvx0ksvITo6Gq6urnjggQfw2WefiftPnz6NpUuXIjMzE02aNEF8fDyeeeYZlJWVITMzE48//jjS09Ph5+eH+++/H++9916Ny0NERCQRBE4ZbEpOTg68vLyQnZ0NT09PvX1FRUVITU1FeHg4lEqllUpYhWtHAAhA43b6a4gZIwhAzhUg/6bmeZNOYnOaIAg4fjVbPLRjiHddlNbiGsT3iIiILK6yv98VsWnMnlVnhum8jPIQ5Oqj16dIovMYAC5m5uPyrQJLlZKIiMhqGISMaPBrjWmJQciMiRBzdUa7SQyH38t1JmnMLizF7YIS/PTTz3B3dzf6r127drUtPRERUZ1jHyEj4uPjER8fL1atNVjVqRGSyMoDk8QwH7fwd8OZ9Fy9bfeOGIGePe8yejrO+ExERA0Bg5A9k0oBFcwLQlI5oDIdhJydZJBKJFDrdClzd/dAI+8GHBSJiMjhsWmslmy6r7m2icucICSrugZHKtXvK6Q2Mizfltj094aIiGwCg1ANaZt+CgpsuNNwdZrGZDqjytRlxg/Rz0Gw9Zyh/d6wmY6IiExh01gNyWQyeHt7IyMjA4BmDpyKo6usrlQAygSgqBiQVjH/T0mZ5lgAkHkCRYbHC2WlEHRWoC8sLARUtvcREgQBBQUFyMjIgLe3N2SyytdeIyIix2V7f8UakMDAQAAQw5DNKcgESvIBZSmgzK782PybQGkB4NIIyL9q9JAbucUoLiuvXRJyneEst91KRW9vb/F7REREZAyDUC1IJBI0adIEjRs3RmlpqbWLY2j778CJP4BuTwERz1Z+7LoEIHU70G8a0PZBo4ds2nEOvx+4LD7/5IEotA1rZMECW46TkxNrgoiIqEoMQhYgk8ls84+utAzIuwwU3wCqmlm5+KbmWJna5LHjerbA59suiM8L1VLO2ExERA2a7bZrUO0pNCu8oyS/6mO1HaSlprNxY0/90KPbTEZERNQQMQjZMydXzddSM0a2qe807VUShADAU1m+v6jUjBmriYiIbBiDkD1T3AlCZtUI3Qk1VQShX58un0maNUJERNTQMQjZM6c7TWPm1AipzKsRah/shXs7NgHAGiEiImr4GITsmVgjZE7T2J0+QmbMMO3lojkmq6CkpiUjIiKyCQxC9kzbWbrUMp2ltQLvdJpOy6likkYiIiIbxyBkREJCAiIjIxEdHW3totSOtmmsOjVC0qqnAQj00gSh69kMQkRE1LAxCBkRHx+PpKQkJCYmWrsotVOtztLaIFR101gTLxcAwLWswpqWjIiIyCYwCNmz6nSWLs7TfDWjaax1oDsA4PzNfNzKZz8hIiJquBiE7JlujVBlS8VfPQTkXNE8NqOzdGMPJVo2docgAEevZNW+nERERFbCIGTPtBMqCipAVUnNzaYZ5Y/N6CMEAP7uzgCA3KKympaOiIjI6hiE7JnCvfxxZf2E5M7lj81oGgMA9zszTOcxCBERUQPGIGTPZHJArunYjOJc08c5uZQ/NqOzNAB4ON8JQsWlNS0dERGR1TEI2TvnO7VClQUhuc5iqhLzPhLaGqHswlKo1ZX0PyIiIrJhDEL2ztlD89XcIFSSZ9Zp3e/UCCVsO4fx3+2raemIiIisikHI3mmDUGUBR1Vc/rhxhFmndddZhX7P+UyuO0ZERA0Sg5C9U2hrhHJMH6OdeXrIHP3+QpXQ9hHSSudyG0RE1AAxCNk7c5rGtLVFbv5mnza4kX5gupbFIERERA0Pg5C9MysI3Rlar12k1Qw9wn31nl/lchtERNQAMQjZO4UZC6/WIAi5Ocux8NGukEslAIDT1ytpeiMiIrJRDEL2Ttvnp7L1xmoQhABgSPtAfHR/BwDAiWvZNSkdERGRVTEI2TttECqrpA+Pto+Q7kzUZgptpFnG42YeF18lIqKGh0HI3tVhjRAAeHCpDSIiasAYhOyddomNUhOdmVWl5fMI1SAIaSdWzC7kUhtERNTwMAjZO6cqgpDuYqxONQhCd2qECktV+HnvxWq/noiIyJoYhOydk6YPT5VBSOoEyBXVPr2HzgzTb68+Ue3XExERWRODkBEJCQmIjIxEdHS0tYtSe0531hGrKgjVoFkMAJzlshq9joiIyBYwCBkRHx+PpKQkJCYmWrsotaetESozFYRqPmKMiIiooWMQsndV9RHSzjjtzCBERESOh0HI3smrGD6vXYzV2bPGl/jkgY7i462n0mt8HiIiovrGIGTvPJtovmZfBYrzDPcX3QlCypoHoU5NvcXHTy49UOPzEBER1TcGIXvnFQJ4NwUEFXDFSJ8nC9QIuTnL9Z5fvlXJ5I1EREQ2hEHIEfhHaL5mGZnnxwI1Qu4K/SB0kuuOERFRA8Eg5Ag8AjVfc9P0t6vVwPaPNI9rMWrMXakfhApLVTU+FxERUX1iEHIEnkGar7nX9bcX3ip/fPtCjU8vk0qw/61BiArx0py2RF3jcxEREdUnBiFHoK0RyqkQhHRHkvV4plaXaOyhRFNfzaSMRawRIiKiBoJByBG4+Gi+FmXpb9fOLSRTAOF31/4yTpqP047kGzh2Javyg4mIiGwAg5Aj0HaE1naM1tIGIVc/y1zGSbPcxo7kGxi5YDfUasEi5yUiIqorDEKOwNlD81U7i7SWNghpZ5+uJRcn/XXHsgtLLXJeIiKiusIg5AicNZ2YDYPQnT5C2vXIaslJpv9xWnf8OoZ/8S+OXM6yyPmJiIgsjUHIEYg1QjmaIfNaFq4Rqjhs/u3VJ3DyWg7iFu+3yPmJiIgsjUHIEWiDEASgNL98e1mR5quT0iKXKSgxPlqMTWRERGSrGIQcgZMLIL0z6aFu85iFm8YKSsosch4iIqL6wiDkCCQSwM1f8/jW+fLtFm4aC/SyTM0SERFRfWEQchRhfTRfz28v31aYpfkqt0wQih/QEg92DcHs+ztY5HxERER1jUHIUfi30XzVXW/szDrN18D2FrmEp9IJn46JwsPRoRY5HxERUV1jEHIUijsdpkvyyrfdTNF8bRVr0UtJJBKDbf+l3LToNYiIiCyBQchRON9ZXb74ThAqLSwfQebuX+eXf+S7fXV+DSIioupiEHIUFWeXLriz8rxUDjh7Wvxyk+5ubvFzEhERWRqDkKNQ3KkR0jaNFWRqvrr6akaVWdjE3mEG2wSBa48REZFtYRByFAY1QjpBqA40clUYbJu3KZlhiIiIbAqDkKOorEaoDiidZOjc1Ftv24JtKXj6xwN1cj0iIqKaYBByFLo1QoJQ3kfI1afOLvnn870Ntm05lYGz6blQqVkzRERE1scgZERCQgIiIyMRHR1t7aJYjjbwqEo0YaiOa4S0pg9ta7Bt8Oc78ev+S3V6XSIiInMwCBkRHx+PpKQkJCYmWrsolqNwKx8dlpdeb0HomX4tcGHOcFyYMxwKWfnHTTuv0KpDVzD+u724nV9Sp+UgIiIyhkHIkbgHaL7mXq+3IKRLrdNR+lZ+CQRBwNTfj2J3SiYWbEupt3IQERFpMQg5Eo9AzdcN04GTqzSPXf3q7fJB3uVrmu1LvYXw6evF59mFpfVWDiIiIi25tQtA9cgzWPM1/UT5tqDO9Xb5L8d1xqiE3Ub3lanUeOz7fRjYtjGu3i5Ec3939G7pi9BGrpBKLT/PEREREcAg5Fh8jMz27Nui3i4fFeqN0x8MQdsZGwz2rT5yDQDw71n9Ncme6B2Od0ZE1kv5iIjI8bBpzJFom8Z01cGs0pVROslw8r1YDOtgpCxGLN6dWsclIiIiR8Yg5EhaDLB2CQAAbs5yfDW+K9oEeJh1fKlKjcISVR2XioiIHBGDkCPxbgoovaxdCtHwjk3MOu6+r3aj+0dbkF9cprf9Zl4x1h67BjUnZyQiohpiEHI07e4rf+zdzHrlAPBMv+Zo7u9W5XEnruYgt6gM7WZuxJm0XBSXqfDJhtOI/XwnXvj1MF754ygStqUgM6+4ynMVlapQUqa2RPGJiMgOSASugmlSTk4OvLy8kJ2dDU9PT2sXxzKKcoA/JgA3k4FHfgcCrNsReUfyDcQt3m/28VGh3ujWrBG+32XYdygq1BtD2gViZKcgBOsM1dcqKVOj5+ytcFfKsf3V/pDUc/8oIiKqH9X5+80gVAm7DEI2RhAEJF3PQZivG9rN3Gix86Z8OBRymX6FZ0pGLmI+2wkASHo/Fq4KDpokIrJH1fn7zaYxsiqJRIJ2QV5wc7ZsKFm656LBtjKdvkQ5hfr9jTJyivDv2Rvg/wuIiBwL/0tMNumxu5rhVkEJ1h27XqPXf7A2CUt2pyIzrwR9W/nh3ZHtMH/zWXF/TlEpAr2U4vN75u9EVkEpFk/ohoFtA2pdfiIiahgYhMjmvBbbBvEDWgIAIgLP4tNNyQCAkEYuuHK70OzzaI/dlJSOTUnpevvu+XwnnuvfAs/e3QK5xaXIKtAs8fHP6QwxCJWq1Fj63wXc3dofrc0c6k9ERA0L+whVgn2E6tfvBy7jn1MZmP9wJyidZAA0HZw/35KMpGs5eH9UO2w/cwMz/3cS3zzWFVKJBP+39SyOX80GAOydPgh3zd5arWt6OMuRW2FY/vdx3TAoIgDf/Xses9adgkIuRfKsoZZ5k0REVOfYWdpCGIRsU1GpSgxKhSUqbD6VDh9XBfq08sPJa9kY/sUuveP7tPTDrpSbxk5l0oU5wzF20R7sS70lPv/v3E1kFZRiWIcm2Hc+EwDQo7mvBd4RERFZEjtLk13ThiAAcFHIMDIqCH1a+QEA2gV54cWBLcX993cOxs9P9cBH93Wo1jXCpq0TQxAAHL50G8/8eBDP/3IIS3anYuw3ezH2m724lV9S6Xky84pxLcv85jwiIqpfrBGqBGuEGq4bucXwdVOIK9cLgoCx3+zF/jvhZk18b7z553GcvJZj1vk6N/XG4UtZBtunxLTCzuQbGN4xCE/2CQcAJKfnIj2nCL1b+KHNjL9RqhJw/N174KF0ssybIyKiSrFpzEIYhOxLmUqNo1ey0CHYGwq5pjJ03/lMjP1mr0XOP31oW1zLKhSH7o+ICsJfR68BANa+2Aftg21neRMiInvGpjEiI+QyKbo28xFDEABEh/mgT0s/9GnphzOzhqBPSz9xX3N/N/zfw53MPv/sv0/rzV+kDUEAxFFpyxMv4dHv9iGroPImNSIiqh+sEaoEa4QcU1p2EdJyitAp1BsAEP/LIaw7XrP5jHT9+Xwv3PfVfwAAmVSCQ28Phpcrm8uIiCyNNUJEtRDopRRDEAAseKQzVsf31qstAoAHu4YYXdPMFG0IAgCVWsCbq48jI7cIKrWADSfSMP67vfjvXPVGtxERUe2wRqgSrBGiivKKy3Dw4m208HdDSCNXFJaosCP5Bvq28sNfR6/hxz0XkXS9vAN2TYbuJ88aCoVciumrjuG/c5lY9VwvNHJVoESlhtJJhtdXHEV2YSkWPtqVC8cSERnBztIWwiBE1aVWC/jr2DUcuHAbrQLccW/HICxPvIyPN5w2+xztgz1x4mp5mIoK9YZSLsX5m/lY+2If9PhIM2nkP6/0Q3N/d4u/ByKiho5ByEIYhMhS3vvrJJbsvgAAWDyhG5744UCNzvPGkLZ6oap3S1+8PTwSEU34+SQi0mIQshAGIbKUgpIyLNl9AQ90CUGglxL/nbsJN4UcUaHeeGjRHnF+o5ro19ofS5/obrBdEAR8uO4UfNwVeL5/SyOvJCKyTwxCFsIgRPUhp6gU5zLy9DpThzRygberk14TmTkWPdYVse0CAQApGXmI+WwHAODEe7Fwd9assXwtqxAyqQQBnkoLvQMiItvCUWNEDYin0gmdmzbCT0+W1+q8PTwCa1/sW+1zPfPTQdzMK8aBC7ewLzVT3L45KQ0frE3CT3svotecf9Djo61Qqfl/ICIiubULQEQafVv5Y8OUvjh+JVus1Wnu54bzN/Nxb8cmmHR3c3gqneAkl+KPA5cxODLAYIFZAOj78TYUlqr0tr28/KjBcXlFZfB0kSO3uAyeXP6DiBwUgxCRDWkb6Im2geXVuEuf6I5Vh64irlczeLsqxO1TYloDAJ7uG45v/03VO0fFEGRKXkkZ3vzzOP4+cR1zH4zCXS18qzUvEhGRPWAfoUqwjxA1BIUlKszfkoz2wV6Y+vsRlKoEyKQSDGjTGCq1Gtezi3A6Ldfgdf/3cCe8tOyI+NzLxQk7XuuvF7iIiBoidpa2EAYhamjOpOUi6Xo2eoT7IuhO7U6ZSo2Wb/1t1usn3d0cQ9oHIirEGzJp+WSNS/+7ACeZFI/0aFon5SYisiQGIQthECJ7se10Bib+kGj28a/FtsGz/VpgyvIjeovH6o4+IyKyVQxCFsIgRPYkPadInJVaSyGXoqRMbfR4D2dNR+qKPhjdHo/d1axOykhEZAkcPk9EBgI8lejWrJHetiPvDDZ5vLEQBAAzVp+waLmIiKyJQYjIgXz5SGfx8f1dguGqMGzm6tLUG10rBKaKBn+2A0t2p6LIzBFqRES2ik1jlWDTGNmja1mF2HgyDeO6N4XSSYawaev09v/5fC90DPHG6yuOYeWhK5Wey8VJhiMzB8NZLqvLIhMRVQv7CFkIgxA5gm1nMnD0chYm9grHpVsF6BDipbf//I08/H0iDXM3njH6+pFRQZg/thOkOqPMiIisiX2EiMhsA9o0xpSY1vBydTIIQQDQ3N8dboryGp+OFY7539Fr+GX/JQiCgOIyFbadzsDlWwV1Xm4iIktgjVAlWCNEpJGRW4RBn+5An1Z++PrRrgCAKcsOY/WRayZfc1/nYHw6JkpvPiIiovrAGiEisqjGHkokvh2Dr8Z3EbfFRAZU+po/D1/FjuSMui4aEVGt2H0QWrt2Ldq0aYNWrVrhu+++s3ZxiBospZMMEkl57c6QdoHo19q/0tfkF3NUGRHZNrsOQmVlZZg6dSr++ecfHD58GHPnzkVmZqa1i0VkF+QyKZY+0R0z7o00ecxv+y+hTGV8wkYiIltg10Fo//79aNeuHYKDg+Hu7o6hQ4di06ZN1i4WkV2J69kMkwe1gouTDEFeSsy5v4O4779zmfgt8TJOXM3GB2uTUFjCGiIisi02HYR27tyJESNGICgoCBKJBKtXrzY4JiEhAWFhYVAqlejRowf2798v7rt27RqCg4PF58HBwbh69Wp9FJ3IYchlUkwd3BqnPhiC3dMGYmx0KHqE+4j7Z6w+gXu/3IXvd6Vi8e5UK5aUiMiQTQeh/Px8REVFISEhwej+5cuXY+rUqZg5cyYOHTqEqKgoxMbGIiODHTSJrEEikUAikeDTMVFo7u9msH/uxjOYvuq4FUpGRGScTQehoUOHYtasWbjvvvuM7v/ss8/w9NNPY+LEiYiMjMTChQvh6uqKxYsXAwCCgoL0aoCuXr2KoKAgk9crLi5GTk6O3j8iqr5QH1f880p/fK0zykzrt/2XMH3VMRSXsZmMiKzPpoNQZUpKSnDw4EHExMSI26RSKWJiYrBnzx4AQPfu3XHixAlcvXoVeXl5+PvvvxEbG2vynLNnz4aXl5f4LzQ0tM7fB5E9G9qhidHtv+2/jF/2Xqrn0hARGWqwQejmzZtQqVQICNCfyyQgIABpaWkAALlcjnnz5mHAgAHo1KkTXnnlFfj6+po85/Tp05GdnS3+u3z5cp2+ByJHdjYjz9pFICKC4dLTdmbkyJEYOXKkWcc6OzvD2dm5jktERADgquBCrURkfQ22RsjPzw8ymQzp6el629PT0xEYGGilUhFRRe7Oxv+/9f2uVDz5QyK4yg8RWVODDUIKhQJdu3bF1q1bxW1qtRpbt25Fz549rVgyItL181M90D3MB6vjexvs23o6A5n5JVYoFRGRhk03jeXl5SElJUV8npqaiiNHjsDHxwdNmzbF1KlTERcXh27duqF79+6YP38+8vPzMXHiRCuWmoh0dQr1xu/Pav5z0jrAHcnp+n2DUjLy4OfOJmkisg6bXn1++/btGDBggMH2uLg4/PDDDwCABQsWYO7cuUhLS0OnTp3wxRdfoEePHha5PlefJ7Kss+m5WPLfBZy+noNDl7LE7ac/GAKlE/sMEZFlVOfvt00HIWtJSEhAQkICVCoVkpOTGYSI6sCstUn4bpdmpulx3UPx4egOkEolVbyKiKhq1QlCDbaPUF2Kj49HUlISEhMTrV0UIrvlqtOJ+rf9l/HdrvNWLA0ROSoGISKyinsi9ecAm/P3aSuVhIgcGYMQEVlF+2AvTB/aVnyuFoCf9lzA5qR0pHCyRSKqJzY9aoyI7Nt9nYMxW6cmaMaak+LjC3OGW6NIRORgWCNERFbj78Fh80RkXQxCRGQ1EokEvz9jfAJUDmglovrAIEREVtXYRK3Qjbziei4JETkiBiEjEhISEBkZiejoaGsXhcjuhfm54e3hEQbbx3+7zwqlISJHwwkVK8GZpYnqz1NLE7HlVIbeNnaYJqKa4ISKRNTgLHy0q8G2DjM3YtbaJJSp1Pjr6DVcyyq0QsmIyJ7VKAgtXboU69atE5+//vrr8Pb2Rq9evXDx4kWLFY6IHIdcZvjrKLe4DN/tSsW4b/fixd8OI+azHVYoGRHZsxoFoY8++gguLi4AgD179iAhIQGffPIJ/Pz88PLLL1u0gETkOJROxn8lJV64DQAoKFHVZ3GIyAHUaELFy5cvo2XLlgCA1atX44EHHsCkSZPQu3dv9O/f35LlIyIHsm5yX/x56CoW705l6CGielGjGiF3d3dkZmYCADZt2oTBgwcDAJRKJQoL2YZPRDXTwt8dr8a2QXSYj7WLQkQOokY1QoMHD8ZTTz2Fzp07Izk5GcOGDQMAnDx5EmFhYZYsHxE5oJaN3bEj+YbRfaUqNZyM9CciIqqJGv02SUhIQM+ePXHjxg2sXLkSvr6+AICDBw9i3LhxFi0gETmeNgEeJvd9tP5UPZaEiOwd5xEyIiEhAQkJCVCpVEhOTuY8QkT1LL+4DO1mbjS5/8Kc4ShVqXE9qwhNfV3rsWRE1BDU+TxCGzZswK5du8TnCQkJ6NSpEx555BHcvn27Jqe0KfHx8UhKSkJiYqK1i0LkkNyc5dj88t0m9y9PvISnfzyAu+duw84KTWjJ6bkYMn8nNpxIq+tiEpEdqFEQeu2115CTkwMAOH78OF555RUMGzYMqampmDp1qkULSESOqVWABw7NGGx03xsrj2P7GU0A+nHPBb19k387jNNpuXj254N1XUQisgM1CkKpqamIjIwEAKxcuRL33nsvPvroIyQkJODvv/+2aAGJyHH5uCkw98GOlR4jlUj0nmcVlNZlkYjIztQoCCkUChQUFAAAtmzZgnvuuQcA4OPjI9YUERFZQkxEQKX7KwYhdRXdHr/79zye+/kgSlXqWpeNiBq+Gg2f79OnD6ZOnYrevXtj//79WL58OQAgOTkZISEhFi0gETk2LxenSvfLpOVB6M/DV5CRW1zp8bPWaUadrT9+HaM6Bde+gETUoNWoRmjBggWQy+VYsWIFvv76awQHa36Z/P333xgyZIhFC0hEjk0qlVS6f93x69AOfn15+VGzz5tTyCY0IqphjVDTpk2xdu1ag+2ff/55rQtERFRde8/fQs8Wvkb3CYIAicQwTJWpOXMIEdUwCAGASqXC6tWrceqUppq5Xbt2GDlyJGQymcUKR0QEAIGeSqTlFJncn1tUioKSMoPtLy8/gmNXsrD2xb5wUcigO21amYpBiIhqGIRSUlIwbNgwXL16FW3atAEAzJ49G6GhoVi3bh1atGhh0UISkWP7+6W+SE7Pxdhv9gIAwnxdcTOvBHnFmvCjUgt4739JBq/78/BVAMCO5BtQyCV6I8pYI0REQA37CE2ePBktWrTA5cuXcejQIRw6dAiXLl1CeHg4Jk+ebOky1ruEhARERkYiOjra2kUhIgCN3BTo0by86UsqkeC7uG7i8+d+OYTlBy6bfP2hS7fxxA8HMPX38j5EZRw1RkSoYRDasWMHPvnkE/j4lK8Q7evrizlz5mDHjh0WK5y1cGZpItsW0cQTPcJ9Kl2TTFfF2acBoLiMQYiIahiEnJ2dkZuba7A9Ly8PCoWi1oUiIjJm5XO98ECXELw7sh0kEgmambnO2LkbeQbb8o30KSIix1OjIHTvvfdi0qRJ2LdvHwRBgCAI2Lt3L5599lmMHDnS0mUkIgIAdG3WCPMeioK/hzMA4PLtQrNeV2qkY3R2YSlWHLyCq1nmnYOI7FONgtAXX3yBFi1aoGfPnlAqlVAqlejVqxdatmyJ+fPnW7iIRETGTR3c2mDbE73DzXrtqkNX8eofRzFqwa6qDyYiu1WjUWPe3t5Ys2YNUlJSxOHzERERaNmypUULR0RUmcGRAZgS0wrzt5wVt8W2C8Di3almn+NmXkldFI2IGgizg1BVq8pv27ZNfPzZZ5/VvERERNXQPax80EaQlxIdQ7wR7ueG1Jv5Zp+jqFSFDSfS0KeVH/zcneuimERko8wOQocPHzbrOGMzuBIR1ZVeLf3w6ZgotGrsjogmnlDIpdgwpS/avL1BPCYqxAtLJnZHlw82Gz3H51uSsWjHebQN9MDPT/WAj6uiyqU9iMg+SAShiqWaHVhOTg68vLyQnZ0NT09PaxeHiKrhrT+P45d9l7D2xT5oF+QJiUSCsGnrjB7rLJfqDacf3SkI8x/uXF9FJSILq87f7xp1liYisnWzRrfHsXfvQftgryprqivOKbT6yDWUlKnx3b/ncTHT/CY2Imp4GISIyC5JJBJ4Kp1q/Po3/zyOWetO4bmfD1mwVERkaxiEiMhhPdff9LqIKw5eAQAkXc/Bqes5BvvVXKuMyC4wCBGRQ+rbys/sJTqeWnoAWQUlyC7ULNp67kYeun+0Be/9dbIui0hE9YBByAguukpk33qE++CnJ3uIM1RX5WpWITq9vxn95m5DUakKL/56GDfzSrBk94VKa4Yu3ypgzRGRjWMQMoKLrhLZt8ggzSiSRq7VWxsxq6AU/eZuQ1pOkbjt0e81Sw1pldzpeP3b/kvo+8k2vHun1qhUpcZ5I2ueEZF1MQgRkcNY8WxPxPVshlfuaQMAaNHYDS383ap1jvScYtzKL5+N+r9zmfhxz0UAwOsrjqLrrM1YcfAKpq86DgDivud+PoiB83ZgzZGrlngrRGQhnEeoEpxHiMgxCIKAracy8NSPB2r0eg+lHIdnDEbLt/42uv/CnOF6cxgdnjEYjdyqVxtFRObjPEJERNUgkUhQVKaq8et93BQ4nZZr9vG/7LtY42sRkWUxCBERAejZ3BcA0DrA3eQxXZs1Mrrd3VmOc9Xo/1OiM4HjhZv5WH/8Olg5T2QdDEJERAB83Z1x5J3BWPtiX6P7g71dMOPeSKP7Gns442y6+UHom3/PY+updABA/0+34/lfDiF8+nq8+efx6heciGqFQYiI6A5vVwUUcv1fiyOjgvB8/xbYPW0gOgR76e2LH6CZkHHbmRtYsC3F5Hn3nMvUe15UqsaTSw/gUmaB3vZf911CmUqNF349hB92p9bmrRCRmRiEiIgq8cW4znh9SFsAgEwqwcwR5bVC4X6mm9F0jft2r9Hthy/fNtj2v6PXsPbYdbz7V1INSktE1cUgRERUDZFNykeg+LnXbuRXfrFhB+3r2UVGjgSuZxcir7isVtcjIkMMQkREJngo5QbbQnxcxceqWs4aXVBiGGzyjYSdtOwi9Jz9D7p+sLlW1yMiQ4Y/5UREDi7M1xUXMgswulOwwb5gbxcsmRgNT6UTgryVevvkUgnKqhGOjNUIGQtCBy9qmtCKdUabEZFlMAgREVXw81M9sOlkOsZ1b2p0/4A2jcXHa+J7Y1TCbgCAQi5FWYn58xHlG6kRWrqnfI6hMpUacpkUUonZpySiamLTGBFRBSGNXPFEn3C4KGRVHtsxpHwkWYFOCPrwvvboHu5T6Wtz7qxmb4q2BkjKJERUZxiEiIhqQSIpDyltAjzEx+N7NMPQ9oGVvvZ2QUml+8UgpHONMpXlm8c2nUzDQwv34PKtgqoPJrIzDEJGJCQkIDIyEtHR0dYuChE1ANte7Y/fn+mJrx/tgjYBHvjsoSgAQGy7qoJQVTVCmhommc5v6hKVGv87eg3P/HQAr684apEZqSf9dBD7L9zihI7kkNhHyIj4+HjEx8eLi7YREVUm3M8N4X6aVew3vny3uN3YqDNdN3OLK91fXKqp/dGtddp19iYm/3ZYfD42OhRdm1XeBGeuW/mV11AR2SPWCBER1RE3ReVB6PzN/Er3GxslllrhNVduF1a/YEQkYhAiIqojte3krG0aU+sMyXet0IE7I0dTq7Rs/yU8tGgPsqrod0RE+hiEiIhsVF5RGQ5duo0nlx4Qt+VWmGfoRp4mCE1bdRz7U29h3qbkGl9PwsFp5IDYR4iIyEY98t0+g22fbDij97y4VH/eovM38+q0TET2hjVCRER16Om+4fBQynFvxybitvgBLTBrdHuLnH/pnou4odPp+mZu1U1jfx6+grdXH6/1EiFE9oA1QkREdeit4ZF4Y0hb3C4oxd7zmRjVKRivxbaFIAgY3qEJnlyaiEOXsmp1jVnryleqz8yvfCQaALy8/CgAoEe4L0ZEBYnbJWDbGDke1ggREdUxuUwKfw9n7H8zBjPujQSgGRLfyE2BB7qG1Pr8x69mi4+LSs2fcPGdNSf0nrOPEDkiBiEionpibBSZp9Kp1uc9f6N8SH1JNWaevl1QyrmDyOExCBERWVEjV4X42MWp6rXNqlJazSU4jK12T+RIGISIiKzI27W8Rqiw1PyV65t4KY1uFwQgu7AU8zadQUpGrrg9p6gUyem5Bkty6C4Ue+xKNogcDYMQEZEV6Qah6qisSe2zTWfw5T8pGPp//4rbhs7/F/d8vhPbz9zQO/bjDaf1nh+8eKtG5SFqqBiEiIisSLdp7N0RkWjk6oQPRrWr8nUCTA99T7xwGwBQqio/5mqWZimOiT8k6h37z+kMvef/pWRWXWgiO8IgRERkRbpLZgzr2ASHZgzGo3c1q/J117OLTO5Lup4jPlarBRRVo8mNMwuRo+E8QkREViSRSLB+cl/kFZehsYfxfj/G5BaZ18k5PbcIE5ckVn0gkYNijRARkZVFBnmie7iP3rb1k/vi5yd7IOn9WIPjPZRyvDK4tfj8xYEtTZ57d0omTqflmtxfkSAABy7cQnZBqdmvIWrIWCNERGSDIoM8Te47+s49uJZdiHmbNQus+ropTB77f1urtwjrpqQ0fL4lGX7uCkgkEtzILcaix7oitl2g3nG/J17G/C3JWDwxGm0DNWUVBAGZ+SXwc3eu1jWJrIk1QkREDUi/1v6QSiUIaeSK5ZPuwqaX74ZCbnr+ocu3Cqt1/pPXNP2LbuaViGuYPfPTQYPjXl95DNeyizBk/r/IKdLUHr2/NgndZm3B2mPXqnVNImtiEDIiISEBkZGRiI6OtnZRiIj0fBfXTXzco7kvWgd4YFSnIDT3d8OjdzXF8/1b1HuZOr67CdvOZGDJ7gsAgNnrT1f+AiIbwiBkRHx8PJKSkpCYyA6GRGQ7nGQSOMkMf227OcuxdWo/zBrdQW8UWn3S7ZAtreZfljKVGuuPX0dGrumRcER1hUGIiMjGPdUnHAAwfWiEyWMkd1ZMlemkEKWT6V/xAZ51149HamL11oqzWmv98N8FPP/LIQz/YledlYnIFAYhIiIb99bwCOx8bQAm9g6r8tgynbXGQhu5mjxOdyLH6igoKcOZKkahGQtCLy07jHs+32l0TqNtZzSTOt7ILcbs9aeQZwPrn/1+4DKmrzoOlZozK9k7BiEiIhsnkUjQ1NdVrPWpTKnOH+5AE+uRAYBcVn6uUB8Xs8sy7pu9iJ2/s9JjJAAu3MzXqwFac+QazmbkYXfKTYPjnXU6ey/aeR6fbjxjdnnqyusrjuG3/Zew6WSatYtCdYxBiIjIjjjLy3+tj4wKMnmcbhNaUx9XLHy0S6Xnjf18J+ZtOoOjZizMev5mPvp/uh3zt5w1o8T6ZQb0Z8auqDqzZFtCZn5JvV6P6h+DEBGRHXmsZzNEhXpj+tC2aNnY3eRxcml5jZBSLoO/R+V9hs6k5+LLf1KqVZb/26oJQqU6zXXGKrWUTuZ18P5o/Sm0nbEBJ69VHcaMOXE1G9/vSq1Wc5faRL8msh+cUJGIyI54Kp2wJr43AOBSZoHJ42Q6QcjZSQovF9Or2deWbi2OBIZJqGKNkKkGwG92ngcAfLrxDJZM7F7tctz7paYztruzDGOjm5r1GvYRsn+sESIislM+7qY7RE+JaSU+9nJxgpdLzTpPm6O4TF3pfoMgpJOEbuYVY82Rq3q1SrXNJtpJIwEgr7gMxytp7mMQsn+sESIislNuJuYUOvB2DPzcnTF9aFssS7yMl2NaG9QItWrsjsu3C1BUWnmIMYdujVBxmRrHr2SjVYC72CRWWdPY5N8O479z+uulVdVclbAtBZ5KOR7rGWZ0v26N00ML9yDpeg6+j+uGQREBBseWMQjZPdYIERHZKYlEgraBHgCA4R2biNu1a4E9068Ftr3aH409lVDo1MpEhXhh89R+2Pxyv0rPf3+XYLPKoRumfvgvFSMW7MKzP2uW7UjJyMWiO01eYrnvRJV/z97Af+cyAQBfbz8n7q8sB125XYC5G89gxpqTJmtzJBIJ1GoB+cVlYsfs3w9cNnosa4TsH2uEiIjs2KrneyG3qAweSjkim3gitp1hrUdFN/M0I6WMzWINAHumD0QjVwXOpOVi1aGrVZ5Pt0Zo7/lbAIDtZ26gTKVGzGfGh+Kfu5GHx77fb3RfZeFEN3QVlJTBQ2m879Pji/djl85Q/oIS46PRGITsH4MQEZEdc1XI4arQ/KqPH9DSrNdkF2oWUXWSGXZb3jt9kDg/kXMlM1eLx5/PxOOLjQeaV/44anS7RAIcvZxl8pyVNY3p9i/KL1aZDEK7KsxnlK8ziaPu/EdsGrN/bBojIiIAwKzR7SGVAPMeigIAOMkN/0ToTtKoMFFjpGvKsiMoMdFZes0R06vU63aOrqiypjHda+09n4n1x68bdIY2NoRft0ZIN/uoG0AQUqsFk8uX2AJBEOp9/qfqYI0QEREBAB69qxke6BIClzudrKsKOs5mzP+TllP9hVQlEpgMTwCgquSPvu7rpiw/Ij5O+XBopdfUDUK6zWHVrRFSqQW9qQnqWplKjeFf7EKglxJLn6j+lAL1If7XQ1h/PE2vNtGWsEaIiIhELjojzSr2EXqid7je84rD3i0lM68EJSrTAeT41WxMX3UMGUZClqmh+kU6243NZaQbfvQfq1GqUuPQpduV1lIBwNTlR9B7zj/ILSpFcnounvnpAE5VMku2JRy7mo0z6bnYkXyjTq9TG+uPa5YpWZ5ovEO6tTEIERGRUbo1Gz8+0R3vjIjU219XQeh0Wi4uZuab3F9SpsZv+y/j/bVJ4rZb+SVQqwWTNUkFJeYv5Kpb46RSA3P+Po37v/oPs9efrvR1qw5fRVpOEf46eh2PfLsPG0+mY+yiPWZftyZ0K8dsuXnMljEIERFRlcJ83Qy26S6Wuvnlu/HJAx0tdr2NZix2mpyumVvo7+PX0XXWZny94xxKVMb7ouTc6QAOAGXqymt2VCr9GqHvd6UCABbvTq2yTAAgQMDNvGLNdYtMB7CM3CLkFZsf0Iy5pbMWmjmtePnFZTijMyeTMTuSb2DNkapHA9oLBiEiIjLp16d74KvxXdDU19Vgn+6ossaeykr77kSFeFXruuk5xVUeU6oS8H9bzuK5Xw5BEIC5G8/g/A3jNUlfby+fq8jYUHnd2hSVicfGXLldgLGL9mBzUrrOuaosOm7ll6D7h1vRbdZmnLiajVd+P4rr2YVVv1DHfyk38fSPB8rLakYSGvHlLsTO34ndFUbN6YpbvB8vLTuCy7dML9FSEwJss8aKnaWJiMikXi38TO6TSCT49/UBKC5Tw8vFqdI/xHc19zVr5frqSL2Zj8+3JIvP5VIJZq07ZfTYlYeuiI9XHLxisL/URB+hn/deMjg2u6AULgoZFHIpZqw+gX2pt7Av9ZbJch67koWOId4G2wDNvEfaNdCuZhVg2aSeJs+jlXjhFs6k5RrU2pizQOz5m5qg+NfRa+jdUv97W1iiwsQfyqc6yMwvQaiPYQC2N6wRIiKiGgv1cRVXua8sCPVu6SceV1dqM+fPjdxijPhyF87fyDP5PpxkEmTmFSPq/U0Y8n+aiSAv3zasxan46nfWnDQ4xtjIslPXK2+y0npjxTG8vfoEEi/c1ttenfdvLDOtPHRFnPAS0ARLXQUlZThw4VaNpxSw1S5MDEJERGQRFQPETJ3O1V4uTtgytR/GdTdv1XdrOH41GwPn7cCZdOOBpFQliE1R2iY4o6HAjL/4UiOTGZk7i7W2Vqemrzel4lw/FcPahMWJeHDhHvyy37CWrCFjECIiIotwd9bvbeGiM8+Qdhbqj+5rb/L104a2rZuCVVOciZmwAeDQpSzxsUotGO1DVHGL9nlJmRrZBaXiayuqrBP3nnOZ+HxzMsoqGcJf28kfq5r/aP8FTW3Rr/vsKwixjxAREVnEqM5B2HIqHZvudBzWXVVeO8JMIpFg26v9UViiQtL1HBSVqvD26hOa4+VSNPdzM1njYWu2nErHxUzzOhRnFZRgyPx/kVtUiv+mDTI603JlNTrjvt0LAAhp5GLymKo6dlelYgwyVZ6aDtO30ZYx1ggREZFlOMtl+ObxbuLz1gEeOvvK/9yE+7khMsgTD3YNQZvA8mNkUolZHX6NaezhXKPX1cYzPx00ur3iW7iWVYif915EWk4R8ktUuHy7wOjEj6UqAcVllS9FYaxPklZuURlyijQ1TisOXsGoBbswKmG3WAula/kBw8kNCyqEM1NByNj3SDccNYRlSXQxCBERkUX9+/oAbJjSF0He5cspGOsTA+g3n0mlErNqNdZN7mOwrYW/fkfsr8d3Mbe4FlexxuRGbjE+3VQ+uq24TG1y7a02b2/QWwC2otRKassGfLodHd/dhGtZhXj1j6M4eiUbRy9nYbWJOYH+O6c/hD6vwpxHpjpfV9z88YbTuGv2VmTkFiHxwi10fG8TlhnrR2SjvaUZhIiIyKJCfVzRNtATbjp9hrxdja8Cr7ukh1wqQRVzHQIAIpt4Gmxr0Vh/wsehHZrg39cHYOGjXc0staGarhlW1eit4lKV3pIfFf13LhMAkJFThBFf7sJvOqHir6OmF6rVOpuRp/fczVludMbtDSf0J628WGHeIG2NkFot6M07VLFG6Ovt55CeU4zuH27FK78fRV5xGaatOg6gYcx2zT5CRERUJ5xkUuyZPhBqQb+/kC5XnSAklUjETtW6PJRy5OrUVkiM1C6F+5XXCI3rHgpAE8hCfVxxT2QANiWlw9/DGTdyq56oUUsplyLfyOSLVSmpYk2y4jI1iitZjf3o5Sx8+c9ZXMsqws28Yky/EyrMdSZNf32zK7cL0OHdjXg4OlRv+828YnGR2OWJl7Du2HW9/drO22uPX8fk3w6L2ytr+rpUIUxdqaQpz1awRsiIhIQEREZGIjo62tpFISJq0Jp4uSDY23QHX1en8v+PCwIwf2wnBOmsUP71+C448HYMuof7mDzHmK4h8HYpr3GadHcLvf0f3d8Bz/Zrgd+fMT5Z4TP9mhvdLtdZdNZYLZQppWWV14Jczy4yOfEjACzYloJjV7LFZTqq66MKa6LN33IWxWVqLN1zUW/7+uNp6P7hFmTmFeOtP08YnEdbI7SpwnIn1ekCNOyLf8XHtlo3xCBkRHx8PJKSkpCYmGjtohAR2TXdprHiMhU6hnjjv+mDkPhWDH56sjuGtA+Es1yGJ3qHGX19qI8L5o6J0qtx8nLRb4bzc3fGtKFtEe7nZrSprI1Op+7tr/YXH+uuNh8Vav4SIabWO9N688/q1fDUpcz8Eqw4eMVoc552m5+7fkf06nRoz61krTVbwaYxIiKyGoXOaDLdkVT+Hs7w9/AXnw+KCECfln5oe2eU2bAOgVh/PA0fjNLMS6TbydpTafpPW3P/8r5EfVv54a7mvmims46aj7tCfFyo13xlfn+hhG3nzD7WFhwzsfSJdvFZzwrBUhA0fX/yS1QGc0fpHydUeF7LgtYRBiEiIrIJxoaUaznJpPj5qR7i808ejMJrsZpaHkD/j65uk1ZFrQM88FpsGwR4KvFg1xAAwMlr5UFAdxSbrf7htrR1x68b3a7tI1RxSL9aEPDu/07i1/2X8L8XDEfwaeVVMvrNljAIERGRTaisA3FF7s5yvdqIwZEBaObriugw032JtOIHtNR73ibAAxFNPOHlIoeTiRAlkQAvDmyJL/9JMbuM5pBIbDdwaZvGCooN5xfS9jdK2Gb6fpSq9N9YTeeIqmsMQkREZBMqqxGqiqtCju2v9jc6oqwqcpkU617sg6pe+lC3UIsHoWBvF5sdWaXtLF1QYlgjpKWdMdyY0gqj52o783VdYWdpIiKyCZWNDDNHTUKQllQqEV/vp9NPSDw3NMP4jdE2z9VEVKh3jV9b18pUAi7czBdnq9a6mVciPl556IrJ11ecu0ilss0gxBohIiKyqv+mDcSZ9Fz0b+1f9cH1oLm/O27m3TLYbqpj8IioIHyx9Wyl52zZ2B2380uQmV+it/2ucB+D+XusTS6VoEwt4It/zpq9lpoxFSdsrGqiSWthjRAREVlVkLcLBrRpXKsaHUt6vr9mHqKezX31tstlUiybdJfB8bqTQpry7ePdMKFXmMF2Hzdn/Pv6AIT6mJ5rqb5ppyKoTQgCgA/X68+VpG1qs7XZphmEiIiIdPRv0xgbpvTF9xPKF5DVZrS7mvtiw5S+esfrBqEneocbPWczH1e9oflaznKpZgbsRq4G+3572jB0aW19pR8+HRNV6fuoKaWR2b0toUwtYN/5TER/uAX/O3oNe85lYr2JEWv1iU1jREREFbQNND2TdMV9ukPuO4YYTrw4rnsopFKJwcSEgOmlRwAgpJEL3BQyo8t8hPu6oYW/O17946jJ19dUZR2ga0OtFvDGymO4mVeit2THztcGoKmvYRCsL6wRIiIiqoKvm2GI0dJdXFbQWUgi2NsFvVv6Yvb9HQEY74StrX0x1lrkoZSbHEknreGCsOaoyxohYyHrapZ1R80xCBEREZnwxbjOGNIuEJPuNr4eGQB0bdYIACCV6K8ztvP1Afj5yfJJIL1dDYNQZSvcuznL8dH9HQy2P9QtRHwc4Gk6oJny0qBWle43VUs1Iiqo2tfSpVKrEaCzjpyWtecXYhAiIiIyYWRUEBY+1lWv1qeiAE8l/n19AA6+PVhv5XmZzpB8AGjq44qmPvpNQJV1EHeSSfFQt1Bse7W/XmfqTx4s7xu0ZEL3ar2f0x8MwcuDW1d6THqO4WKvnko5vhzXWa8ZsCJjTX+6Vh+5hp3JNwy2VxxmX98YhIiIiGop1McVjdwUYu2QMU4yKbZM7YezHw4Vt5nTwBXu54Y18X0wtlsoVj7XU29fZJAngirUspjKVuO6h4q1PQsf7WLyepWteu8kKz+5s1w/QrgoahYprL0UBztLExERWUhEE0/8+XwvNPEyPhxeUSE8hJk5GaOPmwIfP9jR6L6K8/M4yaRiLcuSCdH46+g1qAUBr8e2FY8Z0r4Juof5YP8Fw/mSKqNb/raBHjiqs2CruoYVOx+sTUL/Nv7wUDpVfXAdYBAiIiKyoM5NTdcKaR2aMRjFZSp4udT+j3/FPjY/TIjGc78cwqzR7TGgbWMMaNvY6Oue6dfcaBCSSoCKcx9qn+quxfbluC64e+428XlNOz1n5BbX2Ug1c7BpjIiIqJo6BGuGyfu6GXaANoePm0Kv1qhF4/KaofdHtcPaF02v6l6RqkJq6dXSD0feGVxl5+ZBEQF4e3iEwfbvJ0QbbJPeaW/TDUJNfV0xsXeY2eWsTMWasvrEGiEiIqJqWvhYV3y1LQUTTUygWF2vD2kLQQBGdw5GdFj11lwztnSFubN0RzQxnC+pS4UaLRcnGb4er+lT9MLAlnh9xTExZJnTnPVsvxZYuOOcyf2V9VeqDwxCRERE1RTs7YIP7zMc2l5TnkqnGp9PIat5bYqxkeu6HaL7t/HHd493g/zONcZ0DUHnUG+xb9PQ9oH4YutZNPZwRkau8U7WunMrGTOkfZMalt4y2DRGRETUgMW2D6zxa5t4G87rozu3kbNcKoYgQFPT1CrAQ2wii2jiia2v9MOWV/qZvIa7wrDOZXhHTfgZ2y20xmW3FNYIERERNWBvD4+Ai5MM3+9KxZN9qtdU18LfHd/HdcOiHeex/8ItDOsQCGe5DB8/0AHf/puKt4ZFmnUOXUonKSb0CkdRqQrFZWr0bOELbNZ/zcA2jfHeyHY17mNlSQxCREREDZirQo4Z90bihQEt4e1a/VFogyICMCgiQG/b2OimGBvdtEblGRUVjGlDy4fqH7uSZXCMXGZ87TVrYBAiIiKyA42sXLuy4tme+PPwVbyhE4IA4yPCnGrRr8nSGISIiIio1rqF+aCbkRFvxkKPvA4Xja0u24lkREREZHeMjWpzsuK8QRXZTkmIiIjI7hhtGpPaTvywnZIQERGR3TFWIySXsWmMiIiIHICxZjAnBiEiIiJyBMZCT2FJDZeqrwMMQkRERFRnjDWNuSist9p8RRw+T0RERHVGIpHgm8e6Ir+kDN6uCiSn5aJLU29rF0vEIERERER16p525euhDWjT2IolMcSmMSIiInJYDEJERETksBiEiIiIyGExCBEREZHDYhAiIiIih8UgRERERA6LQYiIiIgcFoMQEREROSyHCEL33XcfGjVqhAcffNDaRSEiIiIb4hBB6KWXXsKPP/5o7WIQERGRjXGIINS/f394eHhYuxhERERkY6wehHbu3IkRI0YgKCgIEokEq1evNjgmISEBYWFhUCqV6NGjB/bv31//BSUiIiK7Y/UglJ+fj6ioKCQkJBjdv3z5ckydOhUzZ87EoUOHEBUVhdjYWGRkZIjHdOrUCe3btzf4d+3atfp6G0RERNQAWX31+aFDh2Lo0KEm93/22Wd4+umnMXHiRADAwoULsW7dOixevBjTpk0DABw5csQiZSkuLkZxcbH4PCcnxyLnJSIiIttk9RqhypSUlODgwYOIiYkRt0mlUsTExGDPnj0Wv97s2bPh5eUl/gsNDbX4NYiIiMh22HQQunnzJlQqFQICAvS2BwQEIC0tzezzxMTEYMyYMVi/fj1CQkJMhqjp06cjOztb/Hf58uValZ+IiIhsm9WbxurDli1bzDrO2dkZzs7OdVwaIiIishU2XSPk5+cHmUyG9PR0ve3p6ekIDAy0UqmIiIjIXth0EFIoFOjatSu2bt0qblOr1di6dSt69uxpxZIRERGRPbB601heXh5SUlLE56mpqThy5Ah8fHzQtGlTTJ06FXFxcejWrRu6d++O+fPnIz8/XxxFRkRERFRTVg9CBw4cwIABA8TnU6dOBQDExcXhhx9+wNixY3Hjxg288847SEtLQ6dOnbBhwwaDDtRERERE1SURBEGwdiFsTUJCAhISEqBSqZCcnIzs7Gx4enpau1hERERkhpycHHh5eZn195tBqBLVuZFERERkG6rz99umO0sTERER1SUGISIiInJYDEJERETksBiEiIiIyGExCBEREZHDYhAiIiIih8UgZERCQgIiIyMRHR1t7aIQERFRHeI8QpXgPEJEREQND+cRIiIiIjIDgxARERE5LAYhIiIiclgMQkREROSwGISIiIjIYTEIERERkcNiECIiIiKHxSBkBCdUJCIicgycULESnFCRiIio4eGEikRERERmYBAiIiIih8UgRERERA6LQYiIiIgcFoMQEREROSwGISIiInJYDEJERETksBiEiIiIyGExCBEREZHDYhAygktsEBEROQYusVEJLrFBRETU8HCJDSIiIiIzMAgRERGRw2IQIiIiIofFIEREREQOi0GIiIiIHBaDEBERETksBiEiIiJyWAxCRERE5LAYhIiIiMhhMQgRERGRw2IQMoJrjRERETkGrjVWCa41RkRE1PBwrTEiIiIiMzAIERERkcNiECIiIiKHxSBEREREDotBiIiIiBwWgxARERE5LAYhIiIiclhyaxfAlmmnWMrJybFySYiIiMhc2r/b5kyVyCBUidzcXABAaGiolUtCRERE1ZWbmwsvL69Kj+HM0pVQq9W4du0aPDw8IJFILHrunJwchIaG4vLly5y12gJ4Py2L99OyeD8ti/fTcuz1XgqCgNzcXAQFBUEqrbwXEGuEKiGVShESElKn1/D09LSrD5+18X5aFu+nZfF+Whbvp+XY472sqiZIi52liYiIyGExCBEREZHDYhCyEmdnZ8ycORPOzs7WLopd4P20LN5Py+L9tCzeT8vhvWRnaSIiInJgrBEiIiIih8UgRERERA6LQYiIiIgcFoMQEREROSwGIStJSEhAWFgYlEolevTogf3791u7SDZn9uzZiI6OhoeHBxo3bozRo0fjzJkzescUFRUhPj4evr6+cHd3xwMPPID09HS9Yy5duoThw4fD1dUVjRs3xmuvvYaysrL6fCs2Z86cOZBIJJgyZYq4jfeyeq5evYpHH30Uvr6+cHFxQYcOHXDgwAFxvyAIeOedd9CkSRO4uLggJiYGZ8+e1TvHrVu3MH78eHh6esLb2xtPPvkk8vLy6vutWJ1KpcKMGTMQHh4OFxcXtGjRAh988IHeOlG8n6bt3LkTI0aMQFBQECQSCVavXq2331L37tixY+jbty+USiVCQ0PxySef1PVbqx8C1btly5YJCoVCWLx4sXDy5Enh6aefFry9vYX09HRrF82mxMbGCkuWLBFOnDghHDlyRBg2bJjQtGlTIS8vTzzm2WefFUJDQ4WtW7cKBw4cEO666y6hV69e4v6ysjKhffv2QkxMjHD48GFh/fr1gp+fnzB9+nRrvCWbsH//fiEsLEzo2LGj8NJLL4nbeS/Nd+vWLaFZs2bChAkThH379gnnz58XNm7cKKSkpIjHzJkzR/Dy8hJWr14tHD16VBg5cqQQHh4uFBYWiscMGTJEiIqKEvbu3Sv8+++/QsuWLYVx48ZZ4y1Z1Ycffij4+voKa9euFVJTU4U//vhDcHd3F/7v//5PPIb307T169cLb731lrBq1SoBgPDnn3/q7bfEvcvOzhYCAgKE8ePHCydOnBB+++03wcXFRVi0aFF9vc06wyBkBd27dxfi4+PF5yqVSggKChJmz55txVLZvoyMDAGAsGPHDkEQBCErK0twcnIS/vjjD/GYU6dOCQCEPXv2CIKg+QUhlUqFtLQ08Zivv/5a8PT0FIqLi+v3DdiA3NxcoVWrVsLmzZuFfv36iUGI97J63njjDaFPnz4m96vVaiEwMFCYO3euuC0rK0twdnYWfvvtN0EQBCEpKUkAICQmJorH/P3334JEIhGuXr1ad4W3QcOHDxeeeOIJvW3333+/MH78eEEQeD+ro2IQstS9++qrr4RGjRrp/ay/8cYbQps2ber4HdU9No3Vs5KSEhw8eBAxMTHiNqlUipiYGOzZs8eKJbN92dnZAAAfHx8AwMGDB1FaWqp3L9u2bYumTZuK93LPnj3o0KEDAgICxGNiY2ORk5ODkydP1mPpbUN8fDyGDx+ud88A3svq+t///odu3bphzJgxaNy4MTp37oxvv/1W3J+amoq0tDS9++nl5YUePXro3U9vb29069ZNPCYmJgZSqRT79u2rvzdjA3r16oWtW7ciOTkZAHD06FHs2rULQ4cOBcD7WRuWund79uzB3XffDYVCIR4TGxuLM2fO4Pbt2/X0buoGF12tZzdv3oRKpdL7YwIAAQEBOH36tJVKZfvUajWmTJmC3r17o3379gCAtLQ0KBQKeHt76x0bEBCAtLQ08Rhj91q7z5EsW7YMhw4dQmJiosE+3svqOX/+PL7++mtMnToVb775JhITEzF58mQoFArExcWJ98PY/dK9n40bN9bbL5fL4ePj43D3c9q0acjJyUHbtm0hk8mgUqnw4YcfYvz48QDA+1kLlrp3aWlpCA8PNziHdl+jRo3qpPz1gUGIGoT4+HicOHECu3btsnZRGqTLly/jpZdewubNm6FUKq1dnAZPrVajW7du+OijjwAAnTt3xokTJ7Bw4ULExcVZuXQNz++//45ffvkFv/76K9q1a4cjR45gypQpCAoK4v2kOsemsXrm5+cHmUxmMBonPT0dgYGBViqVbXvhhRewdu1abNu2DSEhIeL2wMBAlJSUICsrS+943XsZGBho9F5r9zmKgwcPIiMjA126dIFcLodcLseOHTvwxRdfQC6XIyAggPeyGpo0aYLIyEi9bREREbh06RKA8vtR2c95YGAgMjIy9PaXlZXh1q1bDnc/X3vtNUybNg0PP/wwOnTogMceewwvv/wyZs+eDYD3szYsde/s+eefQaieKRQKdO3aFVu3bhW3qdVqbN26FT179rRiyWyPIAh44YUX8Oeff+Kff/4xqJbt2rUrnJyc9O7lmTNncOnSJfFe9uzZE8ePH9f7Id+8eTM8PT0N/pDZs0GDBuH48eM4cuSI+K9bt24YP368+Jj30ny9e/c2mMohOTkZzZo1AwCEh4cjMDBQ737m5ORg3759evczKysLBw8eFI/5559/oFar0aNHj3p4F7ajoKAAUqn+nyOZTAa1Wg2A97M2LHXvevbsiZ07d6K0tFQ8ZvPmzWjTpk2DbhYDwOHz1rBs2TLB2dlZ+OGHH4SkpCRh0qRJgre3t95oHBKE5557TvDy8hK2b98uXL9+XfxXUFAgHvPss88KTZs2Ff755x/hwIEDQs+ePYWePXuK+7VDvu+55x7hyJEjwoYNGwR/f3+HHPJdke6oMUHgvayO/fv3C3K5XPjwww+Fs2fPCr/88ovg6uoq/Pzzz+Ixc+bMEby9vYU1a9YIx44dE0aNGmV0yHLnzp2Fffv2Cbt27RJatWrlEMO9K4qLixOCg4PF4fOrVq0S/Pz8hNdff108hvfTtNzcXOHw4cPC4cOHBQDCZ599Jhw+fFi4ePGiIAiWuXdZWVlCQECA8NhjjwknTpwQli1bJri6unL4PNXcl19+KTRt2lRQKBRC9+7dhb1791q7SDYHgNF/S5YsEY8pLCwUnn/+eaFRo0aCq6urcN999wnXr1/XO8+FCxeEoUOHCi4uLoKfn5/wyiuvCKWlpfX8bmxPxSDEe1k9f/31l9C+fXvB2dlZaNu2rfDNN9/o7Ver1cKMGTOEgIAAwdnZWRg0aJBw5swZvWMyMzOFcePGCe7u7oKnp6cwceJEITc3tz7fhk3IyckRXnrpJaFp06aCUqkUmjdvLrz11lt6Q7V5P03btm2b0d+VcXFxgiBY7t4dPXpU6NOnj+Ds7CwEBwcLc+bMqa+3WKckgqAzdScRERGRA2EfISIiInJYDEJERETksBiEiIiIyGExCBEREZHDYhAiIiIih8UgRERERA6LQYiIiIgcFoMQEVE1bN++HRKJxGBdNiJqmBiEiIiIyGExCBEREZHDYhAiogZFrVZj9uzZCA8Ph4uLC6KiorBixQoA5c1W69atQ8eOHaFUKnHXXXfhxIkTeudYuXIl2rVrB2dnZ4SFhWHevHl6+4uLi/HGG28gNDQUzs7OaNmyJb7//nu9Yw4ePIhu3brB1dUVvXr1MliNnogaBgYhImpQZs+ejR9//BELFy7EyZMn8fLLL+PRRx/Fjh07xGNee+01zJs3D4mJifD398eIESNQWloKQBNgHnroITz88MM4fvw43n33XcyYMQM//PCD+PrHH38cv/32G7744gucOnUKixYtgru7u1453nrrLcybNw8HDhyAXC7HE088US/vn4gsi4uuElGDUVxcDB8fH2zZsgU9e/YUtz/11FMoKCjApEmTMGDAACxbtgxjx44FANy6dQshISH44Ycf8NBDD2H8+PG4ceMGNm3aJL7+9ddfx7p163Dy5EkkJyejTZs22Lx5M2JiYgzKsH37dgwYMABbtmzBoEGDAADr16/H8OHDUVhYCKVSWcd3gYgsiTVCRNRgpKSkoKCgAIMHD4a7u7v478cff8S5c+fE43RDko+PD9q0aYNTp04BAE6dOoXevXvrnbd37944e/YsVCoVjhw5AplMhn79+lValo4dO4qPmzRpAgDIyMio9Xskovolt3YBiIjMlZeXBwBYt24dgoOD9fY5OzvrhaGacnFxMes4Jycn8bFEIgGg6b9ERA0La4SIqMGIjIyEs7MzLl26hJYtW+r9Cw0NFY/bu3ev+Pj27dtITk5GREQEACAiIgK7d+/WO+/u3bvRunVryGQydOjQAWq1Wq/PERHZL9YIEVGD4eHhgVdffRUvv/wy1Go1+vTpg+zsbOzevRuenp5o1qwZAOD999+Hr68vAgIC8NZbb8HPzw+jR48GALzyyiuIjo7GBx98gLFjx2LPnj1YsGABvvrqKwBAWFgY4uLi8MQTT+CLL75AVFQULl68iIyMDDz00EPWeutEVEcYhIioQfnggw/g7++P2bNn4/z58/D29kaXLl3w5ptvik1Tc+bMwUsvvYSzZ8+iU6dO+Ouvv6BQKAAAXbp0we+//4533nkHH3zwAZo0aYL3338fEyZMEK/x9ddf480338Tzzz+PzMxMNG3aFG+++aY13i4R1TGOGiMiu6Ed0XX79m14e3tbuzhE1ACwjxARERE5LAYhIiIiclhsGiMiIiKHxRohIiIiclgMQkREROSwGISIiIjIYTEIERERkcNiECIiIiKHxSBEREREDotBiIiIiBwWgxARERE5LAYhIiIiclj/D0sxOhc8QXrMAAAAAElFTkSuQmCC"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 1.7435619831085205\n",
            "Train loss: 17.569419860839844\n",
            "Test loss: 4.42103910446167\n",
            "dO18 RMSE: 11.068434341874646\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       24.042000           0.345970\n",
            "1       25.240000           0.035950\n",
            "2       25.782000           0.372220\n",
            "3       25.076000           0.230280\n",
            "4       25.966000           0.172480\n",
            "5       27.434000           0.501030\n",
            "6       28.156000           0.999030\n",
            "7       26.836000           0.120880\n",
            "8       28.180000           0.778250\n",
            "9       26.834000           0.094930\n",
            "10      26.644000           0.488430\n",
            "11      26.772000           0.373370\n",
            "12      27.684280           1.216389\n",
            "13      27.403235           0.892280\n",
            "14      24.777670           0.736571\n",
            "15      25.551850           0.312355\n",
            "16      25.115885           0.033519\n",
            "17      25.987815           5.280825\n",
            "18      24.132031           0.389042\n",
            "19      24.898000           0.236070\n",
            "20      23.944000           0.158130\n",
            "21      26.018000           0.964170\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       18.347429          12.769851\n",
            "1       18.732466          13.214819\n",
            "2       17.938549          12.671567\n",
            "3       18.370306          12.917855\n",
            "4       18.307663          12.976246\n",
            "5       43.832699          28.991426\n",
            "6       42.142345          27.902414\n",
            "7       43.109764          28.564310\n",
            "8       42.401196          28.059128\n",
            "9       43.139973          28.582863\n",
            "10      42.672157          28.307558\n",
            "11      42.812130          28.386084\n",
            "12      43.008396          28.474703\n",
            "13      42.233944          27.999012\n",
            "14      27.143351          16.538883\n",
            "15      30.950760          18.092991\n",
            "16      31.703344          18.606936\n",
            "17      24.299488          12.969061\n",
            "18      31.466640          18.271736\n",
            "19      18.394575          12.915168\n",
            "20      18.764395          13.088288\n",
            "21      16.801706          11.765584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-02 23:49:47.061277: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [22,13]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/usr/local/google/home/ruru/Downloads/model_builds/fixed_all_boosted_transformer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Grouped fixed, ablating other columns besides krigin"
      ],
      "metadata": {
        "id": "h1uuaygyDvXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_fixed_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_train_fixed_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_test_fixed_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_validation_fixed_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_normalize = []\n",
        "\n",
        "columns_to_standardize =  [\n",
        "    'lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature','Mean Annual Precipitation']\n",
        "\n",
        "data = load_and_scale(grouped_fixed_fileset, columns_to_normalize, columns_to_standardize)\n",
        "model = train_and_evaluate(data, 'fixed_ablated', training_batch_size=3)\n",
        "model.save(get_model_save_location('fixed_ablated_boosted.keras'), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location('fixed_ablated_boosted._transformer.pkl'))"
      ],
      "metadata": {
        "id": "WQ60MVzlD5DJ",
        "outputId": "98aeef89-38be-4145-9c46-6aacb0cdf86b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         lat       long      VPD       RH       PET  DEM          PA  \\\n",
            "0  -3.995545 -57.589273  0.85167  0.77358  99.99167   61  1005.67358   \n",
            "1  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "2  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "3  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "4  -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "..       ...        ...      ...      ...       ...  ...         ...   \n",
            "64 -3.398397 -54.973982  0.64500  0.81744  97.93333  139   996.36792   \n",
            "65 -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "66 -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "67 -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "68 -3.992300 -54.908000  0.70750  0.80339  97.88333  108  1000.05792   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  krig_mean_residual  \\\n",
            "0                  27.16667                       2273           -0.827845   \n",
            "1                  26.29583                       1897           -0.941845   \n",
            "2                  26.29583                       1897           -0.369845   \n",
            "3                  26.29583                       1897           -0.963845   \n",
            "4                  26.29583                       1897           -0.615845   \n",
            "..                      ...                        ...                 ...   \n",
            "64                 26.00000                       1840            0.760155   \n",
            "65                 26.29583                       1897           -0.621845   \n",
            "66                 26.29583                       1897           -1.175845   \n",
            "67                 26.29583                       1897           -0.209845   \n",
            "68                 26.29583                       1897           -1.063845   \n",
            "\n",
            "    krig_variance_residual  \n",
            "0                 0.728384  \n",
            "1                 0.771084  \n",
            "2                 0.439054  \n",
            "3                 0.900874  \n",
            "4                 0.617224  \n",
            "..                     ...  \n",
            "64                1.046004  \n",
            "65                0.289584  \n",
            "66                1.093624  \n",
            "67                1.070804  \n",
            "68                0.859824  \n",
            "\n",
            "[68 rows x 11 columns]\n",
            "         lat       long      VPD       RH       PET  DEM          PA  \\\n",
            "0  -0.121000 -67.013000  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "1  -0.041000 -66.872000  0.62083  0.82559  92.27500   93  1001.84741   \n",
            "2  -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "3  -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "4  -4.303698 -70.291068  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "5  -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "6  -4.303698 -70.291068  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "7  -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "8  -4.303698 -70.291068  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "9  -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "10 -4.303698 -70.291068  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "11 -3.907000 -66.055000  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "12 -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "13 -4.303698 -70.291068  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "14 -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "15 -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "16 -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "17 -3.907000 -66.055000  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "18 -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "19 -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "20 -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "21 -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "22 -4.303698 -70.291068  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "24 -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "25 -0.121000 -67.013000  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "26 -0.041000 -66.872000  0.62083  0.82559  92.27500   93  1001.84741   \n",
            "27 -3.907000 -66.055000  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "28 -4.304000 -70.291000  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "29 -4.304000 -70.291000  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "30 -3.907183 -66.055146  0.64000  0.82437  89.98333   88  1002.44446   \n",
            "31 -0.121230 -67.013103  0.58917  0.83284  91.16666  101  1000.89270   \n",
            "32 -4.303698 -70.291068  0.73583  0.79822  89.56667  108  1000.05792   \n",
            "33 -3.758534 -66.073754  0.64667  0.82247  90.44167   84  1002.92230   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  krig_mean_residual  \\\n",
            "0                  26.20833                       2830            2.514895   \n",
            "1                  26.37500                       2764            0.194310   \n",
            "2                  26.20833                       2830           -0.015845   \n",
            "3                  26.71667                       2795            2.484155   \n",
            "4                  26.64583                       2708            0.506155   \n",
            "5                  26.20833                       2830            0.752155   \n",
            "6                  26.64583                       2708            1.338155   \n",
            "7                  26.20833                       2830            0.812155   \n",
            "8                  26.64583                       2708            2.522155   \n",
            "9                  26.20833                       2830            0.914155   \n",
            "10                 26.64583                       2708            2.152155   \n",
            "11                 26.71667                       2795            2.120155   \n",
            "12                 26.20833                       2830            2.178155   \n",
            "13                 26.64583                       2708            1.882155   \n",
            "14                 26.20833                       2830            1.312155   \n",
            "15                 26.20833                       2830            0.224155   \n",
            "16                 26.20833                       2830            0.396155   \n",
            "17                 26.71667                       2795            0.126155   \n",
            "18                 26.71667                       2795            2.150155   \n",
            "19                 26.71667                       2795            0.008155   \n",
            "20                 26.71667                       2795            0.042155   \n",
            "21                 26.71667                       2795           -0.139845   \n",
            "22                 26.64583                       2708            2.592155   \n",
            "24                 26.71667                       2795            0.724155   \n",
            "25                 26.20833                       2830            1.264155   \n",
            "26                 26.37500                       2764           -0.435845   \n",
            "27                 26.71667                       2795            1.958155   \n",
            "28                 26.64583                       2708            1.072155   \n",
            "29                 26.64583                       2708            0.840155   \n",
            "30                 26.71667                       2795           -0.625845   \n",
            "31                 26.20833                       2830            0.238155   \n",
            "32                 26.64583                       2708            1.892155   \n",
            "33                 26.71667                       2856            2.168155   \n",
            "\n",
            "    krig_variance_residual  \n",
            "0                 1.112778  \n",
            "1                 1.047335  \n",
            "2                 1.194524  \n",
            "3                 1.126274  \n",
            "4                 1.235124  \n",
            "5                 1.154734  \n",
            "6                -2.801616  \n",
            "7                 1.090434  \n",
            "8                 1.141884  \n",
            "9                 0.988924  \n",
            "10                1.212634  \n",
            "11                0.983704  \n",
            "12                1.142184  \n",
            "13                0.997084  \n",
            "14               -0.389416  \n",
            "15                0.469474  \n",
            "16                1.041474  \n",
            "17                1.037024  \n",
            "18                0.256504  \n",
            "19                0.782084  \n",
            "20                0.911134  \n",
            "21               -1.323696  \n",
            "22                0.598884  \n",
            "24                0.850424  \n",
            "25                1.083774  \n",
            "26                1.221524  \n",
            "27                0.951134  \n",
            "28                1.102634  \n",
            "29                1.209804  \n",
            "30               -0.882726  \n",
            "31                0.316334  \n",
            "32                0.802534  \n",
            "33                0.736684  \n",
            "          lat       long      VPD       RH        PET  DEM          PA  \\\n",
            "0   -6.009707 -61.868657  0.77083  0.79509   93.97500   71  1004.47662   \n",
            "1   -6.009707 -61.868657  0.77083  0.79509   93.97500   71  1004.47662   \n",
            "2   -6.009707 -61.868657  0.77083  0.79509   93.97500   71  1004.47662   \n",
            "3   -6.009707 -61.868657  0.77083  0.79509   93.97500   71  1004.47662   \n",
            "4   -6.009707 -61.868657  0.77083  0.79509   93.97500   71  1004.47662   \n",
            "5  -13.081300 -52.377100  0.99167  0.71167  105.92500  391   966.77917   \n",
            "6  -13.079000 -52.386400  0.99167  0.71167  105.92500  381   967.93964   \n",
            "7  -13.079000 -52.386400  0.99167  0.71167  105.92500  381   967.93964   \n",
            "8  -13.079000 -52.386400  0.99167  0.71167  105.92500  381   967.93964   \n",
            "9  -13.079000 -52.386400  0.99167  0.71167  105.92500  381   967.93964   \n",
            "10 -13.079000 -52.386400  0.99167  0.71167  105.92500  381   967.93964   \n",
            "11 -13.079000 -52.386400  0.99167  0.71167  105.92500  381   967.93964   \n",
            "12 -13.081300 -52.377100  0.99167  0.71167  105.92500  391   966.77917   \n",
            "13 -13.079000 -52.386400  0.99167  0.71167  105.92500  381   967.93964   \n",
            "14  -9.318000 -60.978000  0.72583  0.79095   93.08334  151   994.94250   \n",
            "15  -9.312000 -62.982000  0.63333  0.81416   91.68333  129   997.55707   \n",
            "16  -9.311000 -62.974000  0.63333  0.81416   91.68333  124   998.15204   \n",
            "17  -9.317000 -62.981000  0.63333  0.81416   91.68333  138   996.48682   \n",
            "18  -9.299000 -62.977000  0.63333  0.81416   91.68333  127   997.79504   \n",
            "19  -6.009707 -61.868657  0.77083  0.79509   93.97500   71  1004.47662   \n",
            "20  -6.009707 -61.868657  0.77083  0.79509   93.97500   71  1004.47662   \n",
            "21  -6.009707 -61.868657  0.77083  0.79509   93.97500   71  1004.47662   \n",
            "\n",
            "    Mean Annual Temperature  Mean Annual Precipitation  krig_mean_residual  \\\n",
            "0                  27.20000                       1996            1.878155   \n",
            "1                  27.20000                       1996            0.680155   \n",
            "2                  27.20000                       1996            0.138155   \n",
            "3                  27.20000                       1996            0.844155   \n",
            "4                  27.20000                       1996           -0.045845   \n",
            "5                  25.06667                       1593           -1.513845   \n",
            "6                  25.06667                       1593           -2.235845   \n",
            "7                  25.06667                       1593           -0.915845   \n",
            "8                  25.06667                       1593           -2.259845   \n",
            "9                  25.06667                       1593           -0.913845   \n",
            "10                 25.06667                       1593           -0.723845   \n",
            "11                 25.06667                       1593           -0.851845   \n",
            "12                 25.06667                       1593           -1.764125   \n",
            "13                 25.06667                       1593           -1.483080   \n",
            "14                 25.45833                       2190            1.142485   \n",
            "15                 25.37500                       2344            0.368305   \n",
            "16                 25.37500                       2344            0.804270   \n",
            "17                 25.37500                       2344           -0.067660   \n",
            "18                 25.37500                       2344            1.788124   \n",
            "19                 27.20000                       1996            1.022155   \n",
            "20                 27.20000                       1996            1.976155   \n",
            "21                 27.20000                       1996           -0.097845   \n",
            "\n",
            "    krig_variance_residual  \n",
            "0                 0.915084  \n",
            "1                 1.225104  \n",
            "2                 0.888834  \n",
            "3                 1.030774  \n",
            "4                 1.088574  \n",
            "5                 0.760024  \n",
            "6                 0.262024  \n",
            "7                 1.140174  \n",
            "8                 0.482804  \n",
            "9                 1.166124  \n",
            "10                0.772624  \n",
            "11                0.887684  \n",
            "12                0.044664  \n",
            "13                0.368773  \n",
            "14                0.524482  \n",
            "15                0.948699  \n",
            "16                1.227534  \n",
            "17               -4.019772  \n",
            "18                0.872011  \n",
            "19                1.024984  \n",
            "20                1.102924  \n",
            "21                0.296884  \n",
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('krig_mean_residual_scaler', StandardScaler(),\n",
            "                                 ['krig_mean_residual']),\n",
            "                                ('krig_variance_residual_scaler',\n",
            "                                 StandardScaler(), ['krig_variance_residual']),\n",
            "                                ('lat_standardizer', StandardScaler(), ['lat']),\n",
            "                                ('long_standardizer', StandardScaler(),\n",
            "                                 ['long']),\n",
            "                                ('VPD_standardizer', StandardScaler(), ['VPD']),\n",
            "                                ('RH_standardizer', StandardScaler(), ['RH']),\n",
            "                                ('PET_standardizer', StandardScaler(), ['PET']),\n",
            "                                ('DEM_standardizer', StandardScaler(), ['DEM']),\n",
            "                                ('PA_standardizer', StandardScaler(), ['PA']),\n",
            "                                ('Mean Annual Temperature_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation'])])\n",
            "==================\n",
            "fixed_ablated\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 11)]         0           []                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_20 (S  (None, 9)           0           ['input_7[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 20)           200         ['tf.__operators__.getitem_20[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_18 (S  (None,)             0           ['input_7[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 20)           420         ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_19 (S  (None,)             0           ['input_7[0][0]']                \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " tf.expand_dims_12 (TFOpLambda)  (None, 1)           0           ['tf.__operators__.getitem_18[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " mean_output (Dense)            (None, 1)            21          ['dense_13[0][0]']               \n",
            "                                                                                                  \n",
            " var_output (Dense)             (None, 1)            21          ['dense_13[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_13 (TFOpLambda)  (None, 1)           0           ['tf.__operators__.getitem_19[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " tf.math.multiply_21 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_12[0][0]']      \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.multiply_20 (TFOpLambd  (None, 1)           0           ['mean_output[0][0]']            \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.multiply_22 (TFOpLambd  (None, 1)           0           ['var_output[0][0]']             \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.multiply_23 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_13[0][0]']      \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_27 (TFOpL  (None, 1)           0           ['tf.math.multiply_21[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_26 (TFOpL  (None, 1)           0           ['tf.math.multiply_20[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_29 (TFOpL  (None, 1)           0           ['tf.math.multiply_22[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_30 (TFOpL  (None, 1)           0           ['tf.math.multiply_23[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_28 (TFOpL  (None, 1)           0           ['tf.__operators__.add_27[0][0]',\n",
            " ambda)                                                           'tf.__operators__.add_26[0][0]']\n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1)            0           ['tf.__operators__.add_29[0][0]',\n",
            "                                                                  'tf.__operators__.add_30[0][0]']\n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 2)            0           ['tf.__operators__.add_28[0][0]',\n",
            "                                                                  'lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 662\n",
            "Trainable params: 662\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10000\n",
            "23/23 [==============================] - 1s 11ms/step - loss: 320.1581 - val_loss: 104.6114\n",
            "Epoch 2/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 307.1628 - val_loss: 78.8956\n",
            "Epoch 3/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 284.0358 - val_loss: 75.8350\n",
            "Epoch 4/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 267.8958 - val_loss: 72.0589\n",
            "Epoch 5/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 271.9968 - val_loss: 82.6360\n",
            "Epoch 6/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 264.8977 - val_loss: 67.7310\n",
            "Epoch 7/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 260.4834 - val_loss: 66.7599\n",
            "Epoch 8/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 239.0022 - val_loss: 61.2409\n",
            "Epoch 9/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 224.0683 - val_loss: 58.5017\n",
            "Epoch 10/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 248.3160 - val_loss: 60.1448\n",
            "Epoch 11/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 234.3275 - val_loss: 57.3943\n",
            "Epoch 12/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 229.7677 - val_loss: 53.9337\n",
            "Epoch 13/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 212.6685 - val_loss: 51.9937\n",
            "Epoch 14/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 223.2082 - val_loss: 47.4121\n",
            "Epoch 15/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 211.0469 - val_loss: 49.5056\n",
            "Epoch 16/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 234.2556 - val_loss: 36.9789\n",
            "Epoch 17/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 191.5661 - val_loss: 34.6395\n",
            "Epoch 18/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 181.3978 - val_loss: 33.8945\n",
            "Epoch 19/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 154.8096 - val_loss: 27.4105\n",
            "Epoch 20/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 171.0576 - val_loss: 29.5577\n",
            "Epoch 21/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 154.8006 - val_loss: 35.1567\n",
            "Epoch 22/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 159.4442 - val_loss: 22.9281\n",
            "Epoch 23/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 161.9326 - val_loss: 27.0786\n",
            "Epoch 24/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 152.5455 - val_loss: 23.9377\n",
            "Epoch 25/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 146.9033 - val_loss: 27.8451\n",
            "Epoch 26/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 134.1176 - val_loss: 19.9937\n",
            "Epoch 27/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 147.3315 - val_loss: 18.0027\n",
            "Epoch 28/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 119.8071 - val_loss: 21.6683\n",
            "Epoch 29/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 111.2198 - val_loss: 17.8020\n",
            "Epoch 30/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 136.8212 - val_loss: 14.7413\n",
            "Epoch 31/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 97.5447 - val_loss: 13.8869\n",
            "Epoch 32/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 101.8753 - val_loss: 15.2664\n",
            "Epoch 33/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 104.9982 - val_loss: 14.1738\n",
            "Epoch 34/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 105.4235 - val_loss: 10.9185\n",
            "Epoch 35/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 100.0774 - val_loss: 9.8589\n",
            "Epoch 36/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 82.3268 - val_loss: 9.5093\n",
            "Epoch 37/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 79.5584 - val_loss: 8.1722\n",
            "Epoch 38/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 85.7351 - val_loss: 7.0268\n",
            "Epoch 39/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 78.9545 - val_loss: 6.7515\n",
            "Epoch 40/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 72.7126 - val_loss: 6.4814\n",
            "Epoch 41/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 68.3195 - val_loss: 5.3980\n",
            "Epoch 42/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 63.7304 - val_loss: 4.9132\n",
            "Epoch 43/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 67.6775 - val_loss: 4.5536\n",
            "Epoch 44/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 65.5053 - val_loss: 3.6054\n",
            "Epoch 45/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 55.8457 - val_loss: 3.6143\n",
            "Epoch 46/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 54.6589 - val_loss: 3.1299\n",
            "Epoch 47/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 55.0574 - val_loss: 3.0746\n",
            "Epoch 48/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 49.6876 - val_loss: 2.8404\n",
            "Epoch 49/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 49.9016 - val_loss: 2.5131\n",
            "Epoch 50/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 46.0900 - val_loss: 2.3497\n",
            "Epoch 51/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 48.2094 - val_loss: 2.2731\n",
            "Epoch 52/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 43.7544 - val_loss: 2.1138\n",
            "Epoch 53/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 45.0711 - val_loss: 1.9832\n",
            "Epoch 54/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 39.3775 - val_loss: 2.0089\n",
            "Epoch 55/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 39.5359 - val_loss: 2.0300\n",
            "Epoch 56/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 38.5648 - val_loss: 2.1452\n",
            "Epoch 57/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 38.3019 - val_loss: 2.0349\n",
            "Epoch 58/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 35.0393 - val_loss: 2.1713\n",
            "Epoch 59/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 32.2868 - val_loss: 2.1698\n",
            "Epoch 60/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 32.6793 - val_loss: 2.4896\n",
            "Epoch 61/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 30.6124 - val_loss: 3.2593\n",
            "Epoch 62/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 31.6618 - val_loss: 2.6052\n",
            "Epoch 63/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 38.7120 - val_loss: 3.0119\n",
            "Epoch 64/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 26.1479 - val_loss: 3.5091\n",
            "Epoch 65/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 27.8149 - val_loss: 3.4119\n",
            "Epoch 66/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 24.8338 - val_loss: 3.5087\n",
            "Epoch 67/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 24.6581 - val_loss: 4.4005\n",
            "Epoch 68/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 24.9674 - val_loss: 5.6412\n",
            "Epoch 69/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 22.4795 - val_loss: 4.7013\n",
            "Epoch 70/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 21.5441 - val_loss: 6.0357\n",
            "Epoch 71/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 21.8049 - val_loss: 5.8549\n",
            "Epoch 72/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 19.5893 - val_loss: 6.1491\n",
            "Epoch 73/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 18.7409 - val_loss: 6.9994\n",
            "Epoch 74/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 18.6964 - val_loss: 7.2863\n",
            "Epoch 75/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 17.3693 - val_loss: 7.2855\n",
            "Epoch 76/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 15.5684 - val_loss: 7.6028\n",
            "Epoch 77/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 16.5326 - val_loss: 8.2191\n",
            "Epoch 78/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 15.4314 - val_loss: 8.6252\n",
            "Epoch 79/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 13.7281 - val_loss: 9.6316\n",
            "Epoch 80/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 13.6855 - val_loss: 9.9808\n",
            "Epoch 81/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 13.9411 - val_loss: 9.7487\n",
            "Epoch 82/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 12.8791 - val_loss: 11.7060\n",
            "Epoch 83/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 12.5413 - val_loss: 10.9261\n",
            "Epoch 84/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 12.7851 - val_loss: 13.5985\n",
            "Epoch 85/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 10.5832 - val_loss: 13.6507\n",
            "Epoch 86/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 11.4575 - val_loss: 14.9975\n",
            "Epoch 87/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 11.0371 - val_loss: 12.9693\n",
            "Epoch 88/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 10.2114 - val_loss: 17.7085\n",
            "Epoch 89/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.5094 - val_loss: 16.2556\n",
            "Epoch 90/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.1401 - val_loss: 14.9199\n",
            "Epoch 91/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.3095 - val_loss: 17.8219\n",
            "Epoch 92/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 9.8427 - val_loss: 17.0577\n",
            "Epoch 93/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.9262 - val_loss: 17.9049\n",
            "Epoch 94/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.4786 - val_loss: 18.3885\n",
            "Epoch 95/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 7.7162 - val_loss: 16.3015\n",
            "Epoch 96/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.2601 - val_loss: 22.4208\n",
            "Epoch 97/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 7.1363 - val_loss: 19.9281\n",
            "Epoch 98/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 7.6314 - val_loss: 23.5518\n",
            "Epoch 99/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.2896 - val_loss: 19.6460\n",
            "Epoch 100/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 7.1140 - val_loss: 22.9571\n",
            "Epoch 101/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 6.4936 - val_loss: 22.0239\n",
            "Epoch 102/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 6.3692 - val_loss: 22.0438\n",
            "Epoch 103/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 6.5120 - val_loss: 23.2889\n",
            "Epoch 104/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 5.9232 - val_loss: 24.4258\n",
            "Epoch 105/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 5.5473 - val_loss: 24.4134\n",
            "Epoch 106/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 5.3772 - val_loss: 26.1375\n",
            "Epoch 107/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 5.5123 - val_loss: 29.4143\n",
            "Epoch 108/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 5.4312 - val_loss: 26.5789\n",
            "Epoch 109/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 5.4106 - val_loss: 30.6962\n",
            "Epoch 110/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 4.9762 - val_loss: 29.0270\n",
            "Epoch 111/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 4.3819 - val_loss: 32.0808\n",
            "Epoch 112/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 4.7128 - val_loss: 30.7278\n",
            "Epoch 113/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 4.8514 - val_loss: 29.3205\n",
            "Epoch 114/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 4.1946 - val_loss: 30.2933\n",
            "Epoch 115/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 5.2786 - val_loss: 32.4821\n",
            "Epoch 116/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 4.2881 - val_loss: 29.6690\n",
            "Epoch 117/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 4.2374 - val_loss: 28.7502\n",
            "Epoch 118/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.7453 - val_loss: 32.7179\n",
            "Epoch 119/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.8172 - val_loss: 37.3033\n",
            "Epoch 120/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.8897 - val_loss: 31.3092\n",
            "Epoch 121/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.9818 - val_loss: 34.1633\n",
            "Epoch 122/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.8894 - val_loss: 37.7345\n",
            "Epoch 123/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.3693 - val_loss: 39.3879\n",
            "Epoch 124/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.1569 - val_loss: 38.1037\n",
            "Epoch 125/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.3277 - val_loss: 38.0790\n",
            "Epoch 126/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.1431 - val_loss: 36.9593\n",
            "Epoch 127/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.2373 - val_loss: 35.7416\n",
            "Epoch 128/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.2076 - val_loss: 36.5073\n",
            "Epoch 129/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.7579 - val_loss: 34.5774\n",
            "Epoch 130/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.1321 - val_loss: 37.9538\n",
            "Epoch 131/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.8031 - val_loss: 44.5779\n",
            "Epoch 132/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.8955 - val_loss: 37.4184\n",
            "Epoch 133/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.7786 - val_loss: 43.9614\n",
            "Epoch 134/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.6575 - val_loss: 42.0277\n",
            "Epoch 135/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.7488 - val_loss: 49.1912\n",
            "Epoch 136/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.8800 - val_loss: 41.1992\n",
            "Epoch 137/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.5592 - val_loss: 42.3984\n",
            "Epoch 138/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.8432 - val_loss: 44.3193\n",
            "Epoch 139/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.5704 - val_loss: 46.3310\n",
            "Epoch 140/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.3874 - val_loss: 38.0779\n",
            "Epoch 141/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.6240 - val_loss: 49.2638\n",
            "Epoch 142/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.4505 - val_loss: 42.7221\n",
            "Epoch 143/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.2691 - val_loss: 43.2075\n",
            "Epoch 144/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.5540 - val_loss: 47.9426\n",
            "Epoch 145/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.1960 - val_loss: 53.7518\n",
            "Epoch 146/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.4397 - val_loss: 48.3879\n",
            "Epoch 147/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.3015 - val_loss: 48.2413\n",
            "Epoch 148/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.1844 - val_loss: 48.7297\n",
            "Epoch 149/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.3332 - val_loss: 53.9829\n",
            "Epoch 150/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.2485 - val_loss: 52.2509\n",
            "Epoch 151/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.0879 - val_loss: 45.0650\n",
            "Epoch 152/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.3726 - val_loss: 56.8987\n",
            "Epoch 153/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.2176 - val_loss: 48.3586\n",
            "Epoch 154/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.2735 - val_loss: 46.2051\n",
            "Epoch 155/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.2047 - val_loss: 46.6114\n",
            "Epoch 156/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.0857 - val_loss: 45.7114\n",
            "Epoch 157/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.0111 - val_loss: 49.5965\n",
            "Epoch 158/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.0543 - val_loss: 43.0032\n",
            "Epoch 159/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.9975 - val_loss: 52.6108\n",
            "Epoch 160/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.9136 - val_loss: 52.6746\n",
            "Epoch 161/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.0392 - val_loss: 59.9746\n",
            "Epoch 162/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.9097 - val_loss: 53.8891\n",
            "Epoch 163/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.9961 - val_loss: 42.8774\n",
            "Epoch 164/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.9734 - val_loss: 57.2442\n",
            "Epoch 165/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.0355 - val_loss: 45.2677\n",
            "Epoch 166/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.9940 - val_loss: 51.9291\n",
            "Epoch 167/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.8959 - val_loss: 48.5838\n",
            "Epoch 168/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.8955 - val_loss: 53.8316\n",
            "Epoch 169/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.9513 - val_loss: 48.4318\n",
            "Epoch 170/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.1227 - val_loss: 46.2627\n",
            "Epoch 171/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.9195 - val_loss: 43.8525\n",
            "Epoch 172/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.8311 - val_loss: 49.3675\n",
            "Epoch 173/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7811 - val_loss: 53.1520\n",
            "Epoch 174/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.9033 - val_loss: 49.8858\n",
            "Epoch 175/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.8572 - val_loss: 50.5604\n",
            "Epoch 176/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7564 - val_loss: 55.8503\n",
            "Epoch 177/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.8381 - val_loss: 59.8521\n",
            "Epoch 178/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.9089 - val_loss: 55.8255\n",
            "Epoch 179/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.7383 - val_loss: 55.5544\n",
            "Epoch 180/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.8375 - val_loss: 58.3190\n",
            "Epoch 181/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6971 - val_loss: 49.0991\n",
            "Epoch 182/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.7375 - val_loss: 54.0128\n",
            "Epoch 183/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.8255 - val_loss: 50.8244\n",
            "Epoch 184/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.7387 - val_loss: 55.8795\n",
            "Epoch 185/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.8115 - val_loss: 54.5058\n",
            "Epoch 186/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7947 - val_loss: 56.6290\n",
            "Epoch 187/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7990 - val_loss: 51.1275\n",
            "Epoch 188/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6576 - val_loss: 55.6511\n",
            "Epoch 189/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7489 - val_loss: 53.5253\n",
            "Epoch 190/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7052 - val_loss: 49.9258\n",
            "Epoch 191/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.8808 - val_loss: 54.3221\n",
            "Epoch 192/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6359 - val_loss: 50.9505\n",
            "Epoch 193/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6768 - val_loss: 54.2838\n",
            "Epoch 194/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.6246 - val_loss: 47.7498\n",
            "Epoch 195/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7552 - val_loss: 47.3010\n",
            "Epoch 196/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6846 - val_loss: 50.3452\n",
            "Epoch 197/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6511 - val_loss: 53.5330\n",
            "Epoch 198/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7499 - val_loss: 46.6233\n",
            "Epoch 199/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6262 - val_loss: 54.9552\n",
            "Epoch 200/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.8022 - val_loss: 49.9998\n",
            "Epoch 201/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6369 - val_loss: 51.0276\n",
            "Epoch 202/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6544 - val_loss: 42.0625\n",
            "Epoch 203/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6958 - val_loss: 52.9647\n",
            "Epoch 204/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6478 - val_loss: 49.0520\n",
            "Epoch 205/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6929 - val_loss: 50.9639\n",
            "Epoch 206/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6771 - val_loss: 49.0760\n",
            "Epoch 207/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6634 - val_loss: 52.8220\n",
            "Epoch 208/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6755 - val_loss: 49.9887\n",
            "Epoch 209/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6435 - val_loss: 48.6862\n",
            "Epoch 210/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6028 - val_loss: 50.5799\n",
            "Epoch 211/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6314 - val_loss: 49.8526\n",
            "Epoch 212/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6051 - val_loss: 55.6098\n",
            "Epoch 213/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6344 - val_loss: 49.4029\n",
            "Epoch 214/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6809 - val_loss: 47.2634\n",
            "Epoch 215/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.5887 - val_loss: 48.5014\n",
            "Epoch 216/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6402 - val_loss: 52.5824\n",
            "Epoch 217/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6542 - val_loss: 47.6270\n",
            "Epoch 218/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6133 - val_loss: 42.9422\n",
            "Epoch 219/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6458 - val_loss: 48.0227\n",
            "Epoch 220/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6409 - val_loss: 50.6843\n",
            "Epoch 221/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6021 - val_loss: 50.7388\n",
            "Epoch 222/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6001 - val_loss: 54.6149\n",
            "Epoch 223/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6007 - val_loss: 49.2166\n",
            "Epoch 224/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5940 - val_loss: 41.0592\n",
            "Epoch 225/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7427 - val_loss: 51.2846\n",
            "Epoch 226/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5695 - val_loss: 47.5573\n",
            "Epoch 227/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5971 - val_loss: 47.5999\n",
            "Epoch 228/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6449 - val_loss: 48.6231\n",
            "Epoch 229/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7285 - val_loss: 43.2586\n",
            "Epoch 230/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6591 - val_loss: 53.6301\n",
            "Epoch 231/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5624 - val_loss: 54.9044\n",
            "Epoch 232/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6483 - val_loss: 47.2963\n",
            "Epoch 233/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5608 - val_loss: 48.5848\n",
            "Epoch 234/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5747 - val_loss: 40.3447\n",
            "Epoch 235/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6431 - val_loss: 48.9232\n",
            "Epoch 236/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5706 - val_loss: 50.1981\n",
            "Epoch 237/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5187 - val_loss: 52.5587\n",
            "Epoch 238/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6375 - val_loss: 44.6051\n",
            "Epoch 239/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.6119 - val_loss: 48.9465\n",
            "Epoch 240/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6636 - val_loss: 52.4156\n",
            "Epoch 241/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5904 - val_loss: 54.9884\n",
            "Epoch 242/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5775 - val_loss: 51.8397\n",
            "Epoch 243/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6265 - val_loss: 41.8013\n",
            "Epoch 244/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5540 - val_loss: 47.5576\n",
            "Epoch 245/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5755 - val_loss: 46.2088\n",
            "Epoch 246/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5823 - val_loss: 47.2549\n",
            "Epoch 247/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5080 - val_loss: 41.8067\n",
            "Epoch 248/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5580 - val_loss: 46.9765\n",
            "Epoch 249/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5072 - val_loss: 48.5002\n",
            "Epoch 250/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5376 - val_loss: 45.5942\n",
            "Epoch 251/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5959 - val_loss: 47.2487\n",
            "Epoch 252/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5808 - val_loss: 46.4823\n",
            "Epoch 253/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6423 - val_loss: 47.3768\n",
            "Epoch 254/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.5982 - val_loss: 57.8761\n",
            "Epoch 255/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.5973 - val_loss: 50.3547\n",
            "Epoch 256/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4936 - val_loss: 53.1902\n",
            "Epoch 257/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4980 - val_loss: 48.8553\n",
            "Epoch 258/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4883 - val_loss: 41.4235\n",
            "Epoch 259/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.5057 - val_loss: 41.3833\n",
            "Epoch 260/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5198 - val_loss: 51.6642\n",
            "Epoch 261/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.5784 - val_loss: 48.1556\n",
            "Epoch 262/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5816 - val_loss: 45.5800\n",
            "Epoch 263/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5702 - val_loss: 42.2083\n",
            "Epoch 264/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5169 - val_loss: 42.6499\n",
            "Epoch 265/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5994 - val_loss: 45.7380\n",
            "Epoch 266/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5126 - val_loss: 49.0882\n",
            "Epoch 267/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5116 - val_loss: 46.9627\n",
            "Epoch 268/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5713 - val_loss: 48.8058\n",
            "Epoch 269/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.6003 - val_loss: 41.5299\n",
            "Epoch 270/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.4800 - val_loss: 43.8021\n",
            "Epoch 271/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5239 - val_loss: 43.5657\n",
            "Epoch 272/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5324 - val_loss: 47.4184\n",
            "Epoch 273/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5397 - val_loss: 46.7731\n",
            "Epoch 274/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5152 - val_loss: 44.7503\n",
            "Epoch 275/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5553 - val_loss: 46.4224\n",
            "Epoch 276/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.5234 - val_loss: 38.8657\n",
            "Epoch 277/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5230 - val_loss: 41.1571\n",
            "Epoch 278/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5408 - val_loss: 46.3527\n",
            "Epoch 279/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5207 - val_loss: 45.7006\n",
            "Epoch 280/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4748 - val_loss: 46.0556\n",
            "Epoch 281/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4995 - val_loss: 46.5071\n",
            "Epoch 282/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4906 - val_loss: 44.5841\n",
            "Epoch 283/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5410 - val_loss: 43.3424\n",
            "Epoch 284/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4627 - val_loss: 41.6187\n",
            "Epoch 285/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5795 - val_loss: 48.0821\n",
            "Epoch 286/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5295 - val_loss: 42.0095\n",
            "Epoch 287/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4435 - val_loss: 47.9947\n",
            "Epoch 288/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5120 - val_loss: 41.8529\n",
            "Epoch 289/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5136 - val_loss: 46.0482\n",
            "Epoch 290/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5165 - val_loss: 43.1627\n",
            "Epoch 291/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5027 - val_loss: 44.0850\n",
            "Epoch 292/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5754 - val_loss: 41.9798\n",
            "Epoch 293/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4739 - val_loss: 47.4467\n",
            "Epoch 294/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4778 - val_loss: 47.1438\n",
            "Epoch 295/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4877 - val_loss: 43.7868\n",
            "Epoch 296/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5174 - val_loss: 41.0132\n",
            "Epoch 297/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4925 - val_loss: 39.4702\n",
            "Epoch 298/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4288 - val_loss: 43.5333\n",
            "Epoch 299/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5035 - val_loss: 46.1505\n",
            "Epoch 300/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4317 - val_loss: 44.4439\n",
            "Epoch 301/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5132 - val_loss: 42.3579\n",
            "Epoch 302/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4843 - val_loss: 41.2714\n",
            "Epoch 303/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5222 - val_loss: 45.8100\n",
            "Epoch 304/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4783 - val_loss: 43.5302\n",
            "Epoch 305/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4637 - val_loss: 38.5870\n",
            "Epoch 306/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.4294 - val_loss: 45.8682\n",
            "Epoch 307/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4472 - val_loss: 47.2425\n",
            "Epoch 308/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4541 - val_loss: 39.5302\n",
            "Epoch 309/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4518 - val_loss: 43.5533\n",
            "Epoch 310/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4839 - val_loss: 42.8079\n",
            "Epoch 311/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4533 - val_loss: 42.0202\n",
            "Epoch 312/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5212 - val_loss: 42.9342\n",
            "Epoch 313/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4111 - val_loss: 39.1469\n",
            "Epoch 314/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.4328 - val_loss: 48.2057\n",
            "Epoch 315/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4953 - val_loss: 43.1862\n",
            "Epoch 316/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5299 - val_loss: 47.1594\n",
            "Epoch 317/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4758 - val_loss: 43.3269\n",
            "Epoch 318/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4919 - val_loss: 46.0939\n",
            "Epoch 319/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3997 - val_loss: 40.6878\n",
            "Epoch 320/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4545 - val_loss: 44.8537\n",
            "Epoch 321/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3825 - val_loss: 43.7082\n",
            "Epoch 322/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4362 - val_loss: 42.7636\n",
            "Epoch 323/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4471 - val_loss: 40.9374\n",
            "Epoch 324/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4381 - val_loss: 44.3822\n",
            "Epoch 325/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4412 - val_loss: 44.3895\n",
            "Epoch 326/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4632 - val_loss: 44.3691\n",
            "Epoch 327/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4517 - val_loss: 45.7043\n",
            "Epoch 328/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4627 - val_loss: 42.2786\n",
            "Epoch 329/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4708 - val_loss: 44.5945\n",
            "Epoch 330/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4476 - val_loss: 44.5460\n",
            "Epoch 331/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4132 - val_loss: 40.4570\n",
            "Epoch 332/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4343 - val_loss: 38.5920\n",
            "Epoch 333/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4086 - val_loss: 37.9585\n",
            "Epoch 334/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4181 - val_loss: 45.7464\n",
            "Epoch 335/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4467 - val_loss: 45.9008\n",
            "Epoch 336/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4329 - val_loss: 44.1578\n",
            "Epoch 337/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.3800 - val_loss: 37.1044\n",
            "Epoch 338/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.4104 - val_loss: 44.6545\n",
            "Epoch 339/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.4435 - val_loss: 49.6684\n",
            "Epoch 340/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4190 - val_loss: 47.7671\n",
            "Epoch 341/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.4054 - val_loss: 47.4994\n",
            "Epoch 342/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.5076 - val_loss: 40.5948\n",
            "Epoch 343/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.3599 - val_loss: 42.1752\n",
            "Epoch 344/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.4489 - val_loss: 40.2179\n",
            "Epoch 345/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3843 - val_loss: 44.7634\n",
            "Epoch 346/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4156 - val_loss: 50.3030\n",
            "Epoch 347/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4432 - val_loss: 41.9920\n",
            "Epoch 348/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4436 - val_loss: 48.8865\n",
            "Epoch 349/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3689 - val_loss: 41.1082\n",
            "Epoch 350/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4066 - val_loss: 40.8508\n",
            "Epoch 351/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4131 - val_loss: 47.3397\n",
            "Epoch 352/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4409 - val_loss: 40.5852\n",
            "Epoch 353/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3770 - val_loss: 34.4345\n",
            "Epoch 354/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3888 - val_loss: 43.5754\n",
            "Epoch 355/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4050 - val_loss: 41.7299\n",
            "Epoch 356/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3602 - val_loss: 43.0252\n",
            "Epoch 357/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3810 - val_loss: 41.0710\n",
            "Epoch 358/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3961 - val_loss: 50.7085\n",
            "Epoch 359/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3501 - val_loss: 41.5034\n",
            "Epoch 360/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3798 - val_loss: 41.6050\n",
            "Epoch 361/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3665 - val_loss: 39.4505\n",
            "Epoch 362/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3641 - val_loss: 43.2975\n",
            "Epoch 363/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4237 - val_loss: 38.9516\n",
            "Epoch 364/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.3685 - val_loss: 46.1795\n",
            "Epoch 365/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3434 - val_loss: 41.6455\n",
            "Epoch 366/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3516 - val_loss: 41.3567\n",
            "Epoch 367/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3890 - val_loss: 48.4335\n",
            "Epoch 368/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4029 - val_loss: 48.4912\n",
            "Epoch 369/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.3798 - val_loss: 37.8566\n",
            "Epoch 370/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3813 - val_loss: 41.9578\n",
            "Epoch 371/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3540 - val_loss: 40.4781\n",
            "Epoch 372/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3840 - val_loss: 55.7131\n",
            "Epoch 373/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3718 - val_loss: 46.0724\n",
            "Epoch 374/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3101 - val_loss: 49.5056\n",
            "Epoch 375/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.3993 - val_loss: 44.0784\n",
            "Epoch 376/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.3263 - val_loss: 49.8805\n",
            "Epoch 377/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.3412 - val_loss: 43.3660\n",
            "Epoch 378/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3314 - val_loss: 42.4646\n",
            "Epoch 379/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2727 - val_loss: 40.0234\n",
            "Epoch 380/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4068 - val_loss: 38.7783\n",
            "Epoch 381/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2756 - val_loss: 51.5386\n",
            "Epoch 382/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2923 - val_loss: 40.0593\n",
            "Epoch 383/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3411 - val_loss: 49.0428\n",
            "Epoch 384/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3023 - val_loss: 39.7801\n",
            "Epoch 385/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3147 - val_loss: 40.8718\n",
            "Epoch 386/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3231 - val_loss: 45.6675\n",
            "Epoch 387/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3266 - val_loss: 48.4437\n",
            "Epoch 388/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3563 - val_loss: 42.6007\n",
            "Epoch 389/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3306 - val_loss: 40.4274\n",
            "Epoch 390/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2547 - val_loss: 46.4755\n",
            "Epoch 391/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3901 - val_loss: 45.4298\n",
            "Epoch 392/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3321 - val_loss: 55.3398\n",
            "Epoch 393/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.3535 - val_loss: 46.8176\n",
            "Epoch 394/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2988 - val_loss: 44.6977\n",
            "Epoch 395/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2500 - val_loss: 46.5472\n",
            "Epoch 396/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.3322 - val_loss: 49.7040\n",
            "Epoch 397/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2916 - val_loss: 47.9436\n",
            "Epoch 398/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3516 - val_loss: 49.7707\n",
            "Epoch 399/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2883 - val_loss: 46.0629\n",
            "Epoch 400/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2958 - val_loss: 46.7881\n",
            "Epoch 401/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3163 - val_loss: 42.2528\n",
            "Epoch 402/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2742 - val_loss: 43.7735\n",
            "Epoch 403/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.2586 - val_loss: 47.1569\n",
            "Epoch 404/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2936 - val_loss: 47.2014\n",
            "Epoch 405/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2922 - val_loss: 41.4423\n",
            "Epoch 406/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3249 - val_loss: 47.1678\n",
            "Epoch 407/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.3304 - val_loss: 43.7486\n",
            "Epoch 408/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.2564 - val_loss: 48.8036\n",
            "Epoch 409/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.3176 - val_loss: 51.9506\n",
            "Epoch 410/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.2793 - val_loss: 43.7251\n",
            "Epoch 411/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3302 - val_loss: 48.3859\n",
            "Epoch 412/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2991 - val_loss: 47.7536\n",
            "Epoch 413/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2179 - val_loss: 43.8800\n",
            "Epoch 414/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2978 - val_loss: 47.3363\n",
            "Epoch 415/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.3028 - val_loss: 47.8453\n",
            "Epoch 416/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2570 - val_loss: 57.5275\n",
            "Epoch 417/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2935 - val_loss: 46.7687\n",
            "Epoch 418/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2542 - val_loss: 48.8553\n",
            "Epoch 419/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2641 - val_loss: 46.0708\n",
            "Epoch 420/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2819 - val_loss: 43.0389\n",
            "Epoch 421/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2685 - val_loss: 45.3808\n",
            "Epoch 422/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2468 - val_loss: 56.3191\n",
            "Epoch 423/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2692 - val_loss: 52.9447\n",
            "Epoch 424/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2168 - val_loss: 47.6970\n",
            "Epoch 425/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1923 - val_loss: 44.0588\n",
            "Epoch 426/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2377 - val_loss: 48.4046\n",
            "Epoch 427/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2172 - val_loss: 56.1381\n",
            "Epoch 428/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2272 - val_loss: 48.1384\n",
            "Epoch 429/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2002 - val_loss: 42.9876\n",
            "Epoch 430/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2026 - val_loss: 50.3066\n",
            "Epoch 431/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2593 - val_loss: 46.5122\n",
            "Epoch 432/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2176 - val_loss: 53.2986\n",
            "Epoch 433/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2169 - val_loss: 46.9510\n",
            "Epoch 434/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2376 - val_loss: 49.4334\n",
            "Epoch 435/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2310 - val_loss: 49.7864\n",
            "Epoch 436/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2342 - val_loss: 58.2061\n",
            "Epoch 437/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2438 - val_loss: 46.5783\n",
            "Epoch 438/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1955 - val_loss: 53.2924\n",
            "Epoch 439/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1869 - val_loss: 52.6713\n",
            "Epoch 440/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.2262 - val_loss: 49.5880\n",
            "Epoch 441/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1843 - val_loss: 55.5202\n",
            "Epoch 442/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1719 - val_loss: 47.5931\n",
            "Epoch 443/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1862 - val_loss: 50.3053\n",
            "Epoch 444/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1924 - val_loss: 55.3716\n",
            "Epoch 445/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1533 - val_loss: 48.6193\n",
            "Epoch 446/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1821 - val_loss: 55.6719\n",
            "Epoch 447/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.1937 - val_loss: 53.1887\n",
            "Epoch 448/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.1055 - val_loss: 47.7332\n",
            "Epoch 449/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.1829 - val_loss: 56.8974\n",
            "Epoch 450/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1844 - val_loss: 56.1311\n",
            "Epoch 451/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1413 - val_loss: 57.4319\n",
            "Epoch 452/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1555 - val_loss: 58.6788\n",
            "Epoch 453/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1572 - val_loss: 55.0339\n",
            "Epoch 454/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1739 - val_loss: 56.2966\n",
            "Epoch 455/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1231 - val_loss: 59.2029\n",
            "Epoch 456/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1397 - val_loss: 54.6605\n",
            "Epoch 457/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1599 - val_loss: 56.7831\n",
            "Epoch 458/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1043 - val_loss: 57.3875\n",
            "Epoch 459/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1745 - val_loss: 59.8677\n",
            "Epoch 460/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1077 - val_loss: 65.4570\n",
            "Epoch 461/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1007 - val_loss: 58.3952\n",
            "Epoch 462/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1042 - val_loss: 56.7769\n",
            "Epoch 463/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.0863 - val_loss: 67.4428\n",
            "Epoch 464/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1309 - val_loss: 61.2012\n",
            "Epoch 465/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1014 - val_loss: 56.3900\n",
            "Epoch 466/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0725 - val_loss: 63.9846\n",
            "Epoch 467/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1001 - val_loss: 66.6973\n",
            "Epoch 468/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0231 - val_loss: 65.1414\n",
            "Epoch 469/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0693 - val_loss: 68.4676\n",
            "Epoch 470/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1194 - val_loss: 64.6850\n",
            "Epoch 471/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0533 - val_loss: 68.6059\n",
            "Epoch 472/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0608 - val_loss: 71.7048\n",
            "Epoch 473/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0481 - val_loss: 68.8133\n",
            "Epoch 474/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.0228 - val_loss: 77.6649\n",
            "Epoch 475/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.1274 - val_loss: 75.0905\n",
            "Epoch 476/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0403 - val_loss: 62.4210\n",
            "Epoch 477/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0810 - val_loss: 75.7130\n",
            "Epoch 478/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.0195 - val_loss: 68.5878\n",
            "Epoch 479/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0375 - val_loss: 71.1837\n",
            "Epoch 480/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0340 - val_loss: 83.2191\n",
            "Epoch 481/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0178 - val_loss: 90.1080\n",
            "Epoch 482/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9622 - val_loss: 75.3604\n",
            "Epoch 483/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0372 - val_loss: 76.8498\n",
            "Epoch 484/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0445 - val_loss: 75.6957\n",
            "Epoch 485/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.0120 - val_loss: 90.7789\n",
            "Epoch 486/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9942 - val_loss: 91.1698\n",
            "Epoch 487/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.9593 - val_loss: 84.1373\n",
            "Epoch 488/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9055 - val_loss: 86.1889\n",
            "Epoch 489/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9832 - val_loss: 88.7746\n",
            "Epoch 490/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9421 - val_loss: 88.3260\n",
            "Epoch 491/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.9601 - val_loss: 94.0510\n",
            "Epoch 492/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9379 - val_loss: 95.3855\n",
            "Epoch 493/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8969 - val_loss: 97.9418\n",
            "Epoch 494/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9239 - val_loss: 87.2966\n",
            "Epoch 495/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9374 - val_loss: 97.3759\n",
            "Epoch 496/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9512 - val_loss: 96.6647\n",
            "Epoch 497/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9269 - val_loss: 104.0792\n",
            "Epoch 498/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9112 - val_loss: 120.8355\n",
            "Epoch 499/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.8644 - val_loss: 109.0677\n",
            "Epoch 500/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9171 - val_loss: 110.2593\n",
            "Epoch 501/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.8672 - val_loss: 120.2443\n",
            "Epoch 502/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8957 - val_loss: 107.8813\n",
            "Epoch 503/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8374 - val_loss: 118.8556\n",
            "Epoch 504/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8596 - val_loss: 130.3851\n",
            "Epoch 505/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7843 - val_loss: 126.0039\n",
            "Epoch 506/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7817 - val_loss: 145.3923\n",
            "Epoch 507/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7900 - val_loss: 126.0989\n",
            "Epoch 508/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7880 - val_loss: 159.2199\n",
            "Epoch 509/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7589 - val_loss: 133.2358\n",
            "Epoch 510/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7922 - val_loss: 162.1561\n",
            "Epoch 511/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7531 - val_loss: 161.7809\n",
            "Epoch 512/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8014 - val_loss: 162.2540\n",
            "Epoch 513/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7789 - val_loss: 171.1901\n",
            "Epoch 514/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7491 - val_loss: 166.5345\n",
            "Epoch 515/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8094 - val_loss: 180.6748\n",
            "Epoch 516/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7051 - val_loss: 185.6929\n",
            "Epoch 517/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7538 - val_loss: 196.6345\n",
            "Epoch 518/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7438 - val_loss: 211.1783\n",
            "Epoch 519/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6685 - val_loss: 249.9107\n",
            "Epoch 520/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6982 - val_loss: 272.0444\n",
            "Epoch 521/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7225 - val_loss: 254.7171\n",
            "Epoch 522/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7004 - val_loss: 288.6989\n",
            "Epoch 523/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6882 - val_loss: 282.0915\n",
            "Epoch 524/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6943 - val_loss: 279.5871\n",
            "Epoch 525/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6754 - val_loss: 285.2125\n",
            "Epoch 526/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6728 - val_loss: 341.7017\n",
            "Epoch 527/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7107 - val_loss: 370.8989\n",
            "Epoch 528/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.6834 - val_loss: 375.8567\n",
            "Epoch 529/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.6484 - val_loss: 398.5833\n",
            "Epoch 530/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.6574 - val_loss: 382.0052\n",
            "Epoch 531/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5571 - val_loss: 360.3330\n",
            "Epoch 532/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.6377 - val_loss: 355.8648\n",
            "Epoch 533/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7204 - val_loss: 417.1367\n",
            "Epoch 534/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6563 - val_loss: 492.0623\n",
            "Epoch 535/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5788 - val_loss: 520.0733\n",
            "Epoch 536/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5987 - val_loss: 472.1172\n",
            "Epoch 537/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5788 - val_loss: 550.2684\n",
            "Epoch 538/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5729 - val_loss: 530.2908\n",
            "Epoch 539/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.6434 - val_loss: 574.2127\n",
            "Epoch 540/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6191 - val_loss: 561.9033\n",
            "Epoch 541/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5873 - val_loss: 577.1890\n",
            "Epoch 542/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5321 - val_loss: 584.6547\n",
            "Epoch 543/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5486 - val_loss: 685.3234\n",
            "Epoch 544/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5608 - val_loss: 680.0459\n",
            "Epoch 545/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5119 - val_loss: 787.4642\n",
            "Epoch 546/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5686 - val_loss: 784.0144\n",
            "Epoch 547/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5380 - val_loss: 985.6366\n",
            "Epoch 548/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5557 - val_loss: 750.0048\n",
            "Epoch 549/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4997 - val_loss: 810.4017\n",
            "Epoch 550/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5344 - val_loss: 862.1734\n",
            "Epoch 551/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5750 - val_loss: 1121.5389\n",
            "Epoch 552/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5061 - val_loss: 1228.3971\n",
            "Epoch 553/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4968 - val_loss: 1224.3706\n",
            "Epoch 554/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4901 - val_loss: 1163.4034\n",
            "Epoch 555/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4548 - val_loss: 1351.7155\n",
            "Epoch 556/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4860 - val_loss: 1432.7339\n",
            "Epoch 557/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5214 - val_loss: 1428.6600\n",
            "Epoch 558/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5236 - val_loss: 1409.4833\n",
            "Epoch 559/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4541 - val_loss: 1458.7585\n",
            "Epoch 560/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5133 - val_loss: 1408.0847\n",
            "Epoch 561/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4886 - val_loss: 1304.5692\n",
            "Epoch 562/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4981 - val_loss: 1638.4531\n",
            "Epoch 563/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4973 - val_loss: 1889.2483\n",
            "Epoch 564/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4892 - val_loss: 2100.2910\n",
            "Epoch 565/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4884 - val_loss: 1787.3417\n",
            "Epoch 566/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4781 - val_loss: 1590.5754\n",
            "Epoch 567/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4870 - val_loss: 2279.0894\n",
            "Epoch 568/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4905 - val_loss: 3275.7832\n",
            "Epoch 569/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4747 - val_loss: 1965.8826\n",
            "Epoch 570/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4686 - val_loss: 2436.1350\n",
            "Epoch 571/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4480 - val_loss: 1882.0314\n",
            "Epoch 572/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4705 - val_loss: 2353.8452\n",
            "Epoch 573/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4330 - val_loss: 2419.4902\n",
            "Epoch 574/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5015 - val_loss: 3212.4939\n",
            "Epoch 575/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4810 - val_loss: 2405.4021\n",
            "Epoch 576/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4496 - val_loss: 3180.6475\n",
            "Epoch 577/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4949 - val_loss: 2600.1985\n",
            "Epoch 578/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4677 - val_loss: 2581.5513\n",
            "Epoch 579/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5048 - val_loss: 2536.0613\n",
            "Epoch 580/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4221 - val_loss: 3011.3186\n",
            "Epoch 581/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4565 - val_loss: 2628.4446\n",
            "Epoch 582/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4280 - val_loss: 3917.0454\n",
            "Epoch 583/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4135 - val_loss: 3600.9260\n",
            "Epoch 584/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4260 - val_loss: 3784.2476\n",
            "Epoch 585/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4249 - val_loss: 3993.9138\n",
            "Epoch 586/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4563 - val_loss: 3808.4639\n",
            "Epoch 587/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4535 - val_loss: 3684.9714\n",
            "Epoch 588/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4862 - val_loss: 3537.7642\n",
            "Epoch 589/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3774 - val_loss: 3680.2964\n",
            "Epoch 590/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4399 - val_loss: 3500.0933\n",
            "Epoch 591/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4164 - val_loss: 3907.6155\n",
            "Epoch 592/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4979 - val_loss: 4124.3726\n",
            "Epoch 593/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3892 - val_loss: 4414.2744\n",
            "Epoch 594/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4056 - val_loss: 4847.6885\n",
            "Epoch 595/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3864 - val_loss: 4385.4570\n",
            "Epoch 596/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4480 - val_loss: 6195.7188\n",
            "Epoch 597/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4041 - val_loss: 6283.8296\n",
            "Epoch 598/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3889 - val_loss: 6253.6650\n",
            "Epoch 599/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4006 - val_loss: 5236.1772\n",
            "Epoch 600/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3527 - val_loss: 6255.0610\n",
            "Epoch 601/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3776 - val_loss: 6398.4424\n",
            "Epoch 602/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4344 - val_loss: 6353.6235\n",
            "Epoch 603/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3872 - val_loss: 6556.5312\n",
            "Epoch 604/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3674 - val_loss: 7820.2134\n",
            "Epoch 605/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4565 - val_loss: 6241.9443\n",
            "Epoch 606/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3417 - val_loss: 7880.7227\n",
            "Epoch 607/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4500 - val_loss: 8669.3867\n",
            "Epoch 608/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3867 - val_loss: 7657.2490\n",
            "Epoch 609/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3750 - val_loss: 8404.9600\n",
            "Epoch 610/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3518 - val_loss: 9943.2158\n",
            "Epoch 611/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4408 - val_loss: 7692.2900\n",
            "Epoch 612/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4114 - val_loss: 8150.6157\n",
            "Epoch 613/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3692 - val_loss: 9325.1914\n",
            "Epoch 614/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3747 - val_loss: 9156.4678\n",
            "Epoch 615/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3392 - val_loss: 7977.7690\n",
            "Epoch 616/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3281 - val_loss: 9551.4023\n",
            "Epoch 617/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3437 - val_loss: 10646.0664\n",
            "Epoch 618/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3495 - val_loss: 10103.6426\n",
            "Epoch 619/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4520 - val_loss: 10278.5439\n",
            "Epoch 620/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3317 - val_loss: 12163.3057\n",
            "Epoch 621/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3120 - val_loss: 11929.5176\n",
            "Epoch 622/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3380 - val_loss: 12991.8994\n",
            "Epoch 623/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4140 - val_loss: 15243.0400\n",
            "Epoch 624/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3453 - val_loss: 12155.2520\n",
            "Epoch 625/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3873 - val_loss: 12946.7959\n",
            "Epoch 626/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3180 - val_loss: 11995.2764\n",
            "Epoch 627/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3140 - val_loss: 13829.0459\n",
            "Epoch 628/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3347 - val_loss: 16365.3916\n",
            "Epoch 629/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3535 - val_loss: 13693.3057\n",
            "Epoch 630/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4240 - val_loss: 19152.1777\n",
            "Epoch 631/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3817 - val_loss: 12984.8389\n",
            "Epoch 632/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3206 - val_loss: 13511.4062\n",
            "Epoch 633/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3089 - val_loss: 15436.8057\n",
            "Epoch 634/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3284 - val_loss: 14800.0527\n",
            "Epoch 635/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3385 - val_loss: 14351.9004\n",
            "Epoch 636/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3399 - val_loss: 16014.5508\n",
            "Epoch 637/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3019 - val_loss: 13512.0557\n",
            "Epoch 638/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3291 - val_loss: 17543.8789\n",
            "Epoch 639/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3287 - val_loss: 14492.5312\n",
            "Epoch 640/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3193 - val_loss: 18628.5410\n",
            "Epoch 641/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2819 - val_loss: 17395.8008\n",
            "Epoch 642/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3905 - val_loss: 17923.3320\n",
            "Epoch 643/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2972 - val_loss: 18325.3828\n",
            "Epoch 644/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2626 - val_loss: 21805.1484\n",
            "Epoch 645/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3113 - val_loss: 25458.4062\n",
            "Epoch 646/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3285 - val_loss: 22779.6836\n",
            "Epoch 647/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3176 - val_loss: 25786.9277\n",
            "Epoch 648/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2684 - val_loss: 20850.9805\n",
            "Epoch 649/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3745 - val_loss: 19928.8613\n",
            "Epoch 650/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3255 - val_loss: 23505.8535\n",
            "Epoch 651/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3426 - val_loss: 21407.9844\n",
            "Epoch 652/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3681 - val_loss: 26996.9805\n",
            "Epoch 653/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3377 - val_loss: 22961.1758\n",
            "Epoch 654/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3011 - val_loss: 19127.0469\n",
            "Epoch 655/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3196 - val_loss: 21752.5078\n",
            "Epoch 656/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3065 - val_loss: 18788.9863\n",
            "Epoch 657/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3002 - val_loss: 22337.5254\n",
            "Epoch 658/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3067 - val_loss: 23237.5312\n",
            "Epoch 659/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3117 - val_loss: 19879.7539\n",
            "Epoch 660/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2960 - val_loss: 22621.6445\n",
            "Epoch 661/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2785 - val_loss: 25846.3613\n",
            "Epoch 662/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3489 - val_loss: 28809.1562\n",
            "Epoch 663/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3223 - val_loss: 24620.1309\n",
            "Epoch 664/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2788 - val_loss: 24998.6289\n",
            "Epoch 665/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2935 - val_loss: 30476.8789\n",
            "Epoch 666/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2906 - val_loss: 30132.9121\n",
            "Epoch 667/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2965 - val_loss: 28365.1914\n",
            "Epoch 668/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2429 - val_loss: 34089.7305\n",
            "Epoch 669/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2734 - val_loss: 43304.3203\n",
            "Epoch 670/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2873 - val_loss: 42131.4414\n",
            "Epoch 671/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2958 - val_loss: 38362.0625\n",
            "Epoch 672/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2713 - val_loss: 42384.4844\n",
            "Epoch 673/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2768 - val_loss: 38444.9922\n",
            "Epoch 674/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2722 - val_loss: 33664.3320\n",
            "Epoch 675/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2402 - val_loss: 35330.6523\n",
            "Epoch 676/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2267 - val_loss: 44225.4922\n",
            "Epoch 677/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2534 - val_loss: 40128.2617\n",
            "Epoch 678/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2601 - val_loss: 58068.0664\n",
            "Epoch 679/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2549 - val_loss: 44514.1953\n",
            "Epoch 680/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2790 - val_loss: 40275.2930\n",
            "Epoch 681/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3007 - val_loss: 45902.7578\n",
            "Epoch 682/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2274 - val_loss: 38915.5469\n",
            "Epoch 683/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2963 - val_loss: 37416.8984\n",
            "Epoch 684/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2490 - val_loss: 69186.5234\n",
            "Epoch 685/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2689 - val_loss: 44816.8594\n",
            "Epoch 686/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2226 - val_loss: 50854.6875\n",
            "Epoch 687/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2514 - val_loss: 52892.4531\n",
            "Epoch 688/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3080 - val_loss: 40119.5156\n",
            "Epoch 689/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2861 - val_loss: 42713.0820\n",
            "Epoch 690/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2885 - val_loss: 39954.7734\n",
            "Epoch 691/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2759 - val_loss: 43870.6367\n",
            "Epoch 692/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2957 - val_loss: 44321.4453\n",
            "Epoch 693/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2490 - val_loss: 30044.6016\n",
            "Epoch 694/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2683 - val_loss: 37394.9883\n",
            "Epoch 695/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2739 - val_loss: 35857.0469\n",
            "Epoch 696/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2637 - val_loss: 46125.5430\n",
            "Epoch 697/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2725 - val_loss: 42726.8945\n",
            "Epoch 698/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3462 - val_loss: 49126.0742\n",
            "Epoch 699/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2461 - val_loss: 28333.1895\n",
            "Epoch 700/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2849 - val_loss: 41228.9727\n",
            "Epoch 701/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2841 - val_loss: 41736.0625\n",
            "Epoch 702/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2590 - val_loss: 36248.1445\n",
            "Epoch 703/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2091 - val_loss: 38342.1875\n",
            "Epoch 704/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2587 - val_loss: 44753.8398\n",
            "Epoch 705/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2497 - val_loss: 45972.1367\n",
            "Epoch 706/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2952 - val_loss: 55319.3008\n",
            "Epoch 707/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2546 - val_loss: 51288.0000\n",
            "Epoch 708/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2328 - val_loss: 45668.4570\n",
            "Epoch 709/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2384 - val_loss: 39519.2227\n",
            "Epoch 710/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2413 - val_loss: 49650.0273\n",
            "Epoch 711/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2749 - val_loss: 68295.2266\n",
            "Epoch 712/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2731 - val_loss: 48999.3125\n",
            "Epoch 713/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2702 - val_loss: 51629.8398\n",
            "Epoch 714/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2097 - val_loss: 47922.1250\n",
            "Epoch 715/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2435 - val_loss: 48658.5195\n",
            "Epoch 716/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2456 - val_loss: 53961.8867\n",
            "Epoch 717/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2378 - val_loss: 54711.2461\n",
            "Epoch 718/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2301 - val_loss: 44951.3398\n",
            "Epoch 719/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2494 - val_loss: 46635.8789\n",
            "Epoch 720/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2207 - val_loss: 46811.8008\n",
            "Epoch 721/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2341 - val_loss: 52116.9414\n",
            "Epoch 722/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2484 - val_loss: 58407.9570\n",
            "Epoch 723/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2440 - val_loss: 60429.3867\n",
            "Epoch 724/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2676 - val_loss: 60520.7188\n",
            "Epoch 725/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2405 - val_loss: 48625.4766\n",
            "Epoch 726/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2505 - val_loss: 53483.8047\n",
            "Epoch 727/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2517 - val_loss: 49989.5391\n",
            "Epoch 728/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2312 - val_loss: 67795.9297\n",
            "Epoch 729/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2762 - val_loss: 43176.3477\n",
            "Epoch 730/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2795 - val_loss: 55726.5586\n",
            "Epoch 731/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2490 - val_loss: 56777.3945\n",
            "Epoch 732/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2481 - val_loss: 41154.9648\n",
            "Epoch 733/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2113 - val_loss: 51063.3867\n",
            "Epoch 734/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2921 - val_loss: 51632.6367\n",
            "Epoch 735/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2337 - val_loss: 45384.3555\n",
            "Epoch 736/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2900 - val_loss: 56483.3164\n",
            "Epoch 737/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2150 - val_loss: 63622.7891\n",
            "Epoch 738/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2333 - val_loss: 57108.0938\n",
            "Epoch 739/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2498 - val_loss: 61786.8750\n",
            "Epoch 740/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2040 - val_loss: 60586.1875\n",
            "Epoch 741/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2672 - val_loss: 51726.9648\n",
            "Epoch 742/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2063 - val_loss: 60435.1055\n",
            "Epoch 743/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2375 - val_loss: 53073.4336\n",
            "Epoch 744/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2439 - val_loss: 65565.4922\n",
            "Epoch 745/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2018 - val_loss: 68237.2109\n",
            "Epoch 746/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2544 - val_loss: 67296.5078\n",
            "Epoch 747/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2344 - val_loss: 65177.5234\n",
            "Epoch 748/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2117 - val_loss: 47882.2148\n",
            "Epoch 749/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2000 - val_loss: 51605.3555\n",
            "Epoch 750/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2080 - val_loss: 61518.2109\n",
            "Epoch 751/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2169 - val_loss: 75100.3672\n",
            "Epoch 752/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1977 - val_loss: 81421.8125\n",
            "Epoch 753/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2396 - val_loss: 67531.9688\n",
            "Epoch 754/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2237 - val_loss: 61620.5117\n",
            "Epoch 755/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2117 - val_loss: 81847.3438\n",
            "Epoch 756/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2112 - val_loss: 63544.1836\n",
            "Epoch 757/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2467 - val_loss: 62480.9531\n",
            "Epoch 758/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2127 - val_loss: 63844.1875\n",
            "Epoch 759/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2115 - val_loss: 72920.1172\n",
            "Epoch 760/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1862 - val_loss: 87703.3047\n",
            "Epoch 761/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2124 - val_loss: 78311.8750\n",
            "Epoch 762/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2077 - val_loss: 89444.5000\n",
            "Epoch 763/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2450 - val_loss: 83709.9062\n",
            "Epoch 764/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2150 - val_loss: 74994.8047\n",
            "Epoch 765/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1966 - val_loss: 80494.3594\n",
            "Epoch 766/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2349 - val_loss: 98453.1094\n",
            "Epoch 767/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2097 - val_loss: 68634.0547\n",
            "Epoch 768/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1950 - val_loss: 87964.1094\n",
            "Epoch 769/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2305 - val_loss: 87538.4844\n",
            "Epoch 770/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2135 - val_loss: 70441.2344\n",
            "Epoch 771/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2287 - val_loss: 93051.7422\n",
            "Epoch 772/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2158 - val_loss: 71662.5547\n",
            "Epoch 773/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1957 - val_loss: 69545.1641\n",
            "Epoch 774/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1547 - val_loss: 73481.8281\n",
            "Epoch 775/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1872 - val_loss: 80774.7344\n",
            "Epoch 776/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1988 - val_loss: 94102.3516\n",
            "Epoch 777/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1857 - val_loss: 85490.1953\n",
            "Epoch 778/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1972 - val_loss: 79926.6250\n",
            "Epoch 779/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2674 - val_loss: 93335.2500\n",
            "Epoch 780/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2119 - val_loss: 90383.7500\n",
            "Epoch 781/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2239 - val_loss: 74514.4062\n",
            "Epoch 782/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1761 - val_loss: 92514.5859\n",
            "Epoch 783/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2171 - val_loss: 94277.7969\n",
            "Epoch 784/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2057 - val_loss: 84136.0000\n",
            "Epoch 785/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1870 - val_loss: 76003.2031\n",
            "Epoch 786/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1965 - val_loss: 76573.2031\n",
            "Epoch 787/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2279 - val_loss: 95014.6484\n",
            "Epoch 788/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2555 - val_loss: 85947.9688\n",
            "Epoch 789/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1984 - val_loss: 64954.2578\n",
            "Epoch 790/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2017 - val_loss: 74740.8906\n",
            "Epoch 791/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2432 - val_loss: 76140.6328\n",
            "Epoch 792/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2198 - val_loss: 82149.3359\n",
            "Epoch 793/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1905 - val_loss: 71781.2891\n",
            "Epoch 794/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1829 - val_loss: 98867.1172\n",
            "Epoch 795/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2089 - val_loss: 122909.8750\n",
            "Epoch 796/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1809 - val_loss: 69715.0312\n",
            "Epoch 797/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1878 - val_loss: 94713.5625\n",
            "Epoch 798/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2959 - val_loss: 78563.6250\n",
            "Epoch 799/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2214 - val_loss: 52306.8164\n",
            "Epoch 800/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2230 - val_loss: 86714.2734\n",
            "Epoch 801/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1911 - val_loss: 89038.1875\n",
            "Epoch 802/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2174 - val_loss: 67320.2109\n",
            "Epoch 803/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2194 - val_loss: 62860.7148\n",
            "Epoch 804/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2096 - val_loss: 82041.1328\n",
            "Epoch 805/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1967 - val_loss: 87951.1719\n",
            "Epoch 806/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1683 - val_loss: 73990.1016\n",
            "Epoch 807/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1989 - val_loss: 69159.0469\n",
            "Epoch 808/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2114 - val_loss: 80357.2812\n",
            "Epoch 809/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1912 - val_loss: 68354.5469\n",
            "Epoch 810/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1995 - val_loss: 76521.3359\n",
            "Epoch 811/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2088 - val_loss: 71806.0078\n",
            "Epoch 812/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2139 - val_loss: 76046.1094\n",
            "Epoch 813/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1925 - val_loss: 81598.2734\n",
            "Epoch 814/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2104 - val_loss: 92336.3203\n",
            "Epoch 815/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2004 - val_loss: 83202.4375\n",
            "Epoch 816/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1637 - val_loss: 72461.5312\n",
            "Epoch 817/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1906 - val_loss: 80465.9453\n",
            "Epoch 818/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1726 - val_loss: 96725.7422\n",
            "Epoch 819/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2040 - val_loss: 100571.6484\n",
            "Epoch 820/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1825 - val_loss: 100511.7891\n",
            "Epoch 821/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2119 - val_loss: 83930.9297\n",
            "Epoch 822/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2156 - val_loss: 88641.8125\n",
            "Epoch 823/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1840 - val_loss: 82081.4688\n",
            "Epoch 824/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2312 - val_loss: 96075.8281\n",
            "Epoch 825/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2168 - val_loss: 69940.9062\n",
            "Epoch 826/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1887 - val_loss: 66577.0156\n",
            "Epoch 827/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1917 - val_loss: 95419.6953\n",
            "Epoch 828/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2123 - val_loss: 81800.9375\n",
            "Epoch 829/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1793 - val_loss: 74291.8828\n",
            "Epoch 830/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2171 - val_loss: 93050.3672\n",
            "Epoch 831/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1860 - val_loss: 83235.4609\n",
            "Epoch 832/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1637 - val_loss: 157344.9062\n",
            "Epoch 833/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1922 - val_loss: 89863.0078\n",
            "Epoch 834/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2040 - val_loss: 96023.9375\n",
            "Epoch 835/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1877 - val_loss: 114378.2500\n",
            "Epoch 836/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2433 - val_loss: 82943.2578\n",
            "Epoch 837/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1924 - val_loss: 82996.0703\n",
            "Epoch 838/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1890 - val_loss: 88885.8672\n",
            "Epoch 839/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1862 - val_loss: 106565.7656\n",
            "Epoch 840/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1755 - val_loss: 105724.9766\n",
            "Epoch 841/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1824 - val_loss: 109662.8516\n",
            "Epoch 842/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2060 - val_loss: 114430.3672\n",
            "Epoch 843/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2140 - val_loss: 89276.2891\n",
            "Epoch 844/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1797 - val_loss: 74000.7422\n",
            "Epoch 845/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1849 - val_loss: 87339.5625\n",
            "Epoch 846/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1883 - val_loss: 73495.8438\n",
            "Epoch 847/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2094 - val_loss: 62812.1523\n",
            "Epoch 848/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2187 - val_loss: 84402.7891\n",
            "Epoch 849/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2011 - val_loss: 78205.3203\n",
            "Epoch 850/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1960 - val_loss: 78336.3750\n",
            "Epoch 851/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2013 - val_loss: 85973.5703\n",
            "Epoch 852/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1762 - val_loss: 66051.1797\n",
            "Epoch 853/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1847 - val_loss: 75765.0000\n",
            "Epoch 854/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1884 - val_loss: 55405.8672\n",
            "Epoch 855/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1865 - val_loss: 51991.4062\n",
            "Epoch 856/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1914 - val_loss: 71669.4453\n",
            "Epoch 857/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1712 - val_loss: 74471.1094\n",
            "Epoch 858/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1857 - val_loss: 95950.6016\n",
            "Epoch 859/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1765 - val_loss: 87303.9219\n",
            "Epoch 860/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1744 - val_loss: 92150.1250\n",
            "Epoch 861/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1628 - val_loss: 94985.0312\n",
            "Epoch 862/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2443 - val_loss: 64456.0625\n",
            "Epoch 863/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2038 - val_loss: 70670.4297\n",
            "Epoch 864/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2181 - val_loss: 58387.1680\n",
            "Epoch 865/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1680 - val_loss: 78788.5625\n",
            "Epoch 866/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2045 - val_loss: 88861.8203\n",
            "Epoch 867/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1641 - val_loss: 77956.3516\n",
            "Epoch 868/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1712 - val_loss: 100653.8906\n",
            "Epoch 869/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1986 - val_loss: 89715.5234\n",
            "Epoch 870/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2135 - val_loss: 82128.9219\n",
            "Epoch 871/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1738 - val_loss: 81975.7422\n",
            "Epoch 872/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1883 - val_loss: 72728.0000\n",
            "Epoch 873/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2033 - val_loss: 74593.8750\n",
            "Epoch 874/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1887 - val_loss: 79372.5000\n",
            "Epoch 875/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1840 - val_loss: 91559.4375\n",
            "Epoch 876/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1683 - val_loss: 79231.9922\n",
            "Epoch 877/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1742 - val_loss: 105352.4688\n",
            "Epoch 878/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1833 - val_loss: 87266.4766\n",
            "Epoch 879/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1893 - val_loss: 91146.1719\n",
            "Epoch 880/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1677 - val_loss: 64404.9766\n",
            "Epoch 881/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1559 - val_loss: 88696.7188\n",
            "Epoch 882/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1482 - val_loss: 87731.9844\n",
            "Epoch 883/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1703 - val_loss: 85025.3359\n",
            "Epoch 884/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2028 - val_loss: 86060.3047\n",
            "Epoch 885/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1707 - val_loss: 73589.7734\n",
            "Epoch 886/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1811 - val_loss: 84994.1953\n",
            "Epoch 887/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1961 - val_loss: 97114.0000\n",
            "Epoch 888/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1791 - val_loss: 92805.3438\n",
            "Epoch 889/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1583 - val_loss: 78006.1328\n",
            "Epoch 890/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1729 - val_loss: 79508.6250\n",
            "Epoch 891/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1956 - val_loss: 96935.1641\n",
            "Epoch 892/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1660 - val_loss: 75415.4922\n",
            "Epoch 893/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2298 - val_loss: 75162.4531\n",
            "Epoch 894/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1710 - val_loss: 74538.7891\n",
            "Epoch 895/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1893 - val_loss: 60016.9805\n",
            "Epoch 896/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1622 - val_loss: 62260.9922\n",
            "Epoch 897/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1576 - val_loss: 77098.0547\n",
            "Epoch 898/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1675 - val_loss: 74558.3203\n",
            "Epoch 899/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1805 - val_loss: 68755.7031\n",
            "Epoch 900/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1708 - val_loss: 85992.8281\n",
            "Epoch 901/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2282 - val_loss: 63054.6016\n",
            "Epoch 902/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1521 - val_loss: 61635.6680\n",
            "Epoch 903/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2077 - val_loss: 47565.8867\n",
            "Epoch 904/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1561 - val_loss: 69724.4219\n",
            "Epoch 905/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1741 - val_loss: 73231.2734\n",
            "Epoch 906/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1591 - val_loss: 79418.4297\n",
            "Epoch 907/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1742 - val_loss: 84535.8516\n",
            "Epoch 908/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1704 - val_loss: 93862.8906\n",
            "Epoch 909/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1549 - val_loss: 92182.1953\n",
            "Epoch 910/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2066 - val_loss: 81094.9219\n",
            "Epoch 911/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1977 - val_loss: 66970.0859\n",
            "Epoch 912/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2036 - val_loss: 75296.7578\n",
            "Epoch 913/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1578 - val_loss: 70337.2266\n",
            "Epoch 914/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1547 - val_loss: 79079.0625\n",
            "Epoch 915/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1580 - val_loss: 73940.8750\n",
            "Epoch 916/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1776 - val_loss: 76308.7188\n",
            "Epoch 917/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1589 - val_loss: 89340.4609\n",
            "Epoch 918/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1493 - val_loss: 98590.3516\n",
            "Epoch 919/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1587 - val_loss: 92495.4297\n",
            "Epoch 920/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1467 - val_loss: 90123.9375\n",
            "Epoch 921/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2235 - val_loss: 77861.7188\n",
            "Epoch 922/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1699 - val_loss: 69647.3281\n",
            "Epoch 923/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2090 - val_loss: 73740.8047\n",
            "Epoch 924/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1889 - val_loss: 69845.0625\n",
            "Epoch 925/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1730 - val_loss: 76406.2188\n",
            "Epoch 926/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1564 - val_loss: 81023.2422\n",
            "Epoch 927/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1834 - val_loss: 74853.2266\n",
            "Epoch 928/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1569 - val_loss: 69417.1094\n",
            "Epoch 929/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2202 - val_loss: 68272.4219\n",
            "Epoch 930/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1670 - val_loss: 68845.8750\n",
            "Epoch 931/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1851 - val_loss: 76492.2109\n",
            "Epoch 932/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1756 - val_loss: 66935.0625\n",
            "Epoch 933/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1602 - val_loss: 79055.8438\n",
            "Epoch 934/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1521 - val_loss: 76956.1641\n",
            "Epoch 935/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1464 - val_loss: 96619.0312\n",
            "Epoch 936/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1527 - val_loss: 84484.0391\n",
            "Epoch 937/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1719 - val_loss: 103613.2344\n",
            "Epoch 938/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1617 - val_loss: 87931.2812\n",
            "Epoch 939/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1538 - val_loss: 88123.3672\n",
            "Epoch 940/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1757 - val_loss: 68804.5625\n",
            "Epoch 941/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1785 - val_loss: 77335.6719\n",
            "Epoch 942/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1744 - val_loss: 79046.5547\n",
            "Epoch 943/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1797 - val_loss: 72961.4609\n",
            "Epoch 944/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1706 - val_loss: 64539.0234\n",
            "Epoch 945/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1748 - val_loss: 76722.4375\n",
            "Epoch 946/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1568 - val_loss: 67823.8203\n",
            "Epoch 947/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1847 - val_loss: 70335.1250\n",
            "Epoch 948/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2005 - val_loss: 69606.1719\n",
            "Epoch 949/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1615 - val_loss: 81948.2500\n",
            "Epoch 950/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1873 - val_loss: 66421.2656\n",
            "Epoch 951/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1985 - val_loss: 69609.3516\n",
            "Epoch 952/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1677 - val_loss: 61672.0625\n",
            "Epoch 953/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1816 - val_loss: 78769.7109\n",
            "Epoch 954/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1557 - val_loss: 84799.0000\n",
            "Epoch 955/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2067 - val_loss: 84007.3828\n",
            "Epoch 956/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1847 - val_loss: 59221.0547\n",
            "Epoch 957/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1652 - val_loss: 86385.2891\n",
            "Epoch 958/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1569 - val_loss: 68870.3281\n",
            "Epoch 959/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1897 - val_loss: 67720.9922\n",
            "Epoch 960/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2090 - val_loss: 101146.8984\n",
            "Epoch 961/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1903 - val_loss: 58001.5859\n",
            "Epoch 962/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1487 - val_loss: 78664.7188\n",
            "Epoch 963/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1603 - val_loss: 72846.8281\n",
            "Epoch 964/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2173 - val_loss: 88964.8750\n",
            "Epoch 965/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1470 - val_loss: 81788.1172\n",
            "Epoch 966/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1426 - val_loss: 68265.1250\n",
            "Epoch 967/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1775 - val_loss: 85386.3672\n",
            "Epoch 968/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1512 - val_loss: 91020.6016\n",
            "Epoch 969/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1962 - val_loss: 67445.5312\n",
            "Epoch 970/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1546 - val_loss: 68779.2734\n",
            "Epoch 971/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1706 - val_loss: 79456.1016\n",
            "Epoch 972/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1944 - val_loss: 65119.8164\n",
            "Epoch 973/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1553 - val_loss: 69578.1016\n",
            "Epoch 974/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1881 - val_loss: 64325.2812\n",
            "Epoch 975/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1549 - val_loss: 72682.2109\n",
            "Epoch 976/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2051 - val_loss: 73156.6016\n",
            "Epoch 977/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1745 - val_loss: 58601.2695\n",
            "Epoch 978/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1708 - val_loss: 52042.5625\n",
            "Epoch 979/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2023 - val_loss: 53015.1133\n",
            "Epoch 980/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1691 - val_loss: 63368.3203\n",
            "Epoch 981/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1629 - val_loss: 74927.7500\n",
            "Epoch 982/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1902 - val_loss: 69703.8516\n",
            "Epoch 983/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1791 - val_loss: 71659.8516\n",
            "Epoch 984/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1606 - val_loss: 68065.5547\n",
            "Epoch 985/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1767 - val_loss: 91080.3359\n",
            "Epoch 986/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1621 - val_loss: 84735.1406\n",
            "Epoch 987/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1620 - val_loss: 73110.9531\n",
            "Epoch 988/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1487 - val_loss: 73042.4375\n",
            "Epoch 989/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1635 - val_loss: 73045.1562\n",
            "Epoch 990/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1609 - val_loss: 84333.5859\n",
            "Epoch 991/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2003 - val_loss: 93405.8203\n",
            "Epoch 992/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1571 - val_loss: 71638.2734\n",
            "Epoch 993/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1698 - val_loss: 73007.6484\n",
            "Epoch 994/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1932 - val_loss: 79609.4062\n",
            "Epoch 995/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1571 - val_loss: 62018.3789\n",
            "Epoch 996/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1495 - val_loss: 57747.2109\n",
            "Epoch 997/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1555 - val_loss: 72185.9844\n",
            "Epoch 998/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1756 - val_loss: 82689.6484\n",
            "Epoch 999/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1521 - val_loss: 68924.1953\n",
            "Epoch 1000/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1573 - val_loss: 82701.8672\n",
            "Epoch 1001/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1485 - val_loss: 72790.0078\n",
            "Epoch 1002/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1367 - val_loss: 154846.4375\n",
            "Epoch 1003/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1348 - val_loss: 97234.5000\n",
            "Epoch 1004/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1679 - val_loss: 77520.9531\n",
            "Epoch 1005/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1405 - val_loss: 88569.6016\n",
            "Epoch 1006/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1968 - val_loss: 77455.8984\n",
            "Epoch 1007/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1427 - val_loss: 78890.2344\n",
            "Epoch 1008/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1870 - val_loss: 62136.9219\n",
            "Epoch 1009/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1930 - val_loss: 69807.7344\n",
            "Epoch 1010/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1671 - val_loss: 75314.7109\n",
            "Epoch 1011/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1534 - val_loss: 81545.9062\n",
            "Epoch 1012/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1998 - val_loss: 66497.9844\n",
            "Epoch 1013/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1576 - val_loss: 84660.5312\n",
            "Epoch 1014/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1554 - val_loss: 72387.0156\n",
            "Epoch 1015/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1325 - val_loss: 87587.0625\n",
            "Epoch 1016/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1697 - val_loss: 101997.9766\n",
            "Epoch 1017/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1722 - val_loss: 90411.7734\n",
            "Epoch 1018/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1589 - val_loss: 87545.8828\n",
            "Epoch 1019/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1708 - val_loss: 73721.5312\n",
            "Epoch 1020/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1545 - val_loss: 74854.3906\n",
            "Epoch 1021/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1819 - val_loss: 79496.3125\n",
            "Epoch 1022/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1668 - val_loss: 66994.8750\n",
            "Epoch 1023/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1781 - val_loss: 67909.8906\n",
            "Epoch 1024/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1857 - val_loss: 73334.1250\n",
            "Epoch 1025/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1813 - val_loss: 77224.5859\n",
            "Epoch 1026/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1689 - val_loss: 55916.7656\n",
            "Epoch 1027/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1821 - val_loss: 79407.8125\n",
            "Epoch 1028/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1671 - val_loss: 82347.4609\n",
            "Epoch 1029/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1545 - val_loss: 89493.6250\n",
            "Epoch 1030/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1412 - val_loss: 82511.2891\n",
            "Epoch 1031/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1748 - val_loss: 80256.8047\n",
            "Epoch 1032/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1634 - val_loss: 81183.1719\n",
            "Epoch 1033/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1647 - val_loss: 63270.5742\n",
            "Epoch 1034/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1796 - val_loss: 54081.3086\n",
            "Epoch 1035/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1410 - val_loss: 55870.1914\n",
            "Epoch 1036/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1905 - val_loss: 74971.2422\n",
            "Epoch 1037/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1528 - val_loss: 75187.8359\n",
            "Epoch 1038/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1709 - val_loss: 80328.2812\n",
            "Epoch 1039/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1477 - val_loss: 82656.1172\n",
            "Epoch 1040/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1366 - val_loss: 76647.4375\n",
            "Epoch 1041/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1944 - val_loss: 84494.3359\n",
            "Epoch 1042/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1404 - val_loss: 72449.3906\n",
            "Epoch 1043/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1816 - val_loss: 79685.1406\n",
            "Epoch 1044/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1545 - val_loss: 85604.9688\n",
            "Epoch 1045/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1905 - val_loss: 74791.1875\n",
            "Epoch 1046/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1383 - val_loss: 73383.8906\n",
            "Epoch 1047/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1602 - val_loss: 86217.9922\n",
            "Epoch 1048/10000\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1545 - val_loss: 85126.6328\n",
            "Epoch 1049/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1847 - val_loss: 84932.1953\n",
            "Epoch 1050/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1855 - val_loss: 80861.0000\n",
            "Epoch 1051/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1457 - val_loss: 85519.1016\n",
            "Epoch 1052/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1635 - val_loss: 85498.1797\n",
            "Epoch 1053/10000\n",
            " 1/23 [>.............................] - ETA: 0s - loss: 0.1905Restoring model weights from the end of the best epoch: 53.\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1526 - val_loss: 78419.7344\n",
            "Epoch 1053: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_2707984/581984845.py:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 1.9832416772842407\n",
            "Train loss: 45.07106018066406\n",
            "Test loss: 16.487443923950195\n",
            "dO18 RMSE: 15.189819240941292\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       24.042000           0.345970\n",
            "1       25.240000           0.035950\n",
            "2       25.782000           0.372220\n",
            "3       25.076000           0.230280\n",
            "4       25.966000           0.172480\n",
            "5       27.434000           0.501030\n",
            "6       28.156000           0.999030\n",
            "7       26.836000           0.120880\n",
            "8       28.180000           0.778250\n",
            "9       26.834000           0.094930\n",
            "10      26.644000           0.488430\n",
            "11      26.772000           0.373370\n",
            "12      27.684280           1.216389\n",
            "13      27.403235           0.892280\n",
            "14      24.777670           0.736571\n",
            "15      25.551850           0.312355\n",
            "16      25.115885           0.033519\n",
            "17      25.987815           5.280825\n",
            "18      24.132031           0.389042\n",
            "19      24.898000           0.236070\n",
            "20      23.944000           0.158130\n",
            "21      26.018000           0.964170\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       12.813784           8.872691\n",
            "1       14.783728           9.982981\n",
            "2       15.352287          10.071745\n",
            "3       14.377842           9.669323\n",
            "4       15.793228          10.405943\n",
            "5       10.056499           7.352240\n",
            "6       10.244076           7.435009\n",
            "7        9.916058           7.389081\n",
            "8       10.244997           7.492684\n",
            "9        9.913897           7.395606\n",
            "10       9.883610           7.260664\n",
            "11       9.913189           7.311955\n",
            "12      10.126070           7.231784\n",
            "13      10.075455           7.308174\n",
            "14       5.736795           7.488288\n",
            "15       8.270376          10.893598\n",
            "16       7.826469          11.046714\n",
            "17      11.932503          10.744848\n",
            "18       6.513630          10.178534\n",
            "19      14.099323           9.527592\n",
            "20      12.918242           9.063489\n",
            "21      15.203232           9.646608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-02 23:53:09.160737: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [22,11]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/usr/local/google/home/ruru/Downloads/model_builds/fixed_ablated_boosted._transformer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}