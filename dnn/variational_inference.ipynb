{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnc-br/ddf-isoscapes/blob/new_model_py/dnn/variational_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0IfT3kGwgK6"
      },
      "source": [
        "# Variational model\n",
        "\n",
        "Find the mean/variance of O18 ratios (as well as N15 and C13 in the future) at a particular lat/lon across Brazil."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "henIPlAPCb4i",
        "outputId": "305a1d65-2b76-4d3a-f122-8ef3b096c151",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "henIPlAPCb4i",
        "outputId": "d2840115-c7b8-43e1-d530-18dc0ba2270d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import os\n",
        "from typing import List, Tuple, Dict\n",
        "from dataclasses import dataclass\n",
        "from joblib import dump\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.python.ops import math_ops\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "#@title Model training configuration\n",
        "# See https://zohaib.me/debugging-in-google-collab-notebook/ for tips,\n",
        "# as well as docs for pdb and ipdb.\n",
        "DEBUG = False #@param {type:\"boolean\"}\n",
        "USE_LOCAL_DRIVE = False #@param {type:\"boolean\"}\n",
        "LOCAL_DIR = \"/usr/local/google/home/ruru/Downloads/amazon_sample_data-20230712T203059Z-001\" #@param\n",
        "GDRIVE_DIR = \"MyDrive/amazon_rainforest_files/\" #@param\n",
        "FP_ROOT = LOCAL_DIR\n",
        "\n",
        "MODEL_SAVE_LOCATION = \"MyDrive/amazon_rainforest_files/variational/model\" #@param\n",
        "\n",
        "def get_model_save_location(filename) -> str:\n",
        "  root = '' if USE_LOCAL_DRIVE else '/content/gdrive'\n",
        "  return os.path.join(root, MODEL_SAVE_LOCATION, filename)\n",
        "\n",
        "# Access data stored on Google Drive if not reading data locally.\n",
        "if not USE_LOCAL_DRIVE:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  global FP_ROOT\n",
        "  FP_ROOT = os.path.join('/content/gdrive', GDRIVE_DIR)\n",
        "\n",
        "MODEL_NAME = \"demo_isoscape_model\" #@param\n",
        "\n",
        "TRAINING_SET_PATH = 'amazon_sample_data/canonical/uc_davis_train_fixed_grouped.csv' #@param\n",
        "VALIDATION_SET_PATH = 'amazon_sample_data/canonical/uc_davis_validation_fixed_grouped.csv' #@param\n",
        "TEST_SET_PATH = 'amazon_sample_data/canonical/uc_davis_test_fixed_grouped.csv' #@param\n",
        "\n",
        "if DEBUG:\n",
        "    %pip install -Uqq ipdb\n",
        "    import ipdb\n",
        "    %pdb on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MRzTQkqvRbC"
      },
      "source": [
        "# Import libraries required"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "!if [ ! -d \"/content/ddf_common_stub\" ] ; then git clone -b test https://github.com/tnc-br/ddf_common_stub.git; fi\n",
        "sys.path.append(\"/content/ddf_common_stub/\")\n",
        "import ddfimport\n",
        "ddfimport.ddf_import_common()"
      ],
      "metadata": {
        "id": "AXh86HFwXiax",
        "outputId": "f3ff817e-d2df-42eb-fc32-7637d24a3fe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "executing checkout_branch ...\n",
            "Branch main already checked out.\n",
            "Remember to reload your imports with `importlib.reload(module)`.\n",
            "b''\n",
            "main branch checked out as readonly. You may now use ddf_common imports\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "!if [ ! -d \"/content/ddf_common_stub\" ] ; then git clone -b test https://github.com/tnc-br/ddf_common_stub.git; fi\n",
        "sys.path.append(\"/content/ddf_common_stub/\")\n",
        "import ddfimport\n",
        "ddfimport.ddf_import_common()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import raster\n",
        "import importlib\n",
        "import model\n",
        "importlib.reload(model)\n",
        "importlib.reload(raster)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cab9d31-3dbe-4be7-d0a3-353c2aa302bd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'raster' from '/tmp/ddf_common/raster.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import raster\n",
        "import importlib\n",
        "importlib.reload(raster)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQrEv57zCb4q"
      },
      "source": [
        "# Data preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data nudging for coordinates that are slightly outside the bounds of our rasters."
      ],
      "metadata": {
        "id": "_RDf_XHVuxDR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "klJcnmYgX3I5"
      },
      "outputs": [],
      "source": [
        "def valid_in_all_rasters(lat: float, lon: float, rasters: List[raster.AmazonGeoTiff]) -> bool:\n",
        "  for r in rasters:\n",
        "    if not raster.is_valid_point(lat, lon, r):\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "def nudge_invalid_coords(df: pd.DataFrame, rasters: List[raster.AmazonGeoTiff]):\n",
        "  max_degrees_deviation = 2\n",
        "  for i, row in df.iterrows():\n",
        "    # Get the lat and long for the current row.\n",
        "    lat = df.loc[i, \"lat\"]\n",
        "    lon = df.loc[i, \"long\"]\n",
        "\n",
        "    if valid_in_all_rasters(lat, lon, rasters):\n",
        "      continue\n",
        "\n",
        "    # nudge 0.01 degrees at a time.\n",
        "    for nudge in [x/100.0 for x in range(1, max_degrees_deviation*100)]:\n",
        "      if valid_in_all_rasters(lat + nudge, lon + nudge, rasters):\n",
        "        df.loc[i, \"lat\"] = lat + nudge\n",
        "        df.loc[i, \"long\"] = lon + nudge\n",
        "        break\n",
        "      elif valid_in_all_rasters(lat - nudge, lon - nudge, rasters):\n",
        "        df.loc[i, \"lat\"] = lat - nudge\n",
        "        df.loc[i, \"long\"] = lon - nudge\n",
        "        break\n",
        "      elif valid_in_all_rasters(lat + nudge, lon - nudge, rasters):\n",
        "        df.loc[i, \"lat\"] = lat + nudge\n",
        "        df.loc[i, \"long\"] = lon - nudge\n",
        "        break\n",
        "      elif valid_in_all_rasters(lat - nudge, lon + nudge, rasters):\n",
        "        df.loc[i, \"lat\"] = lat - nudge\n",
        "        df.loc[i, \"long\"] = lon + nudge\n",
        "        break\n",
        "    if df.loc[i, \"lat\"] == lat and df.loc[i, \"long\"] == lon:\n",
        "      raise ValueError(\"Failed to nudge coordinates into valid space\")\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6XMee1aHfcik"
      },
      "outputs": [],
      "source": [
        "def load_dataset(path: str, columns_to_keep: List[str], side_raster_input):\n",
        "  df = pd.read_csv(path, encoding=\"ISO-8859-1\", sep=',')\n",
        "  df = df[df['d18O_cel_variance'].notna()]\n",
        "\n",
        "  df = nudge_invalid_coords(df, list(side_raster_input.values()))\n",
        "\n",
        "  for name, geotiff in side_raster_input.items():\n",
        "    df[name] = df.apply(lambda row: geotiff.value_at(row['long'], row['lat']), axis=1)\n",
        "  for name, geotiff in side_raster_input.items():\n",
        "    df = df[df[name].notnull()]\n",
        "\n",
        "  X = df.drop(df.columns.difference(columns_to_keep), axis=1)\n",
        "  Y = df[[\"d18O_cel_mean\", \"d18O_cel_variance\"]]\n",
        "\n",
        "  return X, Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtkKhMOtb6cS"
      },
      "source": [
        "# Standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "XSDwdvMkb7w8"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class FeaturesToLabels:\n",
        "  def __init__(self, X: pd.DataFrame, Y: pd.DataFrame):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "\n",
        "  def as_tuple(self):\n",
        "    return (self.X, self.Y)\n",
        "\n",
        "\n",
        "def create_feature_scaler(X: pd.DataFrame,\n",
        "                          columns_to_passthrough,\n",
        "                          columns_to_scale,\n",
        "                          columns_to_standardize) -> ColumnTransformer:\n",
        "  columns_to_standardize = columns_to_standardize\n",
        "  feature_scaler = ColumnTransformer(\n",
        "      [(column+'_scaler', MinMaxScaler(), [column]) for column in columns_to_scale] +\n",
        "      [(column+'_standardizer', StandardScaler(), [column]) for column in columns_to_standardize],\n",
        "      remainder='passthrough')\n",
        "  feature_scaler.fit(X)\n",
        "  return feature_scaler\n",
        "\n",
        "def create_label_scaler(Y: pd.DataFrame) -> ColumnTransformer:\n",
        "  label_scaler = ColumnTransformer([\n",
        "      ('mean_std_scaler', StandardScaler(), ['d18O_cel_mean']),\n",
        "      ('var_minmax_scaler', MinMaxScaler(), ['d18O_cel_variance'])],\n",
        "      remainder='passthrough')\n",
        "  label_scaler.fit(Y)\n",
        "  return label_scaler\n",
        "\n",
        "def scale(X: pd.DataFrame, feature_scaler):\n",
        "  # transform() outputs numpy arrays :(  need to convert back to DataFrame.\n",
        "  X_standardized = pd.DataFrame(feature_scaler.transform(X),\n",
        "                        index=X.index, columns=X.columns)\n",
        "  return X_standardized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "_kf2e_fKon2P"
      },
      "outputs": [],
      "source": [
        "# Just a class organization, holds each scaled dataset and the scaler used.\n",
        "# Useful for unscaling predictions.\n",
        "@dataclass\n",
        "class ScaledPartitions():\n",
        "  def __init__(self,\n",
        "               feature_scaler: ColumnTransformer,\n",
        "               label_scaler: ColumnTransformer,\n",
        "               train: FeaturesToLabels, val: FeaturesToLabels,\n",
        "               test: FeaturesToLabels):\n",
        "    self.feature_scaler = feature_scaler\n",
        "    self.label_scaler = label_scaler\n",
        "    self.train = train\n",
        "    self.val = val\n",
        "    self.test = test\n",
        "\n",
        "\n",
        "def load_and_scale(config: Dict,\n",
        "                   columns_to_passthrough: List[str],\n",
        "                   columns_to_scale: List[str],\n",
        "                   columns_to_standardize: List[str],\n",
        "                   extra_columns_from_geotiffs: Dict[str, raster.AmazonGeoTiff]) -> ScaledPartitions:\n",
        "  columns_to_keep = columns_to_passthrough + columns_to_scale + columns_to_standardize\n",
        "  X_train, Y_train = load_dataset(config['TRAIN'], columns_to_keep, extra_columns_from_geotiffs)\n",
        "  X_val, Y_val = load_dataset(config['VALIDATION'], columns_to_keep, extra_columns_from_geotiffs)\n",
        "  X_test, Y_test = load_dataset(config['TEST'], columns_to_keep, extra_columns_from_geotiffs)\n",
        "\n",
        "  # Fit the scaler:\n",
        "  feature_scaler = create_feature_scaler(\n",
        "      X_train,\n",
        "      columns_to_passthrough,\n",
        "      columns_to_scale,\n",
        "      columns_to_standardize)\n",
        "  label_scaler = create_label_scaler(Y_train)\n",
        "\n",
        "  # Apply the scaler:\n",
        "  train = FeaturesToLabels(scale(X_train, feature_scaler), Y_train)\n",
        "  val = FeaturesToLabels(scale(X_val, feature_scaler), Y_val)\n",
        "  test = FeaturesToLabels(scale(X_test, feature_scaler), Y_test)\n",
        "  return ScaledPartitions(feature_scaler, label_scaler, train, val, test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usGznR593LZc"
      },
      "source": [
        "# Model Definition\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khK7C8WvU8ZR"
      },
      "source": [
        "The KL Loss function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "urGjYNNnemX6"
      },
      "outputs": [],
      "source": [
        "def sample_normal_distribution(\n",
        "    mean: tf.Tensor,\n",
        "    stdev: tf.Tensor,\n",
        "    n: int) -> tf.Tensor:\n",
        "    '''\n",
        "    Given a batch of normal distributions described by a mean and stdev in\n",
        "    a tf.Tensor, sample n elements from each distribution and return the mean\n",
        "    and standard deviation per sample.\n",
        "    '''\n",
        "    batch_size = tf.shape(mean)[0]\n",
        "\n",
        "    # Output tensor is (n, batch_size, 1)\n",
        "    sample_values = tfp.distributions.Normal(\n",
        "        loc=mean,\n",
        "        scale=stdev).sample(\n",
        "            sample_shape=n)\n",
        "    # Reshaped tensor will be (batch_size, n)\n",
        "    sample_values = tf.transpose(sample_values)\n",
        "    # Get the mean per sample in the batch.\n",
        "    sample_mean = tf.transpose(tf.math.reduce_mean(sample_values, 2))\n",
        "    sample_stdev = tf.transpose(tf.math.reduce_std(sample_values, 2))\n",
        "\n",
        "    return sample_mean, sample_stdev\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# log(σ2/σ1) + ( σ1^2+(μ1−μ2)^2 ) / 2* σ^2   − 1/2\n",
        "def kl_divergence_helper(real, predicted, sample):\n",
        "    '''\n",
        "    real: tf.Tensor of the real mean and standard deviation of sample to compare\n",
        "    predicted: tf.Tensor of the predicted mean and standard deviation to compare\n",
        "    sample: Whether or not to sample the predicted distribution to get a new\n",
        "            mean and standard deviation.\n",
        "    '''\n",
        "    if real.shape != predicted.shape:\n",
        "      raise ValueError(\n",
        "          f\"real.shape {real.shape} != predicted.shape {predicted.shape}\")\n",
        "\n",
        "    real_value = tf.gather(real, [0], axis=1)\n",
        "    real_std = tf.math.sqrt(tf.gather(real, [1], axis=1))\n",
        "\n",
        "\n",
        "    predicted_value = tf.gather(predicted, [0], axis=1)\n",
        "    predicted_std = tf.math.sqrt(tf.gather(predicted, [1], axis=1))\n",
        "    # If true, sample from the distribution defined by the predicted mean and\n",
        "    # standard deviation to use for mean and stdev used in KL divergence loss.\n",
        "    if sample:\n",
        "      predicted_value, predicted_std = sample_normal_distribution(\n",
        "          mean=predicted_value, stdev=predicted_std, n=15)\n",
        "\n",
        "    kl_loss = -0.5 + tf.math.log(predicted_std/real_std) + \\\n",
        "     (tf.square(real_std) + tf.square(real_value - predicted_value))/ \\\n",
        "     (2*tf.square(predicted_std))\n",
        "\n",
        "    if tf.math.is_nan(tf.math.reduce_mean(kl_loss)):\n",
        "       tf.print(predicted)\n",
        "       sess = tf.compat.v1.Session()\n",
        "       sess.close()\n",
        "\n",
        "    return tf.math.reduce_mean(kl_loss)\n",
        "\n",
        "def kl_divergence(real, predicted):\n",
        "  return kl_divergence_helper(real, predicted, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HCkGSPUo3KqY"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "def get_early_stopping_callback():\n",
        "  return EarlyStopping(monitor='val_loss', patience=1000, min_delta=0.001,\n",
        "                       verbose=1, restore_best_weights=True, start_from_epoch=0)\n",
        "\n",
        "tf.keras.utils.set_random_seed(18731)\n",
        "\n",
        "# I was experimenting with models that took longer to train, and used this\n",
        "# checkpointing callback to periodically save the model. It's optional.\n",
        "def get_checkpoint_callback(model_file):\n",
        "  return ModelCheckpoint(\n",
        "      get_model_save_location(model_file),\n",
        "      monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
        "\n",
        "def train_or_update_variational_model(\n",
        "        sp: ScaledPartitions,\n",
        "        hidden_layers: List[int],\n",
        "        epochs: int,\n",
        "        batch_size: int,\n",
        "        lr: float,\n",
        "        model_file=None,\n",
        "        use_checkpoint=False):\n",
        "  callbacks_list = [get_early_stopping_callback(),\n",
        "                    get_checkpoint_callback(model_file)]\n",
        "  if not use_checkpoint:\n",
        "    inputs = keras.Input(shape=(sp.train.X.shape[1],))\n",
        "    x = inputs\n",
        "    for layer_size in hidden_layers:\n",
        "      x = keras.layers.Dense(\n",
        "          layer_size, activation='relu')(x)\n",
        "    mean_output = keras.layers.Dense(\n",
        "        1, name='mean_output')(x)\n",
        "\n",
        "    # We can not have negative variance. Apply very little variance.\n",
        "    var_output = keras.layers.Dense(\n",
        "        1, name='var_output')(x)\n",
        "\n",
        "    # Invert the normalization on our outputs\n",
        "    mean_scaler = sp.label_scaler.named_transformers_['mean_std_scaler']\n",
        "    untransformed_mean = mean_output * mean_scaler.var_ + mean_scaler.mean_\n",
        "\n",
        "    var_scaler = sp.label_scaler.named_transformers_['var_minmax_scaler']\n",
        "    unscaled_var = var_output * var_scaler.scale_ + var_scaler.min_\n",
        "    untransformed_var = keras.layers.Lambda(lambda t: tf.math.log(1 + tf.exp(t)))(unscaled_var)\n",
        "\n",
        "    # Output mean,  tuples.\n",
        "    outputs = keras.layers.concatenate([untransformed_mean, untransformed_var])\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss=kl_divergence)\n",
        "    model.summary()\n",
        "  else:\n",
        "    model = keras.models.load_model(\n",
        "        get_model_save_location(model_file),\n",
        "        custom_objects={\"kl_divergence\": kl_divergence})\n",
        "  history = model.fit(sp.train.X, sp.train.Y, verbose=1, validation_data=sp.val.as_tuple(), shuffle=True,\n",
        "                      epochs=epochs, batch_size=batch_size, callbacks=callbacks_list)\n",
        "  return history, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DALuUm8UOgNu"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def render_plot_loss(history, name):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title(name + ' model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.yscale(\"log\")\n",
        "  plt.ylim((0, 10))\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['loss', 'val_loss'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def train_and_evaluate(sp: ScaledPartitions, run_id: str, training_batch_size=5):\n",
        "  print(\"==================\")\n",
        "  print(run_id)\n",
        "  history, model = train_or_update_variational_model(\n",
        "      sp, hidden_layers=[20, 20], epochs=5000, batch_size=training_batch_size,\n",
        "      lr=0.0001, model_file=run_id+\".h5\", use_checkpoint=False)\n",
        "  render_plot_loss(history, run_id+\" kl_loss\")\n",
        "  model.save(get_model_save_location(run_id+\".h5\"), save_format=\"h5\")\n",
        "\n",
        "  best_epoch_index = history.history['val_loss'].index(min(history.history['val_loss']))\n",
        "  print('Val loss:', history.history['val_loss'][best_epoch_index])\n",
        "  print('Train loss:', history.history['loss'][best_epoch_index])\n",
        "  print('Test loss:', model.evaluate(x=sp.test.X, y=sp.test.Y, verbose=0))\n",
        "\n",
        "  predictions = model.predict_on_batch(sp.test.X)\n",
        "  predictions = pd.DataFrame(predictions, columns=['d18O_cel_mean', 'd18O_cel_variance'])\n",
        "  rmse = np.sqrt(mean_squared_error(sp.test.Y['d18O_cel_mean'], predictions['d18O_cel_mean']))\n",
        "  print(\"dO18 RMSE: \"+ str(rmse))\n",
        "  print(\"EXPECTED:\")\n",
        "  print(sp.test.Y.to_string())\n",
        "  print()\n",
        "  print(\"PREDICTED:\")\n",
        "  print(predictions.to_string())\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMCbonPjs6Kp"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rPONfgkjvJWz",
        "outputId": "cdab1e43-1079-4340-8860-476535d6336f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: GTiff/GeoTIFF\n",
            "Size is 541 x 467 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.922043, 5.233124)\n",
            "Pixel Size = (0.08333, -0.08333)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 218 x 2\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-74.0, 4.500000000659528)\n",
            "Pixel Size = (0.166666666667993, -0.16666666666799657)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 218 x 2\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-74.0, 4.500000000659528)\n",
            "Pixel Size = (0.166666666667993, -0.16666666666799657)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
              "       'Mean Annual Temperature', 'Mean Annual Precipitation',\n",
              "       'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION',\n",
              "       'ordinary_kriging_linear_d18O_predicted_mean',\n",
              "       'ordinary_kriging_linear_d18O_predicted_variance',\n",
              "       'brisoscape_mean_ISORIX', 'd13C_cel_mean', 'd13C_cel_var'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "grouped_random_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, TRAINING_SET_PATH),\n",
        "    'TEST' : os.path.join(FP_ROOT, VALIDATION_SET_PATH),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, TEST_SET_PATH),\n",
        "}\n",
        "\n",
        "columns_to_passthrough = [\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "columns_to_scale = []\n",
        "columns_to_standardize = [\n",
        "    'lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature', 'Mean Annual Precipitation',\n",
        "    'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION',\n",
        "    'brisoscape_mean_ISORIX', 'd13C_cel_mean', 'd13C_cel_var',\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "\n",
        "# Create artifical columns by querying rasters at each lat/lon in the training\n",
        "# dataset CSV.\n",
        "extra_columns_from_geotiffs = {\n",
        "  \"brisoscape_mean_ISORIX\" : raster.load_raster(raster.get_raster_path(\"brisoscape_mean_ISORIX.tif\")),\n",
        "  \"d13C_cel_mean\" : raster.load_raster(raster.get_raster_path(\"d13C_cel_map_BRAZIL_stack.tiff\"), use_only_band_index=0),\n",
        "  \"d13C_cel_var\" : raster.load_raster(raster.get_raster_path(\"d13C_cel_map_BRAZIL_stack.tiff\"), use_only_band_index=1),\n",
        "  \"ordinary_kriging_linear_d18O_predicted_mean\" : raster.krig_means_isoscape_geotiff(),\n",
        "  \"ordinary_kriging_linear_d18O_predicted_variance\" : raster.krig_variances_isoscape_geotiff(),\n",
        "}\n",
        "\n",
        "data = load_and_scale(grouped_random_fileset, columns_to_passthrough, columns_to_scale, columns_to_standardize, extra_columns_from_geotiffs)\n",
        "data.train.X.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cFuIPM4afQPd",
        "outputId": "6a3f0cd0-ffa7-4ab0-8507-9d5857964c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================\n",
            "demo_isoscape_model\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 16)]         0           []                               \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 20)           340         ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 20)           420         ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " var_output (Dense)             (None, 1)            21          ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " mean_output (Dense)            (None, 1)            21          ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.multiply_1 (TFOpLambda  (None, 1)           0           ['var_output[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.multiply (TFOpLambda)  (None, 1)            0           ['mean_output[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 1)           0           ['tf.math.multiply_1[0][0]']     \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 1)           0           ['tf.math.multiply[0][0]']       \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1)            0           ['tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 2)            0           ['tf.__operators__.add[0][0]',   \n",
            "                                                                  'lambda[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 802\n",
            "Trainable params: 802\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5000\n",
            "23/23 [==============================] - 5s 141ms/step - loss: 1.5372 - val_loss: 8.0820\n",
            "Epoch 2/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.5763 - val_loss: 4.4169\n",
            "Epoch 3/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.5487 - val_loss: 4.7712\n",
            "Epoch 4/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 1.4463 - val_loss: 4.7152\n",
            "Epoch 5/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 1.6353 - val_loss: 4.5083\n",
            "Epoch 6/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.2960 - val_loss: 4.3436\n",
            "Epoch 7/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3620 - val_loss: 5.8760\n",
            "Epoch 8/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.5020 - val_loss: 4.1005\n",
            "Epoch 9/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.3433 - val_loss: 3.8980\n",
            "Epoch 10/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.3887 - val_loss: 4.2335\n",
            "Epoch 11/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3394 - val_loss: 3.9073\n",
            "Epoch 12/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.4531 - val_loss: 3.5252\n",
            "Epoch 13/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.3067 - val_loss: 3.1332\n",
            "Epoch 14/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.2503 - val_loss: 3.8688\n",
            "Epoch 15/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.4654 - val_loss: 3.0467\n",
            "Epoch 16/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3625 - val_loss: 3.1987\n",
            "Epoch 17/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2065 - val_loss: 3.5481\n",
            "Epoch 18/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2970 - val_loss: 3.0498\n",
            "Epoch 19/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1803 - val_loss: 3.3614\n",
            "Epoch 20/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2794 - val_loss: 3.4825\n",
            "Epoch 21/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.3979 - val_loss: 2.6571\n",
            "Epoch 22/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.1066 - val_loss: 2.9626\n",
            "Epoch 23/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.1777 - val_loss: 2.6049\n",
            "Epoch 24/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2366 - val_loss: 2.9659\n",
            "Epoch 25/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3685 - val_loss: 3.0363\n",
            "Epoch 26/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0639 - val_loss: 2.6980\n",
            "Epoch 27/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 1.2090 - val_loss: 2.7938\n",
            "Epoch 28/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.0549 - val_loss: 2.4527\n",
            "Epoch 29/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.1783 - val_loss: 2.9067\n",
            "Epoch 30/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.2055 - val_loss: 2.6791\n",
            "Epoch 31/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.1514 - val_loss: 3.1267\n",
            "Epoch 32/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.1658 - val_loss: 2.7196\n",
            "Epoch 33/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.2075 - val_loss: 2.7761\n",
            "Epoch 34/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.1242 - val_loss: 2.6951\n",
            "Epoch 35/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1336 - val_loss: 2.3484\n",
            "Epoch 36/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1011 - val_loss: 1.9768\n",
            "Epoch 37/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0388 - val_loss: 2.3167\n",
            "Epoch 38/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.3494 - val_loss: 2.4307\n",
            "Epoch 39/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0495 - val_loss: 2.2836\n",
            "Epoch 40/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.1810 - val_loss: 2.1185\n",
            "Epoch 41/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9783 - val_loss: 2.1382\n",
            "Epoch 42/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.1158 - val_loss: 2.4215\n",
            "Epoch 43/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.2151 - val_loss: 2.0220\n",
            "Epoch 44/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.1273 - val_loss: 2.0255\n",
            "Epoch 45/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0873 - val_loss: 2.0736\n",
            "Epoch 46/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1147 - val_loss: 1.9595\n",
            "Epoch 47/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9479 - val_loss: 1.5923\n",
            "Epoch 48/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0315 - val_loss: 2.1294\n",
            "Epoch 49/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9379 - val_loss: 2.5251\n",
            "Epoch 50/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0979 - val_loss: 1.6237\n",
            "Epoch 51/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9660 - val_loss: 1.6888\n",
            "Epoch 52/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0982 - val_loss: 2.3033\n",
            "Epoch 53/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.0109 - val_loss: 1.9186\n",
            "Epoch 54/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9443 - val_loss: 1.8270\n",
            "Epoch 55/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.0334 - val_loss: 1.8540\n",
            "Epoch 56/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0245 - val_loss: 1.8355\n",
            "Epoch 57/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0490 - val_loss: 2.0527\n",
            "Epoch 58/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0309 - val_loss: 1.6255\n",
            "Epoch 59/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1722 - val_loss: 1.4476\n",
            "Epoch 60/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.1308 - val_loss: 1.6664\n",
            "Epoch 61/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0144 - val_loss: 1.3618\n",
            "Epoch 62/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.0936 - val_loss: 1.5841\n",
            "Epoch 63/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.3455 - val_loss: 1.4184\n",
            "Epoch 64/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0617 - val_loss: 1.5322\n",
            "Epoch 65/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8508 - val_loss: 1.6469\n",
            "Epoch 66/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0067 - val_loss: 1.5486\n",
            "Epoch 67/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8138 - val_loss: 1.5636\n",
            "Epoch 68/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9125 - val_loss: 1.7487\n",
            "Epoch 69/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9035 - val_loss: 1.4855\n",
            "Epoch 70/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8777 - val_loss: 1.6458\n",
            "Epoch 71/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0855 - val_loss: 1.6211\n",
            "Epoch 72/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9770 - val_loss: 1.9489\n",
            "Epoch 73/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0455 - val_loss: 1.4535\n",
            "Epoch 74/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9444 - val_loss: 1.5382\n",
            "Epoch 75/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9687 - val_loss: 1.6388\n",
            "Epoch 76/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9445 - val_loss: 1.9199\n",
            "Epoch 77/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8682 - val_loss: 1.7396\n",
            "Epoch 78/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9071 - val_loss: 1.6793\n",
            "Epoch 79/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9222 - val_loss: 1.7697\n",
            "Epoch 80/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8598 - val_loss: 1.5404\n",
            "Epoch 81/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9375 - val_loss: 1.6118\n",
            "Epoch 82/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8380 - val_loss: 1.4147\n",
            "Epoch 83/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.0335 - val_loss: 1.6228\n",
            "Epoch 84/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9477 - val_loss: 1.2517\n",
            "Epoch 85/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9487 - val_loss: 1.5692\n",
            "Epoch 86/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8920 - val_loss: 1.2968\n",
            "Epoch 87/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.1231 - val_loss: 1.3659\n",
            "Epoch 88/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9220 - val_loss: 1.5869\n",
            "Epoch 89/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9461 - val_loss: 1.2414\n",
            "Epoch 90/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8800 - val_loss: 1.2207\n",
            "Epoch 91/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9461 - val_loss: 1.5421\n",
            "Epoch 92/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8963 - val_loss: 1.4205\n",
            "Epoch 93/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9894 - val_loss: 1.2365\n",
            "Epoch 94/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9070 - val_loss: 1.1287\n",
            "Epoch 95/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8288 - val_loss: 1.3601\n",
            "Epoch 96/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8250 - val_loss: 1.1942\n",
            "Epoch 97/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9465 - val_loss: 1.1314\n",
            "Epoch 98/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8673 - val_loss: 1.3562\n",
            "Epoch 99/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0046 - val_loss: 1.2636\n",
            "Epoch 100/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8271 - val_loss: 1.2582\n",
            "Epoch 101/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9552 - val_loss: 1.3977\n",
            "Epoch 102/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9025 - val_loss: 1.5413\n",
            "Epoch 103/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8286 - val_loss: 1.2835\n",
            "Epoch 104/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9876 - val_loss: 1.3374\n",
            "Epoch 105/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9365 - val_loss: 1.2999\n",
            "Epoch 106/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8397 - val_loss: 1.4281\n",
            "Epoch 107/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8733 - val_loss: 1.4321\n",
            "Epoch 108/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8826 - val_loss: 1.4060\n",
            "Epoch 109/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8975 - val_loss: 1.5449\n",
            "Epoch 110/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7939 - val_loss: 1.4462\n",
            "Epoch 111/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8262 - val_loss: 1.2891\n",
            "Epoch 112/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8955 - val_loss: 1.3215\n",
            "Epoch 113/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8705 - val_loss: 1.2227\n",
            "Epoch 114/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8560 - val_loss: 1.2991\n",
            "Epoch 115/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0181 - val_loss: 1.2649\n",
            "Epoch 116/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.0063 - val_loss: 1.2258\n",
            "Epoch 117/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8907 - val_loss: 1.7266\n",
            "Epoch 118/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8987 - val_loss: 1.3526\n",
            "Epoch 119/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7780 - val_loss: 1.0880\n",
            "Epoch 120/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9576 - val_loss: 1.2383\n",
            "Epoch 121/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9013 - val_loss: 1.1629\n",
            "Epoch 122/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8253 - val_loss: 1.1806\n",
            "Epoch 123/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8150 - val_loss: 1.3257\n",
            "Epoch 124/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8886 - val_loss: 1.3239\n",
            "Epoch 125/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8236 - val_loss: 1.3885\n",
            "Epoch 126/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8097 - val_loss: 1.5306\n",
            "Epoch 127/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9381 - val_loss: 1.6384\n",
            "Epoch 128/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8204 - val_loss: 1.4128\n",
            "Epoch 129/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7162 - val_loss: 1.3704\n",
            "Epoch 130/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8940 - val_loss: 1.2940\n",
            "Epoch 131/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7323 - val_loss: 1.2447\n",
            "Epoch 132/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8141 - val_loss: 1.6097\n",
            "Epoch 133/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8208 - val_loss: 1.4820\n",
            "Epoch 134/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9155 - val_loss: 1.2700\n",
            "Epoch 135/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8915 - val_loss: 1.3298\n",
            "Epoch 136/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8590 - val_loss: 1.2681\n",
            "Epoch 137/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8040 - val_loss: 1.3644\n",
            "Epoch 138/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8031 - val_loss: 1.1968\n",
            "Epoch 139/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7366 - val_loss: 1.2424\n",
            "Epoch 140/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7314 - val_loss: 1.2736\n",
            "Epoch 141/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7915 - val_loss: 1.1660\n",
            "Epoch 142/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8675 - val_loss: 1.2238\n",
            "Epoch 143/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8346 - val_loss: 1.1570\n",
            "Epoch 144/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8686 - val_loss: 1.5465\n",
            "Epoch 145/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8937 - val_loss: 1.3478\n",
            "Epoch 146/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9472 - val_loss: 1.2838\n",
            "Epoch 147/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7185 - val_loss: 1.4187\n",
            "Epoch 148/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7527 - val_loss: 1.1924\n",
            "Epoch 149/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8652 - val_loss: 1.3521\n",
            "Epoch 150/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7920 - val_loss: 1.2592\n",
            "Epoch 151/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7519 - val_loss: 1.4638\n",
            "Epoch 152/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8246 - val_loss: 1.3891\n",
            "Epoch 153/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7740 - val_loss: 1.2995\n",
            "Epoch 154/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8122 - val_loss: 1.5534\n",
            "Epoch 155/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8012 - val_loss: 1.6691\n",
            "Epoch 156/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8235 - val_loss: 1.3198\n",
            "Epoch 157/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7163 - val_loss: 1.1712\n",
            "Epoch 158/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8171 - val_loss: 1.3394\n",
            "Epoch 159/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8418 - val_loss: 1.3296\n",
            "Epoch 160/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7382 - val_loss: 1.5621\n",
            "Epoch 161/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8125 - val_loss: 1.3408\n",
            "Epoch 162/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7623 - val_loss: 1.5251\n",
            "Epoch 163/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8807 - val_loss: 1.2474\n",
            "Epoch 164/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8453 - val_loss: 1.2473\n",
            "Epoch 165/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8305 - val_loss: 1.3098\n",
            "Epoch 166/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7308 - val_loss: 1.2849\n",
            "Epoch 167/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8135 - val_loss: 1.2828\n",
            "Epoch 168/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7219 - val_loss: 1.3708\n",
            "Epoch 169/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8228 - val_loss: 1.1283\n",
            "Epoch 170/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8433 - val_loss: 1.4989\n",
            "Epoch 171/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8332 - val_loss: 1.2283\n",
            "Epoch 172/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7671 - val_loss: 1.3978\n",
            "Epoch 173/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8474 - val_loss: 1.1885\n",
            "Epoch 174/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8389 - val_loss: 1.4900\n",
            "Epoch 175/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7067 - val_loss: 1.2961\n",
            "Epoch 176/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7289 - val_loss: 1.4683\n",
            "Epoch 177/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7689 - val_loss: 1.2356\n",
            "Epoch 178/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8013 - val_loss: 1.2396\n",
            "Epoch 179/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7007 - val_loss: 1.3608\n",
            "Epoch 180/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7576 - val_loss: 1.3549\n",
            "Epoch 181/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7874 - val_loss: 1.2833\n",
            "Epoch 182/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7044 - val_loss: 1.5573\n",
            "Epoch 183/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7184 - val_loss: 1.3962\n",
            "Epoch 184/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7230 - val_loss: 1.3840\n",
            "Epoch 185/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7548 - val_loss: 1.4088\n",
            "Epoch 186/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8335 - val_loss: 1.4748\n",
            "Epoch 187/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7198 - val_loss: 1.2439\n",
            "Epoch 188/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8483 - val_loss: 1.2126\n",
            "Epoch 189/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7840 - val_loss: 1.3688\n",
            "Epoch 190/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8248 - val_loss: 1.4076\n",
            "Epoch 191/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7669 - val_loss: 1.3884\n",
            "Epoch 192/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8317 - val_loss: 1.1696\n",
            "Epoch 193/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7532 - val_loss: 1.2907\n",
            "Epoch 194/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7536 - val_loss: 1.2503\n",
            "Epoch 195/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7763 - val_loss: 1.3989\n",
            "Epoch 196/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7617 - val_loss: 1.4706\n",
            "Epoch 197/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7272 - val_loss: 1.3767\n",
            "Epoch 198/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7355 - val_loss: 1.3192\n",
            "Epoch 199/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7157 - val_loss: 1.4099\n",
            "Epoch 200/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7273 - val_loss: 1.2754\n",
            "Epoch 201/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7806 - val_loss: 1.6182\n",
            "Epoch 202/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6818 - val_loss: 1.5280\n",
            "Epoch 203/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7563 - val_loss: 1.3666\n",
            "Epoch 204/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7373 - val_loss: 1.3725\n",
            "Epoch 205/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8314 - val_loss: 1.2545\n",
            "Epoch 206/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7532 - val_loss: 1.2561\n",
            "Epoch 207/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7309 - val_loss: 1.3851\n",
            "Epoch 208/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7332 - val_loss: 1.3371\n",
            "Epoch 209/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7590 - val_loss: 1.4564\n",
            "Epoch 210/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6924 - val_loss: 1.7831\n",
            "Epoch 211/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7676 - val_loss: 1.3883\n",
            "Epoch 212/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8484 - val_loss: 1.4075\n",
            "Epoch 213/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7264 - val_loss: 1.2183\n",
            "Epoch 214/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7311 - val_loss: 1.3486\n",
            "Epoch 215/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7729 - val_loss: 1.4287\n",
            "Epoch 216/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7169 - val_loss: 1.4168\n",
            "Epoch 217/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8447 - val_loss: 1.3839\n",
            "Epoch 218/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7437 - val_loss: 1.2997\n",
            "Epoch 219/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7281 - val_loss: 1.6189\n",
            "Epoch 220/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7166 - val_loss: 1.2520\n",
            "Epoch 221/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7089 - val_loss: 1.4538\n",
            "Epoch 222/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7504 - val_loss: 1.4720\n",
            "Epoch 223/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7532 - val_loss: 1.5345\n",
            "Epoch 224/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6831 - val_loss: 1.3401\n",
            "Epoch 225/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6917 - val_loss: 1.4044\n",
            "Epoch 226/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6668 - val_loss: 1.4810\n",
            "Epoch 227/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7370 - val_loss: 1.3429\n",
            "Epoch 228/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6937 - val_loss: 1.5408\n",
            "Epoch 229/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7907 - val_loss: 1.2567\n",
            "Epoch 230/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7357 - val_loss: 1.3339\n",
            "Epoch 231/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7136 - val_loss: 1.2634\n",
            "Epoch 232/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7861 - val_loss: 1.1641\n",
            "Epoch 233/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7553 - val_loss: 1.2542\n",
            "Epoch 234/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8085 - val_loss: 1.2232\n",
            "Epoch 235/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7072 - val_loss: 1.4040\n",
            "Epoch 236/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8052 - val_loss: 1.4095\n",
            "Epoch 237/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7577 - val_loss: 1.2993\n",
            "Epoch 238/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7560 - val_loss: 1.2683\n",
            "Epoch 239/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7259 - val_loss: 1.1726\n",
            "Epoch 240/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7199 - val_loss: 1.4401\n",
            "Epoch 241/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7296 - val_loss: 1.3197\n",
            "Epoch 242/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7011 - val_loss: 1.6271\n",
            "Epoch 243/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7561 - val_loss: 1.4913\n",
            "Epoch 244/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7150 - val_loss: 1.3837\n",
            "Epoch 245/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7175 - val_loss: 1.4061\n",
            "Epoch 246/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7273 - val_loss: 1.2973\n",
            "Epoch 247/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6942 - val_loss: 1.2433\n",
            "Epoch 248/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6966 - val_loss: 1.4274\n",
            "Epoch 249/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7034 - val_loss: 1.3706\n",
            "Epoch 250/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7574 - val_loss: 1.6006\n",
            "Epoch 251/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7191 - val_loss: 1.2668\n",
            "Epoch 252/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7044 - val_loss: 1.5645\n",
            "Epoch 253/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8265 - val_loss: 1.4568\n",
            "Epoch 254/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7060 - val_loss: 1.3002\n",
            "Epoch 255/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6844 - val_loss: 1.4847\n",
            "Epoch 256/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8385 - val_loss: 1.3222\n",
            "Epoch 257/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7171 - val_loss: 1.4637\n",
            "Epoch 258/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7210 - val_loss: 1.3544\n",
            "Epoch 259/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7323 - val_loss: 1.4292\n",
            "Epoch 260/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6785 - val_loss: 1.3272\n",
            "Epoch 261/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7362 - val_loss: 1.4487\n",
            "Epoch 262/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7126 - val_loss: 1.3284\n",
            "Epoch 263/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7407 - val_loss: 1.3980\n",
            "Epoch 264/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7055 - val_loss: 1.4278\n",
            "Epoch 265/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7520 - val_loss: 1.4088\n",
            "Epoch 266/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7759 - val_loss: 1.5853\n",
            "Epoch 267/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7235 - val_loss: 1.4364\n",
            "Epoch 268/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7835 - val_loss: 1.4235\n",
            "Epoch 269/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7525 - val_loss: 1.3575\n",
            "Epoch 270/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7078 - val_loss: 1.4384\n",
            "Epoch 271/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6880 - val_loss: 1.1913\n",
            "Epoch 272/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7274 - val_loss: 1.4290\n",
            "Epoch 273/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7299 - val_loss: 1.4393\n",
            "Epoch 274/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7638 - val_loss: 1.3159\n",
            "Epoch 275/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7008 - val_loss: 1.2822\n",
            "Epoch 276/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6851 - val_loss: 1.4099\n",
            "Epoch 277/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7342 - val_loss: 1.5446\n",
            "Epoch 278/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7226 - val_loss: 1.6038\n",
            "Epoch 279/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7600 - val_loss: 1.2915\n",
            "Epoch 280/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7197 - val_loss: 1.4616\n",
            "Epoch 281/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7437 - val_loss: 1.5181\n",
            "Epoch 282/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7342 - val_loss: 1.2300\n",
            "Epoch 283/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7611 - val_loss: 1.6058\n",
            "Epoch 284/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6772 - val_loss: 1.4478\n",
            "Epoch 285/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7761 - val_loss: 1.3557\n",
            "Epoch 286/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7702 - val_loss: 1.4527\n",
            "Epoch 287/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7290 - val_loss: 1.4349\n",
            "Epoch 288/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6960 - val_loss: 1.3921\n",
            "Epoch 289/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6887 - val_loss: 1.5465\n",
            "Epoch 290/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7238 - val_loss: 1.3792\n",
            "Epoch 291/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8167 - val_loss: 1.4695\n",
            "Epoch 292/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7054 - val_loss: 1.2278\n",
            "Epoch 293/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7317 - val_loss: 1.4062\n",
            "Epoch 294/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7841 - val_loss: 1.7000\n",
            "Epoch 295/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6847 - val_loss: 1.6517\n",
            "Epoch 296/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7034 - val_loss: 1.2649\n",
            "Epoch 297/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7014 - val_loss: 1.4151\n",
            "Epoch 298/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7003 - val_loss: 1.2670\n",
            "Epoch 299/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7029 - val_loss: 1.3978\n",
            "Epoch 300/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7095 - val_loss: 1.3006\n",
            "Epoch 301/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7139 - val_loss: 1.2545\n",
            "Epoch 302/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7700 - val_loss: 1.4202\n",
            "Epoch 303/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7452 - val_loss: 1.4631\n",
            "Epoch 304/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6687 - val_loss: 1.3857\n",
            "Epoch 305/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7427 - val_loss: 1.4743\n",
            "Epoch 306/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6887 - val_loss: 1.3109\n",
            "Epoch 307/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7776 - val_loss: 1.3943\n",
            "Epoch 308/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6645 - val_loss: 1.3960\n",
            "Epoch 309/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7195 - val_loss: 1.4270\n",
            "Epoch 310/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7978 - val_loss: 1.3412\n",
            "Epoch 311/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7306 - val_loss: 1.2761\n",
            "Epoch 312/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7292 - val_loss: 1.3859\n",
            "Epoch 313/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8292 - val_loss: 1.2452\n",
            "Epoch 314/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7263 - val_loss: 1.2071\n",
            "Epoch 315/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6533 - val_loss: 1.2886\n",
            "Epoch 316/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6538 - val_loss: 1.3673\n",
            "Epoch 317/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7097 - val_loss: 1.4177\n",
            "Epoch 318/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8378 - val_loss: 1.5926\n",
            "Epoch 319/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7046 - val_loss: 1.2852\n",
            "Epoch 320/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6737 - val_loss: 1.4134\n",
            "Epoch 321/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6359 - val_loss: 1.4885\n",
            "Epoch 322/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7283 - val_loss: 1.3171\n",
            "Epoch 323/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7007 - val_loss: 1.4395\n",
            "Epoch 324/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7013 - val_loss: 1.5166\n",
            "Epoch 325/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6989 - val_loss: 1.3158\n",
            "Epoch 326/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7404 - val_loss: 1.3684\n",
            "Epoch 327/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7386 - val_loss: 1.3819\n",
            "Epoch 328/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6983 - val_loss: 1.3471\n",
            "Epoch 329/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7138 - val_loss: 1.2704\n",
            "Epoch 330/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6799 - val_loss: 1.3771\n",
            "Epoch 331/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7286 - val_loss: 1.3828\n",
            "Epoch 332/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6784 - val_loss: 1.4197\n",
            "Epoch 333/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7035 - val_loss: 1.4825\n",
            "Epoch 334/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6864 - val_loss: 1.5063\n",
            "Epoch 335/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7260 - val_loss: 1.3426\n",
            "Epoch 336/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7267 - val_loss: 1.4857\n",
            "Epoch 337/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7136 - val_loss: 1.3771\n",
            "Epoch 338/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7094 - val_loss: 1.3326\n",
            "Epoch 339/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7721 - val_loss: 1.3902\n",
            "Epoch 340/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7220 - val_loss: 1.2526\n",
            "Epoch 341/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7186 - val_loss: 1.3545\n",
            "Epoch 342/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7299 - val_loss: 1.3615\n",
            "Epoch 343/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8390 - val_loss: 1.3066\n",
            "Epoch 344/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7851 - val_loss: 1.2529\n",
            "Epoch 345/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7991 - val_loss: 1.3453\n",
            "Epoch 346/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7700 - val_loss: 1.3344\n",
            "Epoch 347/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6853 - val_loss: 1.3309\n",
            "Epoch 348/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7473 - val_loss: 1.4504\n",
            "Epoch 349/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6236 - val_loss: 1.3621\n",
            "Epoch 350/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6969 - val_loss: 1.3433\n",
            "Epoch 351/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7031 - val_loss: 1.4386\n",
            "Epoch 352/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8118 - val_loss: 1.3258\n",
            "Epoch 353/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6949 - val_loss: 1.4978\n",
            "Epoch 354/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7526 - val_loss: 1.3472\n",
            "Epoch 355/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7062 - val_loss: 1.4025\n",
            "Epoch 356/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7141 - val_loss: 1.4436\n",
            "Epoch 357/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7372 - val_loss: 1.4228\n",
            "Epoch 358/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7336 - val_loss: 1.2507\n",
            "Epoch 359/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7412 - val_loss: 1.3188\n",
            "Epoch 360/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7447 - val_loss: 1.3910\n",
            "Epoch 361/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6450 - val_loss: 1.3429\n",
            "Epoch 362/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6814 - val_loss: 1.3121\n",
            "Epoch 363/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6543 - val_loss: 1.4819\n",
            "Epoch 364/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7408 - val_loss: 1.3243\n",
            "Epoch 365/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7474 - val_loss: 1.4699\n",
            "Epoch 366/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7237 - val_loss: 1.2362\n",
            "Epoch 367/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7153 - val_loss: 1.1923\n",
            "Epoch 368/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7174 - val_loss: 1.3406\n",
            "Epoch 369/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9205 - val_loss: 1.3331\n",
            "Epoch 370/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6561 - val_loss: 1.4067\n",
            "Epoch 371/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8045 - val_loss: 1.3426\n",
            "Epoch 372/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6994 - val_loss: 1.4377\n",
            "Epoch 373/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7022 - val_loss: 1.4880\n",
            "Epoch 374/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7710 - val_loss: 1.4722\n",
            "Epoch 375/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7637 - val_loss: 1.5069\n",
            "Epoch 376/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7055 - val_loss: 1.4693\n",
            "Epoch 377/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6789 - val_loss: 1.5458\n",
            "Epoch 378/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7193 - val_loss: 1.3563\n",
            "Epoch 379/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6365 - val_loss: 1.3282\n",
            "Epoch 380/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7387 - val_loss: 1.3727\n",
            "Epoch 381/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7451 - val_loss: 1.3195\n",
            "Epoch 382/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7507 - val_loss: 1.5612\n",
            "Epoch 383/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7341 - val_loss: 1.2838\n",
            "Epoch 384/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6918 - val_loss: 1.3748\n",
            "Epoch 385/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6841 - val_loss: 1.5232\n",
            "Epoch 386/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7510 - val_loss: 1.3847\n",
            "Epoch 387/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6954 - val_loss: 1.3870\n",
            "Epoch 388/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7210 - val_loss: 1.3635\n",
            "Epoch 389/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6831 - val_loss: 1.4867\n",
            "Epoch 390/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7058 - val_loss: 1.3632\n",
            "Epoch 391/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7199 - val_loss: 1.4428\n",
            "Epoch 392/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6950 - val_loss: 1.4728\n",
            "Epoch 393/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7871 - val_loss: 1.5126\n",
            "Epoch 394/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7772 - val_loss: 1.4032\n",
            "Epoch 395/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6761 - val_loss: 1.3870\n",
            "Epoch 396/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7162 - val_loss: 1.6519\n",
            "Epoch 397/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6650 - val_loss: 1.4208\n",
            "Epoch 398/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6820 - val_loss: 1.3418\n",
            "Epoch 399/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7008 - val_loss: 1.3213\n",
            "Epoch 400/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6954 - val_loss: 1.5197\n",
            "Epoch 401/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6405 - val_loss: 1.4924\n",
            "Epoch 402/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7455 - val_loss: 1.5273\n",
            "Epoch 403/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6943 - val_loss: 1.4938\n",
            "Epoch 404/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6954 - val_loss: 1.5099\n",
            "Epoch 405/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6466 - val_loss: 1.4979\n",
            "Epoch 406/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7481 - val_loss: 1.3711\n",
            "Epoch 407/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8015 - val_loss: 1.5274\n",
            "Epoch 408/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6616 - val_loss: 1.2960\n",
            "Epoch 409/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6940 - val_loss: 1.4312\n",
            "Epoch 410/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6637 - val_loss: 1.4036\n",
            "Epoch 411/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7439 - val_loss: 1.7682\n",
            "Epoch 412/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6614 - val_loss: 1.7750\n",
            "Epoch 413/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6955 - val_loss: 1.5096\n",
            "Epoch 414/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7795 - val_loss: 1.4206\n",
            "Epoch 415/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7346 - val_loss: 1.4754\n",
            "Epoch 416/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7091 - val_loss: 1.3302\n",
            "Epoch 417/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6887 - val_loss: 1.6296\n",
            "Epoch 418/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6871 - val_loss: 1.4320\n",
            "Epoch 419/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6461 - val_loss: 1.3836\n",
            "Epoch 420/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6565 - val_loss: 1.4532\n",
            "Epoch 421/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7437 - val_loss: 1.4870\n",
            "Epoch 422/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7726 - val_loss: 1.5484\n",
            "Epoch 423/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6477 - val_loss: 1.5380\n",
            "Epoch 424/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8020 - val_loss: 1.3552\n",
            "Epoch 425/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7378 - val_loss: 1.3533\n",
            "Epoch 426/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7516 - val_loss: 1.4482\n",
            "Epoch 427/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7384 - val_loss: 1.4752\n",
            "Epoch 428/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7213 - val_loss: 1.4788\n",
            "Epoch 429/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6621 - val_loss: 1.4164\n",
            "Epoch 430/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6649 - val_loss: 1.5803\n",
            "Epoch 431/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7497 - val_loss: 1.3371\n",
            "Epoch 432/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7290 - val_loss: 1.9653\n",
            "Epoch 433/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7042 - val_loss: 1.4558\n",
            "Epoch 434/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6709 - val_loss: 1.5691\n",
            "Epoch 435/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7308 - val_loss: 1.5777\n",
            "Epoch 436/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6789 - val_loss: 1.3892\n",
            "Epoch 437/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6516 - val_loss: 1.4849\n",
            "Epoch 438/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7590 - val_loss: 1.4910\n",
            "Epoch 439/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7045 - val_loss: 1.6073\n",
            "Epoch 440/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6998 - val_loss: 1.4534\n",
            "Epoch 441/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7011 - val_loss: 1.6484\n",
            "Epoch 442/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7260 - val_loss: 1.4650\n",
            "Epoch 443/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6943 - val_loss: 1.7240\n",
            "Epoch 444/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6736 - val_loss: 1.3824\n",
            "Epoch 445/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7036 - val_loss: 1.3619\n",
            "Epoch 446/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7306 - val_loss: 1.6541\n",
            "Epoch 447/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6569 - val_loss: 1.3855\n",
            "Epoch 448/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7319 - val_loss: 1.4956\n",
            "Epoch 449/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6892 - val_loss: 1.6030\n",
            "Epoch 450/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7452 - val_loss: 1.5710\n",
            "Epoch 451/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7523 - val_loss: 1.5388\n",
            "Epoch 452/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7261 - val_loss: 1.7449\n",
            "Epoch 453/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7918 - val_loss: 1.4158\n",
            "Epoch 454/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7253 - val_loss: 1.4733\n",
            "Epoch 455/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6783 - val_loss: 1.5334\n",
            "Epoch 456/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7026 - val_loss: 1.5023\n",
            "Epoch 457/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7581 - val_loss: 1.4248\n",
            "Epoch 458/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6727 - val_loss: 1.5635\n",
            "Epoch 459/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7535 - val_loss: 1.3111\n",
            "Epoch 460/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7274 - val_loss: 1.3650\n",
            "Epoch 461/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7473 - val_loss: 1.4752\n",
            "Epoch 462/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7071 - val_loss: 1.5677\n",
            "Epoch 463/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7985 - val_loss: 1.4401\n",
            "Epoch 464/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6748 - val_loss: 1.4635\n",
            "Epoch 465/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7223 - val_loss: 1.4219\n",
            "Epoch 466/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7434 - val_loss: 1.4478\n",
            "Epoch 467/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7249 - val_loss: 1.5530\n",
            "Epoch 468/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7475 - val_loss: 1.4894\n",
            "Epoch 469/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7138 - val_loss: 1.6264\n",
            "Epoch 470/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7065 - val_loss: 1.5386\n",
            "Epoch 471/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7393 - val_loss: 1.4014\n",
            "Epoch 472/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7659 - val_loss: 1.4391\n",
            "Epoch 473/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6722 - val_loss: 1.5589\n",
            "Epoch 474/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6880 - val_loss: 1.6069\n",
            "Epoch 475/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6888 - val_loss: 1.4064\n",
            "Epoch 476/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6885 - val_loss: 1.5535\n",
            "Epoch 477/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7132 - val_loss: 1.5185\n",
            "Epoch 478/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7126 - val_loss: 1.5099\n",
            "Epoch 479/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6902 - val_loss: 1.4670\n",
            "Epoch 480/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7214 - val_loss: 1.7891\n",
            "Epoch 481/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6891 - val_loss: 1.6582\n",
            "Epoch 482/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6562 - val_loss: 1.8676\n",
            "Epoch 483/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7063 - val_loss: 1.3571\n",
            "Epoch 484/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6819 - val_loss: 1.4339\n",
            "Epoch 485/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6778 - val_loss: 1.4633\n",
            "Epoch 486/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7190 - val_loss: 1.5418\n",
            "Epoch 487/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7166 - val_loss: 1.5347\n",
            "Epoch 488/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6923 - val_loss: 1.5054\n",
            "Epoch 489/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7241 - val_loss: 1.5103\n",
            "Epoch 490/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6410 - val_loss: 2.2655\n",
            "Epoch 491/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7499 - val_loss: 1.6027\n",
            "Epoch 492/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6783 - val_loss: 1.5320\n",
            "Epoch 493/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6924 - val_loss: 1.6195\n",
            "Epoch 494/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6974 - val_loss: 1.6605\n",
            "Epoch 495/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7101 - val_loss: 1.6658\n",
            "Epoch 496/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6853 - val_loss: 1.3583\n",
            "Epoch 497/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6658 - val_loss: 1.5028\n",
            "Epoch 498/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7471 - val_loss: 1.4850\n",
            "Epoch 499/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7462 - val_loss: 1.3378\n",
            "Epoch 500/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7687 - val_loss: 1.5519\n",
            "Epoch 501/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6775 - val_loss: 1.5532\n",
            "Epoch 502/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6839 - val_loss: 1.8893\n",
            "Epoch 503/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7546 - val_loss: 1.5962\n",
            "Epoch 504/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7464 - val_loss: 1.5542\n",
            "Epoch 505/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6837 - val_loss: 1.4248\n",
            "Epoch 506/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6866 - val_loss: 1.7086\n",
            "Epoch 507/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6673 - val_loss: 1.7097\n",
            "Epoch 508/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6643 - val_loss: 1.8654\n",
            "Epoch 509/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7472 - val_loss: 1.6252\n",
            "Epoch 510/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6826 - val_loss: 1.7372\n",
            "Epoch 511/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7013 - val_loss: 1.6551\n",
            "Epoch 512/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7549 - val_loss: 1.5545\n",
            "Epoch 513/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7353 - val_loss: 1.5295\n",
            "Epoch 514/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7342 - val_loss: 1.6197\n",
            "Epoch 515/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8772 - val_loss: 1.6186\n",
            "Epoch 516/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6673 - val_loss: 1.4612\n",
            "Epoch 517/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7450 - val_loss: 1.5779\n",
            "Epoch 518/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7215 - val_loss: 1.7752\n",
            "Epoch 519/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7203 - val_loss: 1.5545\n",
            "Epoch 520/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6844 - val_loss: 1.6062\n",
            "Epoch 521/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7050 - val_loss: 1.6503\n",
            "Epoch 522/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6761 - val_loss: 1.4495\n",
            "Epoch 523/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6892 - val_loss: 1.4182\n",
            "Epoch 524/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6982 - val_loss: 1.9246\n",
            "Epoch 525/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7389 - val_loss: 1.6512\n",
            "Epoch 526/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7268 - val_loss: 1.6967\n",
            "Epoch 527/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6603 - val_loss: 1.7230\n",
            "Epoch 528/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7984 - val_loss: 1.7106\n",
            "Epoch 529/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7822 - val_loss: 1.5484\n",
            "Epoch 530/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7055 - val_loss: 1.5314\n",
            "Epoch 531/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6736 - val_loss: 1.6290\n",
            "Epoch 532/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7278 - val_loss: 1.6209\n",
            "Epoch 533/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7676 - val_loss: 1.9962\n",
            "Epoch 534/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7507 - val_loss: 1.5772\n",
            "Epoch 535/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6186 - val_loss: 1.6037\n",
            "Epoch 536/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6909 - val_loss: 1.6650\n",
            "Epoch 537/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6926 - val_loss: 1.6136\n",
            "Epoch 538/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6913 - val_loss: 1.6821\n",
            "Epoch 539/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7521 - val_loss: 1.5497\n",
            "Epoch 540/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7380 - val_loss: 1.4831\n",
            "Epoch 541/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7374 - val_loss: 1.6045\n",
            "Epoch 542/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6302 - val_loss: 1.7494\n",
            "Epoch 543/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6962 - val_loss: 1.8439\n",
            "Epoch 544/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6927 - val_loss: 1.7301\n",
            "Epoch 545/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6608 - val_loss: 1.5914\n",
            "Epoch 546/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7470 - val_loss: 1.7456\n",
            "Epoch 547/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6380 - val_loss: 1.5514\n",
            "Epoch 548/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7017 - val_loss: 1.6231\n",
            "Epoch 549/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7278 - val_loss: 1.7627\n",
            "Epoch 550/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6148 - val_loss: 2.2416\n",
            "Epoch 551/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6846 - val_loss: 1.6481\n",
            "Epoch 552/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6442 - val_loss: 1.7006\n",
            "Epoch 553/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6834 - val_loss: 1.4550\n",
            "Epoch 554/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6879 - val_loss: 1.7102\n",
            "Epoch 555/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6333 - val_loss: 1.6168\n",
            "Epoch 556/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7211 - val_loss: 1.7657\n",
            "Epoch 557/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7021 - val_loss: 1.6275\n",
            "Epoch 558/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7350 - val_loss: 1.7958\n",
            "Epoch 559/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7133 - val_loss: 1.8296\n",
            "Epoch 560/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6802 - val_loss: 1.7858\n",
            "Epoch 561/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6924 - val_loss: 1.9276\n",
            "Epoch 562/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6453 - val_loss: 1.3702\n",
            "Epoch 563/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7363 - val_loss: 1.8541\n",
            "Epoch 564/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6682 - val_loss: 1.7492\n",
            "Epoch 565/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6573 - val_loss: 1.5809\n",
            "Epoch 566/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7002 - val_loss: 1.5408\n",
            "Epoch 567/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6948 - val_loss: 1.7609\n",
            "Epoch 568/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7336 - val_loss: 1.6338\n",
            "Epoch 569/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7655 - val_loss: 1.9284\n",
            "Epoch 570/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6992 - val_loss: 1.7439\n",
            "Epoch 571/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6840 - val_loss: 1.4621\n",
            "Epoch 572/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7264 - val_loss: 1.6704\n",
            "Epoch 573/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6643 - val_loss: 1.4996\n",
            "Epoch 574/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7380 - val_loss: 1.6236\n",
            "Epoch 575/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7368 - val_loss: 1.4883\n",
            "Epoch 576/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7548 - val_loss: 1.8984\n",
            "Epoch 577/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7297 - val_loss: 1.5996\n",
            "Epoch 578/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7289 - val_loss: 1.6300\n",
            "Epoch 579/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6809 - val_loss: 1.5789\n",
            "Epoch 580/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7132 - val_loss: 1.6959\n",
            "Epoch 581/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6912 - val_loss: 1.7322\n",
            "Epoch 582/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6634 - val_loss: 1.9060\n",
            "Epoch 583/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6369 - val_loss: 1.4955\n",
            "Epoch 584/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6948 - val_loss: 1.5715\n",
            "Epoch 585/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7114 - val_loss: 1.8159\n",
            "Epoch 586/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7046 - val_loss: 1.8033\n",
            "Epoch 587/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7185 - val_loss: 1.7042\n",
            "Epoch 588/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7419 - val_loss: 1.7059\n",
            "Epoch 589/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6609 - val_loss: 1.7290\n",
            "Epoch 590/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7137 - val_loss: 1.6080\n",
            "Epoch 591/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6857 - val_loss: 1.4621\n",
            "Epoch 592/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7806 - val_loss: 1.6957\n",
            "Epoch 593/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6979 - val_loss: 1.7652\n",
            "Epoch 594/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6463 - val_loss: 1.6047\n",
            "Epoch 595/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6775 - val_loss: 1.9040\n",
            "Epoch 596/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7436 - val_loss: 1.6573\n",
            "Epoch 597/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7082 - val_loss: 1.6464\n",
            "Epoch 598/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6496 - val_loss: 1.4985\n",
            "Epoch 599/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7113 - val_loss: 2.4753\n",
            "Epoch 600/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6362 - val_loss: 1.6766\n",
            "Epoch 601/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6451 - val_loss: 1.6688\n",
            "Epoch 602/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7304 - val_loss: 1.5068\n",
            "Epoch 603/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6912 - val_loss: 1.6534\n",
            "Epoch 604/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6589 - val_loss: 1.6234\n",
            "Epoch 605/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7345 - val_loss: 1.5805\n",
            "Epoch 606/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6522 - val_loss: 1.7327\n",
            "Epoch 607/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7905 - val_loss: 1.7022\n",
            "Epoch 608/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6832 - val_loss: 1.5886\n",
            "Epoch 609/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7751 - val_loss: 1.5659\n",
            "Epoch 610/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6944 - val_loss: 1.6078\n",
            "Epoch 611/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7199 - val_loss: 1.8238\n",
            "Epoch 612/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7433 - val_loss: 1.4948\n",
            "Epoch 613/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6272 - val_loss: 1.9422\n",
            "Epoch 614/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7159 - val_loss: 1.6797\n",
            "Epoch 615/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7430 - val_loss: 1.6704\n",
            "Epoch 616/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7229 - val_loss: 1.5322\n",
            "Epoch 617/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6950 - val_loss: 1.7705\n",
            "Epoch 618/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7450 - val_loss: 1.7369\n",
            "Epoch 619/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8556 - val_loss: 1.6121\n",
            "Epoch 620/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6830 - val_loss: 1.6449\n",
            "Epoch 621/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6515 - val_loss: 1.7391\n",
            "Epoch 622/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6855 - val_loss: 1.9699\n",
            "Epoch 623/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8540 - val_loss: 1.6258\n",
            "Epoch 624/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6715 - val_loss: 1.5057\n",
            "Epoch 625/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7168 - val_loss: 1.5458\n",
            "Epoch 626/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6674 - val_loss: 1.6576\n",
            "Epoch 627/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6965 - val_loss: 1.5619\n",
            "Epoch 628/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6731 - val_loss: 1.5643\n",
            "Epoch 629/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7271 - val_loss: 1.6398\n",
            "Epoch 630/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7252 - val_loss: 1.7377\n",
            "Epoch 631/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6938 - val_loss: 1.4941\n",
            "Epoch 632/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6932 - val_loss: 1.7396\n",
            "Epoch 633/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7347 - val_loss: 1.6255\n",
            "Epoch 634/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7019 - val_loss: 1.7754\n",
            "Epoch 635/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6747 - val_loss: 1.8586\n",
            "Epoch 636/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7060 - val_loss: 1.8397\n",
            "Epoch 637/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7038 - val_loss: 1.6157\n",
            "Epoch 638/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6797 - val_loss: 1.4264\n",
            "Epoch 639/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6913 - val_loss: 1.7206\n",
            "Epoch 640/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6955 - val_loss: 2.0358\n",
            "Epoch 641/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6954 - val_loss: 1.6761\n",
            "Epoch 642/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7832 - val_loss: 1.4759\n",
            "Epoch 643/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6786 - val_loss: 1.5227\n",
            "Epoch 644/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6746 - val_loss: 1.9848\n",
            "Epoch 645/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6392 - val_loss: 1.8454\n",
            "Epoch 646/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7575 - val_loss: 1.4896\n",
            "Epoch 647/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6880 - val_loss: 1.6304\n",
            "Epoch 648/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6483 - val_loss: 1.5341\n",
            "Epoch 649/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7449 - val_loss: 1.7897\n",
            "Epoch 650/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6811 - val_loss: 1.6689\n",
            "Epoch 651/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7052 - val_loss: 1.7866\n",
            "Epoch 652/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6968 - val_loss: 1.6612\n",
            "Epoch 653/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7095 - val_loss: 1.7790\n",
            "Epoch 654/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7211 - val_loss: 1.6399\n",
            "Epoch 655/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6865 - val_loss: 1.7534\n",
            "Epoch 656/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6764 - val_loss: 1.6444\n",
            "Epoch 657/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6908 - val_loss: 2.0819\n",
            "Epoch 658/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6946 - val_loss: 1.7243\n",
            "Epoch 659/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7292 - val_loss: 1.5737\n",
            "Epoch 660/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6271 - val_loss: 1.6832\n",
            "Epoch 661/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6941 - val_loss: 1.7455\n",
            "Epoch 662/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7617 - val_loss: 1.7275\n",
            "Epoch 663/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6892 - val_loss: 1.7813\n",
            "Epoch 664/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6795 - val_loss: 1.7867\n",
            "Epoch 665/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6727 - val_loss: 1.5639\n",
            "Epoch 666/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6541 - val_loss: 1.7315\n",
            "Epoch 667/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6704 - val_loss: 1.6128\n",
            "Epoch 668/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6559 - val_loss: 1.6430\n",
            "Epoch 669/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7054 - val_loss: 1.9833\n",
            "Epoch 670/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6829 - val_loss: 1.7036\n",
            "Epoch 671/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6615 - val_loss: 1.7167\n",
            "Epoch 672/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6480 - val_loss: 1.7182\n",
            "Epoch 673/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6961 - val_loss: 1.6963\n",
            "Epoch 674/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7370 - val_loss: 1.5284\n",
            "Epoch 675/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6765 - val_loss: 1.6612\n",
            "Epoch 676/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7004 - val_loss: 1.5356\n",
            "Epoch 677/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6186 - val_loss: 1.5671\n",
            "Epoch 678/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6651 - val_loss: 1.6956\n",
            "Epoch 679/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7329 - val_loss: 1.9509\n",
            "Epoch 680/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6946 - val_loss: 1.6643\n",
            "Epoch 681/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6944 - val_loss: 1.7439\n",
            "Epoch 682/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7076 - val_loss: 1.7795\n",
            "Epoch 683/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7127 - val_loss: 1.8163\n",
            "Epoch 684/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7426 - val_loss: 1.7796\n",
            "Epoch 685/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6522 - val_loss: 1.6470\n",
            "Epoch 686/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7238 - val_loss: 1.8036\n",
            "Epoch 687/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6556 - val_loss: 1.7704\n",
            "Epoch 688/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7636 - val_loss: 1.6087\n",
            "Epoch 689/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6369 - val_loss: 1.7486\n",
            "Epoch 690/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6999 - val_loss: 1.5727\n",
            "Epoch 691/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6771 - val_loss: 1.6469\n",
            "Epoch 692/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7170 - val_loss: 1.5904\n",
            "Epoch 693/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6323 - val_loss: 2.0108\n",
            "Epoch 694/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7010 - val_loss: 1.6642\n",
            "Epoch 695/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7024 - val_loss: 1.9426\n",
            "Epoch 696/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6799 - val_loss: 1.6713\n",
            "Epoch 697/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6953 - val_loss: 1.6128\n",
            "Epoch 698/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7930 - val_loss: 2.0485\n",
            "Epoch 699/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6858 - val_loss: 1.8104\n",
            "Epoch 700/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7343 - val_loss: 1.8352\n",
            "Epoch 701/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7641 - val_loss: 1.8775\n",
            "Epoch 702/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7450 - val_loss: 1.9330\n",
            "Epoch 703/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6893 - val_loss: 1.5800\n",
            "Epoch 704/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7453 - val_loss: 1.5162\n",
            "Epoch 705/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7095 - val_loss: 1.7571\n",
            "Epoch 706/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6777 - val_loss: 1.7909\n",
            "Epoch 707/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6866 - val_loss: 1.6553\n",
            "Epoch 708/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6965 - val_loss: 1.6893\n",
            "Epoch 709/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7087 - val_loss: 1.6938\n",
            "Epoch 710/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6963 - val_loss: 1.6771\n",
            "Epoch 711/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6881 - val_loss: 1.5758\n",
            "Epoch 712/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7590 - val_loss: 1.6748\n",
            "Epoch 713/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7631 - val_loss: 1.6483\n",
            "Epoch 714/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6533 - val_loss: 1.5388\n",
            "Epoch 715/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7079 - val_loss: 1.7947\n",
            "Epoch 716/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7319 - val_loss: 1.7182\n",
            "Epoch 717/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7209 - val_loss: 1.7604\n",
            "Epoch 718/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6498 - val_loss: 1.7189\n",
            "Epoch 719/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6729 - val_loss: 1.6843\n",
            "Epoch 720/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6837 - val_loss: 1.7152\n",
            "Epoch 721/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6924 - val_loss: 1.7709\n",
            "Epoch 722/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7454 - val_loss: 1.7866\n",
            "Epoch 723/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6754 - val_loss: 1.7777\n",
            "Epoch 724/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7466 - val_loss: 1.6941\n",
            "Epoch 725/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7190 - val_loss: 1.7023\n",
            "Epoch 726/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7365 - val_loss: 1.7482\n",
            "Epoch 727/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6632 - val_loss: 1.7091\n",
            "Epoch 728/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6962 - val_loss: 1.7895\n",
            "Epoch 729/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7255 - val_loss: 1.8054\n",
            "Epoch 730/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7167 - val_loss: 1.6651\n",
            "Epoch 731/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6898 - val_loss: 1.6213\n",
            "Epoch 732/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7667 - val_loss: 1.6514\n",
            "Epoch 733/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7303 - val_loss: 2.0082\n",
            "Epoch 734/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7006 - val_loss: 1.8324\n",
            "Epoch 735/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6715 - val_loss: 1.6805\n",
            "Epoch 736/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7742 - val_loss: 1.9691\n",
            "Epoch 737/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6863 - val_loss: 1.8047\n",
            "Epoch 738/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7248 - val_loss: 1.7322\n",
            "Epoch 739/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7160 - val_loss: 1.9879\n",
            "Epoch 740/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6714 - val_loss: 1.9398\n",
            "Epoch 741/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6993 - val_loss: 1.4504\n",
            "Epoch 742/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6668 - val_loss: 1.7962\n",
            "Epoch 743/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7109 - val_loss: 1.9540\n",
            "Epoch 744/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6814 - val_loss: 2.1335\n",
            "Epoch 745/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6525 - val_loss: 1.9365\n",
            "Epoch 746/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7649 - val_loss: 1.7298\n",
            "Epoch 747/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6444 - val_loss: 1.7489\n",
            "Epoch 748/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7093 - val_loss: 1.8685\n",
            "Epoch 749/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7169 - val_loss: 1.6723\n",
            "Epoch 750/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6646 - val_loss: 1.8897\n",
            "Epoch 751/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6821 - val_loss: 1.7745\n",
            "Epoch 752/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6685 - val_loss: 1.8059\n",
            "Epoch 753/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7297 - val_loss: 1.8469\n",
            "Epoch 754/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7485 - val_loss: 1.6960\n",
            "Epoch 755/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7054 - val_loss: 1.7302\n",
            "Epoch 756/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7234 - val_loss: 2.0450\n",
            "Epoch 757/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6733 - val_loss: 1.6886\n",
            "Epoch 758/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7051 - val_loss: 1.8239\n",
            "Epoch 759/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7037 - val_loss: 1.7425\n",
            "Epoch 760/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6824 - val_loss: 1.7173\n",
            "Epoch 761/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7113 - val_loss: 2.0073\n",
            "Epoch 762/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6312 - val_loss: 1.9579\n",
            "Epoch 763/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7082 - val_loss: 1.6728\n",
            "Epoch 764/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6931 - val_loss: 1.7263\n",
            "Epoch 765/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7465 - val_loss: 1.6715\n",
            "Epoch 766/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6657 - val_loss: 1.7037\n",
            "Epoch 767/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6947 - val_loss: 1.9793\n",
            "Epoch 768/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6949 - val_loss: 1.7575\n",
            "Epoch 769/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7222 - val_loss: 1.5125\n",
            "Epoch 770/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7114 - val_loss: 2.2764\n",
            "Epoch 771/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7816 - val_loss: 1.7626\n",
            "Epoch 772/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6931 - val_loss: 1.9799\n",
            "Epoch 773/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6877 - val_loss: 1.5090\n",
            "Epoch 774/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6510 - val_loss: 2.1215\n",
            "Epoch 775/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7075 - val_loss: 1.7602\n",
            "Epoch 776/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7089 - val_loss: 1.8499\n",
            "Epoch 777/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6767 - val_loss: 1.8277\n",
            "Epoch 778/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7236 - val_loss: 1.5459\n",
            "Epoch 779/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7206 - val_loss: 1.7193\n",
            "Epoch 780/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6950 - val_loss: 1.8701\n",
            "Epoch 781/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7006 - val_loss: 1.9643\n",
            "Epoch 782/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6899 - val_loss: 1.9391\n",
            "Epoch 783/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6936 - val_loss: 1.5605\n",
            "Epoch 784/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6737 - val_loss: 1.6113\n",
            "Epoch 785/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6866 - val_loss: 1.8733\n",
            "Epoch 786/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6802 - val_loss: 1.6736\n",
            "Epoch 787/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7275 - val_loss: 1.8868\n",
            "Epoch 788/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7193 - val_loss: 2.0651\n",
            "Epoch 789/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7678 - val_loss: 1.7380\n",
            "Epoch 790/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6784 - val_loss: 1.7948\n",
            "Epoch 791/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7751 - val_loss: 1.9438\n",
            "Epoch 792/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7327 - val_loss: 1.6196\n",
            "Epoch 793/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7004 - val_loss: 2.0816\n",
            "Epoch 794/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6697 - val_loss: 1.5433\n",
            "Epoch 795/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6945 - val_loss: 1.6604\n",
            "Epoch 796/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7349 - val_loss: 1.6215\n",
            "Epoch 797/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6670 - val_loss: 1.6510\n",
            "Epoch 798/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7095 - val_loss: 1.4514\n",
            "Epoch 799/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6817 - val_loss: 1.7875\n",
            "Epoch 800/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6972 - val_loss: 1.9161\n",
            "Epoch 801/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6985 - val_loss: 1.6634\n",
            "Epoch 802/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6892 - val_loss: 1.8243\n",
            "Epoch 803/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7303 - val_loss: 1.7085\n",
            "Epoch 804/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6627 - val_loss: 1.7458\n",
            "Epoch 805/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6986 - val_loss: 1.8971\n",
            "Epoch 806/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7072 - val_loss: 1.7146\n",
            "Epoch 807/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7215 - val_loss: 1.7653\n",
            "Epoch 808/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6736 - val_loss: 1.6868\n",
            "Epoch 809/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7111 - val_loss: 1.8584\n",
            "Epoch 810/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6690 - val_loss: 1.7713\n",
            "Epoch 811/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7059 - val_loss: 1.8082\n",
            "Epoch 812/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6958 - val_loss: 1.8761\n",
            "Epoch 813/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6804 - val_loss: 1.7106\n",
            "Epoch 814/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6966 - val_loss: 1.8639\n",
            "Epoch 815/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7051 - val_loss: 2.1753\n",
            "Epoch 816/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6564 - val_loss: 1.6918\n",
            "Epoch 817/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7086 - val_loss: 1.9629\n",
            "Epoch 818/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6506 - val_loss: 1.7363\n",
            "Epoch 819/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7449 - val_loss: 1.6936\n",
            "Epoch 820/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7215 - val_loss: 1.7627\n",
            "Epoch 821/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7165 - val_loss: 1.7909\n",
            "Epoch 822/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7222 - val_loss: 1.6000\n",
            "Epoch 823/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6777 - val_loss: 1.6627\n",
            "Epoch 824/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7584 - val_loss: 1.7756\n",
            "Epoch 825/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6525 - val_loss: 1.7286\n",
            "Epoch 826/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6794 - val_loss: 1.9771\n",
            "Epoch 827/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6863 - val_loss: 1.7080\n",
            "Epoch 828/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6503 - val_loss: 1.6662\n",
            "Epoch 829/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7007 - val_loss: 1.6185\n",
            "Epoch 830/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6944 - val_loss: 1.6483\n",
            "Epoch 831/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7237 - val_loss: 1.8334\n",
            "Epoch 832/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7571 - val_loss: 2.1347\n",
            "Epoch 833/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6815 - val_loss: 1.6444\n",
            "Epoch 834/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7077 - val_loss: 1.7775\n",
            "Epoch 835/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7532 - val_loss: 1.7113\n",
            "Epoch 836/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7733 - val_loss: 1.6414\n",
            "Epoch 837/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6953 - val_loss: 1.8904\n",
            "Epoch 838/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6645 - val_loss: 1.7200\n",
            "Epoch 839/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6719 - val_loss: 1.7624\n",
            "Epoch 840/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6475 - val_loss: 1.6586\n",
            "Epoch 841/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7122 - val_loss: 2.0049\n",
            "Epoch 842/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7192 - val_loss: 1.6928\n",
            "Epoch 843/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7382 - val_loss: 2.0642\n",
            "Epoch 844/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6176 - val_loss: 1.7139\n",
            "Epoch 845/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6438 - val_loss: 1.8620\n",
            "Epoch 846/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7259 - val_loss: 1.8370\n",
            "Epoch 847/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6939 - val_loss: 1.6550\n",
            "Epoch 848/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7909 - val_loss: 1.5964\n",
            "Epoch 849/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6822 - val_loss: 1.7682\n",
            "Epoch 850/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6625 - val_loss: 1.7401\n",
            "Epoch 851/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6887 - val_loss: 1.8452\n",
            "Epoch 852/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7083 - val_loss: 1.6884\n",
            "Epoch 853/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6790 - val_loss: 1.7653\n",
            "Epoch 854/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6602 - val_loss: 1.5597\n",
            "Epoch 855/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6919 - val_loss: 1.7095\n",
            "Epoch 856/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6655 - val_loss: 1.6691\n",
            "Epoch 857/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7202 - val_loss: 1.7589\n",
            "Epoch 858/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7445 - val_loss: 1.5427\n",
            "Epoch 859/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7394 - val_loss: 1.8916\n",
            "Epoch 860/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6927 - val_loss: 1.9649\n",
            "Epoch 861/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6560 - val_loss: 1.6171\n",
            "Epoch 862/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7165 - val_loss: 1.6042\n",
            "Epoch 863/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6833 - val_loss: 1.6565\n",
            "Epoch 864/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7315 - val_loss: 1.6049\n",
            "Epoch 865/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7454 - val_loss: 1.5468\n",
            "Epoch 866/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7441 - val_loss: 1.8068\n",
            "Epoch 867/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6716 - val_loss: 1.9800\n",
            "Epoch 868/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6640 - val_loss: 1.5844\n",
            "Epoch 869/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7221 - val_loss: 1.7467\n",
            "Epoch 870/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7515 - val_loss: 1.6445\n",
            "Epoch 871/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7087 - val_loss: 1.7971\n",
            "Epoch 872/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6647 - val_loss: 1.6593\n",
            "Epoch 873/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7094 - val_loss: 2.2452\n",
            "Epoch 874/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6931 - val_loss: 1.9150\n",
            "Epoch 875/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7031 - val_loss: 1.7192\n",
            "Epoch 876/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6448 - val_loss: 1.7061\n",
            "Epoch 877/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6612 - val_loss: 1.8324\n",
            "Epoch 878/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7789 - val_loss: 2.0316\n",
            "Epoch 879/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6466 - val_loss: 1.7800\n",
            "Epoch 880/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6894 - val_loss: 1.8985\n",
            "Epoch 881/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6357 - val_loss: 1.8979\n",
            "Epoch 882/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7212 - val_loss: 1.7131\n",
            "Epoch 883/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6784 - val_loss: 1.9118\n",
            "Epoch 884/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6976 - val_loss: 1.5849\n",
            "Epoch 885/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6315 - val_loss: 1.8336\n",
            "Epoch 886/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6823 - val_loss: 1.6732\n",
            "Epoch 887/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6912 - val_loss: 2.0164\n",
            "Epoch 888/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6778 - val_loss: 1.8601\n",
            "Epoch 889/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6535 - val_loss: 1.6627\n",
            "Epoch 890/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7452 - val_loss: 2.0913\n",
            "Epoch 891/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6763 - val_loss: 1.8752\n",
            "Epoch 892/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6666 - val_loss: 1.9376\n",
            "Epoch 893/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7590 - val_loss: 1.9847\n",
            "Epoch 894/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6529 - val_loss: 1.8424\n",
            "Epoch 895/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6657 - val_loss: 1.8303\n",
            "Epoch 896/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6803 - val_loss: 1.6933\n",
            "Epoch 897/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7085 - val_loss: 2.0370\n",
            "Epoch 898/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6542 - val_loss: 1.7838\n",
            "Epoch 899/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6894 - val_loss: 1.6838\n",
            "Epoch 900/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6914 - val_loss: 1.6729\n",
            "Epoch 901/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7778 - val_loss: 1.9662\n",
            "Epoch 902/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6779 - val_loss: 1.8678\n",
            "Epoch 903/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7068 - val_loss: 1.6455\n",
            "Epoch 904/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6698 - val_loss: 2.0264\n",
            "Epoch 905/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6438 - val_loss: 1.9156\n",
            "Epoch 906/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6867 - val_loss: 1.7108\n",
            "Epoch 907/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7720 - val_loss: 1.9015\n",
            "Epoch 908/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7371 - val_loss: 1.6153\n",
            "Epoch 909/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6385 - val_loss: 2.0912\n",
            "Epoch 910/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6794 - val_loss: 1.8826\n",
            "Epoch 911/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6521 - val_loss: 1.7262\n",
            "Epoch 912/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7618 - val_loss: 1.8456\n",
            "Epoch 913/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6407 - val_loss: 1.9898\n",
            "Epoch 914/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7054 - val_loss: 1.7199\n",
            "Epoch 915/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6597 - val_loss: 1.8368\n",
            "Epoch 916/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6642 - val_loss: 1.9296\n",
            "Epoch 917/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7134 - val_loss: 1.8724\n",
            "Epoch 918/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6555 - val_loss: 1.6446\n",
            "Epoch 919/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6664 - val_loss: 1.8008\n",
            "Epoch 920/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5931 - val_loss: 1.7482\n",
            "Epoch 921/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6630 - val_loss: 1.8808\n",
            "Epoch 922/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7163 - val_loss: 1.7752\n",
            "Epoch 923/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6639 - val_loss: 1.9101\n",
            "Epoch 924/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6745 - val_loss: 1.8483\n",
            "Epoch 925/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7361 - val_loss: 1.8928\n",
            "Epoch 926/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7004 - val_loss: 2.2030\n",
            "Epoch 927/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7521 - val_loss: 1.6264\n",
            "Epoch 928/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6264 - val_loss: 1.8464\n",
            "Epoch 929/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6676 - val_loss: 1.9490\n",
            "Epoch 930/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6717 - val_loss: 1.8398\n",
            "Epoch 931/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6623 - val_loss: 2.2319\n",
            "Epoch 932/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7246 - val_loss: 1.9547\n",
            "Epoch 933/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6440 - val_loss: 1.8950\n",
            "Epoch 934/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6887 - val_loss: 1.7058\n",
            "Epoch 935/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7115 - val_loss: 1.6537\n",
            "Epoch 936/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6726 - val_loss: 1.7435\n",
            "Epoch 937/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6851 - val_loss: 1.7396\n",
            "Epoch 938/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7333 - val_loss: 1.9223\n",
            "Epoch 939/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7588 - val_loss: 1.8879\n",
            "Epoch 940/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7525 - val_loss: 1.8479\n",
            "Epoch 941/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6399 - val_loss: 1.7834\n",
            "Epoch 942/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6695 - val_loss: 1.5962\n",
            "Epoch 943/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6159 - val_loss: 1.5994\n",
            "Epoch 944/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6817 - val_loss: 1.5884\n",
            "Epoch 945/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6962 - val_loss: 1.8245\n",
            "Epoch 946/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6653 - val_loss: 1.5930\n",
            "Epoch 947/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7217 - val_loss: 1.7947\n",
            "Epoch 948/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7355 - val_loss: 1.7390\n",
            "Epoch 949/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7055 - val_loss: 1.8484\n",
            "Epoch 950/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6616 - val_loss: 1.9388\n",
            "Epoch 951/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6961 - val_loss: 1.5884\n",
            "Epoch 952/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6398 - val_loss: 1.9728\n",
            "Epoch 953/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6704 - val_loss: 1.7158\n",
            "Epoch 954/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7127 - val_loss: 1.7758\n",
            "Epoch 955/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7446 - val_loss: 1.7571\n",
            "Epoch 956/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7623 - val_loss: 1.9243\n",
            "Epoch 957/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6163 - val_loss: 1.9689\n",
            "Epoch 958/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7297 - val_loss: 1.8190\n",
            "Epoch 959/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7191 - val_loss: 1.6229\n",
            "Epoch 960/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7251 - val_loss: 1.5486\n",
            "Epoch 961/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6696 - val_loss: 2.3349\n",
            "Epoch 962/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6771 - val_loss: 1.5164\n",
            "Epoch 963/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6454 - val_loss: 1.9374\n",
            "Epoch 964/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8225 - val_loss: 1.7914\n",
            "Epoch 965/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6801 - val_loss: 1.9963\n",
            "Epoch 966/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7131 - val_loss: 1.6730\n",
            "Epoch 967/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6908 - val_loss: 2.1415\n",
            "Epoch 968/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7406 - val_loss: 1.6884\n",
            "Epoch 969/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7105 - val_loss: 1.9189\n",
            "Epoch 970/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6703 - val_loss: 1.6329\n",
            "Epoch 971/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6874 - val_loss: 1.8438\n",
            "Epoch 972/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7163 - val_loss: 1.8500\n",
            "Epoch 973/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7091 - val_loss: 1.6345\n",
            "Epoch 974/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6957 - val_loss: 1.8141\n",
            "Epoch 975/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6851 - val_loss: 2.1125\n",
            "Epoch 976/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7369 - val_loss: 1.6756\n",
            "Epoch 977/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6923 - val_loss: 1.7634\n",
            "Epoch 978/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6912 - val_loss: 1.6911\n",
            "Epoch 979/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7064 - val_loss: 1.8954\n",
            "Epoch 980/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6962 - val_loss: 1.7956\n",
            "Epoch 981/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6622 - val_loss: 1.7008\n",
            "Epoch 982/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6634 - val_loss: 1.5433\n",
            "Epoch 983/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7266 - val_loss: 1.7054\n",
            "Epoch 984/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7085 - val_loss: 1.7034\n",
            "Epoch 985/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6976 - val_loss: 1.7153\n",
            "Epoch 986/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6972 - val_loss: 1.7319\n",
            "Epoch 987/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6913 - val_loss: 1.7160\n",
            "Epoch 988/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6964 - val_loss: 1.6671\n",
            "Epoch 989/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6919 - val_loss: 1.7745\n",
            "Epoch 990/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6810 - val_loss: 1.6525\n",
            "Epoch 991/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6362 - val_loss: 1.7332\n",
            "Epoch 992/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6904 - val_loss: 1.8670\n",
            "Epoch 993/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7265 - val_loss: 2.0240\n",
            "Epoch 994/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6964 - val_loss: 1.7722\n",
            "Epoch 995/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6865 - val_loss: 1.9849\n",
            "Epoch 996/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6624 - val_loss: 1.7693\n",
            "Epoch 997/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7014 - val_loss: 1.6301\n",
            "Epoch 998/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7241 - val_loss: 1.8542\n",
            "Epoch 999/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6947 - val_loss: 1.9186\n",
            "Epoch 1000/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6233 - val_loss: 1.9436\n",
            "Epoch 1001/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6420 - val_loss: 1.9024\n",
            "Epoch 1002/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7072 - val_loss: 1.8327\n",
            "Epoch 1003/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7128 - val_loss: 1.8985\n",
            "Epoch 1004/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6678 - val_loss: 2.1506\n",
            "Epoch 1005/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6165 - val_loss: 1.6224\n",
            "Epoch 1006/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6657 - val_loss: 1.6494\n",
            "Epoch 1007/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6472 - val_loss: 1.8893\n",
            "Epoch 1008/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6646 - val_loss: 1.8235\n",
            "Epoch 1009/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7364 - val_loss: 2.0548\n",
            "Epoch 1010/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6443 - val_loss: 1.9883\n",
            "Epoch 1011/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6592 - val_loss: 1.9420\n",
            "Epoch 1012/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6959 - val_loss: 1.9706\n",
            "Epoch 1013/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6651 - val_loss: 1.8587\n",
            "Epoch 1014/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6835 - val_loss: 2.1283\n",
            "Epoch 1015/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6976 - val_loss: 1.6477\n",
            "Epoch 1016/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7147 - val_loss: 1.7745\n",
            "Epoch 1017/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6351 - val_loss: 1.8100\n",
            "Epoch 1018/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7043 - val_loss: 1.8997\n",
            "Epoch 1019/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7169 - val_loss: 1.7520\n",
            "Epoch 1020/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6501 - val_loss: 2.2504\n",
            "Epoch 1021/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6977 - val_loss: 1.9818\n",
            "Epoch 1022/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6646 - val_loss: 1.8922\n",
            "Epoch 1023/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6949 - val_loss: 1.6143\n",
            "Epoch 1024/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7012 - val_loss: 2.0232\n",
            "Epoch 1025/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7032 - val_loss: 1.4892\n",
            "Epoch 1026/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6467 - val_loss: 1.5949\n",
            "Epoch 1027/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6801 - val_loss: 1.8079\n",
            "Epoch 1028/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7017 - val_loss: 1.6581\n",
            "Epoch 1029/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7028 - val_loss: 1.8457\n",
            "Epoch 1030/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6770 - val_loss: 1.7826\n",
            "Epoch 1031/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6610 - val_loss: 1.7737\n",
            "Epoch 1032/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6351 - val_loss: 1.8823\n",
            "Epoch 1033/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6780 - val_loss: 1.7835\n",
            "Epoch 1034/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6777 - val_loss: 1.7259\n",
            "Epoch 1035/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7327 - val_loss: 1.8915\n",
            "Epoch 1036/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6766 - val_loss: 1.7122\n",
            "Epoch 1037/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7053 - val_loss: 1.7309\n",
            "Epoch 1038/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6763 - val_loss: 2.1175\n",
            "Epoch 1039/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7029 - val_loss: 1.8839\n",
            "Epoch 1040/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7151 - val_loss: 1.6500\n",
            "Epoch 1041/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7385 - val_loss: 1.8954\n",
            "Epoch 1042/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6862 - val_loss: 1.9945\n",
            "Epoch 1043/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6621 - val_loss: 1.8772\n",
            "Epoch 1044/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6951 - val_loss: 2.1424\n",
            "Epoch 1045/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7905 - val_loss: 1.7520\n",
            "Epoch 1046/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6542 - val_loss: 1.8882\n",
            "Epoch 1047/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6678 - val_loss: 1.8880\n",
            "Epoch 1048/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7026 - val_loss: 1.5916\n",
            "Epoch 1049/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7643 - val_loss: 1.8032\n",
            "Epoch 1050/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7305 - val_loss: 1.9318\n",
            "Epoch 1051/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6428 - val_loss: 2.0429\n",
            "Epoch 1052/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6706 - val_loss: 1.8768\n",
            "Epoch 1053/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6811 - val_loss: 1.7465\n",
            "Epoch 1054/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7027 - val_loss: 2.0677\n",
            "Epoch 1055/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7161 - val_loss: 1.8564\n",
            "Epoch 1056/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7113 - val_loss: 1.7765\n",
            "Epoch 1057/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6866 - val_loss: 1.8429\n",
            "Epoch 1058/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7060 - val_loss: 1.9725\n",
            "Epoch 1059/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6345 - val_loss: 1.9236\n",
            "Epoch 1060/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8609 - val_loss: 2.5272\n",
            "Epoch 1061/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6722 - val_loss: 1.7731\n",
            "Epoch 1062/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6582 - val_loss: 1.8012\n",
            "Epoch 1063/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6988 - val_loss: 1.6340\n",
            "Epoch 1064/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7578 - val_loss: 2.0031\n",
            "Epoch 1065/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6415 - val_loss: 1.7419\n",
            "Epoch 1066/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6661 - val_loss: 1.9886\n",
            "Epoch 1067/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7002 - val_loss: 1.8527\n",
            "Epoch 1068/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6663 - val_loss: 1.8078\n",
            "Epoch 1069/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6718 - val_loss: 1.7922\n",
            "Epoch 1070/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6359 - val_loss: 1.9104\n",
            "Epoch 1071/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6666 - val_loss: 2.0370\n",
            "Epoch 1072/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6869 - val_loss: 1.9728\n",
            "Epoch 1073/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6583 - val_loss: 2.1873\n",
            "Epoch 1074/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6890 - val_loss: 1.7727\n",
            "Epoch 1075/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7312 - val_loss: 1.9369\n",
            "Epoch 1076/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6395 - val_loss: 1.8034\n",
            "Epoch 1077/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6612 - val_loss: 2.0271\n",
            "Epoch 1078/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8010 - val_loss: 2.0622\n",
            "Epoch 1079/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6555 - val_loss: 1.9820\n",
            "Epoch 1080/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7313 - val_loss: 1.7487\n",
            "Epoch 1081/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7001 - val_loss: 1.7686\n",
            "Epoch 1082/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6934 - val_loss: 1.8977\n",
            "Epoch 1083/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7500 - val_loss: 1.7218\n",
            "Epoch 1084/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6848 - val_loss: 1.6166\n",
            "Epoch 1085/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6970 - val_loss: 1.7749\n",
            "Epoch 1086/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6224 - val_loss: 2.3454\n",
            "Epoch 1087/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6794 - val_loss: 1.9342\n",
            "Epoch 1088/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6997 - val_loss: 1.5654\n",
            "Epoch 1089/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6541 - val_loss: 2.0613\n",
            "Epoch 1090/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7254 - val_loss: 2.2137\n",
            "Epoch 1091/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6994 - val_loss: 1.7601\n",
            "Epoch 1092/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6691 - val_loss: 1.8834\n",
            "Epoch 1093/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8086 - val_loss: 1.7881\n",
            "Epoch 1094/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7067 - val_loss: 1.9086\n",
            "Epoch 1095/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6831 - val_loss: 1.7137\n",
            "Epoch 1096/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7574 - val_loss: 1.8679\n",
            "Epoch 1097/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6241 - val_loss: 1.8067\n",
            "Epoch 1098/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6635 - val_loss: 1.6708\n",
            "Epoch 1099/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6419 - val_loss: 1.7475\n",
            "Epoch 1100/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6484 - val_loss: 1.9361\n",
            "Epoch 1101/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6520 - val_loss: 1.9871\n",
            "Epoch 1102/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6908 - val_loss: 1.7656\n",
            "Epoch 1103/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6576 - val_loss: 1.7000\n",
            "Epoch 1104/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6583 - val_loss: 1.6120\n",
            "Epoch 1105/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6898 - val_loss: 1.7801\n",
            "Epoch 1106/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7511 - val_loss: 1.8659\n",
            "Epoch 1107/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6973 - val_loss: 2.1784\n",
            "Epoch 1108/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6386 - val_loss: 1.8236\n",
            "Epoch 1109/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6551 - val_loss: 1.9432\n",
            "Epoch 1110/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6529 - val_loss: 1.9897\n",
            "Epoch 1111/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7190 - val_loss: 1.7661\n",
            "Epoch 1112/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6452 - val_loss: 2.0191\n",
            "Epoch 1113/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6681 - val_loss: 1.7911\n",
            "Epoch 1114/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7159 - val_loss: 1.9191\n",
            "Epoch 1115/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7006 - val_loss: 1.6896\n",
            "Epoch 1116/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6863 - val_loss: 1.6261\n",
            "Epoch 1117/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6299 - val_loss: 1.9117\n",
            "Epoch 1118/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6837 - val_loss: 1.8200\n",
            "Epoch 1119/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.6756Restoring model weights from the end of the best epoch: 119.\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6621 - val_loss: 2.0175\n",
            "Epoch 1119: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-66f430598e45>:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 1.0879913568496704\n",
            "Train loss: 0.7779841423034668\n",
            "Test loss: 1.3530062437057495\n",
            "dO18 RMSE: 1.37901763540523\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       23.405260           0.148275\n",
            "1       25.725845           0.213718\n",
            "2       25.936000           0.066530\n",
            "3       23.436000           0.134780\n",
            "4       25.414000           0.025930\n",
            "5       25.168000           0.106320\n",
            "6       24.582000           4.062670\n",
            "7       25.108000           0.170620\n",
            "8       23.398000           0.119170\n",
            "9       25.006000           0.272130\n",
            "10      23.768000           0.048420\n",
            "11      23.800000           0.277350\n",
            "12      23.742000           0.118870\n",
            "13      24.038000           0.263970\n",
            "14      24.608000           1.650470\n",
            "15      25.696000           0.791580\n",
            "16      25.524000           0.219580\n",
            "17      25.794000           0.224030\n",
            "18      23.770000           1.004550\n",
            "19      25.912000           0.478970\n",
            "20      25.878000           0.349920\n",
            "21      26.060000           2.584750\n",
            "22      23.328000           0.662170\n",
            "23      22.568000          48.102770\n",
            "24      25.196000           0.410630\n",
            "25      24.656000           0.177280\n",
            "26      26.356000           0.039530\n",
            "27      23.962000           0.309920\n",
            "28      24.848000           0.158420\n",
            "29      25.080000           0.051250\n",
            "30      26.546000           2.143780\n",
            "31      25.682000           0.944720\n",
            "32      24.028000           0.458520\n",
            "33      23.752000           0.524370\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       25.698803           2.155969\n",
            "1       25.770935           1.946571\n",
            "2       25.698782           2.156005\n",
            "3       25.804522           2.431254\n",
            "4       25.527031           3.329686\n",
            "5       25.698782           2.156005\n",
            "6       25.527031           3.329686\n",
            "7       25.698782           2.156005\n",
            "8       25.519785           3.336395\n",
            "9       25.698782           2.156005\n",
            "10      25.527031           3.329686\n",
            "11      25.804461           2.431230\n",
            "12      25.698782           2.156005\n",
            "13      25.527031           3.329686\n",
            "14      25.698782           2.156005\n",
            "15      25.698782           2.156005\n",
            "16      25.698782           2.156005\n",
            "17      25.804461           2.431230\n",
            "18      25.804522           2.431254\n",
            "19      25.804522           2.431254\n",
            "20      25.804522           2.431254\n",
            "21      25.804522           2.431254\n",
            "22      25.527031           3.329686\n",
            "23      25.527031           3.329686\n",
            "24      25.804522           2.431254\n",
            "25      25.698803           2.155969\n",
            "26      25.770935           1.946571\n",
            "27      25.804461           2.431230\n",
            "28      25.519863           3.336434\n",
            "29      25.519863           3.336434\n",
            "30      25.804522           2.431254\n",
            "31      25.698782           2.156005\n",
            "32      25.519785           3.336395\n",
            "33      26.282509           2.486265\n"
          ]
        }
      ],
      "source": [
        "model = train_and_evaluate(data, MODEL_NAME, training_batch_size=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX1HiL-NfR3a",
        "outputId": "7d585f45-f9ed-48c0-a367-487ae22c7f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/MyDrive/amazon_rainforest_files/variational/model/demo_isoscape_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "model.save(get_model_save_location(\n",
        "    f\"{MODEL_NAME}.tf\"), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location(\n",
        "    f\"{MODEL_NAME}.pkl\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "PuGJblTjiDdz",
        "outputId": "ab93ceb5-7895-43d6-fdc1-9e36778f21e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-5a710e4ceee3>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     'ordinary_kriging_linear_d18O_predicted_variance',]\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m raster.generate_isoscapes_from_variational_model(\n\u001b[0m\u001b[1;32m     20\u001b[0m     MODEL_NAME, model, data.feature_scaler, required_geotiffs, False)\n",
            "\u001b[0;32m/tmp/ddf_common/raster.py\u001b[0m in \u001b[0;36mgenerate_isoscapes_from_variational_model\u001b[0;34m(output_geotiff_id, model, required_geotiffs, res_x, res_y)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0mres_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     res_y: int):\n\u001b[0;32m--> 491\u001b[0;31m   \u001b[0minput_geotiffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcolumn_name_to_geotiff_fn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrequired_geotiffs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m   \u001b[0marbitrary_geotiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_geotiffs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'ColumnTransformer' object is not iterable"
          ]
        }
      ],
      "source": [
        "# @title Run this cell to generate an isoscape\n",
        "\n",
        "vi_model = model.TFModel(f\"{MODEL_NAME}.tf\", f\"{MODEL_NAME}.pkl\")\n",
        "\n",
        "required_geotiffs = [\n",
        "    'VPD',\n",
        "    'RH',\n",
        "    'PET',\n",
        "    'DEM',\n",
        "    'PA',\n",
        "    'Mean Annual Temperature',\n",
        "    'Mean Annual Precipitation',\n",
        "    'Iso_Oxi_Stack_mean_TERZER',\n",
        "    'isoscape_fullmodel_d18O_prec_REGRESSION',\n",
        "    \"brisoscape_mean_ISORIX\",\n",
        "    \"d13C_cel_mean\",\n",
        "    \"d13C_cel_var\",\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance',]\n",
        "\n",
        "raster.generate_isoscapes_from_variational_model(\n",
        "    output_geotiff_id=MODEL_NAME, model=vi_model,\n",
        "    required_geotiffs=required_geotiffs,\n",
        "    res_x=1000, res_y=1000)"
      ],
      "metadata": {
        "id": "PuGJblTjiDdz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}