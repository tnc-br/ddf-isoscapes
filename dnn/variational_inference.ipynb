{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnc-br/ddf-isoscapes/blob/new_model_py/dnn/variational_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational model\n",
        "\n",
        "Find the mean/variance of O18 ratios (as well as N15 and C13 in the future) at a particular lat/lon across Brazil."
      ],
      "metadata": {
        "id": "-0IfT3kGwgK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "henIPlAPCb4i",
        "outputId": "305a1d65-2b76-4d3a-f122-8ef3b096c151",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import os\n",
        "from typing import List, Tuple, Dict\n",
        "from dataclasses import dataclass\n",
        "from joblib import dump\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.python.ops import math_ops\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "#@title Model training configuration\n",
        "# See https://zohaib.me/debugging-in-google-collab-notebook/ for tips,\n",
        "# as well as docs for pdb and ipdb.\n",
        "DEBUG = False #@param {type:\"boolean\"}\n",
        "USE_LOCAL_DRIVE = False #@param {type:\"boolean\"}\n",
        "LOCAL_DIR = \"/usr/local/google/home/ruru/Downloads/amazon_sample_data-20230712T203059Z-001\" #@param\n",
        "GDRIVE_DIR = \"MyDrive/amazon_rainforest_files/\" #@param\n",
        "FP_ROOT = LOCAL_DIR\n",
        "\n",
        "MODEL_SAVE_LOCATION = \"MyDrive/amazon_rainforest_files/variational/model\" #@param\n",
        "\n",
        "def get_model_save_location(filename) -> str:\n",
        "  root = '' if USE_LOCAL_DRIVE else '/content/gdrive'\n",
        "  return os.path.join(root, MODEL_SAVE_LOCATION, filename)\n",
        "\n",
        "# Access data stored on Google Drive if not reading data locally.\n",
        "if not USE_LOCAL_DRIVE:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  global FP_ROOT\n",
        "  FP_ROOT = os.path.join('/content/gdrive', GDRIVE_DIR)\n",
        "\n",
        "MODEL_NAME = \"demo_isoscape_model\" #@param\n",
        "\n",
        "TRAINING_SET_PATH = 'amazon_sample_data/canonical/uc_davis_train_fixed_grouped.csv' #@param\n",
        "VALIDATION_SET_PATH = 'amazon_sample_data/canonical/uc_davis_validation_fixed_grouped.csv' #@param\n",
        "TEST_SET_PATH = 'amazon_sample_data/canonical/uc_davis_test_fixed_grouped.csv' #@param\n",
        "\n",
        "if DEBUG:\n",
        "    %pip install -Uqq ipdb\n",
        "    import ipdb\n",
        "    %pdb on"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries required"
      ],
      "metadata": {
        "id": "7MRzTQkqvRbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "!if [ ! -d \"/content/ddf_common_stub\" ] ; then git clone -b test https://github.com/tnc-br/ddf_common_stub.git; fi\n",
        "sys.path.append(\"/content/ddf_common_stub/\")\n",
        "import ddfimport\n",
        "ddfimport.ddf_import_common()"
      ],
      "metadata": {
        "id": "AXh86HFwXiax",
        "outputId": "f3ff817e-d2df-42eb-fc32-7637d24a3fe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "executing checkout_branch ...\n",
            "Branch main already checked out.\n",
            "Remember to reload your imports with `importlib.reload(module)`.\n",
            "b''\n",
            "main branch checked out as readonly. You may now use ddf_common imports\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import raster\n",
        "import importlib\n",
        "import model\n",
        "importlib.reload(model)\n",
        "importlib.reload(raster)"
      ],
      "metadata": {
        "id": "0mUB0y0AXivp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b0681ca-074e-42e4-d34a-e82aa778f85b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'raster' from '/content/gdrive/MyDrive/gen_isoscape/ddf_common/raster.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQrEv57zCb4q"
      },
      "source": [
        "# Data preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path: str, columns_to_keep: List[str], side_raster_input):\n",
        "  df = pd.read_csv(path, encoding=\"ISO-8859-1\", sep=',')\n",
        "  df = df[df['d18O_cel_variance'].notna()]\n",
        "  X = df\n",
        "  X = X.drop(df.columns.difference(columns_to_keep), axis=1)\n",
        "\n",
        "  for name, geotiff in side_raster_input.items():\n",
        "    X[name] = X.apply(lambda row: geotiff.value_at(row['long'], row['lat']), axis=1)\n",
        "\n",
        "  Y = df[[\"d18O_cel_mean\", \"d18O_cel_variance\"]]\n",
        "\n",
        "  print(Y.shape)\n",
        "  for name, geotiff in side_raster_input.items():\n",
        "    Y = Y[X[name].notnull()]\n",
        "  print(Y.shape)\n",
        "\n",
        "  for name, geotiff in side_raster_input.items():\n",
        "    X = X[X[name].notnull()]\n",
        "\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "6XMee1aHfcik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardization"
      ],
      "metadata": {
        "id": "DtkKhMOtb6cS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class FeaturesToLabels:\n",
        "  def __init__(self, X: pd.DataFrame, Y: pd.DataFrame):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "\n",
        "  def as_tuple(self):\n",
        "    return (self.X, self.Y)\n",
        "\n",
        "\n",
        "def create_feature_scaler(X: pd.DataFrame,\n",
        "                          columns_to_passthrough,\n",
        "                          columns_to_scale,\n",
        "                          columns_to_standardize) -> ColumnTransformer:\n",
        "  columns_to_standardize = columns_to_standardize\n",
        "  feature_scaler = ColumnTransformer(\n",
        "      [(column+'_normalizer', MinMaxScaler(), [column]) for column in columns_to_scale] +\n",
        "      [(column+'_standardizer', StandardScaler(), [column]) for column in columns_to_standardize],\n",
        "      remainder='passthrough')\n",
        "  feature_scaler.fit(X)\n",
        "  print(feature_scaler)\n",
        "  return feature_scaler\n",
        "\n",
        "def create_label_scaler(Y: pd.DataFrame) -> ColumnTransformer:\n",
        "  label_scaler = ColumnTransformer([\n",
        "      ('mean_std_scaler', StandardScaler(), ['d18O_cel_mean']),\n",
        "      ('var_minmax_scaler', MinMaxScaler(), ['d18O_cel_variance'])],\n",
        "      remainder='passthrough')\n",
        "  label_scaler.fit(Y)\n",
        "  return label_scaler\n",
        "\n",
        "def scale(X: pd.DataFrame, feature_scaler):\n",
        "  # transform() outputs numpy arrays :(  need to convert back to DataFrame.\n",
        "  X_standardized = pd.DataFrame(feature_scaler.transform(X),\n",
        "                        index=X.index, columns=X.columns)\n",
        "  return X_standardized"
      ],
      "metadata": {
        "id": "XSDwdvMkb7w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just a class organization, holds each scaled dataset and the scaler used.\n",
        "# Useful for unscaling predictions.\n",
        "@dataclass\n",
        "class ScaledPartitions():\n",
        "  def __init__(self,\n",
        "               feature_scaler: ColumnTransformer,\n",
        "               label_scaler: ColumnTransformer,\n",
        "               train: FeaturesToLabels, val: FeaturesToLabels,\n",
        "               test: FeaturesToLabels):\n",
        "    self.feature_scaler = feature_scaler\n",
        "    self.label_scaler = label_scaler\n",
        "    self.train = train\n",
        "    self.val = val\n",
        "    self.test = test\n",
        "\n",
        "\n",
        "def load_and_scale(config: Dict,\n",
        "                   columns_to_passthrough: List[str],\n",
        "                   columns_to_scale: List[str],\n",
        "                   columns_to_standardize: List[str]) -> ScaledPartitions:\n",
        "\n",
        "  geotiff_side_input = {\n",
        "      \"brisoscape_mean_ISORIX\" : raster.load_raster(raster.get_raster_path(\"brisoscape_mean_ISORIX.tif\")),\n",
        "      \"d13C_cel_mean\" : raster.load_raster(raster.get_raster_path(\"d13C_cel_map_BRAZIL_stack.tiff\"), use_only_band_index=0),\n",
        "      \"d13C_cel_var\" : raster.load_raster(raster.get_raster_path(\"d13C_cel_map_BRAZIL_stack.tiff\"), use_only_band_index=1),\n",
        "      # \"gabi_d18O_cel_mean\": raster.load_raster(raster.get_raster_path(\"canonical/d18O_cel_Brazil_stack.tiff\"), use_only_band_index=0),\n",
        "      # \"gabi_d18O_cel_variance\": raster.load_raster(raster.get_raster_path(\"canonical/d18O_cel_Brazil_stack.tiff\"), use_only_band_index=1),\n",
        "      \"ordinary_kriging_linear_d18O_predicted_mean\" : raster.krig_means_isoscape_geotiff(),\n",
        "      \"ordinary_kriging_linear_d18O_predicted_variance\" : raster.krig_variances_isoscape_geotiff(),\n",
        "  }\n",
        "  columns_to_keep = columns_to_passthrough + columns_to_scale + columns_to_standardize\n",
        "  X_train, Y_train = load_dataset(config['TRAIN'], columns_to_keep, geotiff_side_input)\n",
        "  X_val, Y_val = load_dataset(config['VALIDATION'], columns_to_keep, geotiff_side_input)\n",
        "  X_test, Y_test = load_dataset(config['TEST'], columns_to_keep, geotiff_side_input)\n",
        "\n",
        "  # Fit the scaler:\n",
        "  feature_scaler = create_feature_scaler(\n",
        "      X_train,\n",
        "      columns_to_passthrough,\n",
        "      columns_to_scale,\n",
        "      columns_to_standardize)\n",
        "\n",
        "  # Apply the scaler:\n",
        "  label_scaler = create_label_scaler(Y_train)\n",
        "  train = FeaturesToLabels(scale(X_train, feature_scaler), Y_train)\n",
        "  val = FeaturesToLabels(scale(X_val, feature_scaler), Y_val)\n",
        "  test = FeaturesToLabels(scale(X_test, feature_scaler), Y_test)\n",
        "  return ScaledPartitions(feature_scaler, label_scaler, train, val, test)\n"
      ],
      "metadata": {
        "id": "_kf2e_fKon2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition\n",
        "\n"
      ],
      "metadata": {
        "id": "usGznR593LZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KL Loss function:"
      ],
      "metadata": {
        "id": "khK7C8WvU8ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_normal_distribution(\n",
        "    mean: tf.Tensor,\n",
        "    stdev: tf.Tensor,\n",
        "    n: int) -> tf.Tensor:\n",
        "    '''\n",
        "    Given a batch of normal distributions described by a mean and stdev in\n",
        "    a tf.Tensor, sample n elements from each distribution and return the mean\n",
        "    and standard deviation per sample.\n",
        "    '''\n",
        "    batch_size = tf.shape(mean)[0]\n",
        "\n",
        "    # Output tensor is (n, batch_size, 1)\n",
        "    sample_values = tfp.distributions.Normal(\n",
        "        loc=mean,\n",
        "        scale=stdev).sample(\n",
        "            sample_shape=n)\n",
        "    # Reshaped tensor will be (batch_size, n)\n",
        "    sample_values = tf.transpose(sample_values)\n",
        "    # Get the mean per sample in the batch.\n",
        "    sample_mean = tf.transpose(tf.math.reduce_mean(sample_values, 2))\n",
        "    sample_stdev = tf.transpose(tf.math.reduce_std(sample_values, 2))\n",
        "\n",
        "    return sample_mean, sample_stdev\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# log(σ2/σ1) + ( σ1^2+(μ1−μ2)^2 ) / 2* σ^2   − 1/2\n",
        "def kl_divergence_helper(real, predicted, sample):\n",
        "    '''\n",
        "    real: tf.Tensor of the real mean and standard deviation of sample to compare\n",
        "    predicted: tf.Tensor of the predicted mean and standard deviation to compare\n",
        "    sample: Whether or not to sample the predicted distribution to get a new\n",
        "            mean and standard deviation.\n",
        "    '''\n",
        "    if real.shape != predicted.shape:\n",
        "      raise ValueError(\n",
        "          f\"real.shape {real.shape} != predicted.shape {predicted.shape}\")\n",
        "\n",
        "    real_value = tf.gather(real, [0], axis=1)\n",
        "    real_std = tf.math.sqrt(tf.gather(real, [1], axis=1))\n",
        "\n",
        "\n",
        "    predicted_value = tf.gather(predicted, [0], axis=1)\n",
        "    predicted_std = tf.math.sqrt(tf.gather(predicted, [1], axis=1))\n",
        "    # If true, sample from the distribution defined by the predicted mean and\n",
        "    # standard deviation to use for mean and stdev used in KL divergence loss.\n",
        "    if sample:\n",
        "      predicted_value, predicted_std = sample_normal_distribution(\n",
        "          mean=predicted_value, stdev=predicted_std, n=15)\n",
        "\n",
        "    kl_loss = -0.5 + tf.math.log(predicted_std/real_std) + \\\n",
        "     (tf.square(real_std) + tf.square(real_value - predicted_value))/ \\\n",
        "     (2*tf.square(predicted_std))\n",
        "\n",
        "    if tf.math.is_nan(tf.math.reduce_mean(kl_loss)):\n",
        "       tf.print(predicted)\n",
        "       sess = tf.compat.v1.Session()\n",
        "       sess.close()\n",
        "\n",
        "    return tf.math.reduce_mean(kl_loss)\n",
        "\n",
        "def kl_divergence(real, predicted):\n",
        "  return kl_divergence_helper(real, predicted, True)"
      ],
      "metadata": {
        "id": "urGjYNNnemX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "def get_early_stopping_callback():\n",
        "  return EarlyStopping(monitor='val_loss', patience=1000, min_delta=0.001,\n",
        "                       verbose=1, restore_best_weights=True, start_from_epoch=0)\n",
        "\n",
        "tf.keras.utils.set_random_seed(18731)\n",
        "\n",
        "# I was experimenting with models that took longer to train, and used this\n",
        "# checkpointing callback to periodically save the model. It's optional.\n",
        "def get_checkpoint_callback(model_file):\n",
        "  return ModelCheckpoint(\n",
        "      get_model_save_location(model_file),\n",
        "      monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
        "\n",
        "def train_or_update_variational_model(\n",
        "        sp: ScaledPartitions,\n",
        "        hidden_layers: List[int],\n",
        "        epochs: int,\n",
        "        batch_size: int,\n",
        "        lr: float,\n",
        "        model_file=None,\n",
        "        use_checkpoint=False):\n",
        "  callbacks_list = [get_early_stopping_callback(),\n",
        "                    get_checkpoint_callback(model_file)]\n",
        "  if not use_checkpoint:\n",
        "    inputs = keras.Input(shape=(sp.train.X.shape[1],))\n",
        "    x = inputs\n",
        "    for layer_size in hidden_layers:\n",
        "      x = keras.layers.Dense(\n",
        "          layer_size, activation='relu')(x)\n",
        "    mean_output = keras.layers.Dense(\n",
        "        1, name='mean_output')(x)\n",
        "\n",
        "    # We can not have negative variance. Apply very little variance.\n",
        "    var_output = keras.layers.Dense(\n",
        "        1, name='var_output')(x)\n",
        "\n",
        "    # Invert the normalization on our outputs\n",
        "    mean_scaler = sp.label_scaler.named_transformers_['mean_std_scaler']\n",
        "    untransformed_mean = mean_output * mean_scaler.var_ + mean_scaler.mean_\n",
        "\n",
        "    var_scaler = sp.label_scaler.named_transformers_['var_minmax_scaler']\n",
        "    unscaled_var = var_output * var_scaler.scale_ + var_scaler.min_\n",
        "    untransformed_var = keras.layers.Lambda(lambda t: tf.math.log(1 + tf.exp(t)))(unscaled_var)\n",
        "\n",
        "    # Output mean,  tuples.\n",
        "    outputs = keras.layers.concatenate([untransformed_mean, untransformed_var])\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss=kl_divergence)\n",
        "    model.summary()\n",
        "  else:\n",
        "    model = keras.models.load_model(\n",
        "        get_model_save_location(model_file),\n",
        "        custom_objects={\"kl_divergence\": kl_divergence})\n",
        "  history = model.fit(sp.train.X, sp.train.Y, verbose=1, validation_data=sp.val.as_tuple(), shuffle=True,\n",
        "                      epochs=epochs, batch_size=batch_size, callbacks=callbacks_list)\n",
        "  return history, model"
      ],
      "metadata": {
        "id": "HCkGSPUo3KqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def render_plot_loss(history, name):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title(name + ' model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.yscale(\"log\")\n",
        "  plt.ylim((0, 10))\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['loss', 'val_loss'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def destandardize(sd: ScaledPartitions, df: pd.DataFrame):\n",
        "  means = pd.DataFrame(\n",
        "      sd.label_scaler.named_transformers_['var_std_scaler'].inverse_transform(df[['d18O_cel_mean']]),\n",
        "      index=df.index, columns=['d18O_cel_mean'])\n",
        "  vars = df['d18O_cel_variance']\n",
        "  return means.join(vars)\n",
        "\n",
        "def train_and_evaluate(sp: ScaledPartitions, run_id: str, training_batch_size=5):\n",
        "  print(\"==================\")\n",
        "  print(run_id)\n",
        "  history, model = train_or_update_variational_model(\n",
        "      sp, hidden_layers=[20, 20], epochs=5000, batch_size=training_batch_size,\n",
        "      lr=0.0001, model_file=run_id+\".h5\", use_checkpoint=False)\n",
        "  render_plot_loss(history, run_id+\" kl_loss\")\n",
        "  model.save(get_model_save_location(run_id+\".h5\"), save_format=\"h5\")\n",
        "\n",
        "  best_epoch_index = history.history['val_loss'].index(min(history.history['val_loss']))\n",
        "  print('Val loss:', history.history['val_loss'][best_epoch_index])\n",
        "  print('Train loss:', history.history['loss'][best_epoch_index])\n",
        "  print('Test loss:', model.evaluate(x=sp.test.X, y=sp.test.Y, verbose=0))\n",
        "\n",
        "  predictions = model.predict_on_batch(sp.test.X)\n",
        "  predictions = pd.DataFrame(predictions, columns=['d18O_cel_mean', 'd18O_cel_variance'])\n",
        "  rmse = np.sqrt(mean_squared_error(sp.test.Y['d18O_cel_mean'], predictions['d18O_cel_mean']))\n",
        "  print(\"dO18 RMSE: \"+ str(rmse))\n",
        "  print(\"EXPECTED:\")\n",
        "  print(sp.test.Y.to_string())\n",
        "  print()\n",
        "  print(\"PREDICTED:\")\n",
        "  print(predictions.to_string())\n",
        "  return model"
      ],
      "metadata": {
        "id": "DALuUm8UOgNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "bMCbonPjs6Kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_random_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, TRAINING_SET_PATH),\n",
        "    'TEST' : os.path.join(FP_ROOT, VALIDATION_SET_PATH),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, TEST_SET_PATH),\n",
        "}\n",
        "\n",
        "columns_to_passthrough = [\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "columns_to_scale = []\n",
        "columns_to_standardize = [\n",
        "    'lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature', 'Mean Annual Precipitation',\n",
        "    'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION']\n",
        "\n",
        "data = load_and_scale(grouped_random_fileset, columns_to_passthrough, columns_to_scale, columns_to_standardize)\n",
        "data.train.X.columns"
      ],
      "metadata": {
        "id": "rPONfgkjvJWz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "973824a6-9231-4a37-ea01-e1cc79249531",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: GTiff/GeoTIFF\n",
            "Size is 541 x 467 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.922043, 5.233124)\n",
            "Pixel Size = (0.08333, -0.08333)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 218 x 2\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-74.0, 4.500000000659528)\n",
            "Pixel Size = (0.166666666667993, -0.16666666666799657)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 218 x 2\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-74.0, 4.500000000659528)\n",
            "Pixel Size = (0.166666666667993, -0.16666666666799657)\n",
            "(68, 2)\n",
            "(68, 2)\n",
            "(22, 2)\n",
            "(22, 2)\n",
            "(34, 2)\n",
            "(24, 2)\n",
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('lat_standardizer', StandardScaler(), ['lat']),\n",
            "                                ('long_standardizer', StandardScaler(),\n",
            "                                 ['long']),\n",
            "                                ('VPD_standardizer', StandardScaler(), ['VPD']),\n",
            "                                ('RH_standardizer', StandardScaler(), ['RH']),\n",
            "                                ('PET_standardizer', StandardScaler(), ['PET']),\n",
            "                                ('DEM_standardizer', StandardScaler(), ['DEM']),\n",
            "                                ('PA_standardizer'...\n",
            "                                ('Mean Annual Temperature_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation']),\n",
            "                                ('Iso_Oxi_Stack_mean_TERZER_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Iso_Oxi_Stack_mean_TERZER']),\n",
            "                                ('isoscape_fullmodel_d18O_prec_REGRESSION_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['isoscape_fullmodel_d18O_prec_REGRESSION'])])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-bfffc012ca9b>:16: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  Y = Y[X[name].notnull()]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
              "       'Mean Annual Temperature', 'Mean Annual Precipitation',\n",
              "       'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION',\n",
              "       'ordinary_kriging_linear_d18O_predicted_mean',\n",
              "       'ordinary_kriging_linear_d18O_predicted_variance',\n",
              "       'brisoscape_mean_ISORIX', 'd13C_cel_mean', 'd13C_cel_var'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_and_evaluate(data, MODEL_NAME, training_batch_size=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cFuIPM4afQPd",
        "outputId": "ae6f26bd-31ed-4437-835f-abc23facf633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================\n",
            "demo_isoscape_model\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 16)]         0           []                               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 20)           340         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 20)           420         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " var_output (Dense)             (None, 1)            21          ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " mean_output (Dense)            (None, 1)            21          ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.multiply_1 (TFOpLambda  (None, 1)           0           ['var_output[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.multiply (TFOpLambda)  (None, 1)            0           ['mean_output[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 1)           0           ['tf.math.multiply_1[0][0]']     \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 1)           0           ['tf.math.multiply[0][0]']       \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1)            0           ['tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 2)            0           ['tf.__operators__.add[0][0]',   \n",
            "                                                                  'lambda[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 802\n",
            "Trainable params: 802\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5000\n",
            "23/23 [==============================] - 4s 68ms/step - loss: 7.5112 - val_loss: 36.7973\n",
            "Epoch 2/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.6213 - val_loss: 15.7563\n",
            "Epoch 3/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.3312 - val_loss: 12.0727\n",
            "Epoch 4/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.3791 - val_loss: 8.6969\n",
            "Epoch 5/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.5943 - val_loss: 6.1879\n",
            "Epoch 6/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.9973 - val_loss: 4.2610\n",
            "Epoch 7/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6006 - val_loss: 4.0474\n",
            "Epoch 8/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.3233 - val_loss: 2.5670\n",
            "Epoch 9/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.1793 - val_loss: 2.3176\n",
            "Epoch 10/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9744 - val_loss: 2.7738\n",
            "Epoch 11/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9247 - val_loss: 4.1223\n",
            "Epoch 12/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9497 - val_loss: 3.8400\n",
            "Epoch 13/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9306 - val_loss: 3.0083\n",
            "Epoch 14/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8648 - val_loss: 4.0552\n",
            "Epoch 15/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9341 - val_loss: 2.9952\n",
            "Epoch 16/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8571 - val_loss: 3.8236\n",
            "Epoch 17/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8340 - val_loss: 3.9919\n",
            "Epoch 18/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8777 - val_loss: 4.2321\n",
            "Epoch 19/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8863 - val_loss: 3.4593\n",
            "Epoch 20/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9378 - val_loss: 3.5963\n",
            "Epoch 21/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9446 - val_loss: 3.5566\n",
            "Epoch 22/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8354 - val_loss: 3.5807\n",
            "Epoch 23/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8489 - val_loss: 3.1950\n",
            "Epoch 24/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8533 - val_loss: 3.3423\n",
            "Epoch 25/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9223 - val_loss: 3.3950\n",
            "Epoch 26/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8097 - val_loss: 3.3389\n",
            "Epoch 27/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8579 - val_loss: 3.0527\n",
            "Epoch 28/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8378 - val_loss: 2.5052\n",
            "Epoch 29/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8671 - val_loss: 3.6227\n",
            "Epoch 30/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8335 - val_loss: 3.0184\n",
            "Epoch 31/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8626 - val_loss: 4.1640\n",
            "Epoch 32/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8619 - val_loss: 2.9499\n",
            "Epoch 33/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8657 - val_loss: 3.3881\n",
            "Epoch 34/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8142 - val_loss: 2.9509\n",
            "Epoch 35/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8281 - val_loss: 2.7527\n",
            "Epoch 36/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8361 - val_loss: 2.4622\n",
            "Epoch 37/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8043 - val_loss: 3.9163\n",
            "Epoch 38/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9328 - val_loss: 2.7428\n",
            "Epoch 39/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8153 - val_loss: 2.7459\n",
            "Epoch 40/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8710 - val_loss: 2.3442\n",
            "Epoch 41/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7868 - val_loss: 2.8802\n",
            "Epoch 42/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8653 - val_loss: 3.0758\n",
            "Epoch 43/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8389 - val_loss: 2.6452\n",
            "Epoch 44/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8394 - val_loss: 2.4046\n",
            "Epoch 45/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8356 - val_loss: 2.6891\n",
            "Epoch 46/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8800 - val_loss: 2.5811\n",
            "Epoch 47/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7340 - val_loss: 1.8992\n",
            "Epoch 48/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8368 - val_loss: 1.9295\n",
            "Epoch 49/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7981 - val_loss: 2.8774\n",
            "Epoch 50/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8407 - val_loss: 2.1011\n",
            "Epoch 51/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7877 - val_loss: 2.0089\n",
            "Epoch 52/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8975 - val_loss: 1.9524\n",
            "Epoch 53/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7916 - val_loss: 2.2082\n",
            "Epoch 54/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8047 - val_loss: 1.9938\n",
            "Epoch 55/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8063 - val_loss: 2.2707\n",
            "Epoch 56/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8119 - val_loss: 1.6008\n",
            "Epoch 57/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8280 - val_loss: 1.9979\n",
            "Epoch 58/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8402 - val_loss: 1.8547\n",
            "Epoch 59/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9202 - val_loss: 1.7850\n",
            "Epoch 60/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8726 - val_loss: 1.9204\n",
            "Epoch 61/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8144 - val_loss: 1.5695\n",
            "Epoch 62/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8263 - val_loss: 1.8812\n",
            "Epoch 63/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9277 - val_loss: 2.3159\n",
            "Epoch 64/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8547 - val_loss: 2.0782\n",
            "Epoch 65/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7228 - val_loss: 1.8898\n",
            "Epoch 66/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8322 - val_loss: 2.2059\n",
            "Epoch 67/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7558 - val_loss: 1.9793\n",
            "Epoch 68/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7853 - val_loss: 2.3290\n",
            "Epoch 69/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7820 - val_loss: 2.3464\n",
            "Epoch 70/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7875 - val_loss: 2.3492\n",
            "Epoch 71/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8583 - val_loss: 2.1812\n",
            "Epoch 72/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8374 - val_loss: 2.4807\n",
            "Epoch 73/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8458 - val_loss: 1.9387\n",
            "Epoch 74/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7947 - val_loss: 2.3754\n",
            "Epoch 75/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8452 - val_loss: 2.8242\n",
            "Epoch 76/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8006 - val_loss: 2.1656\n",
            "Epoch 77/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7968 - val_loss: 2.2253\n",
            "Epoch 78/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7876 - val_loss: 1.8329\n",
            "Epoch 79/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8031 - val_loss: 1.9375\n",
            "Epoch 80/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7691 - val_loss: 1.7717\n",
            "Epoch 81/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8294 - val_loss: 2.5974\n",
            "Epoch 82/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7508 - val_loss: 2.2664\n",
            "Epoch 83/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8880 - val_loss: 2.7025\n",
            "Epoch 84/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7963 - val_loss: 2.6327\n",
            "Epoch 85/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8234 - val_loss: 3.3853\n",
            "Epoch 86/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7932 - val_loss: 2.2886\n",
            "Epoch 87/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9009 - val_loss: 2.2856\n",
            "Epoch 88/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8025 - val_loss: 2.5027\n",
            "Epoch 89/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8527 - val_loss: 2.1656\n",
            "Epoch 90/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7875 - val_loss: 2.8794\n",
            "Epoch 91/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8246 - val_loss: 4.1579\n",
            "Epoch 92/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7579 - val_loss: 4.0550\n",
            "Epoch 93/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8189 - val_loss: 2.7952\n",
            "Epoch 94/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8253 - val_loss: 2.7272\n",
            "Epoch 95/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7675 - val_loss: 3.0530\n",
            "Epoch 96/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7797 - val_loss: 3.3815\n",
            "Epoch 97/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8510 - val_loss: 2.6474\n",
            "Epoch 98/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8126 - val_loss: 3.5120\n",
            "Epoch 99/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8660 - val_loss: 3.7609\n",
            "Epoch 100/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7564 - val_loss: 3.1080\n",
            "Epoch 101/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8161 - val_loss: 2.8840\n",
            "Epoch 102/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8345 - val_loss: 4.4395\n",
            "Epoch 103/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7726 - val_loss: 3.7125\n",
            "Epoch 104/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8757 - val_loss: 3.8274\n",
            "Epoch 105/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8471 - val_loss: 3.7525\n",
            "Epoch 106/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8326 - val_loss: 2.8837\n",
            "Epoch 107/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8394 - val_loss: 5.0595\n",
            "Epoch 108/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8188 - val_loss: 4.0979\n",
            "Epoch 109/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8532 - val_loss: 5.8889\n",
            "Epoch 110/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8038 - val_loss: 4.9424\n",
            "Epoch 111/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7757 - val_loss: 5.4482\n",
            "Epoch 112/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7987 - val_loss: 5.2650\n",
            "Epoch 113/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7713 - val_loss: 3.9569\n",
            "Epoch 114/5000\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.8361 - val_loss: 3.7698\n",
            "Epoch 115/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8873 - val_loss: 4.2561\n",
            "Epoch 116/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.8634 - val_loss: 4.4990\n",
            "Epoch 117/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8326 - val_loss: 6.9561\n",
            "Epoch 118/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8305 - val_loss: 5.3687\n",
            "Epoch 119/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7930 - val_loss: 4.4733\n",
            "Epoch 120/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8709 - val_loss: 4.7136\n",
            "Epoch 121/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8260 - val_loss: 5.1042\n",
            "Epoch 122/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7848 - val_loss: 4.7431\n",
            "Epoch 123/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8266 - val_loss: 6.5067\n",
            "Epoch 124/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8088 - val_loss: 6.7967\n",
            "Epoch 125/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8203 - val_loss: 6.6829\n",
            "Epoch 126/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7615 - val_loss: 4.9146\n",
            "Epoch 127/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8888 - val_loss: 6.3790\n",
            "Epoch 128/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7672 - val_loss: 5.5094\n",
            "Epoch 129/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7457 - val_loss: 7.2513\n",
            "Epoch 130/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8257 - val_loss: 5.9736\n",
            "Epoch 131/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7185 - val_loss: 6.7481\n",
            "Epoch 132/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8075 - val_loss: 6.8107\n",
            "Epoch 133/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7967 - val_loss: 5.7533\n",
            "Epoch 134/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8916 - val_loss: 5.0046\n",
            "Epoch 135/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8425 - val_loss: 8.2488\n",
            "Epoch 136/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8091 - val_loss: 6.6245\n",
            "Epoch 137/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7819 - val_loss: 9.1509\n",
            "Epoch 138/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7634 - val_loss: 7.3430\n",
            "Epoch 139/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7631 - val_loss: 6.5770\n",
            "Epoch 140/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7456 - val_loss: 7.3745\n",
            "Epoch 141/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7622 - val_loss: 6.7423\n",
            "Epoch 142/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8220 - val_loss: 5.8730\n",
            "Epoch 143/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8070 - val_loss: 6.9771\n",
            "Epoch 144/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8461 - val_loss: 9.4976\n",
            "Epoch 145/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8623 - val_loss: 9.3816\n",
            "Epoch 146/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8959 - val_loss: 8.7087\n",
            "Epoch 147/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7441 - val_loss: 6.2014\n",
            "Epoch 148/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7542 - val_loss: 7.7660\n",
            "Epoch 149/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8212 - val_loss: 9.7693\n",
            "Epoch 150/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7630 - val_loss: 11.2464\n",
            "Epoch 151/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7633 - val_loss: 10.2101\n",
            "Epoch 152/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8125 - val_loss: 12.5597\n",
            "Epoch 153/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7756 - val_loss: 10.0967\n",
            "Epoch 154/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7974 - val_loss: 9.3396\n",
            "Epoch 155/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8312 - val_loss: 11.3643\n",
            "Epoch 156/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8128 - val_loss: 14.9612\n",
            "Epoch 157/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7527 - val_loss: 10.2214\n",
            "Epoch 158/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8106 - val_loss: 10.7756\n",
            "Epoch 159/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8053 - val_loss: 10.6373\n",
            "Epoch 160/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7667 - val_loss: 11.3955\n",
            "Epoch 161/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8265 - val_loss: 13.3078\n",
            "Epoch 162/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7757 - val_loss: 13.6617\n",
            "Epoch 163/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8436 - val_loss: 13.5722\n",
            "Epoch 164/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8234 - val_loss: 10.0026\n",
            "Epoch 165/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8051 - val_loss: 12.8354\n",
            "Epoch 166/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7590 - val_loss: 12.2401\n",
            "Epoch 167/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8077 - val_loss: 9.8495\n",
            "Epoch 168/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7441 - val_loss: 12.4024\n",
            "Epoch 169/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8047 - val_loss: 11.6511\n",
            "Epoch 170/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8250 - val_loss: 13.6092\n",
            "Epoch 171/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8098 - val_loss: 11.8200\n",
            "Epoch 172/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7783 - val_loss: 11.9376\n",
            "Epoch 173/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8421 - val_loss: 13.5438\n",
            "Epoch 174/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8346 - val_loss: 14.4592\n",
            "Epoch 175/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7526 - val_loss: 11.1921\n",
            "Epoch 176/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7314 - val_loss: 13.0773\n",
            "Epoch 177/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7774 - val_loss: 11.6234\n",
            "Epoch 178/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7907 - val_loss: 16.5744\n",
            "Epoch 179/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7365 - val_loss: 13.4677\n",
            "Epoch 180/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7921 - val_loss: 12.1974\n",
            "Epoch 181/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7555 - val_loss: 11.1865\n",
            "Epoch 182/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7386 - val_loss: 10.8850\n",
            "Epoch 183/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7558 - val_loss: 17.3363\n",
            "Epoch 184/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7559 - val_loss: 14.3033\n",
            "Epoch 185/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7725 - val_loss: 17.8009\n",
            "Epoch 186/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7795 - val_loss: 11.9046\n",
            "Epoch 187/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7512 - val_loss: 15.5063\n",
            "Epoch 188/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8497 - val_loss: 12.2049\n",
            "Epoch 189/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7768 - val_loss: 13.7377\n",
            "Epoch 190/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8821 - val_loss: 10.7290\n",
            "Epoch 191/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8233 - val_loss: 21.9927\n",
            "Epoch 192/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8600 - val_loss: 15.3331\n",
            "Epoch 193/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8001 - val_loss: 13.8356\n",
            "Epoch 194/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8071 - val_loss: 23.0219\n",
            "Epoch 195/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8041 - val_loss: 14.6721\n",
            "Epoch 196/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8191 - val_loss: 19.8580\n",
            "Epoch 197/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7609 - val_loss: 17.3324\n",
            "Epoch 198/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7708 - val_loss: 15.7266\n",
            "Epoch 199/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7501 - val_loss: 17.2595\n",
            "Epoch 200/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7501 - val_loss: 15.1185\n",
            "Epoch 201/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8434 - val_loss: 17.8352\n",
            "Epoch 202/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7199 - val_loss: 19.5569\n",
            "Epoch 203/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7703 - val_loss: 16.1040\n",
            "Epoch 204/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7826 - val_loss: 15.7669\n",
            "Epoch 205/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8496 - val_loss: 17.5667\n",
            "Epoch 206/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8006 - val_loss: 16.6694\n",
            "Epoch 207/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7439 - val_loss: 15.5631\n",
            "Epoch 208/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7466 - val_loss: 17.5181\n",
            "Epoch 209/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7504 - val_loss: 16.1584\n",
            "Epoch 210/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7569 - val_loss: 16.3018\n",
            "Epoch 211/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8080 - val_loss: 20.9047\n",
            "Epoch 212/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8780 - val_loss: 20.1097\n",
            "Epoch 213/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7924 - val_loss: 19.4661\n",
            "Epoch 214/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7784 - val_loss: 15.8038\n",
            "Epoch 215/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7793 - val_loss: 17.8013\n",
            "Epoch 216/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7380 - val_loss: 19.5639\n",
            "Epoch 217/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8446 - val_loss: 24.1764\n",
            "Epoch 218/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7845 - val_loss: 22.9909\n",
            "Epoch 219/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7440 - val_loss: 23.1159\n",
            "Epoch 220/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7323 - val_loss: 27.2547\n",
            "Epoch 221/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7594 - val_loss: 20.3512\n",
            "Epoch 222/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8204 - val_loss: 17.8896\n",
            "Epoch 223/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8046 - val_loss: 19.8737\n",
            "Epoch 224/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7240 - val_loss: 17.0624\n",
            "Epoch 225/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7218 - val_loss: 25.0870\n",
            "Epoch 226/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7120 - val_loss: 19.0761\n",
            "Epoch 227/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7573 - val_loss: 18.2114\n",
            "Epoch 228/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7243 - val_loss: 20.3004\n",
            "Epoch 229/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8082 - val_loss: 19.4595\n",
            "Epoch 230/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7764 - val_loss: 18.8242\n",
            "Epoch 231/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7708 - val_loss: 26.0950\n",
            "Epoch 232/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8018 - val_loss: 22.0318\n",
            "Epoch 233/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7890 - val_loss: 20.6089\n",
            "Epoch 234/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8649 - val_loss: 22.4791\n",
            "Epoch 235/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7298 - val_loss: 26.4951\n",
            "Epoch 236/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7991 - val_loss: 26.2026\n",
            "Epoch 237/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7986 - val_loss: 19.0247\n",
            "Epoch 238/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7822 - val_loss: 21.8457\n",
            "Epoch 239/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7625 - val_loss: 29.2128\n",
            "Epoch 240/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7847 - val_loss: 20.3152\n",
            "Epoch 241/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7889 - val_loss: 35.3559\n",
            "Epoch 242/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7520 - val_loss: 21.0081\n",
            "Epoch 243/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7929 - val_loss: 18.2815\n",
            "Epoch 244/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7586 - val_loss: 16.4018\n",
            "Epoch 245/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7822 - val_loss: 28.5063\n",
            "Epoch 246/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7702 - val_loss: 27.0990\n",
            "Epoch 247/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7394 - val_loss: 17.2633\n",
            "Epoch 248/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7415 - val_loss: 20.6141\n",
            "Epoch 249/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7679 - val_loss: 20.5468\n",
            "Epoch 250/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8051 - val_loss: 22.3359\n",
            "Epoch 251/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7449 - val_loss: 21.9050\n",
            "Epoch 252/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7534 - val_loss: 15.8500\n",
            "Epoch 253/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8512 - val_loss: 21.4272\n",
            "Epoch 254/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7185 - val_loss: 19.0218\n",
            "Epoch 255/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7382 - val_loss: 18.2361\n",
            "Epoch 256/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8863 - val_loss: 19.1874\n",
            "Epoch 257/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7595 - val_loss: 20.1180\n",
            "Epoch 258/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7802 - val_loss: 20.7611\n",
            "Epoch 259/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7858 - val_loss: 20.7011\n",
            "Epoch 260/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7159 - val_loss: 19.2486\n",
            "Epoch 261/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7510 - val_loss: 20.2898\n",
            "Epoch 262/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7688 - val_loss: 21.9469\n",
            "Epoch 263/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7740 - val_loss: 17.9972\n",
            "Epoch 264/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7444 - val_loss: 16.9742\n",
            "Epoch 265/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7863 - val_loss: 26.2384\n",
            "Epoch 266/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7931 - val_loss: 25.9759\n",
            "Epoch 267/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7567 - val_loss: 22.3031\n",
            "Epoch 268/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8438 - val_loss: 23.5772\n",
            "Epoch 269/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7963 - val_loss: 23.9631\n",
            "Epoch 270/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7675 - val_loss: 24.2341\n",
            "Epoch 271/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7493 - val_loss: 20.6700\n",
            "Epoch 272/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7491 - val_loss: 30.5289\n",
            "Epoch 273/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7625 - val_loss: 22.1789\n",
            "Epoch 274/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8225 - val_loss: 22.1164\n",
            "Epoch 275/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7443 - val_loss: 23.7941\n",
            "Epoch 276/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7143 - val_loss: 25.1069\n",
            "Epoch 277/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7946 - val_loss: 26.3043\n",
            "Epoch 278/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7554 - val_loss: 29.4936\n",
            "Epoch 279/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7863 - val_loss: 24.9035\n",
            "Epoch 280/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7841 - val_loss: 23.6633\n",
            "Epoch 281/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7991 - val_loss: 23.9109\n",
            "Epoch 282/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7930 - val_loss: 19.2282\n",
            "Epoch 283/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8207 - val_loss: 26.2553\n",
            "Epoch 284/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7341 - val_loss: 21.3424\n",
            "Epoch 285/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8135 - val_loss: 25.8669\n",
            "Epoch 286/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8076 - val_loss: 28.4195\n",
            "Epoch 287/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7728 - val_loss: 31.1978\n",
            "Epoch 288/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7380 - val_loss: 18.5985\n",
            "Epoch 289/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7600 - val_loss: 25.5980\n",
            "Epoch 290/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7430 - val_loss: 20.0061\n",
            "Epoch 291/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8251 - val_loss: 18.4550\n",
            "Epoch 292/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7574 - val_loss: 26.1145\n",
            "Epoch 293/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7996 - val_loss: 29.2853\n",
            "Epoch 294/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8264 - val_loss: 22.0628\n",
            "Epoch 295/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7265 - val_loss: 22.7066\n",
            "Epoch 296/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7717 - val_loss: 26.7213\n",
            "Epoch 297/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7424 - val_loss: 29.6095\n",
            "Epoch 298/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7552 - val_loss: 34.1168\n",
            "Epoch 299/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7323 - val_loss: 28.1893\n",
            "Epoch 300/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7713 - val_loss: 24.0480\n",
            "Epoch 301/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7804 - val_loss: 23.7065\n",
            "Epoch 302/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7922 - val_loss: 22.2242\n",
            "Epoch 303/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7882 - val_loss: 19.5076\n",
            "Epoch 304/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7264 - val_loss: 24.8779\n",
            "Epoch 305/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8009 - val_loss: 23.5574\n",
            "Epoch 306/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7559 - val_loss: 23.2449\n",
            "Epoch 307/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8259 - val_loss: 22.3282\n",
            "Epoch 308/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7267 - val_loss: 28.2555\n",
            "Epoch 309/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7670 - val_loss: 25.1211\n",
            "Epoch 310/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8419 - val_loss: 24.3232\n",
            "Epoch 311/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8000 - val_loss: 20.3561\n",
            "Epoch 312/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7700 - val_loss: 31.0030\n",
            "Epoch 313/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8612 - val_loss: 24.8999\n",
            "Epoch 314/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7804 - val_loss: 26.1005\n",
            "Epoch 315/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7175 - val_loss: 21.7795\n",
            "Epoch 316/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7017 - val_loss: 23.1065\n",
            "Epoch 317/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7563 - val_loss: 28.3132\n",
            "Epoch 318/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8929 - val_loss: 26.0843\n",
            "Epoch 319/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7774 - val_loss: 23.4413\n",
            "Epoch 320/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7351 - val_loss: 29.8359\n",
            "Epoch 321/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6880 - val_loss: 24.7792\n",
            "Epoch 322/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7670 - val_loss: 22.4933\n",
            "Epoch 323/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7650 - val_loss: 24.0572\n",
            "Epoch 324/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7538 - val_loss: 22.3619\n",
            "Epoch 325/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7526 - val_loss: 30.6846\n",
            "Epoch 326/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7835 - val_loss: 26.3691\n",
            "Epoch 327/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7834 - val_loss: 25.2209\n",
            "Epoch 328/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7378 - val_loss: 26.5043\n",
            "Epoch 329/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7829 - val_loss: 25.0831\n",
            "Epoch 330/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7230 - val_loss: 27.7789\n",
            "Epoch 331/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7878 - val_loss: 23.8843\n",
            "Epoch 332/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7147 - val_loss: 22.9958\n",
            "Epoch 333/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7550 - val_loss: 25.6525\n",
            "Epoch 334/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7754 - val_loss: 22.7007\n",
            "Epoch 335/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7930 - val_loss: 27.9142\n",
            "Epoch 336/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7741 - val_loss: 18.0868\n",
            "Epoch 337/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7785 - val_loss: 28.1582\n",
            "Epoch 338/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7808 - val_loss: 32.8286\n",
            "Epoch 339/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8092 - val_loss: 28.8434\n",
            "Epoch 340/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7720 - val_loss: 27.1532\n",
            "Epoch 341/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7736 - val_loss: 23.8184\n",
            "Epoch 342/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7930 - val_loss: 36.0692\n",
            "Epoch 343/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8930 - val_loss: 30.0712\n",
            "Epoch 344/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8275 - val_loss: 19.9498\n",
            "Epoch 345/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8587 - val_loss: 31.6609\n",
            "Epoch 346/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8136 - val_loss: 26.5799\n",
            "Epoch 347/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7217 - val_loss: 22.7765\n",
            "Epoch 348/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8005 - val_loss: 23.5141\n",
            "Epoch 349/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7025 - val_loss: 24.0199\n",
            "Epoch 350/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7313 - val_loss: 25.4902\n",
            "Epoch 351/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7664 - val_loss: 31.5564\n",
            "Epoch 352/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8674 - val_loss: 24.6679\n",
            "Epoch 353/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7720 - val_loss: 23.7350\n",
            "Epoch 354/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7888 - val_loss: 25.0317\n",
            "Epoch 355/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7811 - val_loss: 39.3221\n",
            "Epoch 356/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7676 - val_loss: 25.7502\n",
            "Epoch 357/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7927 - val_loss: 27.7053\n",
            "Epoch 358/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8010 - val_loss: 28.2053\n",
            "Epoch 359/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7676 - val_loss: 39.2113\n",
            "Epoch 360/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8163 - val_loss: 21.1179\n",
            "Epoch 361/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7144 - val_loss: 35.4004\n",
            "Epoch 362/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7400 - val_loss: 25.4045\n",
            "Epoch 363/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7036 - val_loss: 21.5766\n",
            "Epoch 364/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7991 - val_loss: 22.7812\n",
            "Epoch 365/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7982 - val_loss: 29.0192\n",
            "Epoch 366/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7634 - val_loss: 29.2550\n",
            "Epoch 367/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7636 - val_loss: 29.0034\n",
            "Epoch 368/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7732 - val_loss: 25.0946\n",
            "Epoch 369/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.9736 - val_loss: 21.6266\n",
            "Epoch 370/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6957 - val_loss: 19.2041\n",
            "Epoch 371/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8462 - val_loss: 24.6777\n",
            "Epoch 372/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7579 - val_loss: 24.0126\n",
            "Epoch 373/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7600 - val_loss: 29.9811\n",
            "Epoch 374/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8265 - val_loss: 22.9363\n",
            "Epoch 375/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8129 - val_loss: 26.4740\n",
            "Epoch 376/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7597 - val_loss: 22.6957\n",
            "Epoch 377/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7366 - val_loss: 30.2479\n",
            "Epoch 378/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7863 - val_loss: 26.2020\n",
            "Epoch 379/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7084 - val_loss: 29.4196\n",
            "Epoch 380/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8050 - val_loss: 19.6431\n",
            "Epoch 381/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7846 - val_loss: 28.7512\n",
            "Epoch 382/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8601 - val_loss: 23.0575\n",
            "Epoch 383/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7909 - val_loss: 26.3310\n",
            "Epoch 384/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7631 - val_loss: 25.5050\n",
            "Epoch 385/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7424 - val_loss: 28.2871\n",
            "Epoch 386/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8039 - val_loss: 28.4928\n",
            "Epoch 387/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7255 - val_loss: 26.0571\n",
            "Epoch 388/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7791 - val_loss: 27.7297\n",
            "Epoch 389/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7293 - val_loss: 25.8512\n",
            "Epoch 390/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8046 - val_loss: 32.0887\n",
            "Epoch 391/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7683 - val_loss: 26.6009\n",
            "Epoch 392/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7584 - val_loss: 28.2343\n",
            "Epoch 393/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8118 - val_loss: 22.0210\n",
            "Epoch 394/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8572 - val_loss: 26.2545\n",
            "Epoch 395/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7150 - val_loss: 21.9658\n",
            "Epoch 396/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7789 - val_loss: 28.8477\n",
            "Epoch 397/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7158 - val_loss: 29.8275\n",
            "Epoch 398/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7199 - val_loss: 27.9611\n",
            "Epoch 399/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7595 - val_loss: 22.8831\n",
            "Epoch 400/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7479 - val_loss: 26.9739\n",
            "Epoch 401/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7148 - val_loss: 26.8733\n",
            "Epoch 402/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7678 - val_loss: 27.0584\n",
            "Epoch 403/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7389 - val_loss: 22.9346\n",
            "Epoch 404/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7447 - val_loss: 33.1608\n",
            "Epoch 405/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6977 - val_loss: 21.5394\n",
            "Epoch 406/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8101 - val_loss: 27.8370\n",
            "Epoch 407/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8867 - val_loss: 24.4054\n",
            "Epoch 408/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7093 - val_loss: 27.6673\n",
            "Epoch 409/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7269 - val_loss: 25.8148\n",
            "Epoch 410/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7163 - val_loss: 34.4537\n",
            "Epoch 411/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7773 - val_loss: 32.3643\n",
            "Epoch 412/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7200 - val_loss: 21.9310\n",
            "Epoch 413/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7404 - val_loss: 26.5657\n",
            "Epoch 414/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8725 - val_loss: 22.5407\n",
            "Epoch 415/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7886 - val_loss: 28.4078\n",
            "Epoch 416/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7396 - val_loss: 32.6323\n",
            "Epoch 417/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7433 - val_loss: 21.5364\n",
            "Epoch 418/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7438 - val_loss: 23.1399\n",
            "Epoch 419/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6902 - val_loss: 23.7767\n",
            "Epoch 420/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7022 - val_loss: 26.8352\n",
            "Epoch 421/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7659 - val_loss: 28.7072\n",
            "Epoch 422/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8093 - val_loss: 22.4677\n",
            "Epoch 423/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6787 - val_loss: 29.6413\n",
            "Epoch 424/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8711 - val_loss: 27.7084\n",
            "Epoch 425/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7982 - val_loss: 29.2911\n",
            "Epoch 426/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7851 - val_loss: 29.4290\n",
            "Epoch 427/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8017 - val_loss: 29.8871\n",
            "Epoch 428/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7940 - val_loss: 24.5407\n",
            "Epoch 429/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7057 - val_loss: 23.3673\n",
            "Epoch 430/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7337 - val_loss: 27.0647\n",
            "Epoch 431/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8166 - val_loss: 29.3035\n",
            "Epoch 432/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7867 - val_loss: 20.9073\n",
            "Epoch 433/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7701 - val_loss: 23.6399\n",
            "Epoch 434/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7176 - val_loss: 22.5830\n",
            "Epoch 435/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7905 - val_loss: 26.2617\n",
            "Epoch 436/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7056 - val_loss: 27.9643\n",
            "Epoch 437/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6862 - val_loss: 22.8459\n",
            "Epoch 438/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8183 - val_loss: 31.0212\n",
            "Epoch 439/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7641 - val_loss: 26.1897\n",
            "Epoch 440/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7405 - val_loss: 30.5651\n",
            "Epoch 441/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7916 - val_loss: 20.0240\n",
            "Epoch 442/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7788 - val_loss: 26.1870\n",
            "Epoch 443/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7412 - val_loss: 36.2033\n",
            "Epoch 444/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7256 - val_loss: 29.5580\n",
            "Epoch 445/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7368 - val_loss: 28.3520\n",
            "Epoch 446/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7589 - val_loss: 22.8743\n",
            "Epoch 447/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7091 - val_loss: 32.3050\n",
            "Epoch 448/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7757 - val_loss: 30.6086\n",
            "Epoch 449/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7478 - val_loss: 22.2817\n",
            "Epoch 450/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7979 - val_loss: 34.3453\n",
            "Epoch 451/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8185 - val_loss: 27.8627\n",
            "Epoch 452/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7739 - val_loss: 21.0423\n",
            "Epoch 453/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8236 - val_loss: 22.8762\n",
            "Epoch 454/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7903 - val_loss: 22.7156\n",
            "Epoch 455/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7416 - val_loss: 25.1666\n",
            "Epoch 456/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7574 - val_loss: 29.4650\n",
            "Epoch 457/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8071 - val_loss: 16.1366\n",
            "Epoch 458/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7238 - val_loss: 23.4489\n",
            "Epoch 459/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8239 - val_loss: 23.5238\n",
            "Epoch 460/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7638 - val_loss: 31.6476\n",
            "Epoch 461/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8046 - val_loss: 23.2467\n",
            "Epoch 462/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7429 - val_loss: 23.4019\n",
            "Epoch 463/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8518 - val_loss: 22.3142\n",
            "Epoch 464/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7430 - val_loss: 25.4029\n",
            "Epoch 465/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7942 - val_loss: 24.9549\n",
            "Epoch 466/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7910 - val_loss: 19.0540\n",
            "Epoch 467/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7798 - val_loss: 18.9521\n",
            "Epoch 468/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8329 - val_loss: 31.2468\n",
            "Epoch 469/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7597 - val_loss: 17.2903\n",
            "Epoch 470/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7669 - val_loss: 27.6367\n",
            "Epoch 471/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8316 - val_loss: 31.3808\n",
            "Epoch 472/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8135 - val_loss: 22.1115\n",
            "Epoch 473/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7427 - val_loss: 19.4226\n",
            "Epoch 474/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7422 - val_loss: 25.5257\n",
            "Epoch 475/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7311 - val_loss: 24.5058\n",
            "Epoch 476/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7277 - val_loss: 27.4932\n",
            "Epoch 477/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7533 - val_loss: 33.6656\n",
            "Epoch 478/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7520 - val_loss: 24.0887\n",
            "Epoch 479/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7537 - val_loss: 41.8592\n",
            "Epoch 480/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7895 - val_loss: 22.6471\n",
            "Epoch 481/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7305 - val_loss: 21.8500\n",
            "Epoch 482/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6967 - val_loss: 22.0230\n",
            "Epoch 483/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7374 - val_loss: 25.2428\n",
            "Epoch 484/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7569 - val_loss: 27.4214\n",
            "Epoch 485/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7301 - val_loss: 22.1099\n",
            "Epoch 486/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7795 - val_loss: 25.6181\n",
            "Epoch 487/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7714 - val_loss: 28.8685\n",
            "Epoch 488/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7203 - val_loss: 31.0138\n",
            "Epoch 489/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7678 - val_loss: 20.0790\n",
            "Epoch 490/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6843 - val_loss: 19.2981\n",
            "Epoch 491/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7935 - val_loss: 21.2732\n",
            "Epoch 492/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7411 - val_loss: 22.6062\n",
            "Epoch 493/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7571 - val_loss: 25.5988\n",
            "Epoch 494/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7423 - val_loss: 23.9921\n",
            "Epoch 495/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7561 - val_loss: 21.9890\n",
            "Epoch 496/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7275 - val_loss: 20.8294\n",
            "Epoch 497/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7106 - val_loss: 25.5059\n",
            "Epoch 498/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8017 - val_loss: 23.8808\n",
            "Epoch 499/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8138 - val_loss: 23.1612\n",
            "Epoch 500/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8028 - val_loss: 24.1329\n",
            "Epoch 501/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7164 - val_loss: 19.1146\n",
            "Epoch 502/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7366 - val_loss: 27.2378\n",
            "Epoch 503/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7950 - val_loss: 21.9097\n",
            "Epoch 504/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8194 - val_loss: 25.7313\n",
            "Epoch 505/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7517 - val_loss: 24.6955\n",
            "Epoch 506/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7341 - val_loss: 21.6997\n",
            "Epoch 507/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7398 - val_loss: 24.5119\n",
            "Epoch 508/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7061 - val_loss: 25.2702\n",
            "Epoch 509/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7997 - val_loss: 27.4685\n",
            "Epoch 510/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7295 - val_loss: 22.1267\n",
            "Epoch 511/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7463 - val_loss: 24.6583\n",
            "Epoch 512/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8235 - val_loss: 22.9579\n",
            "Epoch 513/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7986 - val_loss: 22.0502\n",
            "Epoch 514/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7877 - val_loss: 24.5887\n",
            "Epoch 515/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9327 - val_loss: 23.4394\n",
            "Epoch 516/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7061 - val_loss: 25.0217\n",
            "Epoch 517/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7928 - val_loss: 23.3825\n",
            "Epoch 518/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7803 - val_loss: 22.9351\n",
            "Epoch 519/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7834 - val_loss: 17.2556\n",
            "Epoch 520/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7248 - val_loss: 21.5169\n",
            "Epoch 521/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7555 - val_loss: 24.8853\n",
            "Epoch 522/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6891 - val_loss: 24.2273\n",
            "Epoch 523/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7260 - val_loss: 24.2426\n",
            "Epoch 524/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7343 - val_loss: 21.5304\n",
            "Epoch 525/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8091 - val_loss: 24.5599\n",
            "Epoch 526/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7685 - val_loss: 23.4844\n",
            "Epoch 527/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7079 - val_loss: 29.1789\n",
            "Epoch 528/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8414 - val_loss: 23.4861\n",
            "Epoch 529/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8414 - val_loss: 17.8141\n",
            "Epoch 530/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7448 - val_loss: 21.1783\n",
            "Epoch 531/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7509 - val_loss: 26.1280\n",
            "Epoch 532/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7903 - val_loss: 17.2266\n",
            "Epoch 533/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8132 - val_loss: 27.3400\n",
            "Epoch 534/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7971 - val_loss: 23.9784\n",
            "Epoch 535/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6588 - val_loss: 24.5289\n",
            "Epoch 536/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7503 - val_loss: 22.8264\n",
            "Epoch 537/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7405 - val_loss: 27.3407\n",
            "Epoch 538/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7399 - val_loss: 21.0823\n",
            "Epoch 539/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7806 - val_loss: 21.1198\n",
            "Epoch 540/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8227 - val_loss: 24.5422\n",
            "Epoch 541/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7717 - val_loss: 19.2507\n",
            "Epoch 542/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6824 - val_loss: 19.3982\n",
            "Epoch 543/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7590 - val_loss: 22.1556\n",
            "Epoch 544/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7592 - val_loss: 20.8838\n",
            "Epoch 545/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7253 - val_loss: 22.3339\n",
            "Epoch 546/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7873 - val_loss: 26.4220\n",
            "Epoch 547/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6883 - val_loss: 19.0708\n",
            "Epoch 548/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7509 - val_loss: 22.8373\n",
            "Epoch 549/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7682 - val_loss: 26.2221\n",
            "Epoch 550/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6567 - val_loss: 27.4777\n",
            "Epoch 551/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7317 - val_loss: 26.2679\n",
            "Epoch 552/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6884 - val_loss: 22.6592\n",
            "Epoch 553/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7315 - val_loss: 18.5216\n",
            "Epoch 554/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7474 - val_loss: 23.9137\n",
            "Epoch 555/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6900 - val_loss: 22.9411\n",
            "Epoch 556/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7787 - val_loss: 26.9651\n",
            "Epoch 557/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7600 - val_loss: 21.1017\n",
            "Epoch 558/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7712 - val_loss: 22.0176\n",
            "Epoch 559/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7473 - val_loss: 24.2762\n",
            "Epoch 560/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7056 - val_loss: 26.5667\n",
            "Epoch 561/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7518 - val_loss: 19.5892\n",
            "Epoch 562/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7031 - val_loss: 27.4339\n",
            "Epoch 563/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7490 - val_loss: 19.6314\n",
            "Epoch 564/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7196 - val_loss: 25.1833\n",
            "Epoch 565/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7114 - val_loss: 21.4425\n",
            "Epoch 566/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7576 - val_loss: 22.9722\n",
            "Epoch 567/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7175 - val_loss: 23.4459\n",
            "Epoch 568/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7938 - val_loss: 21.2869\n",
            "Epoch 569/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8310 - val_loss: 28.7418\n",
            "Epoch 570/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7477 - val_loss: 20.8928\n",
            "Epoch 571/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7483 - val_loss: 37.7440\n",
            "Epoch 572/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7428 - val_loss: 20.1092\n",
            "Epoch 573/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7371 - val_loss: 18.9520\n",
            "Epoch 574/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7885 - val_loss: 23.3726\n",
            "Epoch 575/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7785 - val_loss: 21.3445\n",
            "Epoch 576/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8188 - val_loss: 23.2261\n",
            "Epoch 577/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7757 - val_loss: 18.0884\n",
            "Epoch 578/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7815 - val_loss: 18.6102\n",
            "Epoch 579/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7372 - val_loss: 19.2956\n",
            "Epoch 580/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7686 - val_loss: 24.8360\n",
            "Epoch 581/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7516 - val_loss: 22.1388\n",
            "Epoch 582/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7020 - val_loss: 29.4975\n",
            "Epoch 583/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6851 - val_loss: 18.3423\n",
            "Epoch 584/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7512 - val_loss: 21.8021\n",
            "Epoch 585/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7624 - val_loss: 17.9752\n",
            "Epoch 586/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7745 - val_loss: 17.0895\n",
            "Epoch 587/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7531 - val_loss: 23.3794\n",
            "Epoch 588/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7791 - val_loss: 27.0389\n",
            "Epoch 589/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6970 - val_loss: 22.5089\n",
            "Epoch 590/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7738 - val_loss: 19.0760\n",
            "Epoch 591/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7267 - val_loss: 21.2760\n",
            "Epoch 592/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8201 - val_loss: 20.6935\n",
            "Epoch 593/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7336 - val_loss: 23.3903\n",
            "Epoch 594/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6891 - val_loss: 20.1045\n",
            "Epoch 595/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7210 - val_loss: 17.9871\n",
            "Epoch 596/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7881 - val_loss: 24.9779\n",
            "Epoch 597/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7749 - val_loss: 21.2153\n",
            "Epoch 598/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6794 - val_loss: 19.8716\n",
            "Epoch 599/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7822 - val_loss: 20.7994\n",
            "Epoch 600/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6773 - val_loss: 16.9127\n",
            "Epoch 601/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6857 - val_loss: 17.0650\n",
            "Epoch 602/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7928 - val_loss: 19.4196\n",
            "Epoch 603/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7327 - val_loss: 18.7320\n",
            "Epoch 604/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6978 - val_loss: 19.1514\n",
            "Epoch 605/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7562 - val_loss: 31.0904\n",
            "Epoch 606/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6985 - val_loss: 19.4630\n",
            "Epoch 607/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8549 - val_loss: 18.5694\n",
            "Epoch 608/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7381 - val_loss: 27.7199\n",
            "Epoch 609/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8312 - val_loss: 21.5932\n",
            "Epoch 610/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7669 - val_loss: 20.8608\n",
            "Epoch 611/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7517 - val_loss: 19.9319\n",
            "Epoch 612/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7887 - val_loss: 17.6606\n",
            "Epoch 613/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6739 - val_loss: 23.0930\n",
            "Epoch 614/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7630 - val_loss: 16.7213\n",
            "Epoch 615/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7884 - val_loss: 19.6415\n",
            "Epoch 616/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7842 - val_loss: 18.7207\n",
            "Epoch 617/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7526 - val_loss: 17.6095\n",
            "Epoch 618/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8032 - val_loss: 23.9525\n",
            "Epoch 619/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8862 - val_loss: 21.8430\n",
            "Epoch 620/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7456 - val_loss: 21.4642\n",
            "Epoch 621/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7056 - val_loss: 15.6299\n",
            "Epoch 622/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7207 - val_loss: 22.0944\n",
            "Epoch 623/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8967 - val_loss: 30.3212\n",
            "Epoch 624/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7331 - val_loss: 21.2548\n",
            "Epoch 625/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7694 - val_loss: 18.2789\n",
            "Epoch 626/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7314 - val_loss: 18.9617\n",
            "Epoch 627/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7452 - val_loss: 19.1208\n",
            "Epoch 628/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7164 - val_loss: 18.4854\n",
            "Epoch 629/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7766 - val_loss: 24.9048\n",
            "Epoch 630/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7573 - val_loss: 21.4766\n",
            "Epoch 631/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7410 - val_loss: 20.9101\n",
            "Epoch 632/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7315 - val_loss: 25.5062\n",
            "Epoch 633/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7907 - val_loss: 21.3156\n",
            "Epoch 634/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7525 - val_loss: 19.8988\n",
            "Epoch 635/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7212 - val_loss: 19.4150\n",
            "Epoch 636/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7481 - val_loss: 14.0877\n",
            "Epoch 637/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7791 - val_loss: 18.8220\n",
            "Epoch 638/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7334 - val_loss: 19.6744\n",
            "Epoch 639/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7303 - val_loss: 15.4594\n",
            "Epoch 640/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7529 - val_loss: 20.5824\n",
            "Epoch 641/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7404 - val_loss: 21.0954\n",
            "Epoch 642/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8317 - val_loss: 16.8344\n",
            "Epoch 643/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7325 - val_loss: 18.0665\n",
            "Epoch 644/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7202 - val_loss: 20.9980\n",
            "Epoch 645/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6793 - val_loss: 25.7128\n",
            "Epoch 646/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8378 - val_loss: 22.6450\n",
            "Epoch 647/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7358 - val_loss: 16.2941\n",
            "Epoch 648/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6878 - val_loss: 20.0544\n",
            "Epoch 649/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8099 - val_loss: 18.1619\n",
            "Epoch 650/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7313 - val_loss: 16.3716\n",
            "Epoch 651/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7860 - val_loss: 24.2880\n",
            "Epoch 652/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7289 - val_loss: 20.2627\n",
            "Epoch 653/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7374 - val_loss: 20.1354\n",
            "Epoch 654/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7582 - val_loss: 19.5255\n",
            "Epoch 655/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7290 - val_loss: 18.1788\n",
            "Epoch 656/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7094 - val_loss: 21.1504\n",
            "Epoch 657/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7552 - val_loss: 20.5340\n",
            "Epoch 658/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7550 - val_loss: 22.0805\n",
            "Epoch 659/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7785 - val_loss: 24.5316\n",
            "Epoch 660/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6719 - val_loss: 17.6053\n",
            "Epoch 661/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7332 - val_loss: 24.1503\n",
            "Epoch 662/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7990 - val_loss: 18.5707\n",
            "Epoch 663/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7320 - val_loss: 21.5292\n",
            "Epoch 664/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7294 - val_loss: 17.4268\n",
            "Epoch 665/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7287 - val_loss: 21.4657\n",
            "Epoch 666/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7018 - val_loss: 15.6703\n",
            "Epoch 667/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7268 - val_loss: 19.8843\n",
            "Epoch 668/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7205 - val_loss: 20.6183\n",
            "Epoch 669/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7651 - val_loss: 23.9493\n",
            "Epoch 670/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7076 - val_loss: 19.4452\n",
            "Epoch 671/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6977 - val_loss: 18.6637\n",
            "Epoch 672/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6723 - val_loss: 21.8684\n",
            "Epoch 673/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7542 - val_loss: 22.4309\n",
            "Epoch 674/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7767 - val_loss: 29.9948\n",
            "Epoch 675/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7167 - val_loss: 14.3542\n",
            "Epoch 676/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7576 - val_loss: 24.4935\n",
            "Epoch 677/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6659 - val_loss: 20.2884\n",
            "Epoch 678/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6999 - val_loss: 18.9880\n",
            "Epoch 679/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7925 - val_loss: 20.4319\n",
            "Epoch 680/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7424 - val_loss: 21.0997\n",
            "Epoch 681/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7382 - val_loss: 20.1887\n",
            "Epoch 682/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7597 - val_loss: 19.4167\n",
            "Epoch 683/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7508 - val_loss: 20.2882\n",
            "Epoch 684/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7999 - val_loss: 18.8299\n",
            "Epoch 685/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6978 - val_loss: 22.1950\n",
            "Epoch 686/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7805 - val_loss: 18.2219\n",
            "Epoch 687/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7276 - val_loss: 15.5142\n",
            "Epoch 688/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8171 - val_loss: 19.8248\n",
            "Epoch 689/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6843 - val_loss: 20.3741\n",
            "Epoch 690/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7407 - val_loss: 17.5414\n",
            "Epoch 691/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7250 - val_loss: 18.3746\n",
            "Epoch 692/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7548 - val_loss: 14.6867\n",
            "Epoch 693/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6841 - val_loss: 18.3347\n",
            "Epoch 694/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7418 - val_loss: 20.6818\n",
            "Epoch 695/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7495 - val_loss: 27.3644\n",
            "Epoch 696/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7317 - val_loss: 22.5579\n",
            "Epoch 697/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7502 - val_loss: 19.4320\n",
            "Epoch 698/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8524 - val_loss: 17.6224\n",
            "Epoch 699/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7442 - val_loss: 19.2688\n",
            "Epoch 700/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7974 - val_loss: 16.4769\n",
            "Epoch 701/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8003 - val_loss: 19.2326\n",
            "Epoch 702/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7786 - val_loss: 17.2053\n",
            "Epoch 703/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7453 - val_loss: 19.8758\n",
            "Epoch 704/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7798 - val_loss: 18.0113\n",
            "Epoch 705/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7552 - val_loss: 14.9582\n",
            "Epoch 706/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7276 - val_loss: 21.6120\n",
            "Epoch 707/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7273 - val_loss: 17.1118\n",
            "Epoch 708/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7476 - val_loss: 17.1445\n",
            "Epoch 709/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7266 - val_loss: 20.3125\n",
            "Epoch 710/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7308 - val_loss: 16.2915\n",
            "Epoch 711/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7405 - val_loss: 17.5808\n",
            "Epoch 712/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8362 - val_loss: 14.9900\n",
            "Epoch 713/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8407 - val_loss: 16.2521\n",
            "Epoch 714/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7021 - val_loss: 22.0199\n",
            "Epoch 715/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7554 - val_loss: 18.0213\n",
            "Epoch 716/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7817 - val_loss: 18.3096\n",
            "Epoch 717/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7684 - val_loss: 17.7776\n",
            "Epoch 718/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6997 - val_loss: 17.5448\n",
            "Epoch 719/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7052 - val_loss: 20.2630\n",
            "Epoch 720/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7290 - val_loss: 22.5543\n",
            "Epoch 721/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7462 - val_loss: 19.0542\n",
            "Epoch 722/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8062 - val_loss: 17.3848\n",
            "Epoch 723/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7141 - val_loss: 13.7959\n",
            "Epoch 724/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7914 - val_loss: 16.6142\n",
            "Epoch 725/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7554 - val_loss: 16.6046\n",
            "Epoch 726/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7879 - val_loss: 16.8251\n",
            "Epoch 727/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7219 - val_loss: 15.8176\n",
            "Epoch 728/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7456 - val_loss: 14.9718\n",
            "Epoch 729/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7633 - val_loss: 16.0342\n",
            "Epoch 730/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7609 - val_loss: 17.9006\n",
            "Epoch 731/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7504 - val_loss: 14.3996\n",
            "Epoch 732/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8502 - val_loss: 19.3979\n",
            "Epoch 733/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7818 - val_loss: 16.3694\n",
            "Epoch 734/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7405 - val_loss: 18.7809\n",
            "Epoch 735/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7218 - val_loss: 15.5461\n",
            "Epoch 736/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8342 - val_loss: 16.2019\n",
            "Epoch 737/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7525 - val_loss: 14.8980\n",
            "Epoch 738/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7660 - val_loss: 19.7952\n",
            "Epoch 739/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7658 - val_loss: 15.1016\n",
            "Epoch 740/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6971 - val_loss: 15.9454\n",
            "Epoch 741/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7557 - val_loss: 17.4206\n",
            "Epoch 742/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7054 - val_loss: 17.8008\n",
            "Epoch 743/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7374 - val_loss: 17.4044\n",
            "Epoch 744/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7230 - val_loss: 14.4545\n",
            "Epoch 745/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6943 - val_loss: 15.0059\n",
            "Epoch 746/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8605 - val_loss: 18.1206\n",
            "Epoch 747/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6848 - val_loss: 18.7715\n",
            "Epoch 748/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7447 - val_loss: 15.1790\n",
            "Epoch 749/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7647 - val_loss: 17.1635\n",
            "Epoch 750/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7064 - val_loss: 18.4709\n",
            "Epoch 751/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7398 - val_loss: 14.8520\n",
            "Epoch 752/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7391 - val_loss: 14.7095\n",
            "Epoch 753/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7650 - val_loss: 11.7792\n",
            "Epoch 754/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8115 - val_loss: 14.4708\n",
            "Epoch 755/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7458 - val_loss: 13.9089\n",
            "Epoch 756/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7264 - val_loss: 13.3938\n",
            "Epoch 757/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7238 - val_loss: 16.6967\n",
            "Epoch 758/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7487 - val_loss: 12.9683\n",
            "Epoch 759/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7594 - val_loss: 16.7007\n",
            "Epoch 760/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7245 - val_loss: 13.1146\n",
            "Epoch 761/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7560 - val_loss: 17.0466\n",
            "Epoch 762/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6727 - val_loss: 15.2717\n",
            "Epoch 763/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7639 - val_loss: 15.0628\n",
            "Epoch 764/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7455 - val_loss: 18.6835\n",
            "Epoch 765/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7951 - val_loss: 19.6558\n",
            "Epoch 766/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7155 - val_loss: 16.1843\n",
            "Epoch 767/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7386 - val_loss: 15.2405\n",
            "Epoch 768/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7381 - val_loss: 18.9013\n",
            "Epoch 769/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7632 - val_loss: 22.2416\n",
            "Epoch 770/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7699 - val_loss: 16.2018\n",
            "Epoch 771/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8403 - val_loss: 20.3149\n",
            "Epoch 772/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7592 - val_loss: 15.9732\n",
            "Epoch 773/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7214 - val_loss: 15.8410\n",
            "Epoch 774/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6978 - val_loss: 21.7667\n",
            "Epoch 775/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7489 - val_loss: 15.8542\n",
            "Epoch 776/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7637 - val_loss: 15.6968\n",
            "Epoch 777/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7359 - val_loss: 21.9012\n",
            "Epoch 778/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7941 - val_loss: 19.4762\n",
            "Epoch 779/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7702 - val_loss: 18.2024\n",
            "Epoch 780/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7272 - val_loss: 17.0801\n",
            "Epoch 781/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7623 - val_loss: 15.4790\n",
            "Epoch 782/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7536 - val_loss: 18.2600\n",
            "Epoch 783/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7368 - val_loss: 16.9875\n",
            "Epoch 784/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7391 - val_loss: 16.1649\n",
            "Epoch 785/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7540 - val_loss: 14.3077\n",
            "Epoch 786/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7391 - val_loss: 18.9554\n",
            "Epoch 787/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7837 - val_loss: 14.4107\n",
            "Epoch 788/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7691 - val_loss: 14.2154\n",
            "Epoch 789/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8275 - val_loss: 17.2495\n",
            "Epoch 790/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7064 - val_loss: 18.5216\n",
            "Epoch 791/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8542 - val_loss: 16.0670\n",
            "Epoch 792/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7854 - val_loss: 17.9394\n",
            "Epoch 793/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7464 - val_loss: 18.5348\n",
            "Epoch 794/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7240 - val_loss: 14.1219\n",
            "Epoch 795/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7376 - val_loss: 16.6866\n",
            "Epoch 796/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8092 - val_loss: 16.1722\n",
            "Epoch 797/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7100 - val_loss: 17.7885\n",
            "Epoch 798/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7577 - val_loss: 18.9119\n",
            "Epoch 799/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7472 - val_loss: 15.5239\n",
            "Epoch 800/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7417 - val_loss: 17.1077\n",
            "Epoch 801/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7560 - val_loss: 14.9102\n",
            "Epoch 802/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7527 - val_loss: 16.4293\n",
            "Epoch 803/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7907 - val_loss: 16.9298\n",
            "Epoch 804/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6977 - val_loss: 18.0868\n",
            "Epoch 805/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7474 - val_loss: 19.4344\n",
            "Epoch 806/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7614 - val_loss: 15.3473\n",
            "Epoch 807/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7972 - val_loss: 18.2603\n",
            "Epoch 808/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7023 - val_loss: 16.1862\n",
            "Epoch 809/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7606 - val_loss: 18.7345\n",
            "Epoch 810/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7110 - val_loss: 25.1965\n",
            "Epoch 811/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7711 - val_loss: 15.2277\n",
            "Epoch 812/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7486 - val_loss: 18.6369\n",
            "Epoch 813/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7240 - val_loss: 19.0545\n",
            "Epoch 814/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7387 - val_loss: 14.2200\n",
            "Epoch 815/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7640 - val_loss: 15.1276\n",
            "Epoch 816/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7107 - val_loss: 13.6014\n",
            "Epoch 817/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7522 - val_loss: 13.9786\n",
            "Epoch 818/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6916 - val_loss: 14.1090\n",
            "Epoch 819/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7776 - val_loss: 19.1878\n",
            "Epoch 820/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7779 - val_loss: 20.8654\n",
            "Epoch 821/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7992 - val_loss: 15.0778\n",
            "Epoch 822/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7546 - val_loss: 20.6532\n",
            "Epoch 823/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7048 - val_loss: 13.4063\n",
            "Epoch 824/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7853 - val_loss: 15.6681\n",
            "Epoch 825/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7060 - val_loss: 12.7393\n",
            "Epoch 826/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7120 - val_loss: 18.8450\n",
            "Epoch 827/5000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.7250 - val_loss: 14.7691\n",
            "Epoch 828/5000\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.6919 - val_loss: 16.8028\n",
            "Epoch 829/5000\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7592 - val_loss: 16.5830\n",
            "Epoch 830/5000\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7514 - val_loss: 13.2180\n",
            "Epoch 831/5000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.7875 - val_loss: 16.3373\n",
            "Epoch 832/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8234 - val_loss: 14.0376\n",
            "Epoch 833/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7364 - val_loss: 14.8301\n",
            "Epoch 834/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7326 - val_loss: 10.9389\n",
            "Epoch 835/5000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.8002 - val_loss: 12.8258\n",
            "Epoch 836/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.8151 - val_loss: 12.5968\n",
            "Epoch 837/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.7380 - val_loss: 19.0894\n",
            "Epoch 838/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7204 - val_loss: 13.3325\n",
            "Epoch 839/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7135 - val_loss: 11.9521\n",
            "Epoch 840/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6824 - val_loss: 13.9125\n",
            "Epoch 841/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7665 - val_loss: 12.7001\n",
            "Epoch 842/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7593 - val_loss: 16.4127\n",
            "Epoch 843/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7864 - val_loss: 12.3460\n",
            "Epoch 844/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6653 - val_loss: 16.5086\n",
            "Epoch 845/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6985 - val_loss: 15.9597\n",
            "Epoch 846/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7736 - val_loss: 18.3116\n",
            "Epoch 847/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7569 - val_loss: 11.1867\n",
            "Epoch 848/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8305 - val_loss: 13.6458\n",
            "Epoch 849/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7281 - val_loss: 16.2422\n",
            "Epoch 850/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7006 - val_loss: 13.2606\n",
            "Epoch 851/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7118 - val_loss: 11.3471\n",
            "Epoch 852/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7464 - val_loss: 13.7809\n",
            "Epoch 853/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7166 - val_loss: 13.1169\n",
            "Epoch 854/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6965 - val_loss: 17.8259\n",
            "Epoch 855/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7301 - val_loss: 16.9917\n",
            "Epoch 856/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7108 - val_loss: 13.4502\n",
            "Epoch 857/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7564 - val_loss: 13.1950\n",
            "Epoch 858/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7901 - val_loss: 17.1364\n",
            "Epoch 859/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8018 - val_loss: 12.4216\n",
            "Epoch 860/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7288 - val_loss: 17.2079\n",
            "Epoch 861/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7055 - val_loss: 12.0407\n",
            "Epoch 862/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7574 - val_loss: 14.2258\n",
            "Epoch 863/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7292 - val_loss: 13.4624\n",
            "Epoch 864/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7365 - val_loss: 13.4642\n",
            "Epoch 865/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8107 - val_loss: 14.1766\n",
            "Epoch 866/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7943 - val_loss: 15.6147\n",
            "Epoch 867/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7141 - val_loss: 10.5914\n",
            "Epoch 868/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7252 - val_loss: 13.3879\n",
            "Epoch 869/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7786 - val_loss: 13.3704\n",
            "Epoch 870/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7922 - val_loss: 14.4912\n",
            "Epoch 871/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7457 - val_loss: 14.3417\n",
            "Epoch 872/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7056 - val_loss: 15.5183\n",
            "Epoch 873/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7573 - val_loss: 13.8606\n",
            "Epoch 874/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7371 - val_loss: 14.4596\n",
            "Epoch 875/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7544 - val_loss: 13.6967\n",
            "Epoch 876/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6798 - val_loss: 17.2943\n",
            "Epoch 877/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6974 - val_loss: 17.2395\n",
            "Epoch 878/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8477 - val_loss: 15.4088\n",
            "Epoch 879/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6591 - val_loss: 11.9276\n",
            "Epoch 880/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7186 - val_loss: 12.1321\n",
            "Epoch 881/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6839 - val_loss: 14.9263\n",
            "Epoch 882/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7674 - val_loss: 9.9071\n",
            "Epoch 883/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7299 - val_loss: 14.3508\n",
            "Epoch 884/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7331 - val_loss: 11.0109\n",
            "Epoch 885/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6807 - val_loss: 13.2883\n",
            "Epoch 886/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7266 - val_loss: 12.5885\n",
            "Epoch 887/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7490 - val_loss: 17.2833\n",
            "Epoch 888/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7255 - val_loss: 12.7392\n",
            "Epoch 889/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7032 - val_loss: 14.7873\n",
            "Epoch 890/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7859 - val_loss: 16.3034\n",
            "Epoch 891/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7300 - val_loss: 18.1167\n",
            "Epoch 892/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6893 - val_loss: 14.8128\n",
            "Epoch 893/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8121 - val_loss: 15.0227\n",
            "Epoch 894/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6957 - val_loss: 17.2982\n",
            "Epoch 895/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7186 - val_loss: 14.3951\n",
            "Epoch 896/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7164 - val_loss: 15.5288\n",
            "Epoch 897/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7373 - val_loss: 14.5866\n",
            "Epoch 898/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6978 - val_loss: 16.4293\n",
            "Epoch 899/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7593 - val_loss: 22.0724\n",
            "Epoch 900/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7365 - val_loss: 17.3531\n",
            "Epoch 901/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8349 - val_loss: 11.5649\n",
            "Epoch 902/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7466 - val_loss: 14.7425\n",
            "Epoch 903/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7464 - val_loss: 15.9042\n",
            "Epoch 904/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7174 - val_loss: 12.8886\n",
            "Epoch 905/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6761 - val_loss: 11.2690\n",
            "Epoch 906/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7390 - val_loss: 12.6872\n",
            "Epoch 907/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8158 - val_loss: 13.2244\n",
            "Epoch 908/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8030 - val_loss: 15.7587\n",
            "Epoch 909/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6609 - val_loss: 13.9217\n",
            "Epoch 910/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7312 - val_loss: 13.7395\n",
            "Epoch 911/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6827 - val_loss: 13.1008\n",
            "Epoch 912/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8200 - val_loss: 15.5760\n",
            "Epoch 913/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6953 - val_loss: 13.2746\n",
            "Epoch 914/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7365 - val_loss: 13.2699\n",
            "Epoch 915/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7133 - val_loss: 11.5236\n",
            "Epoch 916/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6917 - val_loss: 15.9516\n",
            "Epoch 917/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7397 - val_loss: 13.0349\n",
            "Epoch 918/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6876 - val_loss: 14.6052\n",
            "Epoch 919/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7045 - val_loss: 15.9199\n",
            "Epoch 920/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6398 - val_loss: 15.1693\n",
            "Epoch 921/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7074 - val_loss: 13.4089\n",
            "Epoch 922/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7627 - val_loss: 16.2080\n",
            "Epoch 923/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7105 - val_loss: 15.6365\n",
            "Epoch 924/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7146 - val_loss: 10.3757\n",
            "Epoch 925/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7923 - val_loss: 13.0913\n",
            "Epoch 926/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7467 - val_loss: 14.8545\n",
            "Epoch 927/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8386 - val_loss: 11.5059\n",
            "Epoch 928/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6664 - val_loss: 14.0794\n",
            "Epoch 929/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7444 - val_loss: 10.7889\n",
            "Epoch 930/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7360 - val_loss: 12.4340\n",
            "Epoch 931/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7474 - val_loss: 14.6167\n",
            "Epoch 932/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7711 - val_loss: 11.2103\n",
            "Epoch 933/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6708 - val_loss: 11.4934\n",
            "Epoch 934/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7510 - val_loss: 17.8805\n",
            "Epoch 935/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7603 - val_loss: 12.0437\n",
            "Epoch 936/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7146 - val_loss: 12.8909\n",
            "Epoch 937/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7295 - val_loss: 13.7961\n",
            "Epoch 938/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8070 - val_loss: 10.2623\n",
            "Epoch 939/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8115 - val_loss: 12.9748\n",
            "Epoch 940/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7922 - val_loss: 16.0889\n",
            "Epoch 941/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6703 - val_loss: 10.7540\n",
            "Epoch 942/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7144 - val_loss: 15.1782\n",
            "Epoch 943/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6737 - val_loss: 10.5741\n",
            "Epoch 944/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7097 - val_loss: 14.0224\n",
            "Epoch 945/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7284 - val_loss: 10.6306\n",
            "Epoch 946/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7160 - val_loss: 13.3621\n",
            "Epoch 947/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7579 - val_loss: 10.4732\n",
            "Epoch 948/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7774 - val_loss: 13.0912\n",
            "Epoch 949/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7749 - val_loss: 16.2399\n",
            "Epoch 950/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7049 - val_loss: 14.6592\n",
            "Epoch 951/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7266 - val_loss: 11.9036\n",
            "Epoch 952/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6755 - val_loss: 12.0145\n",
            "Epoch 953/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7192 - val_loss: 13.1046\n",
            "Epoch 954/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7663 - val_loss: 16.0143\n",
            "Epoch 955/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7639 - val_loss: 13.1690\n",
            "Epoch 956/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8499 - val_loss: 9.0081\n",
            "Epoch 957/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6690 - val_loss: 9.6632\n",
            "Epoch 958/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7448 - val_loss: 11.6128\n",
            "Epoch 959/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7711 - val_loss: 11.7983\n",
            "Epoch 960/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7827 - val_loss: 12.8675\n",
            "Epoch 961/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7228 - val_loss: 11.1845\n",
            "Epoch 962/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7369 - val_loss: 13.4981\n",
            "Epoch 963/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6951 - val_loss: 10.7362\n",
            "Epoch 964/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8684 - val_loss: 11.8803\n",
            "Epoch 965/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7381 - val_loss: 13.7020\n",
            "Epoch 966/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7361 - val_loss: 11.4999\n",
            "Epoch 967/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7235 - val_loss: 14.4008\n",
            "Epoch 968/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7933 - val_loss: 15.7045\n",
            "Epoch 969/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7663 - val_loss: 10.1430\n",
            "Epoch 970/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7142 - val_loss: 13.9648\n",
            "Epoch 971/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7137 - val_loss: 13.5231\n",
            "Epoch 972/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7429 - val_loss: 12.1482\n",
            "Epoch 973/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7638 - val_loss: 11.6652\n",
            "Epoch 974/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7347 - val_loss: 11.3450\n",
            "Epoch 975/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7437 - val_loss: 14.9820\n",
            "Epoch 976/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7532 - val_loss: 10.4676\n",
            "Epoch 977/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7440 - val_loss: 11.5935\n",
            "Epoch 978/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7547 - val_loss: 12.8193\n",
            "Epoch 979/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7309 - val_loss: 12.1819\n",
            "Epoch 980/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7275 - val_loss: 11.0031\n",
            "Epoch 981/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7232 - val_loss: 9.7093\n",
            "Epoch 982/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7000 - val_loss: 12.6797\n",
            "Epoch 983/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7757 - val_loss: 10.0097\n",
            "Epoch 984/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7496 - val_loss: 12.4755\n",
            "Epoch 985/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7440 - val_loss: 14.2775\n",
            "Epoch 986/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7166 - val_loss: 11.7258\n",
            "Epoch 987/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7444 - val_loss: 9.5465\n",
            "Epoch 988/5000\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.7519 - val_loss: 12.6458\n",
            "Epoch 989/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7113 - val_loss: 12.8806\n",
            "Epoch 990/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7230 - val_loss: 12.5532\n",
            "Epoch 991/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6641 - val_loss: 13.9206\n",
            "Epoch 992/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7379 - val_loss: 10.0465\n",
            "Epoch 993/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7744 - val_loss: 11.1659\n",
            "Epoch 994/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7511 - val_loss: 13.2638\n",
            "Epoch 995/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7349 - val_loss: 9.9474\n",
            "Epoch 996/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7013 - val_loss: 12.0023\n",
            "Epoch 997/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7557 - val_loss: 13.6900\n",
            "Epoch 998/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7708 - val_loss: 11.5879\n",
            "Epoch 999/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7332 - val_loss: 9.5851\n",
            "Epoch 1000/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6656 - val_loss: 11.8558\n",
            "Epoch 1001/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6630 - val_loss: 13.0986\n",
            "Epoch 1002/5000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.7569 - val_loss: 12.3241\n",
            "Epoch 1003/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7579 - val_loss: 13.2686\n",
            "Epoch 1004/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7092 - val_loss: 10.5665\n",
            "Epoch 1005/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6646 - val_loss: 11.3395\n",
            "Epoch 1006/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7162 - val_loss: 12.8881\n",
            "Epoch 1007/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6905 - val_loss: 10.2152\n",
            "Epoch 1008/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7397 - val_loss: 10.4359\n",
            "Epoch 1009/5000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.7710 - val_loss: 10.9780\n",
            "Epoch 1010/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.6906 - val_loss: 14.2991\n",
            "Epoch 1011/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6976 - val_loss: 11.5764\n",
            "Epoch 1012/5000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.7197 - val_loss: 9.5910\n",
            "Epoch 1013/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7218 - val_loss: 11.2494\n",
            "Epoch 1014/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7243 - val_loss: 12.1736\n",
            "Epoch 1015/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7443 - val_loss: 13.2218\n",
            "Epoch 1016/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7677 - val_loss: 14.9738\n",
            "Epoch 1017/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6835 - val_loss: 10.1984\n",
            "Epoch 1018/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7515 - val_loss: 13.0184\n",
            "Epoch 1019/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7538 - val_loss: 10.6700\n",
            "Epoch 1020/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7000 - val_loss: 12.6121\n",
            "Epoch 1021/5000\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.7418 - val_loss: 14.2705\n",
            "Epoch 1022/5000\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.6995 - val_loss: 10.7674\n",
            "Epoch 1023/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.7130 - val_loss: 11.3595\n",
            "Epoch 1024/5000\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.7765 - val_loss: 11.5525\n",
            "Epoch 1025/5000\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.7581 - val_loss: 12.0463\n",
            "Epoch 1026/5000\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.6851 - val_loss: 15.6146\n",
            "Epoch 1027/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7128 - val_loss: 10.8122\n",
            "Epoch 1028/5000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.7429 - val_loss: 9.8049\n",
            "Epoch 1029/5000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.7487 - val_loss: 11.0873\n",
            "Epoch 1030/5000\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.7277 - val_loss: 9.5180\n",
            "Epoch 1031/5000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.7081 - val_loss: 10.2813\n",
            "Epoch 1032/5000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.6949 - val_loss: 10.9011\n",
            "Epoch 1033/5000\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.7393 - val_loss: 11.3142\n",
            "Epoch 1034/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7215 - val_loss: 11.3161\n",
            "Epoch 1035/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7686 - val_loss: 10.7702\n",
            "Epoch 1036/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6993 - val_loss: 11.0369\n",
            "Epoch 1037/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7447 - val_loss: 14.0228\n",
            "Epoch 1038/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7082 - val_loss: 13.6979\n",
            "Epoch 1039/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7361 - val_loss: 9.8768\n",
            "Epoch 1040/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7398 - val_loss: 12.9214\n",
            "Epoch 1041/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7570 - val_loss: 11.0505\n",
            "Epoch 1042/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7309 - val_loss: 12.9050\n",
            "Epoch 1043/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6958 - val_loss: 13.0663\n",
            "Epoch 1044/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7419 - val_loss: 10.4981\n",
            "Epoch 1045/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8138 - val_loss: 10.4563\n",
            "Epoch 1046/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7245 - val_loss: 8.4929\n",
            "Epoch 1047/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6910 - val_loss: 12.6998\n",
            "Epoch 1048/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7394 - val_loss: 9.6915\n",
            "Epoch 1049/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.8335 - val_loss: 10.3151\n",
            "Epoch 1050/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7805 - val_loss: 9.3310\n",
            "Epoch 1051/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6989 - val_loss: 11.8586\n",
            "Epoch 1052/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7135 - val_loss: 12.3429\n",
            "Epoch 1053/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7396 - val_loss: 10.5283\n",
            "Epoch 1054/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7503 - val_loss: 9.6593\n",
            "Epoch 1055/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7626 - val_loss: 9.0064\n",
            "Epoch 1056/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7603 - val_loss: 9.9066\n",
            "Epoch 1057/5000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7297 - val_loss: 9.6527\n",
            "Epoch 1058/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7605 - val_loss: 7.5177\n",
            "Epoch 1059/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6641 - val_loss: 14.0830\n",
            "Epoch 1060/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9207 - val_loss: 10.4565\n",
            "Epoch 1061/5000\n",
            "14/23 [=================>............] - ETA: 0s - loss: 0.7776Restoring model weights from the end of the best epoch: 61.\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7128 - val_loss: 7.5756\n",
            "Epoch 1061: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-b383c653ffcb>:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 1.5694530010223389\n",
            "Train loss: 0.8144264221191406\n",
            "Test loss: 1.577889084815979\n",
            "dO18 RMSE: 2.059728943305019\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       23.405260           0.148275\n",
            "1       25.725845           0.213718\n",
            "2       25.936000           0.066530\n",
            "3       23.436000           0.134780\n",
            "5       25.168000           0.106320\n",
            "7       25.108000           0.170620\n",
            "9       25.006000           0.272130\n",
            "11      23.800000           0.277350\n",
            "12      23.742000           0.118870\n",
            "14      24.608000           1.650470\n",
            "15      25.696000           0.791580\n",
            "16      25.524000           0.219580\n",
            "17      25.794000           0.224030\n",
            "18      23.770000           1.004550\n",
            "19      25.912000           0.478970\n",
            "20      25.878000           0.349920\n",
            "21      26.060000           2.584750\n",
            "24      25.196000           0.410630\n",
            "25      24.656000           0.177280\n",
            "26      26.356000           0.039530\n",
            "27      23.962000           0.309920\n",
            "30      26.546000           2.143780\n",
            "31      25.682000           0.944720\n",
            "33      23.752000           0.524370\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       23.239494           3.121233\n",
            "1       23.481453           3.097517\n",
            "2       23.239521           3.121201\n",
            "3       23.111525           2.729036\n",
            "4       23.239521           3.121201\n",
            "5       23.239521           3.121201\n",
            "6       23.239521           3.121201\n",
            "7       23.111502           2.729061\n",
            "8       23.239521           3.121201\n",
            "9       23.239521           3.121201\n",
            "10      23.239521           3.121201\n",
            "11      23.239521           3.121201\n",
            "12      23.111502           2.729061\n",
            "13      23.111525           2.729036\n",
            "14      23.111525           2.729036\n",
            "15      23.111525           2.729036\n",
            "16      23.111525           2.729036\n",
            "17      23.111525           2.729036\n",
            "18      23.239494           3.121233\n",
            "19      23.481453           3.097517\n",
            "20      23.111502           2.729061\n",
            "21      23.111525           2.729036\n",
            "22      23.239521           3.121201\n",
            "23      23.225403           2.758444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(get_model_save_location(\n",
        "    f\"{MODEL_NAME}.tf\"), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location(\n",
        "    f\"{MODEL_NAME}.pkl\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX1HiL-NfR3a",
        "outputId": "20ce9c12-a4f1-4e50-c395-e7e4fb58b341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/MyDrive/amazon_rainforest_files/variational/model/demo_isoscape_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to generate an isoscape\n",
        "\n",
        "vi_model = model.TFModel(f\"{MODEL_NAME}.tf\", f\"{MODEL_NAME}.pkl\")\n",
        "\n",
        "required_geotiffs = [\n",
        "    'VPD',\n",
        "    'RH',\n",
        "    'PET',\n",
        "    'DEM',\n",
        "    'PA',\n",
        "    'Mean Annual Temperature',\n",
        "    'Mean Annual Precipitation',\n",
        "    'Iso_Oxi_Stack_mean_TERZER',\n",
        "    'isoscape_fullmodel_d18O_prec_REGRESSION',\n",
        "    \"brisoscape_mean_ISORIX\",\n",
        "    \"d13C_cel_mean\",\n",
        "    \"d13C_cel_var\",\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance',]\n",
        "\n",
        "raster.generate_isoscapes_from_variational_model(\n",
        "    MODEL_NAME, vi_model, required_geotiffs, 1000, 1000)"
      ],
      "metadata": {
        "id": "PuGJblTjiDdz",
        "outputId": "7cd33454-ecfa-465a-e115-4c803c27871b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: GTiff/GeoTIFF\n",
            "Size is 941 x 937 x 12\n",
            "Projection is GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n",
            "Origin = (-74.0000000000241, 5.29166666665704)\n",
            "Pixel Size = (0.04166666666665718, -0.04166666666667143)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 941 x 937 x 12\n",
            "Projection is GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n",
            "Origin = (-74.0000000000241, 5.29166666665704)\n",
            "Pixel Size = (0.04166666666665718, -0.04166666666667143)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 942 x 936 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-74.0, 5.25)\n",
            "Pixel Size = (0.041666666666666664, -0.041666666666666664)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 5418 x 4683 x 2\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.991666667, 5.275)\n",
            "Pixel Size = (0.008333333333333335, -0.008333333333333333)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 5418 x 4683 x 2\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.991666667, 5.275)\n",
            "Pixel Size = (0.008333333333333335, -0.008333333333333333)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 941 x 937 x 12\n",
            "Projection is GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n",
            "Origin = (-74.0000000000241, 5.29166666665704)\n",
            "Pixel Size = (0.04166666666665718, -0.04166666666667143)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 234 x 1\n",
            "Projection is \n",
            "Origin = (-74.0, 5.333333333999995)\n",
            "Pixel Size = (0.16666666666808508, -0.16666666666808505)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 940 x 936 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.975139313, 5.266527396)\n",
            "Pixel Size = (0.0416666665, -0.041666666500000005)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 234 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-74.0, 5.333333334)\n",
            "Pixel Size = (0.1666666666680851, -0.16666666666666666)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 541 x 467 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.922043, 5.233124)\n",
            "Pixel Size = (0.08333, -0.08333)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 218 x 2\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-74.0, 4.500000000659528)\n",
            "Pixel Size = (0.166666666667993, -0.16666666666799657)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 218 x 2\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-74.0, 4.500000000659528)\n",
            "Pixel Size = (0.166666666667993, -0.16666666666799657)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 941/941 [01:18<00:00, 11.95it/s]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}