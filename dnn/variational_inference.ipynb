{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnc-br/ddf-isoscapes/blob/new_model_py/dnn/variational_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0IfT3kGwgK6"
      },
      "source": [
        "# Variational model\n",
        "\n",
        "Find the mean/variance of O18 ratios (as well as N15 and C13 in the future) at a particular lat/lon across Brazil."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "henIPlAPCb4i",
        "outputId": "c3aeebf6-3855-42b9-a8cf-20e66f56a328",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Cloning into 'ddf_common_stub'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 11 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (11/11), 5.50 KiB | 5.50 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n",
            "executing checkout_branch ...\n",
            "b''\n",
            "main branch checked out as readonly. You may now use ddf_common imports\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import os\n",
        "from typing import List, Tuple, Dict\n",
        "from dataclasses import dataclass\n",
        "from joblib import dump\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import tensorflow_probability as tfp\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.python.ops import math_ops\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#@title Model training configuration\n",
        "# See https://zohaib.me/debugging-in-google-collab-notebook/ for tips,\n",
        "# as well as docs for pdb and ipdb.\n",
        "DEBUG = False #@param {type:\"boolean\"}\n",
        "USE_LOCAL_DRIVE = False #@param {type:\"boolean\"}\n",
        "LOCAL_DIR = \"/usr/local/google/home/ruru/Downloads/amazon_sample_data-20230712T203059Z-001\" #@param\n",
        "GDRIVE_DIR = \"MyDrive/amazon_rainforest_files/\" #@param\n",
        "FP_ROOT = LOCAL_DIR\n",
        "\n",
        "MODEL_SAVE_LOCATION = \"MyDrive/amazon_rainforest_files/variational/model\" #@param\n",
        "\n",
        "def get_model_save_location(filename) -> str:\n",
        "  root = '' if USE_LOCAL_DRIVE else '/content/gdrive'\n",
        "  return os.path.join(root, MODEL_SAVE_LOCATION, filename)\n",
        "\n",
        "# Access data stored on Google Drive if not reading data locally.\n",
        "if not USE_LOCAL_DRIVE:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  global FP_ROOT\n",
        "  FP_ROOT = os.path.join('/content/gdrive', GDRIVE_DIR)\n",
        "\n",
        "MODEL_NAME = \"demo_isoscape_model\" #@param\n",
        "\n",
        "TRAINING_SET_PATH = 'amazon_sample_data/canonical/uc_davis_train_fixed_grouped.csv' #@param\n",
        "VALIDATION_SET_PATH = 'amazon_sample_data/canonical/uc_davis_validation_fixed_grouped.csv' #@param\n",
        "TEST_SET_PATH = 'amazon_sample_data/canonical/uc_davis_test_fixed_grouped.csv' #@param\n",
        "\n",
        "if DEBUG:\n",
        "    %pip install -Uqq ipdb\n",
        "    import ipdb\n",
        "    %pdb on\n",
        "\n",
        "import sys\n",
        "\n",
        "!if [ ! -d \"/content/ddf_common_stub\" ] ; then git clone -b test https://github.com/tnc-br/ddf_common_stub.git; fi\n",
        "sys.path.append(\"/content/ddf_common_stub/\")\n",
        "import ddfimport\n",
        "ddfimport.ddf_import_common()\n",
        "\n",
        "import raster\n",
        "import model\n",
        "import importlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQrEv57zCb4q"
      },
      "source": [
        "# Data preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6XMee1aHfcik"
      },
      "outputs": [],
      "source": [
        "def valid_in_all_rasters(\n",
        "    lat: float,\n",
        "    lon: float,\n",
        "    rasters: List[raster.AmazonGeoTiff]) -> bool:\n",
        "  '''\n",
        "    Args:\n",
        "        lat (float): The latitude coordinate.\n",
        "        lon (float): The longitude coordinate.\n",
        "        rasters (List[AmazonGeoTiff]): lat, lon are checked to be valid coords\n",
        "          in each of these rasters\n",
        "\n",
        "    Returns:\n",
        "        bool: True if (lat, lon) is valid in all rasters, False if invalid in\n",
        "          at least one.\n",
        "  '''\n",
        "  for r in rasters:\n",
        "    if not raster.is_valid_point(lat, lon, r):\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "def nudge_invalid_coords(df: pd.DataFrame, rasters: List[raster.AmazonGeoTiff]):\n",
        "  '''\n",
        "    Given a Pandas DataFrame with latitude and longitude columns, maybe\n",
        "    perturb the latitude and longitude (i.e. nudge) values to fit within the\n",
        "    bounds of every AmazonGeoTiff in `rasters`.\n",
        "\n",
        "    This may be necessary as some rasters have slightly different coordinate\n",
        "    systems than the one used by data providers. Samples very close to borders\n",
        "    are particularly susceptible to being out-of-bounds and will need nudging.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): A dataframe with \"lat\" and \"long\" columns.\n",
        "        rasters (List[AmazonGeoTiff]): A list of rasters to use to decide whether\n",
        "          to nudge coordindates. If a row in the dataframe does not have a value\n",
        "          in this raster, nudge it.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if (lat, lon) is valid in all rasters, False if invalid in\n",
        "          at least one.\n",
        "  '''\n",
        "  max_degrees_deviation = 2\n",
        "  for i, row in df.iterrows():\n",
        "    # Get the lat and long for the current row.\n",
        "    lat = df.loc[i, \"lat\"]\n",
        "    lon = df.loc[i, \"long\"]\n",
        "\n",
        "    if valid_in_all_rasters(lat, lon, rasters):\n",
        "      continue\n",
        "\n",
        "    # nudge 0.01 degrees at a time.\n",
        "    for nudge in [x/100.0 for x in range(1, max_degrees_deviation*100)]:\n",
        "      if valid_in_all_rasters(lat + nudge, lon + nudge, rasters):\n",
        "        df.loc[i, \"lat\"] = lat + nudge\n",
        "        df.loc[i, \"long\"] = lon + nudge\n",
        "        break\n",
        "      elif valid_in_all_rasters(lat - nudge, lon - nudge, rasters):\n",
        "        df.loc[i, \"lat\"] = lat - nudge\n",
        "        df.loc[i, \"long\"] = lon - nudge\n",
        "        break\n",
        "      elif valid_in_all_rasters(lat + nudge, lon - nudge, rasters):\n",
        "        df.loc[i, \"lat\"] = lat + nudge\n",
        "        df.loc[i, \"long\"] = lon - nudge\n",
        "        break\n",
        "      elif valid_in_all_rasters(lat - nudge, lon + nudge, rasters):\n",
        "        df.loc[i, \"lat\"] = lat - nudge\n",
        "        df.loc[i, \"long\"] = lon + nudge\n",
        "        break\n",
        "    if df.loc[i, \"lat\"] == lat and df.loc[i, \"long\"] == lon:\n",
        "      raise ValueError(\"Failed to nudge coordinates into valid space\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def load_dataset(path: str, columns_to_keep: List[str], side_raster_input):\n",
        "  df = pd.read_csv(path, encoding=\"ISO-8859-1\", sep=',')\n",
        "  df = df[df['d18O_cel_variance'].notna()]\n",
        "\n",
        "  df = nudge_invalid_coords(df, list(side_raster_input.values()))\n",
        "\n",
        "  for name, geotiff in side_raster_input.items():\n",
        "    df[name] = df.apply(lambda row: geotiff.value_at(row['long'], row['lat']), axis=1)\n",
        "  for name, geotiff in side_raster_input.items():\n",
        "    df = df[df[name].notnull()]\n",
        "\n",
        "  X = df.drop(df.columns.difference(columns_to_keep), axis=1)\n",
        "  Y = df[[\"d18O_cel_mean\", \"d18O_cel_variance\"]]\n",
        "\n",
        "  return X, Y\n",
        "\n",
        "@dataclass\n",
        "class FeaturesToLabels:\n",
        "  def __init__(self, X: pd.DataFrame, Y: pd.DataFrame):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "\n",
        "  def as_tuple(self):\n",
        "    return (self.X, self.Y)\n",
        "\n",
        "\n",
        "def create_feature_scaler(X: pd.DataFrame,\n",
        "                          columns_to_scale,\n",
        "                          columns_to_standardize) -> ColumnTransformer:\n",
        "  columns_to_standardize = columns_to_standardize\n",
        "  feature_scaler = ColumnTransformer(\n",
        "      [(column+'_scaler', MinMaxScaler(), [column]) for column in columns_to_scale] +\n",
        "      [(column+'_standardizer', StandardScaler(), [column]) for column in columns_to_standardize])\n",
        "  feature_scaler.fit(X)\n",
        "  return feature_scaler\n",
        "\n",
        "def create_label_scaler(Y: pd.DataFrame) -> ColumnTransformer:\n",
        "  label_scaler = ColumnTransformer([\n",
        "      ('mean_std_scaler', StandardScaler(), ['d18O_cel_mean']),\n",
        "      ('var_minmax_scaler', MinMaxScaler(), ['d18O_cel_variance'])])\n",
        "  label_scaler.fit(Y)\n",
        "  return label_scaler\n",
        "\n",
        "def scale(X: pd.DataFrame, feature_scaler):\n",
        "  # transform() outputs numpy arrays :(  need to convert back to DataFrame.\n",
        "  X_standardized = pd.DataFrame(feature_scaler.transform(X),\n",
        "                        index=X.index, columns=X.columns)\n",
        "  return X_standardized\n",
        "\n",
        "# Holds each scaled dataset and the scaler used. Used for unscaling predictions.\n",
        "@dataclass\n",
        "class ScaledPartitions():\n",
        "  def __init__(self,\n",
        "               feature_scaler: ColumnTransformer,\n",
        "               label_scaler: ColumnTransformer,\n",
        "               train: FeaturesToLabels, val: FeaturesToLabels,\n",
        "               test: FeaturesToLabels):\n",
        "    self.feature_scaler = feature_scaler\n",
        "    self.label_scaler = label_scaler\n",
        "    self.train = train\n",
        "    self.val = val\n",
        "    self.test = test\n",
        "\n",
        "\n",
        "def load_and_scale(config: Dict,\n",
        "                   columns_to_passthrough: List[str],\n",
        "                   columns_to_scale: List[str],\n",
        "                   columns_to_standardize: List[str],\n",
        "                   extra_columns_from_geotiffs: Dict[str, raster.AmazonGeoTiff]) -> ScaledPartitions:\n",
        "  columns_to_keep = columns_to_passthrough + columns_to_scale + columns_to_standardize\n",
        "  X_train, Y_train = load_dataset(config['TRAIN'], columns_to_keep, extra_columns_from_geotiffs)\n",
        "  X_val, Y_val = load_dataset(config['VALIDATION'], columns_to_keep, extra_columns_from_geotiffs)\n",
        "  X_test, Y_test = load_dataset(config['TEST'], columns_to_keep, extra_columns_from_geotiffs)\n",
        "\n",
        "  # Fit the scaler:\n",
        "  feature_scaler = create_feature_scaler(\n",
        "      X_train,\n",
        "      columns_to_scale,\n",
        "      columns_to_standardize)\n",
        "  label_scaler = create_label_scaler(Y_train)\n",
        "\n",
        "  # Apply the scaler:\n",
        "  train = FeaturesToLabels(scale(X_train, feature_scaler), Y_train)\n",
        "  val = FeaturesToLabels(scale(X_val, feature_scaler), Y_val)\n",
        "  test = FeaturesToLabels(scale(X_test, feature_scaler), Y_test)\n",
        "  return ScaledPartitions(feature_scaler, label_scaler, train, val, test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usGznR593LZc"
      },
      "source": [
        "# Model Definition\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HCkGSPUo3KqY"
      },
      "outputs": [],
      "source": [
        "def sample_normal_distribution(\n",
        "    mean: tf.Tensor,\n",
        "    stdev: tf.Tensor,\n",
        "    n: int) -> tf.Tensor:\n",
        "    '''\n",
        "    Given a batch of normal distributions described by a mean and stdev in\n",
        "    a tf.Tensor, sample n elements from each distribution and return the mean\n",
        "    and standard deviation per sample.\n",
        "    '''\n",
        "    batch_size = tf.shape(mean)[0]\n",
        "\n",
        "    # Output tensor is (n, batch_size, 1)\n",
        "    sample_values = tfp.distributions.Normal(\n",
        "        loc=mean,\n",
        "        scale=stdev).sample(\n",
        "            sample_shape=n)\n",
        "    # Reshaped tensor will be (batch_size, n)\n",
        "    sample_values = tf.transpose(sample_values)\n",
        "    # Get the mean per sample in the batch.\n",
        "    sample_mean = tf.transpose(tf.math.reduce_mean(sample_values, 2))\n",
        "    sample_stdev = tf.transpose(tf.math.reduce_std(sample_values, 2))\n",
        "\n",
        "    return sample_mean, sample_stdev\n",
        "\n",
        "# log(σ2/σ1) + ( σ1^2+(μ1−μ2)^2 ) / 2* σ^2   − 1/2\n",
        "def kl_divergence_helper(real, predicted, sample):\n",
        "    '''\n",
        "    real: tf.Tensor of the real mean and standard deviation of sample to compare\n",
        "    predicted: tf.Tensor of the predicted mean and standard deviation to compare\n",
        "    sample: Whether or not to sample the predicted distribution to get a new\n",
        "            mean and standard deviation.\n",
        "    '''\n",
        "    if real.shape != predicted.shape:\n",
        "      raise ValueError(\n",
        "          f\"real.shape {real.shape} != predicted.shape {predicted.shape}\")\n",
        "\n",
        "    real_value = tf.gather(real, [0], axis=1)\n",
        "    real_std = tf.math.sqrt(tf.gather(real, [1], axis=1))\n",
        "\n",
        "\n",
        "    predicted_value = tf.gather(predicted, [0], axis=1)\n",
        "    predicted_std = tf.math.sqrt(tf.gather(predicted, [1], axis=1))\n",
        "    # If true, sample from the distribution defined by the predicted mean and\n",
        "    # standard deviation to use for mean and stdev used in KL divergence loss.\n",
        "    if sample:\n",
        "      predicted_value, predicted_std = sample_normal_distribution(\n",
        "          mean=predicted_value, stdev=predicted_std, n=15)\n",
        "\n",
        "    kl_loss = -0.5 + tf.math.log(predicted_std/real_std) + \\\n",
        "     (tf.square(real_std) + tf.square(real_value - predicted_value))/ \\\n",
        "     (2*tf.square(predicted_std))\n",
        "\n",
        "    if tf.math.is_nan(tf.math.reduce_mean(kl_loss)):\n",
        "       tf.print(predicted)\n",
        "       sess = tf.compat.v1.Session()\n",
        "       sess.close()\n",
        "\n",
        "    return tf.math.reduce_mean(kl_loss)\n",
        "\n",
        "def kl_divergence(real, predicted):\n",
        "  return kl_divergence_helper(real, predicted, True)\n",
        "\n",
        "# Stop training the model early if no improvements are made after 1000 epochs.\n",
        "def get_early_stopping_callback():\n",
        "  return EarlyStopping(monitor='val_loss', patience=1000, min_delta=0.001,\n",
        "                       verbose=1, restore_best_weights=True, start_from_epoch=0)\n",
        "\n",
        "tf.keras.utils.set_random_seed(18731)\n",
        "\n",
        "# I was experimenting with models that took longer to train, and used this\n",
        "# checkpointing callback to periodically save the model. It's optional.\n",
        "def get_checkpoint_callback(model_file):\n",
        "  return ModelCheckpoint(\n",
        "      get_model_save_location(model_file),\n",
        "      monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
        "\n",
        "# Trains the variational inference model.\n",
        "def train_or_update_variational_model(\n",
        "        sp: ScaledPartitions,\n",
        "        hidden_layers: List[int],\n",
        "        epochs: int,\n",
        "        batch_size: int,\n",
        "        lr: float,\n",
        "        model_file=None,\n",
        "        use_checkpoint=False):\n",
        "  callbacks_list = [get_early_stopping_callback(),\n",
        "                    get_checkpoint_callback(model_file)]\n",
        "  if not use_checkpoint:\n",
        "    inputs = keras.Input(shape=(sp.train.X.shape[1],))\n",
        "    x = inputs\n",
        "    for layer_size in hidden_layers:\n",
        "      x = keras.layers.Dense(\n",
        "          layer_size, activation='relu')(x)\n",
        "    mean_output = keras.layers.Dense(\n",
        "        1, name='mean_output')(x)\n",
        "    var_output = keras.layers.Dense(\n",
        "        1, name='var_output')(x)\n",
        "\n",
        "    # Invert the normalization on our outputs\n",
        "    mean_scaler = sp.label_scaler.named_transformers_['mean_std_scaler']\n",
        "    untransformed_mean = mean_output * mean_scaler.var_ + mean_scaler.mean_\n",
        "\n",
        "    var_scaler = sp.label_scaler.named_transformers_['var_minmax_scaler']\n",
        "    unscaled_var = var_output * var_scaler.scale_ + var_scaler.min_\n",
        "    untransformed_var = keras.layers.Lambda(lambda t: tf.math.log(1 + tf.exp(t)))(unscaled_var)\n",
        "\n",
        "    # Output mean, variance  tuples.\n",
        "    outputs = keras.layers.concatenate([untransformed_mean, untransformed_var])\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss=kl_divergence)\n",
        "    model.summary()\n",
        "  else:\n",
        "    model = keras.models.load_model(\n",
        "        get_model_save_location(model_file),\n",
        "        custom_objects={\"kl_divergence\": kl_divergence})\n",
        "  history = model.fit(\n",
        "    sp.train.X, sp.train.Y, verbose=1,\n",
        "    validation_data=sp.val.as_tuple(), shuffle=True,\n",
        "    epochs=epochs, batch_size=batch_size, callbacks=callbacks_list)\n",
        "  return history, model\n",
        "\n",
        "def render_plot_loss(history, name):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title(name + ' model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.yscale(\"log\")\n",
        "  plt.ylim((0, 10))\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['loss', 'val_loss'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def train_and_evaluate(sp: ScaledPartitions, run_id: str, training_batch_size=5):\n",
        "  print(\"==================\")\n",
        "  print(run_id)\n",
        "  history, model = train_or_update_variational_model(\n",
        "      sp, hidden_layers=[20, 20], epochs=5000, batch_size=training_batch_size,\n",
        "      lr=0.0001, model_file=run_id+\".h5\", use_checkpoint=False)\n",
        "  render_plot_loss(history, run_id+\" kl_loss\")\n",
        "  model.save(get_model_save_location(run_id+\".h5\"), save_format=\"h5\")\n",
        "\n",
        "  best_epoch_index = history.history['val_loss'].index(min(history.history['val_loss']))\n",
        "  print('Val loss:', history.history['val_loss'][best_epoch_index])\n",
        "  print('Train loss:', history.history['loss'][best_epoch_index])\n",
        "  print('Test loss:', model.evaluate(x=sp.test.X, y=sp.test.Y, verbose=0))\n",
        "\n",
        "  predictions = model.predict_on_batch(sp.test.X)\n",
        "  predictions = pd.DataFrame(predictions, columns=['d18O_cel_mean', 'd18O_cel_variance'])\n",
        "  rmse = np.sqrt(mean_squared_error(sp.test.Y['d18O_cel_mean'], predictions['d18O_cel_mean']))\n",
        "  print(\"dO18 RMSE: \"+ str(rmse))\n",
        "  print(\"EXPECTED:\")\n",
        "  print(sp.test.Y.to_string())\n",
        "  print()\n",
        "  print(\"PREDICTED:\")\n",
        "  print(predictions.to_string())\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMCbonPjs6Kp"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4FCWouJs94I"
      },
      "source": [
        "## Run data prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "rPONfgkjvJWz",
        "outputId": "60ffd0fb-9318-4f58-98ea-88eb43e4adca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: GTiff/GeoTIFF\n",
            "Size is 541 x 467 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.922043, 5.233124)\n",
            "Pixel Size = (0.08333, -0.08333)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 218 x 2\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-74.0, 4.500000000659528)\n",
            "Pixel Size = (0.166666666667993, -0.16666666666799657)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 218 x 2\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-74.0, 4.500000000659528)\n",
            "Pixel Size = (0.166666666667993, -0.16666666666799657)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 234 x 1\n",
            "Projection is GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n",
            "Origin = (-74.0, 5.333333333999995)\n",
            "Pixel Size = (0.16666666666808508, -0.16666666666808505)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 234 x 1\n",
            "Projection is GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n",
            "Origin = (-74.0, 5.333333333999995)\n",
            "Pixel Size = (0.16666666666808508, -0.16666666666808505)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
              "       'Mean Annual Temperature', 'Mean Annual Precipitation',\n",
              "       'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION',\n",
              "       'ordinary_kriging_linear_d18O_predicted_mean',\n",
              "       'ordinary_kriging_linear_d18O_predicted_variance',\n",
              "       'brisoscape_mean_ISORIX', 'd13C_cel_mean', 'd13C_cel_var'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "grouped_random_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, TRAINING_SET_PATH),\n",
        "    'TEST' : os.path.join(FP_ROOT, VALIDATION_SET_PATH),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, TEST_SET_PATH),\n",
        "}\n",
        "\n",
        "columns_to_passthrough = [\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "columns_to_scale = []\n",
        "columns_to_standardize = [\n",
        "    'lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature', 'Mean Annual Precipitation',\n",
        "    'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION',\n",
        "    'brisoscape_mean_ISORIX', 'd13C_cel_mean', 'd13C_cel_var',\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "\n",
        "# Create artifical columns by querying rasters at each lat/lon in the training\n",
        "# dataset CSV.\n",
        "extra_columns_from_geotiffs = {\n",
        "  \"brisoscape_mean_ISORIX\" : raster.load_raster(raster.get_raster_path(\"brisoscape_mean_ISORIX.tif\")),\n",
        "  \"d13C_cel_mean\" : raster.load_raster(raster.get_raster_path(\"d13C_cel_map_BRAZIL_stack.tiff\"), use_only_band_index=0),\n",
        "  \"d13C_cel_var\" : raster.load_raster(raster.get_raster_path(\"d13C_cel_map_BRAZIL_stack.tiff\"), use_only_band_index=1),\n",
        "  \"ordinary_kriging_linear_d18O_predicted_mean\" : raster.krig_means_isoscape_geotiff(),\n",
        "  \"ordinary_kriging_linear_d18O_predicted_variance\" : raster.krig_variances_isoscape_geotiff(),\n",
        "}\n",
        "\n",
        "data = load_and_scale(grouped_random_fileset, columns_to_passthrough, columns_to_scale, columns_to_standardize, extra_columns_from_geotiffs)\n",
        "data.train.X.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfJ8tq64s94J"
      },
      "source": [
        "## Train and save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cFuIPM4afQPd",
        "outputId": "bc100d5d-7e79-40b8-a6ea-53134a79a425",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================\n",
            "demo_isoscape_model\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 16)]         0           []                               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 20)           340         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 20)           420         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " var_output (Dense)             (None, 1)            21          ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " mean_output (Dense)            (None, 1)            21          ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.multiply_1 (TFOpLambda  (None, 1)           0           ['var_output[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.multiply (TFOpLambda)  (None, 1)            0           ['mean_output[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 1)           0           ['tf.math.multiply_1[0][0]']     \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 1)           0           ['tf.math.multiply[0][0]']       \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1)            0           ['tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 2)            0           ['tf.__operators__.add[0][0]',   \n",
            "                                                                  'lambda[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 802\n",
            "Trainable params: 802\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5000\n",
            "23/23 [==============================] - 5s 108ms/step - loss: 1.2969 - val_loss: 16.3101\n",
            "Epoch 2/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.4071 - val_loss: 8.8556\n",
            "Epoch 3/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.3162 - val_loss: 8.4547\n",
            "Epoch 4/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.2235 - val_loss: 8.5783\n",
            "Epoch 5/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.4264 - val_loss: 7.8233\n",
            "Epoch 6/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.1134 - val_loss: 8.7793\n",
            "Epoch 7/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1659 - val_loss: 7.6066\n",
            "Epoch 8/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.3193 - val_loss: 7.2833\n",
            "Epoch 9/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1531 - val_loss: 5.7523\n",
            "Epoch 10/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.1647 - val_loss: 6.0647\n",
            "Epoch 11/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.1204 - val_loss: 7.2622\n",
            "Epoch 12/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.2192 - val_loss: 5.8735\n",
            "Epoch 13/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.1160 - val_loss: 4.4369\n",
            "Epoch 14/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0505 - val_loss: 5.3971\n",
            "Epoch 15/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.2136 - val_loss: 3.9169\n",
            "Epoch 16/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.1366 - val_loss: 4.7007\n",
            "Epoch 17/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.0239 - val_loss: 4.5050\n",
            "Epoch 18/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.0408 - val_loss: 5.4397\n",
            "Epoch 19/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0354 - val_loss: 3.9612\n",
            "Epoch 20/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.0915 - val_loss: 3.9132\n",
            "Epoch 21/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.1875 - val_loss: 3.9320\n",
            "Epoch 22/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9485 - val_loss: 3.9213\n",
            "Epoch 23/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.9185 - val_loss: 3.3309\n",
            "Epoch 24/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.0122 - val_loss: 3.6055\n",
            "Epoch 25/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.1046 - val_loss: 3.5660\n",
            "Epoch 26/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8723 - val_loss: 3.3620\n",
            "Epoch 27/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9659 - val_loss: 3.3291\n",
            "Epoch 28/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8817 - val_loss: 2.6785\n",
            "Epoch 29/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9333 - val_loss: 3.5435\n",
            "Epoch 30/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9984 - val_loss: 2.8563\n",
            "Epoch 31/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.9081 - val_loss: 4.0025\n",
            "Epoch 32/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.9753 - val_loss: 2.5872\n",
            "Epoch 33/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0134 - val_loss: 3.2561\n",
            "Epoch 34/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9173 - val_loss: 2.6325\n",
            "Epoch 35/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9145 - val_loss: 2.7812\n",
            "Epoch 36/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.9143 - val_loss: 2.5713\n",
            "Epoch 37/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8472 - val_loss: 3.7811\n",
            "Epoch 38/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0999 - val_loss: 2.8179\n",
            "Epoch 39/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8568 - val_loss: 2.5251\n",
            "Epoch 40/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.9501 - val_loss: 2.2215\n",
            "Epoch 41/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8217 - val_loss: 2.6392\n",
            "Epoch 42/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9280 - val_loss: 2.6149\n",
            "Epoch 43/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9994 - val_loss: 2.5270\n",
            "Epoch 44/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9243 - val_loss: 2.3668\n",
            "Epoch 45/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8834 - val_loss: 2.5350\n",
            "Epoch 46/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8995 - val_loss: 2.4634\n",
            "Epoch 47/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7835 - val_loss: 1.8735\n",
            "Epoch 48/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8243 - val_loss: 2.1623\n",
            "Epoch 49/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7529 - val_loss: 2.7215\n",
            "Epoch 50/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8973 - val_loss: 2.1745\n",
            "Epoch 51/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7796 - val_loss: 2.0761\n",
            "Epoch 52/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8831 - val_loss: 2.4421\n",
            "Epoch 53/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8423 - val_loss: 2.1195\n",
            "Epoch 54/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7679 - val_loss: 2.1048\n",
            "Epoch 55/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8603 - val_loss: 2.5152\n",
            "Epoch 56/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8462 - val_loss: 1.8736\n",
            "Epoch 57/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8798 - val_loss: 2.1027\n",
            "Epoch 58/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8391 - val_loss: 1.8635\n",
            "Epoch 59/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9622 - val_loss: 2.1646\n",
            "Epoch 60/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9038 - val_loss: 2.0536\n",
            "Epoch 61/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8269 - val_loss: 1.8231\n",
            "Epoch 62/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8961 - val_loss: 1.9054\n",
            "Epoch 63/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0910 - val_loss: 2.1394\n",
            "Epoch 64/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8730 - val_loss: 1.8551\n",
            "Epoch 65/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7062 - val_loss: 2.0756\n",
            "Epoch 66/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8281 - val_loss: 2.0029\n",
            "Epoch 67/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6948 - val_loss: 2.0503\n",
            "Epoch 68/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7603 - val_loss: 1.8351\n",
            "Epoch 69/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7405 - val_loss: 2.1699\n",
            "Epoch 70/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7642 - val_loss: 1.6638\n",
            "Epoch 71/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9131 - val_loss: 1.7229\n",
            "Epoch 72/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8184 - val_loss: 1.9672\n",
            "Epoch 73/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8731 - val_loss: 1.5286\n",
            "Epoch 74/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8011 - val_loss: 1.7933\n",
            "Epoch 75/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8154 - val_loss: 1.9710\n",
            "Epoch 76/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7789 - val_loss: 1.9145\n",
            "Epoch 77/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7245 - val_loss: 1.9043\n",
            "Epoch 78/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7407 - val_loss: 1.9028\n",
            "Epoch 79/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7833 - val_loss: 1.8025\n",
            "Epoch 80/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6946 - val_loss: 1.6433\n",
            "Epoch 81/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7949 - val_loss: 1.6663\n",
            "Epoch 82/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7118 - val_loss: 1.6155\n",
            "Epoch 83/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8721 - val_loss: 1.7026\n",
            "Epoch 84/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8081 - val_loss: 1.8053\n",
            "Epoch 85/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7925 - val_loss: 1.8102\n",
            "Epoch 86/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7421 - val_loss: 1.7212\n",
            "Epoch 87/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9048 - val_loss: 1.7251\n",
            "Epoch 88/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7639 - val_loss: 1.6495\n",
            "Epoch 89/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7919 - val_loss: 1.5082\n",
            "Epoch 90/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7516 - val_loss: 1.8083\n",
            "Epoch 91/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8018 - val_loss: 1.7839\n",
            "Epoch 92/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7786 - val_loss: 1.8988\n",
            "Epoch 93/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7964 - val_loss: 1.6725\n",
            "Epoch 94/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7605 - val_loss: 1.4998\n",
            "Epoch 95/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6993 - val_loss: 1.6035\n",
            "Epoch 96/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7187 - val_loss: 1.5495\n",
            "Epoch 97/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8172 - val_loss: 1.5997\n",
            "Epoch 98/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7443 - val_loss: 1.4289\n",
            "Epoch 99/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8141 - val_loss: 1.7945\n",
            "Epoch 100/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7088 - val_loss: 1.6612\n",
            "Epoch 101/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7790 - val_loss: 1.7819\n",
            "Epoch 102/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7615 - val_loss: 1.7641\n",
            "Epoch 103/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7029 - val_loss: 1.4134\n",
            "Epoch 104/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8287 - val_loss: 1.4751\n",
            "Epoch 105/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7980 - val_loss: 1.5407\n",
            "Epoch 106/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7241 - val_loss: 1.6635\n",
            "Epoch 107/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7635 - val_loss: 1.4663\n",
            "Epoch 108/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7392 - val_loss: 1.5672\n",
            "Epoch 109/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7891 - val_loss: 1.6065\n",
            "Epoch 110/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7031 - val_loss: 1.8484\n",
            "Epoch 111/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7015 - val_loss: 1.3419\n",
            "Epoch 112/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7506 - val_loss: 1.6339\n",
            "Epoch 113/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7439 - val_loss: 1.4077\n",
            "Epoch 114/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7481 - val_loss: 1.3795\n",
            "Epoch 115/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8621 - val_loss: 1.5196\n",
            "Epoch 116/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8354 - val_loss: 1.7606\n",
            "Epoch 117/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7737 - val_loss: 1.7112\n",
            "Epoch 118/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7720 - val_loss: 1.4894\n",
            "Epoch 119/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6942 - val_loss: 1.4597\n",
            "Epoch 120/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7906 - val_loss: 1.6635\n",
            "Epoch 121/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7769 - val_loss: 1.5763\n",
            "Epoch 122/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7197 - val_loss: 1.4285\n",
            "Epoch 123/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7310 - val_loss: 1.7833\n",
            "Epoch 124/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7548 - val_loss: 1.6538\n",
            "Epoch 125/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7314 - val_loss: 1.7027\n",
            "Epoch 126/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7008 - val_loss: 1.5671\n",
            "Epoch 127/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8191 - val_loss: 1.6265\n",
            "Epoch 128/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7182 - val_loss: 1.5949\n",
            "Epoch 129/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6636 - val_loss: 1.4492\n",
            "Epoch 130/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7678 - val_loss: 1.4929\n",
            "Epoch 131/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6454 - val_loss: 1.3816\n",
            "Epoch 132/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7410 - val_loss: 1.6798\n",
            "Epoch 133/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7146 - val_loss: 1.4941\n",
            "Epoch 134/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7993 - val_loss: 1.5018\n",
            "Epoch 135/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7746 - val_loss: 1.7127\n",
            "Epoch 136/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7699 - val_loss: 1.2900\n",
            "Epoch 137/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7226 - val_loss: 1.9163\n",
            "Epoch 138/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7083 - val_loss: 1.5546\n",
            "Epoch 139/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6665 - val_loss: 1.6015\n",
            "Epoch 140/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6729 - val_loss: 1.4443\n",
            "Epoch 141/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7170 - val_loss: 1.4434\n",
            "Epoch 142/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7658 - val_loss: 1.3497\n",
            "Epoch 143/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7323 - val_loss: 1.6628\n",
            "Epoch 144/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7638 - val_loss: 1.6181\n",
            "Epoch 145/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7689 - val_loss: 1.4840\n",
            "Epoch 146/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8113 - val_loss: 1.4224\n",
            "Epoch 147/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6653 - val_loss: 1.4360\n",
            "Epoch 148/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6985 - val_loss: 1.6188\n",
            "Epoch 149/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7669 - val_loss: 1.5690\n",
            "Epoch 150/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7217 - val_loss: 1.6061\n",
            "Epoch 151/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6776 - val_loss: 1.3766\n",
            "Epoch 152/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7433 - val_loss: 1.7283\n",
            "Epoch 153/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6977 - val_loss: 1.6219\n",
            "Epoch 154/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7159 - val_loss: 1.2818\n",
            "Epoch 155/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7311 - val_loss: 1.5173\n",
            "Epoch 156/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7605 - val_loss: 1.7613\n",
            "Epoch 157/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6610 - val_loss: 1.5570\n",
            "Epoch 158/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7272 - val_loss: 1.5092\n",
            "Epoch 159/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7359 - val_loss: 1.5222\n",
            "Epoch 160/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6922 - val_loss: 1.5802\n",
            "Epoch 161/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7086 - val_loss: 1.7074\n",
            "Epoch 162/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6882 - val_loss: 1.4367\n",
            "Epoch 163/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7901 - val_loss: 1.3308\n",
            "Epoch 164/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7640 - val_loss: 1.5351\n",
            "Epoch 165/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7509 - val_loss: 1.5261\n",
            "Epoch 166/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6577 - val_loss: 1.5864\n",
            "Epoch 167/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7503 - val_loss: 1.4508\n",
            "Epoch 168/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6619 - val_loss: 1.4357\n",
            "Epoch 169/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7392 - val_loss: 1.5968\n",
            "Epoch 170/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7903 - val_loss: 1.5320\n",
            "Epoch 171/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7729 - val_loss: 1.5523\n",
            "Epoch 172/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7004 - val_loss: 1.4051\n",
            "Epoch 173/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7360 - val_loss: 1.5113\n",
            "Epoch 174/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7615 - val_loss: 1.4303\n",
            "Epoch 175/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6649 - val_loss: 1.4720\n",
            "Epoch 176/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6710 - val_loss: 1.5037\n",
            "Epoch 177/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7183 - val_loss: 1.4458\n",
            "Epoch 178/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7090 - val_loss: 1.3233\n",
            "Epoch 179/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6469 - val_loss: 1.4036\n",
            "Epoch 180/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7119 - val_loss: 1.5447\n",
            "Epoch 181/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6953 - val_loss: 1.4512\n",
            "Epoch 182/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6631 - val_loss: 1.5471\n",
            "Epoch 183/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6960 - val_loss: 1.4215\n",
            "Epoch 184/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6793 - val_loss: 1.4619\n",
            "Epoch 185/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7100 - val_loss: 1.4494\n",
            "Epoch 186/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7518 - val_loss: 1.5861\n",
            "Epoch 187/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6777 - val_loss: 1.4570\n",
            "Epoch 188/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7751 - val_loss: 1.5915\n",
            "Epoch 189/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7219 - val_loss: 1.4648\n",
            "Epoch 190/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7739 - val_loss: 1.4536\n",
            "Epoch 191/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7289 - val_loss: 1.8154\n",
            "Epoch 192/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7667 - val_loss: 1.6137\n",
            "Epoch 193/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7130 - val_loss: 1.3875\n",
            "Epoch 194/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7040 - val_loss: 1.8183\n",
            "Epoch 195/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7239 - val_loss: 1.5924\n",
            "Epoch 196/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7159 - val_loss: 1.7050\n",
            "Epoch 197/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6822 - val_loss: 1.5283\n",
            "Epoch 198/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7018 - val_loss: 1.5678\n",
            "Epoch 199/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6811 - val_loss: 1.4906\n",
            "Epoch 200/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6909 - val_loss: 1.5558\n",
            "Epoch 201/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7185 - val_loss: 1.6016\n",
            "Epoch 202/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6588 - val_loss: 1.4104\n",
            "Epoch 203/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7123 - val_loss: 1.4489\n",
            "Epoch 204/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6983 - val_loss: 1.5766\n",
            "Epoch 205/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7693 - val_loss: 1.4733\n",
            "Epoch 206/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7001 - val_loss: 1.5697\n",
            "Epoch 207/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6843 - val_loss: 1.6827\n",
            "Epoch 208/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6740 - val_loss: 1.5451\n",
            "Epoch 209/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6943 - val_loss: 1.7536\n",
            "Epoch 210/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6700 - val_loss: 1.7749\n",
            "Epoch 211/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7253 - val_loss: 1.8790\n",
            "Epoch 212/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8084 - val_loss: 1.6306\n",
            "Epoch 213/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6895 - val_loss: 1.4893\n",
            "Epoch 214/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6894 - val_loss: 1.5303\n",
            "Epoch 215/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7286 - val_loss: 1.5211\n",
            "Epoch 216/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6773 - val_loss: 1.4735\n",
            "Epoch 217/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7863 - val_loss: 1.5436\n",
            "Epoch 218/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7094 - val_loss: 1.8497\n",
            "Epoch 219/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6843 - val_loss: 2.0244\n",
            "Epoch 220/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6737 - val_loss: 1.8124\n",
            "Epoch 221/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6813 - val_loss: 1.5663\n",
            "Epoch 222/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7178 - val_loss: 1.3882\n",
            "Epoch 223/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7303 - val_loss: 1.6056\n",
            "Epoch 224/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6564 - val_loss: 1.4596\n",
            "Epoch 225/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6619 - val_loss: 1.7307\n",
            "Epoch 226/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6335 - val_loss: 1.5840\n",
            "Epoch 227/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7085 - val_loss: 1.4806\n",
            "Epoch 228/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6627 - val_loss: 1.4236\n",
            "Epoch 229/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7374 - val_loss: 1.6315\n",
            "Epoch 230/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6983 - val_loss: 1.6314\n",
            "Epoch 231/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6835 - val_loss: 1.5348\n",
            "Epoch 232/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7421 - val_loss: 1.6176\n",
            "Epoch 233/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7160 - val_loss: 1.5848\n",
            "Epoch 234/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7669 - val_loss: 1.4824\n",
            "Epoch 235/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6857 - val_loss: 1.7364\n",
            "Epoch 236/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7383 - val_loss: 2.0638\n",
            "Epoch 237/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7134 - val_loss: 1.5997\n",
            "Epoch 238/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7146 - val_loss: 1.7886\n",
            "Epoch 239/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6808 - val_loss: 1.6214\n",
            "Epoch 240/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6976 - val_loss: 1.7037\n",
            "Epoch 241/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7155 - val_loss: 1.7272\n",
            "Epoch 242/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6800 - val_loss: 1.4799\n",
            "Epoch 243/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7129 - val_loss: 1.6051\n",
            "Epoch 244/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6801 - val_loss: 1.4209\n",
            "Epoch 245/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6920 - val_loss: 1.8295\n",
            "Epoch 246/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6891 - val_loss: 1.4125\n",
            "Epoch 247/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6668 - val_loss: 1.3849\n",
            "Epoch 248/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6776 - val_loss: 1.6617\n",
            "Epoch 249/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6917 - val_loss: 1.4230\n",
            "Epoch 250/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7227 - val_loss: 1.5975\n",
            "Epoch 251/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6789 - val_loss: 1.6887\n",
            "Epoch 252/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6769 - val_loss: 1.4606\n",
            "Epoch 253/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7919 - val_loss: 1.5972\n",
            "Epoch 254/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6791 - val_loss: 1.7363\n",
            "Epoch 255/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6926 - val_loss: 1.4801\n",
            "Epoch 256/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7858 - val_loss: 1.3910\n",
            "Epoch 257/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7010 - val_loss: 1.7197\n",
            "Epoch 258/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6890 - val_loss: 1.5454\n",
            "Epoch 259/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7192 - val_loss: 1.5707\n",
            "Epoch 260/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6539 - val_loss: 1.6159\n",
            "Epoch 261/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7153 - val_loss: 1.6023\n",
            "Epoch 262/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6842 - val_loss: 1.5632\n",
            "Epoch 263/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7077 - val_loss: 1.6055\n",
            "Epoch 264/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6687 - val_loss: 1.7484\n",
            "Epoch 265/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7199 - val_loss: 1.8056\n",
            "Epoch 266/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7336 - val_loss: 1.8322\n",
            "Epoch 267/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6942 - val_loss: 1.7136\n",
            "Epoch 268/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7471 - val_loss: 1.6865\n",
            "Epoch 269/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7309 - val_loss: 1.7514\n",
            "Epoch 270/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6886 - val_loss: 1.7727\n",
            "Epoch 271/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6777 - val_loss: 1.5879\n",
            "Epoch 272/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7111 - val_loss: 1.6062\n",
            "Epoch 273/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6992 - val_loss: 1.7873\n",
            "Epoch 274/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7400 - val_loss: 1.7437\n",
            "Epoch 275/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6782 - val_loss: 1.5766\n",
            "Epoch 276/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6655 - val_loss: 1.5800\n",
            "Epoch 277/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7219 - val_loss: 1.6542\n",
            "Epoch 278/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7010 - val_loss: 1.9022\n",
            "Epoch 279/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7158 - val_loss: 1.6051\n",
            "Epoch 280/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7027 - val_loss: 1.6084\n",
            "Epoch 281/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7142 - val_loss: 1.8656\n",
            "Epoch 282/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7062 - val_loss: 1.6548\n",
            "Epoch 283/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7230 - val_loss: 1.5252\n",
            "Epoch 284/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6591 - val_loss: 1.6286\n",
            "Epoch 285/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7470 - val_loss: 1.6276\n",
            "Epoch 286/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7225 - val_loss: 1.7902\n",
            "Epoch 287/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7081 - val_loss: 1.8283\n",
            "Epoch 288/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6841 - val_loss: 1.5906\n",
            "Epoch 289/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6760 - val_loss: 1.8096\n",
            "Epoch 290/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6965 - val_loss: 1.5306\n",
            "Epoch 291/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7735 - val_loss: 1.6867\n",
            "Epoch 292/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6881 - val_loss: 1.7448\n",
            "Epoch 293/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6965 - val_loss: 1.8937\n",
            "Epoch 294/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7621 - val_loss: 1.6785\n",
            "Epoch 295/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6710 - val_loss: 1.9535\n",
            "Epoch 296/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6851 - val_loss: 1.7072\n",
            "Epoch 297/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6709 - val_loss: 1.8153\n",
            "Epoch 298/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6897 - val_loss: 1.8432\n",
            "Epoch 299/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6893 - val_loss: 1.6733\n",
            "Epoch 300/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6940 - val_loss: 1.6342\n",
            "Epoch 301/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6971 - val_loss: 1.7355\n",
            "Epoch 302/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7411 - val_loss: 1.6274\n",
            "Epoch 303/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7227 - val_loss: 1.6209\n",
            "Epoch 304/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6603 - val_loss: 1.7111\n",
            "Epoch 305/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7399 - val_loss: 1.7679\n",
            "Epoch 306/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6812 - val_loss: 1.5943\n",
            "Epoch 307/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7552 - val_loss: 1.5601\n",
            "Epoch 308/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6558 - val_loss: 1.9547\n",
            "Epoch 309/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7048 - val_loss: 1.7593\n",
            "Epoch 310/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7665 - val_loss: 1.5389\n",
            "Epoch 311/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7269 - val_loss: 1.6336\n",
            "Epoch 312/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6943 - val_loss: 1.7118\n",
            "Epoch 313/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8009 - val_loss: 1.6649\n",
            "Epoch 314/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7070 - val_loss: 1.7158\n",
            "Epoch 315/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6412 - val_loss: 1.5841\n",
            "Epoch 316/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6425 - val_loss: 1.6920\n",
            "Epoch 317/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6862 - val_loss: 1.7822\n",
            "Epoch 318/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8063 - val_loss: 1.9350\n",
            "Epoch 319/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6954 - val_loss: 1.5969\n",
            "Epoch 320/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6649 - val_loss: 1.9283\n",
            "Epoch 321/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6186 - val_loss: 1.5989\n",
            "Epoch 322/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7101 - val_loss: 1.5121\n",
            "Epoch 323/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6850 - val_loss: 1.6931\n",
            "Epoch 324/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6831 - val_loss: 1.7513\n",
            "Epoch 325/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6893 - val_loss: 1.7095\n",
            "Epoch 326/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7189 - val_loss: 1.6240\n",
            "Epoch 327/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7102 - val_loss: 1.4540\n",
            "Epoch 328/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6959 - val_loss: 1.5044\n",
            "Epoch 329/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6919 - val_loss: 1.6214\n",
            "Epoch 330/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6729 - val_loss: 1.6885\n",
            "Epoch 331/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7003 - val_loss: 1.7007\n",
            "Epoch 332/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6732 - val_loss: 1.6944\n",
            "Epoch 333/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6865 - val_loss: 1.8385\n",
            "Epoch 334/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6631 - val_loss: 1.5595\n",
            "Epoch 335/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7061 - val_loss: 1.7036\n",
            "Epoch 336/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7051 - val_loss: 1.7577\n",
            "Epoch 337/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7022 - val_loss: 2.4215\n",
            "Epoch 338/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6940 - val_loss: 1.8512\n",
            "Epoch 339/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7529 - val_loss: 1.8951\n",
            "Epoch 340/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7131 - val_loss: 1.9523\n",
            "Epoch 341/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7014 - val_loss: 2.0730\n",
            "Epoch 342/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7191 - val_loss: 1.9036\n",
            "Epoch 343/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7862 - val_loss: 1.5404\n",
            "Epoch 344/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7655 - val_loss: 1.5939\n",
            "Epoch 345/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7817 - val_loss: 1.8640\n",
            "Epoch 346/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7470 - val_loss: 1.7974\n",
            "Epoch 347/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6743 - val_loss: 1.6866\n",
            "Epoch 348/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7202 - val_loss: 1.5548\n",
            "Epoch 349/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6164 - val_loss: 1.7657\n",
            "Epoch 350/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6813 - val_loss: 1.7788\n",
            "Epoch 351/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6970 - val_loss: 1.7146\n",
            "Epoch 352/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7870 - val_loss: 1.6422\n",
            "Epoch 353/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6832 - val_loss: 1.8264\n",
            "Epoch 354/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7386 - val_loss: 1.7914\n",
            "Epoch 355/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6855 - val_loss: 1.9140\n",
            "Epoch 356/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6941 - val_loss: 1.9630\n",
            "Epoch 357/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7194 - val_loss: 1.8328\n",
            "Epoch 358/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7185 - val_loss: 1.7667\n",
            "Epoch 359/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7167 - val_loss: 2.2695\n",
            "Epoch 360/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7169 - val_loss: 1.7576\n",
            "Epoch 361/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6338 - val_loss: 2.3049\n",
            "Epoch 362/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6611 - val_loss: 1.8162\n",
            "Epoch 363/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6392 - val_loss: 1.6775\n",
            "Epoch 364/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7271 - val_loss: 1.7179\n",
            "Epoch 365/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7309 - val_loss: 1.6494\n",
            "Epoch 366/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7125 - val_loss: 1.6186\n",
            "Epoch 367/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7058 - val_loss: 1.8187\n",
            "Epoch 368/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7153 - val_loss: 1.8408\n",
            "Epoch 369/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9110 - val_loss: 1.6539\n",
            "Epoch 370/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6455 - val_loss: 1.6027\n",
            "Epoch 371/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7915 - val_loss: 1.8666\n",
            "Epoch 372/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6928 - val_loss: 1.8905\n",
            "Epoch 373/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6861 - val_loss: 1.8742\n",
            "Epoch 374/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7469 - val_loss: 1.7594\n",
            "Epoch 375/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7505 - val_loss: 1.9384\n",
            "Epoch 376/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6847 - val_loss: 1.8211\n",
            "Epoch 377/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6760 - val_loss: 1.8360\n",
            "Epoch 378/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7061 - val_loss: 1.8251\n",
            "Epoch 379/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6272 - val_loss: 1.9691\n",
            "Epoch 380/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7392 - val_loss: 1.7014\n",
            "Epoch 381/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7275 - val_loss: 1.8784\n",
            "Epoch 382/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7240 - val_loss: 1.7598\n",
            "Epoch 383/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7154 - val_loss: 1.6959\n",
            "Epoch 384/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6781 - val_loss: 1.8141\n",
            "Epoch 385/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6757 - val_loss: 1.9820\n",
            "Epoch 386/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7303 - val_loss: 1.7754\n",
            "Epoch 387/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6934 - val_loss: 1.9247\n",
            "Epoch 388/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7035 - val_loss: 1.6755\n",
            "Epoch 389/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6777 - val_loss: 1.6989\n",
            "Epoch 390/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6951 - val_loss: 1.8711\n",
            "Epoch 391/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7134 - val_loss: 2.1069\n",
            "Epoch 392/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6836 - val_loss: 1.9384\n",
            "Epoch 393/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7695 - val_loss: 1.6103\n",
            "Epoch 394/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7667 - val_loss: 1.7295\n",
            "Epoch 395/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6735 - val_loss: 1.7739\n",
            "Epoch 396/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7018 - val_loss: 1.8270\n",
            "Epoch 397/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6568 - val_loss: 1.8132\n",
            "Epoch 398/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6674 - val_loss: 2.0122\n",
            "Epoch 399/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6941 - val_loss: 1.7879\n",
            "Epoch 400/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6834 - val_loss: 1.6809\n",
            "Epoch 401/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6396 - val_loss: 1.8779\n",
            "Epoch 402/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7373 - val_loss: 2.0523\n",
            "Epoch 403/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6877 - val_loss: 1.5627\n",
            "Epoch 404/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6841 - val_loss: 2.2759\n",
            "Epoch 405/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6369 - val_loss: 1.8992\n",
            "Epoch 406/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7374 - val_loss: 1.8705\n",
            "Epoch 407/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7840 - val_loss: 1.7013\n",
            "Epoch 408/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6552 - val_loss: 1.8203\n",
            "Epoch 409/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6883 - val_loss: 1.9397\n",
            "Epoch 410/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6519 - val_loss: 2.2598\n",
            "Epoch 411/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7298 - val_loss: 2.2462\n",
            "Epoch 412/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6543 - val_loss: 1.9189\n",
            "Epoch 413/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6825 - val_loss: 1.7730\n",
            "Epoch 414/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7679 - val_loss: 1.7938\n",
            "Epoch 415/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7263 - val_loss: 1.9628\n",
            "Epoch 416/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6975 - val_loss: 2.3038\n",
            "Epoch 417/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6790 - val_loss: 1.8772\n",
            "Epoch 418/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6711 - val_loss: 1.6489\n",
            "Epoch 419/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6308 - val_loss: 1.8869\n",
            "Epoch 420/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6462 - val_loss: 1.8905\n",
            "Epoch 421/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7280 - val_loss: 1.8587\n",
            "Epoch 422/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7584 - val_loss: 1.6605\n",
            "Epoch 423/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6420 - val_loss: 1.8265\n",
            "Epoch 424/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7901 - val_loss: 1.9577\n",
            "Epoch 425/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7208 - val_loss: 2.1530\n",
            "Epoch 426/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7481 - val_loss: 1.6947\n",
            "Epoch 427/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7166 - val_loss: 1.9746\n",
            "Epoch 428/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7130 - val_loss: 2.1447\n",
            "Epoch 429/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6520 - val_loss: 2.0129\n",
            "Epoch 430/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6588 - val_loss: 2.1765\n",
            "Epoch 431/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7368 - val_loss: 1.8444\n",
            "Epoch 432/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7271 - val_loss: 1.9460\n",
            "Epoch 433/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6974 - val_loss: 1.9938\n",
            "Epoch 434/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6624 - val_loss: 1.8685\n",
            "Epoch 435/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7192 - val_loss: 1.9102\n",
            "Epoch 436/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6727 - val_loss: 1.8192\n",
            "Epoch 437/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6475 - val_loss: 1.8799\n",
            "Epoch 438/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7548 - val_loss: 2.0118\n",
            "Epoch 439/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6909 - val_loss: 1.9239\n",
            "Epoch 440/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7065 - val_loss: 2.0560\n",
            "Epoch 441/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6843 - val_loss: 1.7566\n",
            "Epoch 442/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7130 - val_loss: 1.8523\n",
            "Epoch 443/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6836 - val_loss: 2.1346\n",
            "Epoch 444/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6671 - val_loss: 1.8907\n",
            "Epoch 445/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6958 - val_loss: 1.9087\n",
            "Epoch 446/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7346 - val_loss: 1.9990\n",
            "Epoch 447/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6492 - val_loss: 1.8768\n",
            "Epoch 448/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7190 - val_loss: 2.2413\n",
            "Epoch 449/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6782 - val_loss: 1.8437\n",
            "Epoch 450/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7383 - val_loss: 2.1729\n",
            "Epoch 451/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7493 - val_loss: 2.1844\n",
            "Epoch 452/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7145 - val_loss: 1.8505\n",
            "Epoch 453/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7673 - val_loss: 1.8863\n",
            "Epoch 454/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7166 - val_loss: 1.7499\n",
            "Epoch 455/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6664 - val_loss: 1.9362\n",
            "Epoch 456/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6899 - val_loss: 2.1367\n",
            "Epoch 457/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7531 - val_loss: 1.8035\n",
            "Epoch 458/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6660 - val_loss: 1.8316\n",
            "Epoch 459/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7312 - val_loss: 1.8928\n",
            "Epoch 460/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7170 - val_loss: 2.2642\n",
            "Epoch 461/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7395 - val_loss: 1.9127\n",
            "Epoch 462/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6967 - val_loss: 2.0790\n",
            "Epoch 463/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7813 - val_loss: 2.1074\n",
            "Epoch 464/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6710 - val_loss: 1.9502\n",
            "Epoch 465/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7066 - val_loss: 1.9191\n",
            "Epoch 466/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7329 - val_loss: 1.8958\n",
            "Epoch 467/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7138 - val_loss: 1.8037\n",
            "Epoch 468/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7368 - val_loss: 2.4612\n",
            "Epoch 469/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6970 - val_loss: 1.8456\n",
            "Epoch 470/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6966 - val_loss: 2.1095\n",
            "Epoch 471/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7195 - val_loss: 2.4652\n",
            "Epoch 472/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7755 - val_loss: 1.9428\n",
            "Epoch 473/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6578 - val_loss: 1.7869\n",
            "Epoch 474/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6744 - val_loss: 2.0965\n",
            "Epoch 475/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6879 - val_loss: 2.1249\n",
            "Epoch 476/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6859 - val_loss: 2.1527\n",
            "Epoch 477/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7157 - val_loss: 2.5775\n",
            "Epoch 478/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7155 - val_loss: 2.0233\n",
            "Epoch 479/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6784 - val_loss: 2.4153\n",
            "Epoch 480/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7132 - val_loss: 2.0166\n",
            "Epoch 481/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6846 - val_loss: 1.8760\n",
            "Epoch 482/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6529 - val_loss: 2.1001\n",
            "Epoch 483/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6964 - val_loss: 1.9467\n",
            "Epoch 484/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6724 - val_loss: 2.0800\n",
            "Epoch 485/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6662 - val_loss: 1.7554\n",
            "Epoch 486/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7096 - val_loss: 2.2889\n",
            "Epoch 487/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7120 - val_loss: 1.9607\n",
            "Epoch 488/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6829 - val_loss: 2.0746\n",
            "Epoch 489/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7192 - val_loss: 1.7905\n",
            "Epoch 490/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6312 - val_loss: 1.9953\n",
            "Epoch 491/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7437 - val_loss: 1.9490\n",
            "Epoch 492/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6674 - val_loss: 1.9989\n",
            "Epoch 493/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6942 - val_loss: 2.1734\n",
            "Epoch 494/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6847 - val_loss: 2.1331\n",
            "Epoch 495/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6999 - val_loss: 1.7608\n",
            "Epoch 496/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6777 - val_loss: 1.8290\n",
            "Epoch 497/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6650 - val_loss: 2.2081\n",
            "Epoch 498/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7390 - val_loss: 1.7905\n",
            "Epoch 499/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7387 - val_loss: 2.0160\n",
            "Epoch 500/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7662 - val_loss: 1.9876\n",
            "Epoch 501/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6708 - val_loss: 1.6556\n",
            "Epoch 502/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6745 - val_loss: 2.2583\n",
            "Epoch 503/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7434 - val_loss: 2.0289\n",
            "Epoch 504/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7294 - val_loss: 2.0370\n",
            "Epoch 505/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6717 - val_loss: 1.9250\n",
            "Epoch 506/5000\n",
            "23/23 [==============================] - 1s 33ms/step - loss: 0.6723 - val_loss: 2.0938\n",
            "Epoch 507/5000\n",
            "23/23 [==============================] - 1s 30ms/step - loss: 0.6617 - val_loss: 2.0979\n",
            "Epoch 508/5000\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.6592 - val_loss: 2.0845\n",
            "Epoch 509/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7416 - val_loss: 2.4149\n",
            "Epoch 510/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6810 - val_loss: 2.1698\n",
            "Epoch 511/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6955 - val_loss: 2.2277\n",
            "Epoch 512/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7535 - val_loss: 1.9174\n",
            "Epoch 513/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7272 - val_loss: 1.8131\n",
            "Epoch 514/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7207 - val_loss: 2.0850\n",
            "Epoch 515/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8694 - val_loss: 2.0574\n",
            "Epoch 516/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6692 - val_loss: 1.9976\n",
            "Epoch 517/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7396 - val_loss: 1.8745\n",
            "Epoch 518/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7161 - val_loss: 1.9478\n",
            "Epoch 519/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7047 - val_loss: 2.0944\n",
            "Epoch 520/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6724 - val_loss: 1.9972\n",
            "Epoch 521/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6999 - val_loss: 2.1178\n",
            "Epoch 522/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6681 - val_loss: 1.9229\n",
            "Epoch 523/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6829 - val_loss: 1.8542\n",
            "Epoch 524/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6843 - val_loss: 2.1116\n",
            "Epoch 525/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7298 - val_loss: 1.9908\n",
            "Epoch 526/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7198 - val_loss: 1.8896\n",
            "Epoch 527/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6561 - val_loss: 2.2874\n",
            "Epoch 528/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8015 - val_loss: 2.1639\n",
            "Epoch 529/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7736 - val_loss: 1.7527\n",
            "Epoch 530/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6979 - val_loss: 2.0212\n",
            "Epoch 531/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6704 - val_loss: 2.0375\n",
            "Epoch 532/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7108 - val_loss: 1.9101\n",
            "Epoch 533/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7589 - val_loss: 2.2962\n",
            "Epoch 534/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7461 - val_loss: 2.2070\n",
            "Epoch 535/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6148 - val_loss: 2.0760\n",
            "Epoch 536/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6849 - val_loss: 1.9208\n",
            "Epoch 537/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6867 - val_loss: 2.1599\n",
            "Epoch 538/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6851 - val_loss: 1.8075\n",
            "Epoch 539/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7449 - val_loss: 1.8554\n",
            "Epoch 540/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7243 - val_loss: 2.0122\n",
            "Epoch 541/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7183 - val_loss: 1.8103\n",
            "Epoch 542/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6180 - val_loss: 1.9104\n",
            "Epoch 543/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6949 - val_loss: 1.9406\n",
            "Epoch 544/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6860 - val_loss: 1.9418\n",
            "Epoch 545/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6485 - val_loss: 2.1574\n",
            "Epoch 546/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7471 - val_loss: 2.3134\n",
            "Epoch 547/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6302 - val_loss: 2.0390\n",
            "Epoch 548/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6964 - val_loss: 2.0475\n",
            "Epoch 549/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7213 - val_loss: 2.0978\n",
            "Epoch 550/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6131 - val_loss: 2.2016\n",
            "Epoch 551/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6764 - val_loss: 2.0344\n",
            "Epoch 552/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6369 - val_loss: 2.0720\n",
            "Epoch 553/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6744 - val_loss: 1.8684\n",
            "Epoch 554/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6824 - val_loss: 2.2384\n",
            "Epoch 555/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6251 - val_loss: 1.9800\n",
            "Epoch 556/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7125 - val_loss: 1.9831\n",
            "Epoch 557/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6953 - val_loss: 1.9466\n",
            "Epoch 558/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7269 - val_loss: 1.9295\n",
            "Epoch 559/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7066 - val_loss: 1.9670\n",
            "Epoch 560/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6772 - val_loss: 1.9409\n",
            "Epoch 561/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6897 - val_loss: 1.8351\n",
            "Epoch 562/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6301 - val_loss: 2.2103\n",
            "Epoch 563/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7317 - val_loss: 1.8812\n",
            "Epoch 564/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6569 - val_loss: 2.1828\n",
            "Epoch 565/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6482 - val_loss: 1.8968\n",
            "Epoch 566/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7006 - val_loss: 1.8851\n",
            "Epoch 567/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6928 - val_loss: 2.2623\n",
            "Epoch 568/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7254 - val_loss: 1.9371\n",
            "Epoch 569/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7611 - val_loss: 2.1705\n",
            "Epoch 570/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6949 - val_loss: 2.1598\n",
            "Epoch 571/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6759 - val_loss: 2.4468\n",
            "Epoch 572/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7227 - val_loss: 1.7681\n",
            "Epoch 573/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6509 - val_loss: 1.8184\n",
            "Epoch 574/5000\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.7196 - val_loss: 1.8275\n",
            "Epoch 575/5000\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.7303 - val_loss: 1.8196\n",
            "Epoch 576/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.7515 - val_loss: 2.2867\n",
            "Epoch 577/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7252 - val_loss: 2.0282\n",
            "Epoch 578/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7301 - val_loss: 1.8177\n",
            "Epoch 579/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.6712 - val_loss: 2.0082\n",
            "Epoch 580/5000\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.7106 - val_loss: 2.0213\n",
            "Epoch 581/5000\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.6829 - val_loss: 2.3627\n",
            "Epoch 582/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6613 - val_loss: 2.7401\n",
            "Epoch 583/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6288 - val_loss: 1.8022\n",
            "Epoch 584/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6870 - val_loss: 1.9445\n",
            "Epoch 585/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7122 - val_loss: 1.9236\n",
            "Epoch 586/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6986 - val_loss: 1.8265\n",
            "Epoch 587/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7153 - val_loss: 2.1869\n",
            "Epoch 588/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7369 - val_loss: 2.4998\n",
            "Epoch 589/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6554 - val_loss: 2.0184\n",
            "Epoch 590/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7065 - val_loss: 2.1421\n",
            "Epoch 591/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6755 - val_loss: 2.2218\n",
            "Epoch 592/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7711 - val_loss: 2.0228\n",
            "Epoch 593/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6974 - val_loss: 2.1623\n",
            "Epoch 594/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6417 - val_loss: 2.2249\n",
            "Epoch 595/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6736 - val_loss: 2.1868\n",
            "Epoch 596/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7364 - val_loss: 2.1187\n",
            "Epoch 597/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7059 - val_loss: 2.2981\n",
            "Epoch 598/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6516 - val_loss: 2.2593\n",
            "Epoch 599/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6957 - val_loss: 2.5755\n",
            "Epoch 600/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6335 - val_loss: 1.9063\n",
            "Epoch 601/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6421 - val_loss: 1.8085\n",
            "Epoch 602/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7204 - val_loss: 2.0273\n",
            "Epoch 603/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6835 - val_loss: 2.0918\n",
            "Epoch 604/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6543 - val_loss: 1.9764\n",
            "Epoch 605/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7344 - val_loss: 2.3856\n",
            "Epoch 606/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6465 - val_loss: 2.0536\n",
            "Epoch 607/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7902 - val_loss: 1.8952\n",
            "Epoch 608/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6822 - val_loss: 2.1733\n",
            "Epoch 609/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7658 - val_loss: 1.9782\n",
            "Epoch 610/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6880 - val_loss: 1.9208\n",
            "Epoch 611/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7130 - val_loss: 2.0706\n",
            "Epoch 612/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7412 - val_loss: 1.9546\n",
            "Epoch 613/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6218 - val_loss: 2.4825\n",
            "Epoch 614/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7017 - val_loss: 1.8562\n",
            "Epoch 615/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7299 - val_loss: 1.9793\n",
            "Epoch 616/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7172 - val_loss: 2.3425\n",
            "Epoch 617/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6860 - val_loss: 1.9785\n",
            "Epoch 618/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7407 - val_loss: 2.5116\n",
            "Epoch 619/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8531 - val_loss: 2.0053\n",
            "Epoch 620/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6792 - val_loss: 2.1518\n",
            "Epoch 621/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6458 - val_loss: 1.8777\n",
            "Epoch 622/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6841 - val_loss: 2.2501\n",
            "Epoch 623/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8520 - val_loss: 2.6642\n",
            "Epoch 624/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6672 - val_loss: 1.8334\n",
            "Epoch 625/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7086 - val_loss: 1.9761\n",
            "Epoch 626/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6597 - val_loss: 2.0964\n",
            "Epoch 627/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6890 - val_loss: 1.7710\n",
            "Epoch 628/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6671 - val_loss: 2.1247\n",
            "Epoch 629/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7196 - val_loss: 2.0410\n",
            "Epoch 630/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7220 - val_loss: 2.1882\n",
            "Epoch 631/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6888 - val_loss: 2.1193\n",
            "Epoch 632/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6865 - val_loss: 2.2261\n",
            "Epoch 633/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7252 - val_loss: 2.0840\n",
            "Epoch 634/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6935 - val_loss: 1.9285\n",
            "Epoch 635/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6734 - val_loss: 2.1382\n",
            "Epoch 636/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7010 - val_loss: 1.8785\n",
            "Epoch 637/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6920 - val_loss: 2.1516\n",
            "Epoch 638/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6774 - val_loss: 1.9517\n",
            "Epoch 639/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6885 - val_loss: 2.0529\n",
            "Epoch 640/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6905 - val_loss: 2.0817\n",
            "Epoch 641/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6825 - val_loss: 2.0875\n",
            "Epoch 642/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7669 - val_loss: 1.7843\n",
            "Epoch 643/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6696 - val_loss: 2.1840\n",
            "Epoch 644/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6673 - val_loss: 2.3173\n",
            "Epoch 645/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6349 - val_loss: 2.2848\n",
            "Epoch 646/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7623 - val_loss: 2.3634\n",
            "Epoch 647/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6816 - val_loss: 1.8835\n",
            "Epoch 648/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6458 - val_loss: 2.1269\n",
            "Epoch 649/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7279 - val_loss: 2.0762\n",
            "Epoch 650/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6745 - val_loss: 1.9561\n",
            "Epoch 651/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6915 - val_loss: 2.1668\n",
            "Epoch 652/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6997 - val_loss: 2.2072\n",
            "Epoch 653/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7088 - val_loss: 2.3377\n",
            "Epoch 654/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7137 - val_loss: 2.0776\n",
            "Epoch 655/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6812 - val_loss: 2.1888\n",
            "Epoch 656/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6680 - val_loss: 1.8508\n",
            "Epoch 657/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6860 - val_loss: 2.2872\n",
            "Epoch 658/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6845 - val_loss: 2.1829\n",
            "Epoch 659/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7152 - val_loss: 2.7547\n",
            "Epoch 660/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6274 - val_loss: 2.0931\n",
            "Epoch 661/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6854 - val_loss: 2.4055\n",
            "Epoch 662/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7478 - val_loss: 1.9800\n",
            "Epoch 663/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6816 - val_loss: 2.3393\n",
            "Epoch 664/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6778 - val_loss: 2.0889\n",
            "Epoch 665/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6701 - val_loss: 2.2670\n",
            "Epoch 666/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6520 - val_loss: 1.9744\n",
            "Epoch 667/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6639 - val_loss: 2.2112\n",
            "Epoch 668/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6465 - val_loss: 2.0582\n",
            "Epoch 669/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6963 - val_loss: 2.7168\n",
            "Epoch 670/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6736 - val_loss: 2.0351\n",
            "Epoch 671/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6563 - val_loss: 1.9418\n",
            "Epoch 672/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6439 - val_loss: 2.1610\n",
            "Epoch 673/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6828 - val_loss: 2.4008\n",
            "Epoch 674/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7306 - val_loss: 2.5862\n",
            "Epoch 675/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6696 - val_loss: 1.9465\n",
            "Epoch 676/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6939 - val_loss: 2.1581\n",
            "Epoch 677/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6120 - val_loss: 2.2955\n",
            "Epoch 678/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6579 - val_loss: 2.0247\n",
            "Epoch 679/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7235 - val_loss: 2.2389\n",
            "Epoch 680/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6913 - val_loss: 2.4154\n",
            "Epoch 681/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6940 - val_loss: 2.3293\n",
            "Epoch 682/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6977 - val_loss: 2.4191\n",
            "Epoch 683/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7081 - val_loss: 2.0827\n",
            "Epoch 684/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7307 - val_loss: 1.9540\n",
            "Epoch 685/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6507 - val_loss: 2.1944\n",
            "Epoch 686/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7134 - val_loss: 2.0371\n",
            "Epoch 687/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6461 - val_loss: 2.2140\n",
            "Epoch 688/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7641 - val_loss: 2.2788\n",
            "Epoch 689/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6335 - val_loss: 2.2586\n",
            "Epoch 690/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6992 - val_loss: 1.9669\n",
            "Epoch 691/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6719 - val_loss: 2.2155\n",
            "Epoch 692/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7090 - val_loss: 1.8836\n",
            "Epoch 693/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6229 - val_loss: 2.2798\n",
            "Epoch 694/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6969 - val_loss: 2.0540\n",
            "Epoch 695/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6984 - val_loss: 2.3561\n",
            "Epoch 696/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6795 - val_loss: 2.4209\n",
            "Epoch 697/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6914 - val_loss: 2.0368\n",
            "Epoch 698/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7798 - val_loss: 2.4420\n",
            "Epoch 699/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6766 - val_loss: 2.1811\n",
            "Epoch 700/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7254 - val_loss: 2.4445\n",
            "Epoch 701/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7615 - val_loss: 2.4195\n",
            "Epoch 702/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7410 - val_loss: 2.3138\n",
            "Epoch 703/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6765 - val_loss: 2.3805\n",
            "Epoch 704/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7447 - val_loss: 2.0617\n",
            "Epoch 705/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6991 - val_loss: 2.2288\n",
            "Epoch 706/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6707 - val_loss: 2.3207\n",
            "Epoch 707/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6834 - val_loss: 2.3227\n",
            "Epoch 708/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6901 - val_loss: 2.0168\n",
            "Epoch 709/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7027 - val_loss: 2.3444\n",
            "Epoch 710/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6943 - val_loss: 2.1367\n",
            "Epoch 711/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6817 - val_loss: 2.1395\n",
            "Epoch 712/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7523 - val_loss: 1.8625\n",
            "Epoch 713/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7600 - val_loss: 2.0308\n",
            "Epoch 714/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6463 - val_loss: 2.5045\n",
            "Epoch 715/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7027 - val_loss: 2.2133\n",
            "Epoch 716/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7134 - val_loss: 2.3271\n",
            "Epoch 717/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7174 - val_loss: 2.3943\n",
            "Epoch 718/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6415 - val_loss: 2.3382\n",
            "Epoch 719/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6730 - val_loss: 2.2775\n",
            "Epoch 720/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6806 - val_loss: 2.4826\n",
            "Epoch 721/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6848 - val_loss: 2.2629\n",
            "Epoch 722/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7412 - val_loss: 2.1697\n",
            "Epoch 723/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6661 - val_loss: 2.0134\n",
            "Epoch 724/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7430 - val_loss: 2.0843\n",
            "Epoch 725/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7141 - val_loss: 1.9815\n",
            "Epoch 726/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7224 - val_loss: 2.4121\n",
            "Epoch 727/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6572 - val_loss: 1.8647\n",
            "Epoch 728/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6941 - val_loss: 1.9955\n",
            "Epoch 729/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7214 - val_loss: 2.0181\n",
            "Epoch 730/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7112 - val_loss: 2.2626\n",
            "Epoch 731/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6806 - val_loss: 2.0447\n",
            "Epoch 732/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7509 - val_loss: 2.1144\n",
            "Epoch 733/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7248 - val_loss: 2.2388\n",
            "Epoch 734/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7018 - val_loss: 2.5619\n",
            "Epoch 735/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6709 - val_loss: 2.1600\n",
            "Epoch 736/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7609 - val_loss: 2.4036\n",
            "Epoch 737/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6750 - val_loss: 1.9351\n",
            "Epoch 738/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7175 - val_loss: 2.4145\n",
            "Epoch 739/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7039 - val_loss: 2.3639\n",
            "Epoch 740/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6661 - val_loss: 2.1338\n",
            "Epoch 741/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6974 - val_loss: 2.3086\n",
            "Epoch 742/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6580 - val_loss: 2.2208\n",
            "Epoch 743/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7060 - val_loss: 2.4999\n",
            "Epoch 744/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6767 - val_loss: 2.4233\n",
            "Epoch 745/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6525 - val_loss: 2.2237\n",
            "Epoch 746/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7523 - val_loss: 2.3681\n",
            "Epoch 747/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6373 - val_loss: 2.2029\n",
            "Epoch 748/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7099 - val_loss: 2.2346\n",
            "Epoch 749/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7091 - val_loss: 2.1685\n",
            "Epoch 750/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6569 - val_loss: 2.6028\n",
            "Epoch 751/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6783 - val_loss: 2.1637\n",
            "Epoch 752/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6652 - val_loss: 2.4193\n",
            "Epoch 753/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7220 - val_loss: 2.1956\n",
            "Epoch 754/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7410 - val_loss: 2.1866\n",
            "Epoch 755/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7029 - val_loss: 2.0366\n",
            "Epoch 756/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7240 - val_loss: 2.4094\n",
            "Epoch 757/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6683 - val_loss: 2.5570\n",
            "Epoch 758/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7028 - val_loss: 2.2445\n",
            "Epoch 759/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6886 - val_loss: 2.0480\n",
            "Epoch 760/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6778 - val_loss: 2.1737\n",
            "Epoch 761/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7072 - val_loss: 2.4096\n",
            "Epoch 762/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6231 - val_loss: 2.3358\n",
            "Epoch 763/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7034 - val_loss: 2.3599\n",
            "Epoch 764/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6858 - val_loss: 2.2946\n",
            "Epoch 765/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7398 - val_loss: 2.3802\n",
            "Epoch 766/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6611 - val_loss: 2.4527\n",
            "Epoch 767/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6900 - val_loss: 2.0401\n",
            "Epoch 768/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6876 - val_loss: 2.4730\n",
            "Epoch 769/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7140 - val_loss: 2.4793\n",
            "Epoch 770/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7070 - val_loss: 2.4284\n",
            "Epoch 771/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7777 - val_loss: 2.2767\n",
            "Epoch 772/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6882 - val_loss: 2.2065\n",
            "Epoch 773/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6804 - val_loss: 2.1804\n",
            "Epoch 774/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6472 - val_loss: 2.8165\n",
            "Epoch 775/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7016 - val_loss: 2.2384\n",
            "Epoch 776/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6957 - val_loss: 2.2469\n",
            "Epoch 777/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6754 - val_loss: 2.2992\n",
            "Epoch 778/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7132 - val_loss: 2.3747\n",
            "Epoch 779/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7115 - val_loss: 2.3267\n",
            "Epoch 780/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6907 - val_loss: 2.5455\n",
            "Epoch 781/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6907 - val_loss: 2.5089\n",
            "Epoch 782/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6827 - val_loss: 2.6861\n",
            "Epoch 783/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6857 - val_loss: 2.3227\n",
            "Epoch 784/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6706 - val_loss: 2.2562\n",
            "Epoch 785/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6818 - val_loss: 2.2340\n",
            "Epoch 786/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6630 - val_loss: 2.7063\n",
            "Epoch 787/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7184 - val_loss: 2.2607\n",
            "Epoch 788/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7144 - val_loss: 2.4547\n",
            "Epoch 789/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7575 - val_loss: 2.1823\n",
            "Epoch 790/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6692 - val_loss: 2.4471\n",
            "Epoch 791/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7646 - val_loss: 2.6238\n",
            "Epoch 792/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7271 - val_loss: 2.2127\n",
            "Epoch 793/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6934 - val_loss: 2.6743\n",
            "Epoch 794/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6636 - val_loss: 2.3339\n",
            "Epoch 795/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6928 - val_loss: 2.2367\n",
            "Epoch 796/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7269 - val_loss: 2.0552\n",
            "Epoch 797/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6579 - val_loss: 2.0707\n",
            "Epoch 798/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7068 - val_loss: 2.4391\n",
            "Epoch 799/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6797 - val_loss: 2.5105\n",
            "Epoch 800/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6871 - val_loss: 2.3937\n",
            "Epoch 801/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6905 - val_loss: 2.2300\n",
            "Epoch 802/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6848 - val_loss: 2.5208\n",
            "Epoch 803/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7288 - val_loss: 2.3662\n",
            "Epoch 804/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6609 - val_loss: 2.2884\n",
            "Epoch 805/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6993 - val_loss: 2.3979\n",
            "Epoch 806/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6961 - val_loss: 2.3170\n",
            "Epoch 807/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7164 - val_loss: 2.4942\n",
            "Epoch 808/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6738 - val_loss: 2.2340\n",
            "Epoch 809/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7067 - val_loss: 2.2817\n",
            "Epoch 810/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6682 - val_loss: 2.9584\n",
            "Epoch 811/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6974 - val_loss: 2.2175\n",
            "Epoch 812/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6917 - val_loss: 2.4140\n",
            "Epoch 813/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6790 - val_loss: 2.4739\n",
            "Epoch 814/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6924 - val_loss: 2.0986\n",
            "Epoch 815/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6948 - val_loss: 2.3247\n",
            "Epoch 816/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6492 - val_loss: 2.0828\n",
            "Epoch 817/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7043 - val_loss: 2.4233\n",
            "Epoch 818/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6449 - val_loss: 2.2905\n",
            "Epoch 819/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7449 - val_loss: 2.6385\n",
            "Epoch 820/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7157 - val_loss: 3.0806\n",
            "Epoch 821/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7014 - val_loss: 2.3342\n",
            "Epoch 822/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7193 - val_loss: 2.4872\n",
            "Epoch 823/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6731 - val_loss: 2.0931\n",
            "Epoch 824/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7613 - val_loss: 2.2939\n",
            "Epoch 825/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6505 - val_loss: 2.1929\n",
            "Epoch 826/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6755 - val_loss: 3.0823\n",
            "Epoch 827/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6755 - val_loss: 2.1053\n",
            "Epoch 828/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6446 - val_loss: 2.3287\n",
            "Epoch 829/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6911 - val_loss: 2.3142\n",
            "Epoch 830/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6853 - val_loss: 2.2453\n",
            "Epoch 831/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7152 - val_loss: 2.6551\n",
            "Epoch 832/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7482 - val_loss: 2.7967\n",
            "Epoch 833/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6789 - val_loss: 2.4792\n",
            "Epoch 834/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7083 - val_loss: 2.1064\n",
            "Epoch 835/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7464 - val_loss: 2.3132\n",
            "Epoch 836/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7772 - val_loss: 2.3360\n",
            "Epoch 837/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6909 - val_loss: 2.8090\n",
            "Epoch 838/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6545 - val_loss: 2.2046\n",
            "Epoch 839/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6658 - val_loss: 2.2077\n",
            "Epoch 840/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6405 - val_loss: 2.2814\n",
            "Epoch 841/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7101 - val_loss: 2.3992\n",
            "Epoch 842/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7171 - val_loss: 2.3328\n",
            "Epoch 843/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7303 - val_loss: 2.1078\n",
            "Epoch 844/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6145 - val_loss: 2.3139\n",
            "Epoch 845/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6405 - val_loss: 2.4145\n",
            "Epoch 846/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7204 - val_loss: 2.5593\n",
            "Epoch 847/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6862 - val_loss: 2.0255\n",
            "Epoch 848/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7833 - val_loss: 2.1228\n",
            "Epoch 849/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6743 - val_loss: 2.4266\n",
            "Epoch 850/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6588 - val_loss: 2.5156\n",
            "Epoch 851/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6922 - val_loss: 2.1412\n",
            "Epoch 852/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7059 - val_loss: 2.2545\n",
            "Epoch 853/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6812 - val_loss: 2.3414\n",
            "Epoch 854/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6571 - val_loss: 2.5099\n",
            "Epoch 855/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6877 - val_loss: 2.7496\n",
            "Epoch 856/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6634 - val_loss: 2.4039\n",
            "Epoch 857/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7186 - val_loss: 2.5887\n",
            "Epoch 858/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7395 - val_loss: 2.7730\n",
            "Epoch 859/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7271 - val_loss: 2.4293\n",
            "Epoch 860/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6913 - val_loss: 2.9985\n",
            "Epoch 861/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6526 - val_loss: 2.1084\n",
            "Epoch 862/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7194 - val_loss: 2.2902\n",
            "Epoch 863/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6759 - val_loss: 2.4370\n",
            "Epoch 864/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7298 - val_loss: 2.3146\n",
            "Epoch 865/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7386 - val_loss: 2.5366\n",
            "Epoch 866/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7355 - val_loss: 2.4291\n",
            "Epoch 867/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6661 - val_loss: 2.0753\n",
            "Epoch 868/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6597 - val_loss: 2.4011\n",
            "Epoch 869/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7146 - val_loss: 2.3149\n",
            "Epoch 870/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7491 - val_loss: 2.4594\n",
            "Epoch 871/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7029 - val_loss: 2.3413\n",
            "Epoch 872/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6610 - val_loss: 2.7382\n",
            "Epoch 873/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7100 - val_loss: 2.4568\n",
            "Epoch 874/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6882 - val_loss: 2.6842\n",
            "Epoch 875/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6974 - val_loss: 2.3339\n",
            "Epoch 876/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6418 - val_loss: 2.5669\n",
            "Epoch 877/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6560 - val_loss: 2.8789\n",
            "Epoch 878/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7718 - val_loss: 2.4023\n",
            "Epoch 879/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6503 - val_loss: 2.2033\n",
            "Epoch 880/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6887 - val_loss: 2.1674\n",
            "Epoch 881/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6282 - val_loss: 2.4017\n",
            "Epoch 882/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7148 - val_loss: 2.0572\n",
            "Epoch 883/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6741 - val_loss: 2.6724\n",
            "Epoch 884/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6935 - val_loss: 2.0795\n",
            "Epoch 885/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6256 - val_loss: 2.5662\n",
            "Epoch 886/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6711 - val_loss: 2.0412\n",
            "Epoch 887/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6891 - val_loss: 2.7995\n",
            "Epoch 888/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6719 - val_loss: 2.3592\n",
            "Epoch 889/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6446 - val_loss: 2.4592\n",
            "Epoch 890/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7360 - val_loss: 2.8199\n",
            "Epoch 891/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6743 - val_loss: 3.2378\n",
            "Epoch 892/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6652 - val_loss: 2.6798\n",
            "Epoch 893/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7543 - val_loss: 2.8558\n",
            "Epoch 894/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6501 - val_loss: 2.9875\n",
            "Epoch 895/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6623 - val_loss: 2.6876\n",
            "Epoch 896/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6758 - val_loss: 2.5181\n",
            "Epoch 897/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7062 - val_loss: 2.5421\n",
            "Epoch 898/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6434 - val_loss: 2.6083\n",
            "Epoch 899/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6739 - val_loss: 2.8168\n",
            "Epoch 900/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6834 - val_loss: 2.7690\n",
            "Epoch 901/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7763 - val_loss: 2.5394\n",
            "Epoch 902/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6707 - val_loss: 2.6483\n",
            "Epoch 903/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7082 - val_loss: 2.9316\n",
            "Epoch 904/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6691 - val_loss: 2.4673\n",
            "Epoch 905/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6403 - val_loss: 2.4337\n",
            "Epoch 906/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6764 - val_loss: 2.3961\n",
            "Epoch 907/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7656 - val_loss: 2.6435\n",
            "Epoch 908/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7222 - val_loss: 2.5541\n",
            "Epoch 909/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6385 - val_loss: 2.8279\n",
            "Epoch 910/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6754 - val_loss: 2.2062\n",
            "Epoch 911/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6511 - val_loss: 2.4888\n",
            "Epoch 912/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7538 - val_loss: 2.9872\n",
            "Epoch 913/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6328 - val_loss: 2.6550\n",
            "Epoch 914/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7021 - val_loss: 2.4793\n",
            "Epoch 915/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6518 - val_loss: 2.5933\n",
            "Epoch 916/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6656 - val_loss: 2.8484\n",
            "Epoch 917/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7131 - val_loss: 2.2582\n",
            "Epoch 918/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6518 - val_loss: 2.5906\n",
            "Epoch 919/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6617 - val_loss: 2.4555\n",
            "Epoch 920/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5879 - val_loss: 2.7022\n",
            "Epoch 921/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6638 - val_loss: 2.4246\n",
            "Epoch 922/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7117 - val_loss: 3.0058\n",
            "Epoch 923/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6614 - val_loss: 2.6446\n",
            "Epoch 924/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6685 - val_loss: 2.6671\n",
            "Epoch 925/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7268 - val_loss: 2.6780\n",
            "Epoch 926/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6956 - val_loss: 3.2235\n",
            "Epoch 927/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7464 - val_loss: 2.2779\n",
            "Epoch 928/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6229 - val_loss: 2.8749\n",
            "Epoch 929/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6638 - val_loss: 2.4014\n",
            "Epoch 930/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6657 - val_loss: 2.2918\n",
            "Epoch 931/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6406 - val_loss: 2.7468\n",
            "Epoch 932/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7170 - val_loss: 2.6875\n",
            "Epoch 933/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6373 - val_loss: 2.4157\n",
            "Epoch 934/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6820 - val_loss: 3.0827\n",
            "Epoch 935/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7042 - val_loss: 2.2317\n",
            "Epoch 936/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6688 - val_loss: 2.4512\n",
            "Epoch 937/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6799 - val_loss: 2.4793\n",
            "Epoch 938/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7253 - val_loss: 2.2661\n",
            "Epoch 939/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7485 - val_loss: 2.9209\n",
            "Epoch 940/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7506 - val_loss: 2.8885\n",
            "Epoch 941/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6373 - val_loss: 2.3527\n",
            "Epoch 942/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6699 - val_loss: 2.7393\n",
            "Epoch 943/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6087 - val_loss: 2.3359\n",
            "Epoch 944/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6807 - val_loss: 2.3188\n",
            "Epoch 945/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6945 - val_loss: 2.5335\n",
            "Epoch 946/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6634 - val_loss: 2.5466\n",
            "Epoch 947/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7221 - val_loss: 2.2104\n",
            "Epoch 948/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7335 - val_loss: 2.7538\n",
            "Epoch 949/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6944 - val_loss: 3.0579\n",
            "Epoch 950/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6613 - val_loss: 2.5671\n",
            "Epoch 951/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6954 - val_loss: 2.1816\n",
            "Epoch 952/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6359 - val_loss: 2.5583\n",
            "Epoch 953/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6644 - val_loss: 2.9752\n",
            "Epoch 954/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7079 - val_loss: 2.8350\n",
            "Epoch 955/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7434 - val_loss: 2.4645\n",
            "Epoch 956/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7547 - val_loss: 2.2407\n",
            "Epoch 957/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6095 - val_loss: 2.2813\n",
            "Epoch 958/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7273 - val_loss: 2.4573\n",
            "Epoch 959/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7119 - val_loss: 2.5283\n",
            "Epoch 960/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7204 - val_loss: 2.6360\n",
            "Epoch 961/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6667 - val_loss: 2.9230\n",
            "Epoch 962/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6690 - val_loss: 2.4271\n",
            "Epoch 963/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6439 - val_loss: 2.7287\n",
            "Epoch 964/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8277 - val_loss: 2.1924\n",
            "Epoch 965/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6709 - val_loss: 2.6217\n",
            "Epoch 966/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7121 - val_loss: 2.6251\n",
            "Epoch 967/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6849 - val_loss: 3.1661\n",
            "Epoch 968/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7392 - val_loss: 3.3693\n",
            "Epoch 969/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7049 - val_loss: 2.3184\n",
            "Epoch 970/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6681 - val_loss: 2.7845\n",
            "Epoch 971/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6808 - val_loss: 2.7402\n",
            "Epoch 972/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7152 - val_loss: 2.2793\n",
            "Epoch 973/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7037 - val_loss: 2.2730\n",
            "Epoch 974/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6924 - val_loss: 2.5569\n",
            "Epoch 975/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6806 - val_loss: 3.1001\n",
            "Epoch 976/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7366 - val_loss: 2.2571\n",
            "Epoch 977/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6855 - val_loss: 2.7246\n",
            "Epoch 978/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6871 - val_loss: 2.9610\n",
            "Epoch 979/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7074 - val_loss: 3.0280\n",
            "Epoch 980/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6951 - val_loss: 2.5445\n",
            "Epoch 981/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6586 - val_loss: 2.5567\n",
            "Epoch 982/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6630 - val_loss: 2.4977\n",
            "Epoch 983/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7200 - val_loss: 2.6599\n",
            "Epoch 984/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7068 - val_loss: 2.8809\n",
            "Epoch 985/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6934 - val_loss: 2.7083\n",
            "Epoch 986/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6968 - val_loss: 2.6214\n",
            "Epoch 987/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6844 - val_loss: 2.2911\n",
            "Epoch 988/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6889 - val_loss: 2.7376\n",
            "Epoch 989/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6935 - val_loss: 2.7004\n",
            "Epoch 990/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6797 - val_loss: 3.2100\n",
            "Epoch 991/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6382 - val_loss: 3.0429\n",
            "Epoch 992/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6860 - val_loss: 2.4062\n",
            "Epoch 993/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7169 - val_loss: 2.4603\n",
            "Epoch 994/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6949 - val_loss: 3.1329\n",
            "Epoch 995/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6883 - val_loss: 2.3675\n",
            "Epoch 996/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6596 - val_loss: 2.8847\n",
            "Epoch 997/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6939 - val_loss: 2.7444\n",
            "Epoch 998/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7245 - val_loss: 2.9008\n",
            "Epoch 999/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6890 - val_loss: 2.4578\n",
            "Epoch 1000/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6230 - val_loss: 2.4645\n",
            "Epoch 1001/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6444 - val_loss: 3.1167\n",
            "Epoch 1002/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6988 - val_loss: 2.4895\n",
            "Epoch 1003/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7097 - val_loss: 2.8277\n",
            "Epoch 1004/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6636 - val_loss: 2.5931\n",
            "Epoch 1005/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6131 - val_loss: 2.6728\n",
            "Epoch 1006/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6627 - val_loss: 2.8164\n",
            "Epoch 1007/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6435 - val_loss: 2.3509\n",
            "Epoch 1008/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6567 - val_loss: 2.5808\n",
            "Epoch 1009/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7362 - val_loss: 2.4531\n",
            "Epoch 1010/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6391 - val_loss: 3.4352\n",
            "Epoch 1011/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6532 - val_loss: 2.8618\n",
            "Epoch 1012/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6961 - val_loss: 2.3995\n",
            "Epoch 1013/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6584 - val_loss: 2.6745\n",
            "Epoch 1014/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6714 - val_loss: 2.8986\n",
            "Epoch 1015/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6964 - val_loss: 2.5656\n",
            "Epoch 1016/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7107 - val_loss: 3.7443\n",
            "Epoch 1017/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6331 - val_loss: 2.7834\n",
            "Epoch 1018/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7011 - val_loss: 2.9042\n",
            "Epoch 1019/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7148 - val_loss: 2.5976\n",
            "Epoch 1020/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6465 - val_loss: 2.9015\n",
            "Epoch 1021/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6984 - val_loss: 3.3904\n",
            "Epoch 1022/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6650 - val_loss: 2.3882\n",
            "Epoch 1023/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6946 - val_loss: 2.2119\n",
            "Epoch 1024/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6983 - val_loss: 2.6783\n",
            "Epoch 1025/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6997 - val_loss: 2.4915\n",
            "Epoch 1026/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6435 - val_loss: 2.7543\n",
            "Epoch 1027/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6812 - val_loss: 2.5449\n",
            "Epoch 1028/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6993 - val_loss: 2.2281\n",
            "Epoch 1029/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6998 - val_loss: 2.8205\n",
            "Epoch 1030/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6749 - val_loss: 2.4463\n",
            "Epoch 1031/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6553 - val_loss: 2.7915\n",
            "Epoch 1032/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6317 - val_loss: 2.7082\n",
            "Epoch 1033/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6738 - val_loss: 2.7457\n",
            "Epoch 1034/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6704 - val_loss: 2.8072\n",
            "Epoch 1035/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7278 - val_loss: 2.7827\n",
            "Epoch 1036/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6716 - val_loss: 2.8093\n",
            "Epoch 1037/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7069 - val_loss: 3.4840\n",
            "Epoch 1038/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6748 - val_loss: 3.3530\n",
            "Epoch 1039/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7031 - val_loss: 2.3631\n",
            "Epoch 1040/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7130 - val_loss: 3.0063\n",
            "Epoch 1041/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7392 - val_loss: 2.5986\n",
            "Epoch 1042/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6814 - val_loss: 3.1251\n",
            "Epoch 1043/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6572 - val_loss: 2.3906\n",
            "Epoch 1044/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6926 - val_loss: 2.7967\n",
            "Epoch 1045/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7860 - val_loss: 2.7190\n",
            "Epoch 1046/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6422 - val_loss: 2.3909\n",
            "Epoch 1047/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6660 - val_loss: 3.0845\n",
            "Epoch 1048/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6995 - val_loss: 2.6207\n",
            "Epoch 1049/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7561 - val_loss: 2.6005\n",
            "Epoch 1050/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7294 - val_loss: 2.6734\n",
            "Epoch 1051/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6351 - val_loss: 2.7004\n",
            "Epoch 1052/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6715 - val_loss: 2.7355\n",
            "Epoch 1053/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6764 - val_loss: 2.5001\n",
            "Epoch 1054/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6951 - val_loss: 2.6129\n",
            "Epoch 1055/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7138 - val_loss: 2.5059\n",
            "Epoch 1056/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7045 - val_loss: 2.9522\n",
            "Epoch 1057/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6829 - val_loss: 2.5929\n",
            "Epoch 1058/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6990 - val_loss: 2.3654\n",
            "Epoch 1059/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6353 - val_loss: 3.3556\n",
            "Epoch 1060/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8517 - val_loss: 3.0444\n",
            "Epoch 1061/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6676 - val_loss: 2.3715\n",
            "Epoch 1062/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6556 - val_loss: 2.6860\n",
            "Epoch 1063/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6962 - val_loss: 2.7285\n",
            "Epoch 1064/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7524 - val_loss: 2.9421\n",
            "Epoch 1065/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6322 - val_loss: 3.4035\n",
            "Epoch 1066/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6661 - val_loss: 2.9980\n",
            "Epoch 1067/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6944 - val_loss: 2.6898\n",
            "Epoch 1068/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6643 - val_loss: 2.8701\n",
            "Epoch 1069/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6684 - val_loss: 2.6416\n",
            "Epoch 1070/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6265 - val_loss: 2.5272\n",
            "Epoch 1071/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6629 - val_loss: 2.6335\n",
            "Epoch 1072/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6772 - val_loss: 2.6179\n",
            "Epoch 1073/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6502 - val_loss: 2.6213\n",
            "Epoch 1074/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6848 - val_loss: 2.7454\n",
            "Epoch 1075/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7295 - val_loss: 2.8525\n",
            "Epoch 1076/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6377 - val_loss: 3.0781\n",
            "Epoch 1077/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6511 - val_loss: 2.4585\n",
            "Epoch 1078/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7888 - val_loss: 2.4482\n",
            "Epoch 1079/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6512 - val_loss: 2.7992\n",
            "Epoch 1080/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7332 - val_loss: 2.6934\n",
            "Epoch 1081/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6964 - val_loss: 2.7565\n",
            "Epoch 1082/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6879 - val_loss: 2.8572\n",
            "Epoch 1083/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7460 - val_loss: 2.8898\n",
            "Epoch 1084/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6852 - val_loss: 2.6894\n",
            "Epoch 1085/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6936 - val_loss: 2.8345\n",
            "Epoch 1086/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6192 - val_loss: 2.8418\n",
            "Epoch 1087/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6730 - val_loss: 2.9618\n",
            "Epoch 1088/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6951 - val_loss: 2.3079\n",
            "Epoch 1089/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6503 - val_loss: 2.8506\n",
            "Epoch 1090/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7213 - val_loss: 2.8109\n",
            "Epoch 1091/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6983 - val_loss: 3.5850\n",
            "Epoch 1092/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6680 - val_loss: 2.8066\n",
            "Epoch 1093/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8112 - val_loss: 2.5676\n",
            "Epoch 1094/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6996 - val_loss: 2.4534\n",
            "Epoch 1095/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6760 - val_loss: 2.6467\n",
            "Epoch 1096/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7529 - val_loss: 2.6370\n",
            "Epoch 1097/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6207 - val_loss: 2.7473\n",
            "Epoch 1098/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6622 - val_loss: 2.7465\n",
            "Epoch 1099/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6359 - val_loss: 2.6934\n",
            "Epoch 1100/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6468 - val_loss: 3.2135\n",
            "Epoch 1101/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6471 - val_loss: 2.5940\n",
            "Epoch 1102/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6808 - val_loss: 2.4109\n",
            "Epoch 1103/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6517 - val_loss: 2.9359\n",
            "Epoch 1104/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6554 - val_loss: 2.4705\n",
            "Epoch 1105/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6904 - val_loss: 3.0309\n",
            "Epoch 1106/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7471 - val_loss: 2.8363\n",
            "Epoch 1107/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6985 - val_loss: 2.8224\n",
            "Epoch 1108/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6348 - val_loss: 2.5418\n",
            "Epoch 1109/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6563 - val_loss: 2.6613\n",
            "Epoch 1110/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6541 - val_loss: 3.0937\n",
            "Epoch 1111/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7185 - val_loss: 2.7227\n",
            "Epoch 1112/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6393 - val_loss: 2.5584\n",
            "Epoch 1113/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6672 - val_loss: 2.6516\n",
            "Epoch 1114/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7051 - val_loss: 2.9473\n",
            "Epoch 1115/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6960 - val_loss: 3.3889\n",
            "Epoch 1116/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6824 - val_loss: 2.6460\n",
            "Epoch 1117/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6230 - val_loss: 3.4191\n",
            "Epoch 1118/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6775 - val_loss: 3.3860\n",
            "Epoch 1119/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6617 - val_loss: 3.1458\n",
            "Epoch 1120/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6747 - val_loss: 2.7126\n",
            "Epoch 1121/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6597 - val_loss: 3.2423\n",
            "Epoch 1122/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6634 - val_loss: 2.9590\n",
            "Epoch 1123/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6975 - val_loss: 3.1367\n",
            "Epoch 1124/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7174 - val_loss: 3.0085\n",
            "Epoch 1125/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6807 - val_loss: 2.7252\n",
            "Epoch 1126/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6596 - val_loss: 3.2375\n",
            "Epoch 1127/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7031 - val_loss: 2.5010\n",
            "Epoch 1128/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6799 - val_loss: 2.7463\n",
            "Epoch 1129/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6794 - val_loss: 2.9788\n",
            "Epoch 1130/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6545 - val_loss: 3.0883\n",
            "Epoch 1131/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6957 - val_loss: 2.6237\n",
            "Epoch 1132/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6550 - val_loss: 3.1541\n",
            "Epoch 1133/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6980 - val_loss: 3.2057\n",
            "Epoch 1134/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6553 - val_loss: 3.0315\n",
            "Epoch 1135/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6660 - val_loss: 3.1192\n",
            "Epoch 1136/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6671 - val_loss: 2.9489\n",
            "Epoch 1137/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6584 - val_loss: 2.6938\n",
            "Epoch 1138/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6603 - val_loss: 2.9978\n",
            "Epoch 1139/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6500 - val_loss: 2.9106\n",
            "Epoch 1140/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6525 - val_loss: 2.7778\n",
            "Epoch 1141/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6997 - val_loss: 2.8961\n",
            "Epoch 1142/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6652 - val_loss: 3.4963\n",
            "Epoch 1143/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7166 - val_loss: 3.3580\n",
            "Epoch 1144/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6672 - val_loss: 3.1550\n",
            "Epoch 1145/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7546 - val_loss: 3.0742\n",
            "Epoch 1146/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7705 - val_loss: 2.9039\n",
            "Epoch 1147/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6745 - val_loss: 3.3454\n",
            "Epoch 1148/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6704 - val_loss: 2.9840\n",
            "Epoch 1149/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6873 - val_loss: 3.0071\n",
            "Epoch 1150/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6421 - val_loss: 3.1072\n",
            "Epoch 1151/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7075 - val_loss: 2.4773\n",
            "Epoch 1152/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6587 - val_loss: 2.9181\n",
            "Epoch 1153/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7096 - val_loss: 2.9683\n",
            "Epoch 1154/5000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.5991Restoring model weights from the end of the best epoch: 154.\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6568 - val_loss: 2.5531\n",
            "Epoch 1154: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-929b9a8d7ad2>:130: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 1.2817505598068237\n",
            "Train loss: 0.7158575654029846\n",
            "Test loss: 1.271590232849121\n",
            "dO18 RMSE: 1.0329887384253253\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       23.405260           0.148275\n",
            "1       25.725845           0.213718\n",
            "2       25.936000           0.066530\n",
            "3       23.436000           0.134780\n",
            "4       25.414000           0.025930\n",
            "5       25.168000           0.106320\n",
            "6       24.582000           4.062670\n",
            "7       25.108000           0.170620\n",
            "8       23.398000           0.119170\n",
            "9       25.006000           0.272130\n",
            "10      23.768000           0.048420\n",
            "11      23.800000           0.277350\n",
            "12      23.742000           0.118870\n",
            "13      24.038000           0.263970\n",
            "14      24.608000           1.650470\n",
            "15      25.696000           0.791580\n",
            "16      25.524000           0.219580\n",
            "17      25.794000           0.224030\n",
            "18      23.770000           1.004550\n",
            "19      25.912000           0.478970\n",
            "20      25.878000           0.349920\n",
            "21      26.060000           2.584750\n",
            "22      23.328000           0.662170\n",
            "23      22.568000          48.102770\n",
            "24      25.196000           0.410630\n",
            "25      24.656000           0.177280\n",
            "26      26.356000           0.039530\n",
            "27      23.962000           0.309920\n",
            "28      24.848000           0.158420\n",
            "29      25.080000           0.051250\n",
            "30      26.546000           2.143780\n",
            "31      25.682000           0.944720\n",
            "32      24.028000           0.458520\n",
            "33      23.752000           0.524370\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       25.341579           4.112389\n",
            "1       25.227219           3.790609\n",
            "2       25.341574           4.112326\n",
            "3       25.255587           3.451169\n",
            "4       23.527769           4.266356\n",
            "5       25.341574           4.112326\n",
            "6       23.527769           4.266356\n",
            "7       25.341574           4.112326\n",
            "8       23.494198           4.278221\n",
            "9       25.341574           4.112326\n",
            "10      23.527769           4.266356\n",
            "11      25.255573           3.451202\n",
            "12      25.341574           4.112326\n",
            "13      23.527769           4.266356\n",
            "14      25.341574           4.112326\n",
            "15      25.341574           4.112326\n",
            "16      25.341574           4.112326\n",
            "17      25.255573           3.451202\n",
            "18      25.255587           3.451169\n",
            "19      25.255587           3.451169\n",
            "20      25.255587           3.451169\n",
            "21      25.255587           3.451169\n",
            "22      23.527769           4.266356\n",
            "23      23.527769           4.266356\n",
            "24      25.255587           3.451169\n",
            "25      25.341579           4.112389\n",
            "26      25.227219           3.790609\n",
            "27      25.255573           3.451202\n",
            "28      23.494205           4.278162\n",
            "29      23.494205           4.278162\n",
            "30      25.255587           3.451169\n",
            "31      25.341574           4.112326\n",
            "32      23.494198           4.278221\n",
            "33      25.566378           3.538143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/MyDrive/amazon_rainforest_files/variational/model/demo_isoscape_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "vi_model = train_and_evaluate(data, MODEL_NAME, training_batch_size=3)\n",
        "\n",
        "vi_model.save(get_model_save_location(\n",
        "    f\"{MODEL_NAME}.tf\"), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location(\n",
        "    f\"{MODEL_NAME}.pkl\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NIxjItEs94J"
      },
      "source": [
        "## Generate isoscapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PuGJblTjiDdz",
        "outputId": "814c45ba-42f5-43e0-fa49-4504b68d6cb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: GTiff/GeoTIFF\n",
            "Size is 941 x 937 x 12\n",
            "Projection is GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n",
            "Origin = (-74.0000000000241, 5.29166666665704)\n",
            "Pixel Size = (0.04166666666665718, -0.04166666666667143)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 941 x 937 x 12\n",
            "Projection is GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n",
            "Origin = (-74.0000000000241, 5.29166666665704)\n",
            "Pixel Size = (0.04166666666665718, -0.04166666666667143)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 942 x 936 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-74.0, 5.25)\n",
            "Pixel Size = (0.041666666666666664, -0.041666666666666664)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 5418 x 4683 x 2\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.991666667, 5.275)\n",
            "Pixel Size = (0.008333333333333335, -0.008333333333333333)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 5418 x 4683 x 2\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.991666667, 5.275)\n",
            "Pixel Size = (0.008333333333333335, -0.008333333333333333)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 941 x 937 x 12\n",
            "Projection is GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n",
            "Origin = (-74.0000000000241, 5.29166666665704)\n",
            "Pixel Size = (0.04166666666665718, -0.04166666666667143)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 234 x 1\n",
            "Projection is \n",
            "Origin = (-74.0, 5.333333333999995)\n",
            "Pixel Size = (0.16666666666808508, -0.16666666666808505)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 940 x 936 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.975139313, 5.266527396)\n",
            "Pixel Size = (0.0416666665, -0.041666666500000005)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 234 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-74.0, 5.333333334)\n",
            "Pixel Size = (0.1666666666680851, -0.16666666666666666)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 541 x 467 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.922043, 5.233124)\n",
            "Pixel Size = (0.08333, -0.08333)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 218 x 2\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-74.0, 4.500000000659528)\n",
            "Pixel Size = (0.166666666667993, -0.16666666666799657)\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 218 x 2\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-74.0, 4.500000000659528)\n",
            "Pixel Size = (0.166666666667993, -0.16666666666799657)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1001/1001 [02:31<00:00,  6.60it/s]\n"
          ]
        }
      ],
      "source": [
        "# @title Run this cell to generate an isoscape\n",
        "\n",
        "vi_model = model.TFModel(\n",
        "    get_model_save_location(f\"{MODEL_NAME}.tf\"),\n",
        "    get_model_save_location(f\"{MODEL_NAME}.pkl\"))\n",
        "\n",
        "required_geotiffs = [\n",
        "    'VPD',\n",
        "    'RH',\n",
        "    'PET',\n",
        "    'DEM',\n",
        "    'PA',\n",
        "    'Mean Annual Temperature',\n",
        "    'Mean Annual Precipitation',\n",
        "    'Iso_Oxi_Stack_mean_TERZER',\n",
        "    'isoscape_fullmodel_d18O_prec_REGRESSION',\n",
        "    \"brisoscape_mean_ISORIX\",\n",
        "    \"d13C_cel_mean\",\n",
        "    \"d13C_cel_var\",\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance',]\n",
        "\n",
        "raster.generate_isoscapes_from_variational_model(\n",
        "    model=vi_model, required_geotiffs=required_geotiffs, output_geotiff=MODEL_NAME, res_x=1000, res_y=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd5hotPTs94K"
      },
      "source": [
        "## Render the isoscapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEsO7--Bs94K"
      },
      "outputs": [],
      "source": [
        "from matplotlib import rc\n",
        "rc('animation', html='jshtml')\n",
        "\n",
        "means_anim = raster.animate(raster.load_raster(raster.get_raster_path(MODEL_NAME+\".tiff\"), use_only_band_index=0), 1, 1)\n",
        "means_anim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7ie-IKDs94K"
      },
      "outputs": [],
      "source": [
        "vars_anim = raster.animate(raster.load_raster(raster.get_raster_path(MODEL_NAME+\".tiff\"), use_only_band_index=1), 1, 1)\n",
        "vars_anim"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}