{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnc-br/ddf-isoscapes/blob/new_kl/dnn/briso_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational model\n",
        "\n",
        "Find the mean/variance of O18 ratios (as well as N15 and C13 in the future) at a particular lat/lon across Brazil. At the bottom of the colab, train and evaluate 4 different versions of the model with different data partitioning strategies."
      ],
      "metadata": {
        "id": "-0IfT3kGwgK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "henIPlAPCb4i",
        "outputId": "d7d20faf-3c73-427d-9bdb-bca6e12e2e12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import os\n",
        "from typing import List, Tuple, Dict\n",
        "from dataclasses import dataclass\n",
        "from joblib import dump\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.python.ops import math_ops\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "#@title Debugging\n",
        "# See https://zohaib.me/debugging-in-google-collab-notebook/ for tips,\n",
        "# as well as docs for pdb and ipdb.\n",
        "DEBUG = False #@param {type:\"boolean\"}\n",
        "USE_LOCAL_DRIVE = False #@param {type:\"boolean\"}\n",
        "LOCAL_DIR = \"/usr/local/google/home/ruru/Downloads/amazon_sample_data-20230712T203059Z-001\" #@param\n",
        "GDRIVE_DIR = \"MyDrive/amazon_rainforest_files/\" #@param\n",
        "FP_ROOT = LOCAL_DIR\n",
        "\n",
        "MODEL_SAVE_LOCATION = \"MyDrive/amazon_rainforest_files/variational/model\" #@param\n",
        "\n",
        "def get_model_save_location(filename) -> str:\n",
        "  root = '' if USE_LOCAL_DRIVE else '/content/gdrive'\n",
        "  return os.path.join(root, MODEL_SAVE_LOCATION, filename)\n",
        "\n",
        "# Access data stored on Google Drive if not reading data locally.\n",
        "if not USE_LOCAL_DRIVE:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  global FP_ROOT\n",
        "  FP_ROOT = os.path.join('/content/gdrive', GDRIVE_DIR)\n",
        "\n",
        "if DEBUG:\n",
        "    %pip install -Uqq ipdb\n",
        "    import ipdb\n",
        "    %pdb on"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "!if [ ! -d \"/content/ddf_common_stub\" ] ; then git clone -b test https://github.com/tnc-br/ddf_common_stub.git; fi\n",
        "sys.path.append(\"/content/ddf_common_stub/\")\n",
        "import ddfimport\n",
        "ddfimport.ddf_import_common()\n",
        "\n"
      ],
      "metadata": {
        "id": "AXh86HFwXiax",
        "outputId": "3829fc7e-8c97-48d7-f6e9-c752d3a64959",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ddf_common_stub'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 11 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (11/11), 5.50 KiB | 5.50 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n",
            "executing checkout_branch ...\n",
            "b''\n",
            "main branch checked out as readonly. You may now use ddf_common imports\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import raster"
      ],
      "metadata": {
        "id": "0mUB0y0AXivp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQrEv57zCb4q"
      },
      "source": [
        "# Data preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path: str, columns_to_keep: List[str], side_raster_input):\n",
        "  df = pd.read_csv(path, encoding=\"ISO-8859-1\", sep=',')\n",
        "  df = df[df['d18O_cel_variance'].notna()]\n",
        "  X = df\n",
        "  X = X.drop(df.columns.difference(columns_to_keep), axis=1)\n",
        "\n",
        "  for name, geotiff in side_raster_input.items():\n",
        "    X[name] = X.apply(lambda row: geotiff.value_at(row['long'], row['lat']), axis=1)\n",
        "    # The last two kriging columns need to remain last. Move new columns forward.\n",
        "    X.insert(len(X.columns)-3, name, X.pop(name))\n",
        "\n",
        "  Y = df[[\"d18O_cel_mean\", \"d18O_cel_variance\"]]\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "6XMee1aHfcik"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardization"
      ],
      "metadata": {
        "id": "DtkKhMOtb6cS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class FeaturesToLabels:\n",
        "  def __init__(self, X: pd.DataFrame, Y: pd.DataFrame):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "\n",
        "  def as_tuple(self):\n",
        "    return (self.X, self.Y)\n",
        "\n",
        "\n",
        "def create_feature_scaler(X: pd.DataFrame,\n",
        "                          columns_to_passthrough,\n",
        "                          columns_to_scale,\n",
        "                          columns_to_standardize) -> ColumnTransformer:\n",
        "  columns_to_standardize = columns_to_standardize\n",
        "  feature_scaler = ColumnTransformer(\n",
        "      [(column+'_normalizer', MinMaxScaler(), [column]) for column in columns_to_scale] +\n",
        "      [(column+'_standardizer', StandardScaler(), [column]) for column in columns_to_standardize],\n",
        "      remainder='passthrough')\n",
        "  feature_scaler.fit(X)\n",
        "  print(feature_scaler)\n",
        "  return feature_scaler\n",
        "\n",
        "def create_label_scaler(Y: pd.DataFrame) -> ColumnTransformer:\n",
        "  label_scaler = ColumnTransformer([\n",
        "      ('mean_std_scaler', StandardScaler(), ['d18O_cel_mean']),\n",
        "      ('var_minmax_scaler', MinMaxScaler(), ['d18O_cel_variance'])],\n",
        "      remainder='passthrough')\n",
        "  label_scaler.fit(Y)\n",
        "  return label_scaler\n",
        "\n",
        "def scale(X: pd.DataFrame, feature_scaler):\n",
        "  # transform() outputs numpy arrays :(  need to convert back to DataFrame.\n",
        "  X_standardized = pd.DataFrame(feature_scaler.transform(X),\n",
        "                        index=X.index, columns=X.columns)\n",
        "  return X_standardized"
      ],
      "metadata": {
        "id": "XSDwdvMkb7w8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just a class organization, holds each scaled dataset and the scaler used.\n",
        "# Useful for unscaling predictions.\n",
        "@dataclass\n",
        "class ScaledPartitions():\n",
        "  def __init__(self,\n",
        "               feature_scaler: ColumnTransformer,\n",
        "               label_scaler: ColumnTransformer,\n",
        "               train: FeaturesToLabels, val: FeaturesToLabels,\n",
        "               test: FeaturesToLabels):\n",
        "    self.feature_scaler = feature_scaler\n",
        "    self.label_scaler = label_scaler\n",
        "    self.train = train\n",
        "    self.val = val\n",
        "    self.test = test\n",
        "\n",
        "\n",
        "def load_and_scale(config: Dict,\n",
        "                   columns_to_passthrough: List[str],\n",
        "                   columns_to_scale: List[str],\n",
        "                   columns_to_standardize: List[str]) -> ScaledPartitions:\n",
        "\n",
        "  geotiff_side_input = {\n",
        "      \"brisoscape_mean_ISORIX\" : raster.load_raster(raster.get_raster_path(\"brisoscape_mean_ISORIX.tif\")),\n",
        "      # \"d13C_cel_mean\" : raster.load_raster(raster.get_raster_path(\"d13C_cel_Brazil_stack_terra_null.tiff\"), use_only_band_index=0),\n",
        "      # \"d13C_cel_var\" : raster.load_raster(raster.get_raster_path(\"d13C_cel_Brazil_stack_terra_null.tiff\"), use_only_band_index=1)\n",
        "  }\n",
        "  columns_to_keep = columns_to_passthrough + columns_to_scale + columns_to_standardize\n",
        "  X_train, Y_train = load_dataset(config['TRAIN'], columns_to_keep, geotiff_side_input)\n",
        "  X_val, Y_val = load_dataset(config['VALIDATION'], columns_to_keep, geotiff_side_input)\n",
        "  X_test, Y_test = load_dataset(config['TEST'], columns_to_keep, geotiff_side_input)\n",
        "\n",
        "  # Fit the scaler:\n",
        "  feature_scaler = create_feature_scaler(\n",
        "      X_train,\n",
        "      columns_to_passthrough,\n",
        "      columns_to_scale,\n",
        "      columns_to_standardize)\n",
        "\n",
        "  # Apply the scaler:\n",
        "  label_scaler = create_label_scaler(Y_train)\n",
        "  train = FeaturesToLabels(scale(X_train, feature_scaler), Y_train)\n",
        "  val = FeaturesToLabels(scale(X_val, feature_scaler), Y_val)\n",
        "  test = FeaturesToLabels(scale(X_test, feature_scaler), Y_test)\n",
        "  return ScaledPartitions(feature_scaler, label_scaler, train, val, test)\n"
      ],
      "metadata": {
        "id": "_kf2e_fKon2P"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition\n",
        "\n"
      ],
      "metadata": {
        "id": "usGznR593LZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KL Loss function:"
      ],
      "metadata": {
        "id": "khK7C8WvU8ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_normal_distribution(\n",
        "    mean: tf.Tensor,\n",
        "    stdev: tf.Tensor,\n",
        "    n: int) -> tf.Tensor:\n",
        "    '''\n",
        "    Given a batch of normal distributions described by a mean and stdev in\n",
        "    a tf.Tensor, sample n elements from each distribution and return the mean\n",
        "    and standard deviation per sample.\n",
        "    '''\n",
        "    batch_size = tf.shape(mean)[0]\n",
        "\n",
        "    # Output tensor is (n, batch_size, 1)\n",
        "    sample_values = tfp.distributions.Normal(\n",
        "        loc=mean,\n",
        "        scale=stdev).sample(\n",
        "            sample_shape=n)\n",
        "    # Reshaped tensor will be (batch_size, n)\n",
        "    sample_values = tf.transpose(sample_values)\n",
        "    # Get the mean per sample in the batch.\n",
        "    sample_mean = tf.transpose(tf.math.reduce_mean(sample_values, 2))\n",
        "    sample_stdev = tf.transpose(tf.math.reduce_std(sample_values, 2))\n",
        "\n",
        "    return sample_mean, sample_stdev\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# log(σ2/σ1) + ( σ1^2+(μ1−μ2)^2 ) / 2* σ^2   − 1/2\n",
        "def kl_divergence_helper(real, predicted, sample):\n",
        "    '''\n",
        "    real: tf.Tensor of the real mean and standard deviation of sample to compare\n",
        "    predicted: tf.Tensor of the predicted mean and standard deviation to compare\n",
        "    sample: Whether or not to sample the predicted distribution to get a new\n",
        "            mean and standard deviation.\n",
        "    '''\n",
        "    if real.shape != predicted.shape:\n",
        "      raise ValueError(\n",
        "          f\"real.shape {real.shape} != predicted.shape {predicted.shape}\")\n",
        "\n",
        "    real_value = tf.gather(real, [0], axis=1)\n",
        "    real_std = tf.math.sqrt(tf.gather(real, [1], axis=1))\n",
        "\n",
        "\n",
        "    predicted_value = tf.gather(predicted, [0], axis=1)\n",
        "    predicted_std = tf.math.sqrt(tf.gather(predicted, [1], axis=1))\n",
        "    # If true, sample from the distribution defined by the predicted mean and\n",
        "    # standard deviation to use for mean and stdev used in KL divergence loss.\n",
        "    if sample:\n",
        "      predicted_value, predicted_std = sample_normal_distribution(\n",
        "          mean=predicted_value, stdev=predicted_std, n=15)\n",
        "\n",
        "    kl_loss = -0.5 + tf.math.log(predicted_std/real_std) + \\\n",
        "     (tf.square(real_std) + tf.square(real_value - predicted_value))/ \\\n",
        "     (2*tf.square(predicted_std))\n",
        "\n",
        "    if tf.math.is_nan(tf.math.reduce_mean(kl_loss)):\n",
        "       tf.print(predicted)\n",
        "       sess = tf.compat.v1.Session()\n",
        "       sess.close()\n",
        "\n",
        "    return tf.math.reduce_mean(kl_loss)\n",
        "\n",
        "def kl_divergence(real, predicted):\n",
        "  return kl_divergence_helper(real, predicted, True)"
      ],
      "metadata": {
        "id": "urGjYNNnemX6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the loss function:"
      ],
      "metadata": {
        "id": "fJzBFWQVeqNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model definition"
      ],
      "metadata": {
        "id": "8rI6qPRh7oO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "def get_early_stopping_callback():\n",
        "  return EarlyStopping(monitor='val_loss', patience=1000, min_delta=0.001,\n",
        "                       verbose=1, restore_best_weights=True, start_from_epoch=0)\n",
        "\n",
        "tf.keras.utils.set_random_seed(18731)\n",
        "\n",
        "# I was experimenting with models that took longer to train, and used this\n",
        "# checkpointing callback to periodically save the model. It's optional.\n",
        "def get_checkpoint_callback(model_file):\n",
        "  return ModelCheckpoint(\n",
        "      get_model_save_location(model_file),\n",
        "      monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
        "\n",
        "def train_or_update_variational_model(\n",
        "        sp: ScaledPartitions,\n",
        "        hidden_layers: List[int],\n",
        "        epochs: int,\n",
        "        batch_size: int,\n",
        "        lr: float,\n",
        "        model_file=None,\n",
        "        use_checkpoint=False):\n",
        "  callbacks_list = [get_early_stopping_callback(),\n",
        "                    get_checkpoint_callback(model_file)]\n",
        "  if not use_checkpoint:\n",
        "    inputs = keras.Input(shape=(sp.train.X.shape[1],))\n",
        "    x = inputs\n",
        "    for layer_size in hidden_layers:\n",
        "      x = keras.layers.Dense(\n",
        "          layer_size, activation='relu')(x)\n",
        "    mean_output = keras.layers.Dense(\n",
        "        1, name='mean_output')(x)\n",
        "\n",
        "    # We can not have negative variance. Apply very little variance.\n",
        "    var_output = keras.layers.Dense(\n",
        "        1, name='var_output')(x)\n",
        "\n",
        "    # Invert the normalization on our outputs\n",
        "    mean_scaler = sp.label_scaler.named_transformers_['mean_std_scaler']\n",
        "    untransformed_mean = mean_output * mean_scaler.var_ + mean_scaler.mean_\n",
        "\n",
        "    var_scaler = sp.label_scaler.named_transformers_['var_minmax_scaler']\n",
        "    unscaled_var = var_output * var_scaler.scale_ + var_scaler.min_\n",
        "    untransformed_var = keras.layers.Lambda(lambda t: tf.math.log(1 + tf.exp(t)))(unscaled_var)\n",
        "\n",
        "    # Output mean,  tuples.\n",
        "    outputs = keras.layers.concatenate([untransformed_mean, untransformed_var])\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss=kl_divergence)\n",
        "    model.summary()\n",
        "  else:\n",
        "    model = keras.models.load_model(\n",
        "        get_model_save_location(model_file),\n",
        "        custom_objects={\"kl_divergence\": kl_divergence})\n",
        "  history = model.fit(sp.train.X, sp.train.Y, verbose=1, validation_data=sp.val.as_tuple(), shuffle=True,\n",
        "                      epochs=epochs, batch_size=batch_size, callbacks=callbacks_list)\n",
        "  return history, model"
      ],
      "metadata": {
        "id": "HCkGSPUo3KqY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def render_plot_loss(history, name):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title(name + ' model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.yscale(\"log\")\n",
        "  plt.ylim((0, 10))\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['loss', 'val_loss'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def destandardize(sd: ScaledPartitions, df: pd.DataFrame):\n",
        "  means = pd.DataFrame(\n",
        "      sd.label_scaler.named_transformers_['var_std_scaler'].inverse_transform(df[['d18O_cel_mean']]),\n",
        "      index=df.index, columns=['d18O_cel_mean'])\n",
        "  vars = df['d18O_cel_variance']\n",
        "  return means.join(vars)\n",
        "\n",
        "def train_and_evaluate(sp: ScaledPartitions, run_id: str, training_batch_size=5):\n",
        "  print(\"==================\")\n",
        "  print(run_id)\n",
        "  history, model = train_or_update_variational_model(\n",
        "      sp, hidden_layers=[20, 20], epochs=5000, batch_size=training_batch_size,\n",
        "      lr=0.0001, model_file=run_id+\".h5\", use_checkpoint=False)\n",
        "  render_plot_loss(history, run_id+\" kl_loss\")\n",
        "  model.save(get_model_save_location(run_id+\".h5\"), save_format=\"h5\")\n",
        "\n",
        "  best_epoch_index = history.history['val_loss'].index(min(history.history['val_loss']))\n",
        "  print('Val loss:', history.history['val_loss'][best_epoch_index])\n",
        "  print('Train loss:', history.history['loss'][best_epoch_index])\n",
        "  print('Test loss:', model.evaluate(x=sp.test.X, y=sp.test.Y, verbose=0))\n",
        "\n",
        "  predictions = model.predict_on_batch(sp.test.X)\n",
        "  predictions = pd.DataFrame(predictions, columns=['d18O_cel_mean', 'd18O_cel_variance'])\n",
        "  rmse = np.sqrt(mean_squared_error(sp.test.Y['d18O_cel_mean'], predictions['d18O_cel_mean']))\n",
        "  print(\"dO18 RMSE: \"+ str(rmse))\n",
        "  print(\"EXPECTED:\")\n",
        "  print(sp.test.Y.to_string())\n",
        "  print()\n",
        "  print(\"PREDICTED:\")\n",
        "  print(predictions.to_string())\n",
        "  return model"
      ],
      "metadata": {
        "id": "DALuUm8UOgNu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Grouped, random (jupyter crashed halfway so I reloaded a checkpoint)\n",
        "\n",
        "We can't (easily) generate isoscapes with these because the isoscapes for 'predkrig_br_lat_ISORG', 'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION' are not easily retrievable... but I'm curious how much better the model is if these columns are included."
      ],
      "metadata": {
        "id": "n8pNR1CrvF2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_random_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_train_random_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_test_random_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_validation_random_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_passthrough = [\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "columns_to_scale = []\n",
        "columns_to_standardize = [\n",
        "    'lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature', 'Mean Annual Precipitation',\n",
        "    'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION']\n",
        "\n",
        "data = load_and_scale(grouped_random_fileset, columns_to_passthrough, columns_to_scale, columns_to_standardize)\n",
        "model = train_and_evaluate(data, \"random_all_0809_ensemble\", training_batch_size=3)\n",
        "model.save(get_model_save_location(\"random_all_0809_ensemble.tf\"), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location('random_all_0809_ensemble.pkl'))\n"
      ],
      "metadata": {
        "id": "rPONfgkjvJWz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82cc0839-f938-462f-cc2a-d4c48b0b66e4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Driver: GTiff/GeoTIFF\n",
            "Size is 541 x 467 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.922043, 5.233124)\n",
            "Pixel Size = (0.08333, -0.08333)\n",
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('lat_standardizer', StandardScaler(), ['lat']),\n",
            "                                ('long_standardizer', StandardScaler(),\n",
            "                                 ['long']),\n",
            "                                ('VPD_standardizer', StandardScaler(), ['VPD']),\n",
            "                                ('RH_standardizer', StandardScaler(), ['RH']),\n",
            "                                ('PET_standardizer', StandardScaler(), ['PET']),\n",
            "                                ('DEM_standardizer', StandardScaler(), ['DEM']),\n",
            "                                ('PA_standardizer'...\n",
            "                                ('Mean Annual Temperature_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation']),\n",
            "                                ('Iso_Oxi_Stack_mean_TERZER_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Iso_Oxi_Stack_mean_TERZER']),\n",
            "                                ('isoscape_fullmodel_d18O_prec_REGRESSION_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['isoscape_fullmodel_d18O_prec_REGRESSION'])])\n",
            "==================\n",
            "random_all_boosted_residuals_0809_ensemble\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 14)]         0           []                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 12)          0           ['input_1[0][0]']                \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 20)           260         ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None,)             0           ['input_1[0][0]']                \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 20)           420         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2 (Sl  (None,)             0           ['input_1[0][0]']                \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " tf.expand_dims (TFOpLambda)    (None, 1)            0           ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " pred_mean_residual (Dense)     (None, 1)            21          ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " tf.expand_dims_1 (TFOpLambda)  (None, 1)            0           ['tf.__operators__.getitem_2[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " pred_var_output (Dense)        (None, 1)            21          ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 1)           0           ['tf.expand_dims[0][0]',         \n",
            " da)                                                              'pred_mean_residual[0][0]']     \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1)            0           ['tf.expand_dims_1[0][0]',       \n",
            "                                                                  'pred_var_output[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 2)            0           ['tf.__operators__.add[0][0]',   \n",
            "                                                                  'lambda[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 722\n",
            "Trainable params: 722\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-256f3fba58ab>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrouped_random_fileset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_passthrough\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_standardize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"random_all_boosted_residuals_0809_ensemble\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_model_save_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"random_all_boosted_residuals_0809_ensemble.keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_scaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_model_save_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'random_all_boosted_residuals_0809_ensemble.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-b383c653ffcb>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(sp, run_id, training_batch_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   history, model = train_or_update_variational_model(\n\u001b[0m\u001b[1;32m     25\u001b[0m       \u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       lr=0.0001, model_file=run_id+\".h5\", use_checkpoint=False)\n",
            "\u001b[0;32m<ipython-input-13-e326eb4ccdaf>\u001b[0m in \u001b[0;36mtrain_or_update_variational_model\u001b[0;34m(sp, hidden_layers, epochs, batch_size, lr, model_file, use_checkpoint)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mget_model_save_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         custom_objects={\"kl_divergence\": kl_divergence})\n\u001b[0;32m---> 63\u001b[0;31m   history = model.fit(sp.train.X, sp.train.Y, verbose=0, epochs=epochs,\n\u001b[0m\u001b[1;32m     64\u001b[0m                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1727\u001b[0m                             \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m                         )\n\u001b[0;32m-> 1729\u001b[0;31m                     val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1730\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Grouped, random, ablating other models except kriging\n",
        "\n",
        "We can generate isoscapes for this model easily."
      ],
      "metadata": {
        "id": "opmE3xc0vcY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_random_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_train_random_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_test_random_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_validation_random_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_passthrough = [\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "columns_to_scale = []\n",
        "columns_to_standardize = ['lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature', 'Mean Annual Precipitation',]\n",
        "\n",
        "data = load_and_scale(grouped_random_fileset, columns_to_passthrough, columns_to_scale, columns_to_standardize)\n",
        "model = train_and_evaluate(data, \"random_ablated_0809_ensemble.tf\", training_batch_size=3)\n",
        "model.save(get_model_save_location(\"random_ablated_0809_ensemble.tf\"), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location('random_ablated_0809_ensemble.pkl'))"
      ],
      "metadata": {
        "id": "KCebsA7XvfRH",
        "outputId": "b862bfcf-c453-484c-a36b-213b9432b974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: GTiff/GeoTIFF\n",
            "Size is 541 x 467 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.922043, 5.233124)\n",
            "Pixel Size = (0.08333, -0.08333)\n",
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('lat_standardizer', StandardScaler(), ['lat']),\n",
            "                                ('long_standardizer', StandardScaler(),\n",
            "                                 ['long']),\n",
            "                                ('VPD_standardizer', StandardScaler(), ['VPD']),\n",
            "                                ('RH_standardizer', StandardScaler(), ['RH']),\n",
            "                                ('PET_standardizer', StandardScaler(), ['PET']),\n",
            "                                ('DEM_standardizer', StandardScaler(), ['DEM']),\n",
            "                                ('PA_standardizer', StandardScaler(), ['PA']),\n",
            "                                ('Mean Annual Temperature_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation'])])\n",
            "==================\n",
            "random_ablated_0809_ensemble.tf\n",
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_12 (InputLayer)          [(None, 12)]         0           []                               \n",
            "                                                                                                  \n",
            " dense_22 (Dense)               (None, 20)           260         ['input_12[0][0]']               \n",
            "                                                                                                  \n",
            " dense_23 (Dense)               (None, 20)           420         ['dense_22[0][0]']               \n",
            "                                                                                                  \n",
            " var_output (Dense)             (None, 1)            21          ['dense_23[0][0]']               \n",
            "                                                                                                  \n",
            " mean_output (Dense)            (None, 1)            21          ['dense_23[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.multiply_9 (TFOpLambda  (None, 1)           0           ['var_output[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.multiply_8 (TFOpLambda  (None, 1)           0           ['mean_output[0][0]']            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_15 (TFOpL  (None, 1)           0           ['tf.math.multiply_9[0][0]']     \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_14 (TFOpL  (None, 1)           0           ['tf.math.multiply_8[0][0]']     \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1)            0           ['tf.__operators__.add_15[0][0]']\n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenate)   (None, 2)            0           ['tf.__operators__.add_14[0][0]',\n",
            "                                                                  'lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 722\n",
            "Trainable params: 722\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5000\n",
            "33/33 [==============================] - 2s 14ms/step - loss: 4.0471 - val_loss: 2.9210\n",
            "Epoch 2/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.5039 - val_loss: 1.6343\n",
            "Epoch 3/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 2.2861 - val_loss: 1.1421\n",
            "Epoch 4/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 2.4323 - val_loss: 1.0783\n",
            "Epoch 5/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 2.4436 - val_loss: 0.7421\n",
            "Epoch 6/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.0721 - val_loss: 0.9578\n",
            "Epoch 7/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.1205 - val_loss: 0.8860\n",
            "Epoch 8/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.1397 - val_loss: 0.8133\n",
            "Epoch 9/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.9988 - val_loss: 0.8526\n",
            "Epoch 10/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.4083 - val_loss: 0.9020\n",
            "Epoch 11/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.2180 - val_loss: 0.7775\n",
            "Epoch 12/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 2.2156 - val_loss: 1.2474\n",
            "Epoch 13/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.8874 - val_loss: 1.1757\n",
            "Epoch 14/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.8419 - val_loss: 0.7834\n",
            "Epoch 15/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.5418 - val_loss: 0.9097\n",
            "Epoch 16/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.1458 - val_loss: 0.7623\n",
            "Epoch 17/5000\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 1.7943 - val_loss: 0.6591\n",
            "Epoch 18/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.2554 - val_loss: 0.7548\n",
            "Epoch 19/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.7998 - val_loss: 0.8562\n",
            "Epoch 20/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.1505 - val_loss: 1.2900\n",
            "Epoch 21/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.7261 - val_loss: 0.8734\n",
            "Epoch 22/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.0998 - val_loss: 0.9286\n",
            "Epoch 23/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.1167 - val_loss: 0.7808\n",
            "Epoch 24/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.9970 - val_loss: 1.0099\n",
            "Epoch 25/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.3782 - val_loss: 0.8839\n",
            "Epoch 26/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.8761 - val_loss: 1.0086\n",
            "Epoch 27/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.8698 - val_loss: 1.2248\n",
            "Epoch 28/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.0732 - val_loss: 0.9525\n",
            "Epoch 29/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.7795 - val_loss: 1.2342\n",
            "Epoch 30/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.0652 - val_loss: 0.8271\n",
            "Epoch 31/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.8069 - val_loss: 0.8110\n",
            "Epoch 32/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6348 - val_loss: 0.9835\n",
            "Epoch 33/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.4241 - val_loss: 1.1288\n",
            "Epoch 34/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.9888 - val_loss: 0.9989\n",
            "Epoch 35/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.2460 - val_loss: 0.8065\n",
            "Epoch 36/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.9095 - val_loss: 1.0802\n",
            "Epoch 37/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.9959 - val_loss: 0.7809\n",
            "Epoch 38/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.8375 - val_loss: 0.6899\n",
            "Epoch 39/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7196 - val_loss: 1.0193\n",
            "Epoch 40/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.9655 - val_loss: 1.2276\n",
            "Epoch 41/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6359 - val_loss: 1.1625\n",
            "Epoch 42/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.8036 - val_loss: 1.1607\n",
            "Epoch 43/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3674 - val_loss: 1.0494\n",
            "Epoch 44/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.7403 - val_loss: 0.7658\n",
            "Epoch 45/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 2.0112 - val_loss: 0.8944\n",
            "Epoch 46/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.8401 - val_loss: 0.6917\n",
            "Epoch 47/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.9709 - val_loss: 0.8472\n",
            "Epoch 48/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6951 - val_loss: 0.8242\n",
            "Epoch 49/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.7476 - val_loss: 0.6764\n",
            "Epoch 50/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.7965 - val_loss: 0.6906\n",
            "Epoch 51/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.8140 - val_loss: 0.6192\n",
            "Epoch 52/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7161 - val_loss: 0.8327\n",
            "Epoch 53/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7147 - val_loss: 0.7376\n",
            "Epoch 54/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6339 - val_loss: 0.7325\n",
            "Epoch 55/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4596 - val_loss: 0.7323\n",
            "Epoch 56/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4766 - val_loss: 0.9189\n",
            "Epoch 57/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4240 - val_loss: 0.8291\n",
            "Epoch 58/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6822 - val_loss: 0.5522\n",
            "Epoch 59/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.9438 - val_loss: 0.6133\n",
            "Epoch 60/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.7025 - val_loss: 0.5123\n",
            "Epoch 61/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.7394 - val_loss: 0.7026\n",
            "Epoch 62/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.8349 - val_loss: 0.9957\n",
            "Epoch 63/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5394 - val_loss: 0.6839\n",
            "Epoch 64/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5548 - val_loss: 0.9154\n",
            "Epoch 65/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5820 - val_loss: 0.6154\n",
            "Epoch 66/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6584 - val_loss: 0.7306\n",
            "Epoch 67/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5288 - val_loss: 0.8718\n",
            "Epoch 68/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3653 - val_loss: 0.7501\n",
            "Epoch 69/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.4609 - val_loss: 0.9931\n",
            "Epoch 70/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.8160 - val_loss: 0.8017\n",
            "Epoch 71/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7504 - val_loss: 0.6342\n",
            "Epoch 72/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6087 - val_loss: 0.7358\n",
            "Epoch 73/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5512 - val_loss: 1.0126\n",
            "Epoch 74/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.8891 - val_loss: 0.7436\n",
            "Epoch 75/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4928 - val_loss: 0.8122\n",
            "Epoch 76/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5071 - val_loss: 0.8918\n",
            "Epoch 77/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.0670 - val_loss: 0.8591\n",
            "Epoch 78/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5545 - val_loss: 0.5822\n",
            "Epoch 79/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.8042 - val_loss: 1.1168\n",
            "Epoch 80/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.7915 - val_loss: 0.5783\n",
            "Epoch 81/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.7164 - val_loss: 0.6314\n",
            "Epoch 82/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.9809 - val_loss: 0.6512\n",
            "Epoch 83/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6607 - val_loss: 0.8121\n",
            "Epoch 84/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6242 - val_loss: 0.7306\n",
            "Epoch 85/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6834 - val_loss: 0.7953\n",
            "Epoch 86/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5414 - val_loss: 0.7678\n",
            "Epoch 87/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5363 - val_loss: 0.7317\n",
            "Epoch 88/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.8149 - val_loss: 1.0421\n",
            "Epoch 89/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6174 - val_loss: 0.8136\n",
            "Epoch 90/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.9357 - val_loss: 0.7255\n",
            "Epoch 91/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7433 - val_loss: 0.6536\n",
            "Epoch 92/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3985 - val_loss: 0.7004\n",
            "Epoch 93/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5264 - val_loss: 0.7289\n",
            "Epoch 94/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6660 - val_loss: 0.8783\n",
            "Epoch 95/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.4998 - val_loss: 0.6387\n",
            "Epoch 96/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4827 - val_loss: 0.7513\n",
            "Epoch 97/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7813 - val_loss: 0.9209\n",
            "Epoch 98/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.0419 - val_loss: 0.6171\n",
            "Epoch 99/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4166 - val_loss: 0.6641\n",
            "Epoch 100/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.7531 - val_loss: 0.7676\n",
            "Epoch 101/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.7940 - val_loss: 1.0496\n",
            "Epoch 102/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4422 - val_loss: 0.8234\n",
            "Epoch 103/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6074 - val_loss: 0.7077\n",
            "Epoch 104/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6998 - val_loss: 0.6300\n",
            "Epoch 105/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.5273 - val_loss: 0.8844\n",
            "Epoch 106/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6931 - val_loss: 0.8754\n",
            "Epoch 107/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5312 - val_loss: 0.8315\n",
            "Epoch 108/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.5404 - val_loss: 0.6268\n",
            "Epoch 109/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6192 - val_loss: 0.9886\n",
            "Epoch 110/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 2.0685 - val_loss: 0.5909\n",
            "Epoch 111/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2447 - val_loss: 0.7915\n",
            "Epoch 112/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.7897 - val_loss: 0.7667\n",
            "Epoch 113/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6478 - val_loss: 0.7372\n",
            "Epoch 114/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.8092 - val_loss: 0.7734\n",
            "Epoch 115/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5294 - val_loss: 1.0062\n",
            "Epoch 116/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.7852 - val_loss: 0.8051\n",
            "Epoch 117/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4617 - val_loss: 0.7287\n",
            "Epoch 118/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3279 - val_loss: 0.7513\n",
            "Epoch 119/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3478 - val_loss: 0.8612\n",
            "Epoch 120/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.8128 - val_loss: 0.8224\n",
            "Epoch 121/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3550 - val_loss: 0.6501\n",
            "Epoch 122/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.4443 - val_loss: 0.9070\n",
            "Epoch 123/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.5232 - val_loss: 0.5348\n",
            "Epoch 124/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.5075 - val_loss: 0.7470\n",
            "Epoch 125/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.6439 - val_loss: 0.6586\n",
            "Epoch 126/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5986 - val_loss: 0.9194\n",
            "Epoch 127/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.9949 - val_loss: 0.5433\n",
            "Epoch 128/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5195 - val_loss: 0.9005\n",
            "Epoch 129/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.9183 - val_loss: 0.9507\n",
            "Epoch 130/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 2.1808 - val_loss: 0.7209\n",
            "Epoch 131/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5694 - val_loss: 1.2893\n",
            "Epoch 132/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5129 - val_loss: 0.8571\n",
            "Epoch 133/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5221 - val_loss: 0.8376\n",
            "Epoch 134/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4179 - val_loss: 0.7288\n",
            "Epoch 135/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6950 - val_loss: 0.6825\n",
            "Epoch 136/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4688 - val_loss: 0.7622\n",
            "Epoch 137/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3976 - val_loss: 0.7817\n",
            "Epoch 138/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5812 - val_loss: 0.8435\n",
            "Epoch 139/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6233 - val_loss: 0.6175\n",
            "Epoch 140/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3103 - val_loss: 1.0934\n",
            "Epoch 141/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6470 - val_loss: 1.0155\n",
            "Epoch 142/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.8990 - val_loss: 0.8692\n",
            "Epoch 143/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5202 - val_loss: 0.6639\n",
            "Epoch 144/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3844 - val_loss: 0.8769\n",
            "Epoch 145/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4396 - val_loss: 1.0272\n",
            "Epoch 146/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4886 - val_loss: 0.8125\n",
            "Epoch 147/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6685 - val_loss: 0.7447\n",
            "Epoch 148/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4672 - val_loss: 0.9026\n",
            "Epoch 149/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4423 - val_loss: 0.9503\n",
            "Epoch 150/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6105 - val_loss: 0.7122\n",
            "Epoch 151/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.5735 - val_loss: 0.8701\n",
            "Epoch 152/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6203 - val_loss: 0.7898\n",
            "Epoch 153/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5311 - val_loss: 0.8953\n",
            "Epoch 154/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5112 - val_loss: 0.8081\n",
            "Epoch 155/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7052 - val_loss: 1.1005\n",
            "Epoch 156/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5373 - val_loss: 0.8826\n",
            "Epoch 157/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.8839 - val_loss: 1.0312\n",
            "Epoch 158/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4252 - val_loss: 1.2210\n",
            "Epoch 159/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4030 - val_loss: 0.6655\n",
            "Epoch 160/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5960 - val_loss: 1.1064\n",
            "Epoch 161/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3181 - val_loss: 0.7762\n",
            "Epoch 162/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.4316 - val_loss: 0.9342\n",
            "Epoch 163/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3843 - val_loss: 0.9431\n",
            "Epoch 164/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5650 - val_loss: 0.7556\n",
            "Epoch 165/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5799 - val_loss: 1.0139\n",
            "Epoch 166/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3457 - val_loss: 1.0383\n",
            "Epoch 167/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5519 - val_loss: 0.8745\n",
            "Epoch 168/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4386 - val_loss: 0.9504\n",
            "Epoch 169/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5127 - val_loss: 0.6158\n",
            "Epoch 170/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3454 - val_loss: 0.8885\n",
            "Epoch 171/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3918 - val_loss: 0.6863\n",
            "Epoch 172/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2839 - val_loss: 0.6873\n",
            "Epoch 173/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5046 - val_loss: 0.6695\n",
            "Epoch 174/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.9609 - val_loss: 0.7772\n",
            "Epoch 175/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3772 - val_loss: 0.7324\n",
            "Epoch 176/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4949 - val_loss: 0.9603\n",
            "Epoch 177/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4442 - val_loss: 0.7491\n",
            "Epoch 178/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1710 - val_loss: 0.6961\n",
            "Epoch 179/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6659 - val_loss: 0.9082\n",
            "Epoch 180/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.5350 - val_loss: 0.9415\n",
            "Epoch 181/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3555 - val_loss: 0.6907\n",
            "Epoch 182/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4336 - val_loss: 0.6936\n",
            "Epoch 183/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.8358 - val_loss: 0.8057\n",
            "Epoch 184/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6544 - val_loss: 0.6770\n",
            "Epoch 185/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5315 - val_loss: 0.7911\n",
            "Epoch 186/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2092 - val_loss: 1.4284\n",
            "Epoch 187/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2942 - val_loss: 1.1764\n",
            "Epoch 188/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3054 - val_loss: 0.5183\n",
            "Epoch 189/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.4195 - val_loss: 0.8332\n",
            "Epoch 190/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4394 - val_loss: 0.9568\n",
            "Epoch 191/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4147 - val_loss: 2.4418\n",
            "Epoch 192/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5004 - val_loss: 0.7572\n",
            "Epoch 193/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3033 - val_loss: 1.2522\n",
            "Epoch 194/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5346 - val_loss: 0.8061\n",
            "Epoch 195/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7371 - val_loss: 0.8127\n",
            "Epoch 196/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3935 - val_loss: 0.8927\n",
            "Epoch 197/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1740 - val_loss: 0.8364\n",
            "Epoch 198/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5638 - val_loss: 1.2613\n",
            "Epoch 199/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4148 - val_loss: 0.8654\n",
            "Epoch 200/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4053 - val_loss: 1.1017\n",
            "Epoch 201/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.4795 - val_loss: 0.7871\n",
            "Epoch 202/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4127 - val_loss: 0.7636\n",
            "Epoch 203/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4974 - val_loss: 0.7577\n",
            "Epoch 204/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6401 - val_loss: 0.9490\n",
            "Epoch 205/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.8151 - val_loss: 0.7942\n",
            "Epoch 206/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.5291 - val_loss: 0.8571\n",
            "Epoch 207/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2675 - val_loss: 0.6702\n",
            "Epoch 208/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4519 - val_loss: 0.8379\n",
            "Epoch 209/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2947 - val_loss: 0.8708\n",
            "Epoch 210/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2751 - val_loss: 0.8110\n",
            "Epoch 211/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3639 - val_loss: 0.8322\n",
            "Epoch 212/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3099 - val_loss: 0.7965\n",
            "Epoch 213/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3217 - val_loss: 0.9380\n",
            "Epoch 214/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3650 - val_loss: 1.7102\n",
            "Epoch 215/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.6147 - val_loss: 0.7115\n",
            "Epoch 216/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4143 - val_loss: 1.0149\n",
            "Epoch 217/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5958 - val_loss: 1.2828\n",
            "Epoch 218/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3565 - val_loss: 0.7369\n",
            "Epoch 219/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.7411 - val_loss: 0.9123\n",
            "Epoch 220/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.3334 - val_loss: 1.0115\n",
            "Epoch 221/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2994 - val_loss: 0.8127\n",
            "Epoch 222/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3511 - val_loss: 0.8076\n",
            "Epoch 223/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4622 - val_loss: 1.4208\n",
            "Epoch 224/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.4324 - val_loss: 0.9837\n",
            "Epoch 225/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1168 - val_loss: 1.0353\n",
            "Epoch 226/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5438 - val_loss: 1.0388\n",
            "Epoch 227/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3382 - val_loss: 0.6536\n",
            "Epoch 228/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4488 - val_loss: 0.7153\n",
            "Epoch 229/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3269 - val_loss: 1.1315\n",
            "Epoch 230/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5003 - val_loss: 1.1236\n",
            "Epoch 231/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4616 - val_loss: 1.0413\n",
            "Epoch 232/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.0475 - val_loss: 0.8367\n",
            "Epoch 233/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4228 - val_loss: 1.5212\n",
            "Epoch 234/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4032 - val_loss: 0.9144\n",
            "Epoch 235/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.3965 - val_loss: 1.3036\n",
            "Epoch 236/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5534 - val_loss: 1.1288\n",
            "Epoch 237/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3403 - val_loss: 0.9936\n",
            "Epoch 238/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3344 - val_loss: 0.8171\n",
            "Epoch 239/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4937 - val_loss: 0.8457\n",
            "Epoch 240/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2204 - val_loss: 0.8458\n",
            "Epoch 241/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1670 - val_loss: 1.2131\n",
            "Epoch 242/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5326 - val_loss: 1.0927\n",
            "Epoch 243/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3116 - val_loss: 1.5104\n",
            "Epoch 244/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1814 - val_loss: 0.9183\n",
            "Epoch 245/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6632 - val_loss: 0.8863\n",
            "Epoch 246/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6285 - val_loss: 0.6828\n",
            "Epoch 247/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0651 - val_loss: 0.8065\n",
            "Epoch 248/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3127 - val_loss: 0.7295\n",
            "Epoch 249/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.7331 - val_loss: 0.9083\n",
            "Epoch 250/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1567 - val_loss: 0.9731\n",
            "Epoch 251/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6732 - val_loss: 1.0134\n",
            "Epoch 252/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3071 - val_loss: 0.8236\n",
            "Epoch 253/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2766 - val_loss: 0.7940\n",
            "Epoch 254/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.8472 - val_loss: 0.7961\n",
            "Epoch 255/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2264 - val_loss: 0.9178\n",
            "Epoch 256/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3685 - val_loss: 1.1690\n",
            "Epoch 257/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2138 - val_loss: 0.8031\n",
            "Epoch 258/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1604 - val_loss: 1.0500\n",
            "Epoch 259/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2845 - val_loss: 0.8586\n",
            "Epoch 260/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2755 - val_loss: 0.7914\n",
            "Epoch 261/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6446 - val_loss: 0.8240\n",
            "Epoch 262/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4490 - val_loss: 1.0250\n",
            "Epoch 263/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3039 - val_loss: 1.3018\n",
            "Epoch 264/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4998 - val_loss: 0.7909\n",
            "Epoch 265/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2353 - val_loss: 0.7986\n",
            "Epoch 266/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3129 - val_loss: 0.9336\n",
            "Epoch 267/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3210 - val_loss: 0.8957\n",
            "Epoch 268/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2815 - val_loss: 1.1724\n",
            "Epoch 269/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6674 - val_loss: 1.0355\n",
            "Epoch 270/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4337 - val_loss: 1.1206\n",
            "Epoch 271/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2994 - val_loss: 1.0312\n",
            "Epoch 272/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3459 - val_loss: 0.7077\n",
            "Epoch 273/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3550 - val_loss: 0.9457\n",
            "Epoch 274/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2831 - val_loss: 0.7676\n",
            "Epoch 275/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3130 - val_loss: 1.0668\n",
            "Epoch 276/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3455 - val_loss: 0.9199\n",
            "Epoch 277/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2131 - val_loss: 0.7937\n",
            "Epoch 278/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.7840 - val_loss: 1.0855\n",
            "Epoch 279/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.0510 - val_loss: 0.9372\n",
            "Epoch 280/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3575 - val_loss: 1.0365\n",
            "Epoch 281/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4500 - val_loss: 0.9854\n",
            "Epoch 282/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3780 - val_loss: 0.9082\n",
            "Epoch 283/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2695 - val_loss: 1.5492\n",
            "Epoch 284/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2089 - val_loss: 0.8485\n",
            "Epoch 285/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2444 - val_loss: 0.8185\n",
            "Epoch 286/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2291 - val_loss: 1.2071\n",
            "Epoch 287/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2407 - val_loss: 0.8983\n",
            "Epoch 288/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1471 - val_loss: 1.1966\n",
            "Epoch 289/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2621 - val_loss: 0.9011\n",
            "Epoch 290/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.4054 - val_loss: 0.7918\n",
            "Epoch 291/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3927 - val_loss: 0.9904\n",
            "Epoch 292/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2059 - val_loss: 1.0114\n",
            "Epoch 293/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2785 - val_loss: 0.7697\n",
            "Epoch 294/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2247 - val_loss: 1.0341\n",
            "Epoch 295/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2383 - val_loss: 1.0130\n",
            "Epoch 296/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2972 - val_loss: 1.0504\n",
            "Epoch 297/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4194 - val_loss: 0.8423\n",
            "Epoch 298/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.3753 - val_loss: 0.7903\n",
            "Epoch 299/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.3054 - val_loss: 1.1105\n",
            "Epoch 300/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2865 - val_loss: 1.0900\n",
            "Epoch 301/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2679 - val_loss: 0.8919\n",
            "Epoch 302/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1769 - val_loss: 1.1268\n",
            "Epoch 303/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.3276 - val_loss: 1.0349\n",
            "Epoch 304/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0638 - val_loss: 0.9520\n",
            "Epoch 305/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2080 - val_loss: 0.9901\n",
            "Epoch 306/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3542 - val_loss: 0.7004\n",
            "Epoch 307/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4155 - val_loss: 0.9391\n",
            "Epoch 308/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.4768 - val_loss: 0.9463\n",
            "Epoch 309/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2174 - val_loss: 0.9359\n",
            "Epoch 310/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1059 - val_loss: 0.8334\n",
            "Epoch 311/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.4360 - val_loss: 0.8339\n",
            "Epoch 312/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1845 - val_loss: 0.7008\n",
            "Epoch 313/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3199 - val_loss: 1.1380\n",
            "Epoch 314/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2364 - val_loss: 0.9336\n",
            "Epoch 315/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1515 - val_loss: 0.9980\n",
            "Epoch 316/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1355 - val_loss: 0.9791\n",
            "Epoch 317/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2895 - val_loss: 0.9289\n",
            "Epoch 318/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3541 - val_loss: 0.7486\n",
            "Epoch 319/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.3449 - val_loss: 1.0219\n",
            "Epoch 320/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1522 - val_loss: 1.0423\n",
            "Epoch 321/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2792 - val_loss: 0.8135\n",
            "Epoch 322/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3387 - val_loss: 1.0128\n",
            "Epoch 323/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5370 - val_loss: 0.9657\n",
            "Epoch 324/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1951 - val_loss: 0.7200\n",
            "Epoch 325/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2966 - val_loss: 1.2329\n",
            "Epoch 326/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1408 - val_loss: 0.8287\n",
            "Epoch 327/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1518 - val_loss: 0.7865\n",
            "Epoch 328/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0691 - val_loss: 0.9016\n",
            "Epoch 329/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4347 - val_loss: 0.5589\n",
            "Epoch 330/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1264 - val_loss: 0.7097\n",
            "Epoch 331/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1296 - val_loss: 1.0760\n",
            "Epoch 332/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2170 - val_loss: 1.0318\n",
            "Epoch 333/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1467 - val_loss: 0.9239\n",
            "Epoch 334/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3821 - val_loss: 1.2255\n",
            "Epoch 335/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1166 - val_loss: 0.8190\n",
            "Epoch 336/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2463 - val_loss: 1.1267\n",
            "Epoch 337/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3012 - val_loss: 0.8002\n",
            "Epoch 338/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3359 - val_loss: 0.7414\n",
            "Epoch 339/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.8536 - val_loss: 0.9860\n",
            "Epoch 340/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2086 - val_loss: 1.3885\n",
            "Epoch 341/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2904 - val_loss: 0.8169\n",
            "Epoch 342/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3432 - val_loss: 0.8142\n",
            "Epoch 343/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3823 - val_loss: 0.9588\n",
            "Epoch 344/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0845 - val_loss: 0.6411\n",
            "Epoch 345/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2012 - val_loss: 0.9813\n",
            "Epoch 346/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.2530 - val_loss: 0.9654\n",
            "Epoch 347/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5284 - val_loss: 0.7327\n",
            "Epoch 348/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1321 - val_loss: 0.8655\n",
            "Epoch 349/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1427 - val_loss: 0.8810\n",
            "Epoch 350/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4076 - val_loss: 0.8828\n",
            "Epoch 351/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1939 - val_loss: 1.0033\n",
            "Epoch 352/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1375 - val_loss: 0.7042\n",
            "Epoch 353/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2340 - val_loss: 0.7010\n",
            "Epoch 354/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0843 - val_loss: 0.7807\n",
            "Epoch 355/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1452 - val_loss: 0.9675\n",
            "Epoch 356/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1159 - val_loss: 1.0511\n",
            "Epoch 357/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1468 - val_loss: 0.8753\n",
            "Epoch 358/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0027 - val_loss: 0.9255\n",
            "Epoch 359/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0729 - val_loss: 1.0635\n",
            "Epoch 360/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1303 - val_loss: 0.8755\n",
            "Epoch 361/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0848 - val_loss: 0.9192\n",
            "Epoch 362/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1152 - val_loss: 0.9268\n",
            "Epoch 363/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1289 - val_loss: 1.0827\n",
            "Epoch 364/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2021 - val_loss: 0.9649\n",
            "Epoch 365/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9766 - val_loss: 1.2052\n",
            "Epoch 366/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1502 - val_loss: 1.2701\n",
            "Epoch 367/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2505 - val_loss: 0.9640\n",
            "Epoch 368/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2430 - val_loss: 0.7883\n",
            "Epoch 369/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3670 - val_loss: 1.2374\n",
            "Epoch 370/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3026 - val_loss: 1.0320\n",
            "Epoch 371/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1371 - val_loss: 1.3886\n",
            "Epoch 372/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0030 - val_loss: 0.9819\n",
            "Epoch 373/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1374 - val_loss: 0.7822\n",
            "Epoch 374/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4223 - val_loss: 0.5903\n",
            "Epoch 375/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1835 - val_loss: 0.9798\n",
            "Epoch 376/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1726 - val_loss: 0.9405\n",
            "Epoch 377/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1990 - val_loss: 0.8719\n",
            "Epoch 378/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.3765 - val_loss: 0.7915\n",
            "Epoch 379/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2261 - val_loss: 0.9197\n",
            "Epoch 380/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2499 - val_loss: 0.8794\n",
            "Epoch 381/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.7210 - val_loss: 1.1936\n",
            "Epoch 382/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1680 - val_loss: 0.7492\n",
            "Epoch 383/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1667 - val_loss: 0.8142\n",
            "Epoch 384/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3268 - val_loss: 0.9090\n",
            "Epoch 385/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1909 - val_loss: 0.7451\n",
            "Epoch 386/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0983 - val_loss: 0.8899\n",
            "Epoch 387/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2113 - val_loss: 1.2664\n",
            "Epoch 388/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4163 - val_loss: 0.7170\n",
            "Epoch 389/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2419 - val_loss: 0.7618\n",
            "Epoch 390/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2275 - val_loss: 0.7837\n",
            "Epoch 391/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9980 - val_loss: 0.8539\n",
            "Epoch 392/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1789 - val_loss: 0.8066\n",
            "Epoch 393/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1039 - val_loss: 0.8401\n",
            "Epoch 394/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2621 - val_loss: 1.0609\n",
            "Epoch 395/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1490 - val_loss: 0.9069\n",
            "Epoch 396/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0365 - val_loss: 0.9155\n",
            "Epoch 397/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1153 - val_loss: 0.8739\n",
            "Epoch 398/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2025 - val_loss: 1.0255\n",
            "Epoch 399/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.1268 - val_loss: 1.5422\n",
            "Epoch 400/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2562 - val_loss: 1.1262\n",
            "Epoch 401/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0948 - val_loss: 0.8538\n",
            "Epoch 402/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1262 - val_loss: 0.8939\n",
            "Epoch 403/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0920 - val_loss: 1.3570\n",
            "Epoch 404/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1773 - val_loss: 0.7671\n",
            "Epoch 405/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2251 - val_loss: 0.6671\n",
            "Epoch 406/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1618 - val_loss: 0.9796\n",
            "Epoch 407/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1661 - val_loss: 1.0639\n",
            "Epoch 408/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2180 - val_loss: 1.1510\n",
            "Epoch 409/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0826 - val_loss: 1.1580\n",
            "Epoch 410/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4057 - val_loss: 1.1572\n",
            "Epoch 411/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1243 - val_loss: 0.8365\n",
            "Epoch 412/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1383 - val_loss: 0.9612\n",
            "Epoch 413/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1299 - val_loss: 0.9417\n",
            "Epoch 414/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.3253 - val_loss: 0.9231\n",
            "Epoch 415/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0524 - val_loss: 0.8022\n",
            "Epoch 416/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1862 - val_loss: 0.9862\n",
            "Epoch 417/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2677 - val_loss: 0.6821\n",
            "Epoch 418/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.8523 - val_loss: 0.7744\n",
            "Epoch 419/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9410 - val_loss: 0.8672\n",
            "Epoch 420/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2492 - val_loss: 0.6365\n",
            "Epoch 421/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0768 - val_loss: 0.6776\n",
            "Epoch 422/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0583 - val_loss: 0.6490\n",
            "Epoch 423/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0569 - val_loss: 0.7404\n",
            "Epoch 424/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1025 - val_loss: 0.6306\n",
            "Epoch 425/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0494 - val_loss: 0.8521\n",
            "Epoch 426/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9892 - val_loss: 0.7342\n",
            "Epoch 427/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2452 - val_loss: 0.8255\n",
            "Epoch 428/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0071 - val_loss: 0.9662\n",
            "Epoch 429/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2378 - val_loss: 0.7829\n",
            "Epoch 430/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2800 - val_loss: 0.9251\n",
            "Epoch 431/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1460 - val_loss: 1.1839\n",
            "Epoch 432/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0893 - val_loss: 0.6542\n",
            "Epoch 433/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1843 - val_loss: 0.8468\n",
            "Epoch 434/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2570 - val_loss: 1.1896\n",
            "Epoch 435/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1548 - val_loss: 0.8125\n",
            "Epoch 436/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0615 - val_loss: 1.1866\n",
            "Epoch 437/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1228 - val_loss: 0.9412\n",
            "Epoch 438/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1129 - val_loss: 0.8414\n",
            "Epoch 439/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0632 - val_loss: 0.8951\n",
            "Epoch 440/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0254 - val_loss: 0.7521\n",
            "Epoch 441/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0338 - val_loss: 0.7188\n",
            "Epoch 442/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0482 - val_loss: 1.2181\n",
            "Epoch 443/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1878 - val_loss: 0.7122\n",
            "Epoch 444/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1436 - val_loss: 1.0266\n",
            "Epoch 445/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1329 - val_loss: 0.9337\n",
            "Epoch 446/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1884 - val_loss: 0.8472\n",
            "Epoch 447/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0051 - val_loss: 0.9474\n",
            "Epoch 448/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1485 - val_loss: 0.8214\n",
            "Epoch 449/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0419 - val_loss: 0.8719\n",
            "Epoch 450/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1831 - val_loss: 0.8924\n",
            "Epoch 451/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0386 - val_loss: 0.8255\n",
            "Epoch 452/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9898 - val_loss: 0.6746\n",
            "Epoch 453/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0837 - val_loss: 0.8883\n",
            "Epoch 454/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0823 - val_loss: 0.7613\n",
            "Epoch 455/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1723 - val_loss: 0.7500\n",
            "Epoch 456/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0603 - val_loss: 0.9419\n",
            "Epoch 457/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1405 - val_loss: 0.8933\n",
            "Epoch 458/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0096 - val_loss: 0.6585\n",
            "Epoch 459/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0080 - val_loss: 0.7284\n",
            "Epoch 460/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1064 - val_loss: 0.8144\n",
            "Epoch 461/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0501 - val_loss: 0.7223\n",
            "Epoch 462/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3781 - val_loss: 0.9015\n",
            "Epoch 463/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.8412 - val_loss: 0.6847\n",
            "Epoch 464/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2421 - val_loss: 0.7008\n",
            "Epoch 465/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0788 - val_loss: 0.8589\n",
            "Epoch 466/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0850 - val_loss: 0.7855\n",
            "Epoch 467/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0480 - val_loss: 0.6835\n",
            "Epoch 468/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1561 - val_loss: 0.7757\n",
            "Epoch 469/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9057 - val_loss: 0.7741\n",
            "Epoch 470/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0492 - val_loss: 0.6748\n",
            "Epoch 471/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0990 - val_loss: 0.6982\n",
            "Epoch 472/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9383 - val_loss: 1.0363\n",
            "Epoch 473/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1901 - val_loss: 0.9102\n",
            "Epoch 474/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0194 - val_loss: 0.7848\n",
            "Epoch 475/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0321 - val_loss: 1.0406\n",
            "Epoch 476/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1288 - val_loss: 1.1264\n",
            "Epoch 477/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0795 - val_loss: 1.0558\n",
            "Epoch 478/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0565 - val_loss: 0.7412\n",
            "Epoch 479/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2953 - val_loss: 0.6867\n",
            "Epoch 480/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9831 - val_loss: 0.8501\n",
            "Epoch 481/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0821 - val_loss: 0.8843\n",
            "Epoch 482/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9728 - val_loss: 0.8874\n",
            "Epoch 483/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1588 - val_loss: 0.8912\n",
            "Epoch 484/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0197 - val_loss: 0.7086\n",
            "Epoch 485/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9736 - val_loss: 0.8432\n",
            "Epoch 486/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1140 - val_loss: 0.6125\n",
            "Epoch 487/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1422 - val_loss: 0.8574\n",
            "Epoch 488/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3426 - val_loss: 0.8316\n",
            "Epoch 489/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9923 - val_loss: 0.6090\n",
            "Epoch 490/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9335 - val_loss: 0.8502\n",
            "Epoch 491/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9961 - val_loss: 0.7574\n",
            "Epoch 492/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9666 - val_loss: 0.8325\n",
            "Epoch 493/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0545 - val_loss: 0.7394\n",
            "Epoch 494/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6076 - val_loss: 0.7340\n",
            "Epoch 495/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9335 - val_loss: 0.8290\n",
            "Epoch 496/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0172 - val_loss: 0.7487\n",
            "Epoch 497/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9688 - val_loss: 0.7671\n",
            "Epoch 498/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9742 - val_loss: 0.7403\n",
            "Epoch 499/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1181 - val_loss: 0.5975\n",
            "Epoch 500/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.0312 - val_loss: 0.7528\n",
            "Epoch 501/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0916 - val_loss: 0.8281\n",
            "Epoch 502/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0155 - val_loss: 0.7767\n",
            "Epoch 503/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0123 - val_loss: 0.8220\n",
            "Epoch 504/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0703 - val_loss: 1.0094\n",
            "Epoch 505/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0763 - val_loss: 0.8413\n",
            "Epoch 506/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0468 - val_loss: 0.8241\n",
            "Epoch 507/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0191 - val_loss: 0.8852\n",
            "Epoch 508/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0834 - val_loss: 0.7853\n",
            "Epoch 509/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0109 - val_loss: 0.8274\n",
            "Epoch 510/5000\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 1.0500 - val_loss: 0.8530\n",
            "Epoch 511/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9600 - val_loss: 0.6706\n",
            "Epoch 512/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0400 - val_loss: 0.9146\n",
            "Epoch 513/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1541 - val_loss: 0.7081\n",
            "Epoch 514/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1338 - val_loss: 0.9337\n",
            "Epoch 515/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1017 - val_loss: 1.0176\n",
            "Epoch 516/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9762 - val_loss: 1.0059\n",
            "Epoch 517/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0067 - val_loss: 0.7004\n",
            "Epoch 518/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0726 - val_loss: 0.7552\n",
            "Epoch 519/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9398 - val_loss: 0.8744\n",
            "Epoch 520/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1204 - val_loss: 0.9319\n",
            "Epoch 521/5000\n",
            "33/33 [==============================] - 0s 13ms/step - loss: 1.9773 - val_loss: 0.7790\n",
            "Epoch 522/5000\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 1.0228 - val_loss: 0.7555\n",
            "Epoch 523/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.1360 - val_loss: 0.8698\n",
            "Epoch 524/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1189 - val_loss: 0.5796\n",
            "Epoch 525/5000\n",
            "33/33 [==============================] - 0s 12ms/step - loss: 1.1714 - val_loss: 0.8321\n",
            "Epoch 526/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.0999 - val_loss: 0.6886\n",
            "Epoch 527/5000\n",
            "33/33 [==============================] - 0s 14ms/step - loss: 1.3040 - val_loss: 0.7914\n",
            "Epoch 528/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0536 - val_loss: 0.8276\n",
            "Epoch 529/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0855 - val_loss: 0.7437\n",
            "Epoch 530/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0930 - val_loss: 0.7906\n",
            "Epoch 531/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0190 - val_loss: 0.8068\n",
            "Epoch 532/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9290 - val_loss: 0.8228\n",
            "Epoch 533/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0538 - val_loss: 0.7613\n",
            "Epoch 534/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1636 - val_loss: 0.6011\n",
            "Epoch 535/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0931 - val_loss: 0.7540\n",
            "Epoch 536/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9098 - val_loss: 0.7411\n",
            "Epoch 537/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0051 - val_loss: 0.7745\n",
            "Epoch 538/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1697 - val_loss: 0.7835\n",
            "Epoch 539/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1020 - val_loss: 0.7832\n",
            "Epoch 540/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0225 - val_loss: 0.9128\n",
            "Epoch 541/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0366 - val_loss: 0.9180\n",
            "Epoch 542/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2763 - val_loss: 0.8963\n",
            "Epoch 543/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1698 - val_loss: 0.7676\n",
            "Epoch 544/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1448 - val_loss: 0.8003\n",
            "Epoch 545/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0244 - val_loss: 0.7603\n",
            "Epoch 546/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0651 - val_loss: 0.9251\n",
            "Epoch 547/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9575 - val_loss: 0.7165\n",
            "Epoch 548/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0167 - val_loss: 1.0493\n",
            "Epoch 549/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9016 - val_loss: 0.6983\n",
            "Epoch 550/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9452 - val_loss: 0.8306\n",
            "Epoch 551/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0098 - val_loss: 0.9201\n",
            "Epoch 552/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0317 - val_loss: 0.6979\n",
            "Epoch 553/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1641 - val_loss: 0.7796\n",
            "Epoch 554/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0154 - val_loss: 0.7634\n",
            "Epoch 555/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8226 - val_loss: 0.7167\n",
            "Epoch 556/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3204 - val_loss: 0.9338\n",
            "Epoch 557/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0361 - val_loss: 0.8920\n",
            "Epoch 558/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9901 - val_loss: 0.7743\n",
            "Epoch 559/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9900 - val_loss: 0.8329\n",
            "Epoch 560/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9962 - val_loss: 0.6913\n",
            "Epoch 561/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0619 - val_loss: 0.7070\n",
            "Epoch 562/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0726 - val_loss: 0.7019\n",
            "Epoch 563/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1675 - val_loss: 0.7982\n",
            "Epoch 564/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9903 - val_loss: 0.7749\n",
            "Epoch 565/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1063 - val_loss: 0.8197\n",
            "Epoch 566/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9816 - val_loss: 0.6485\n",
            "Epoch 567/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0510 - val_loss: 1.0114\n",
            "Epoch 568/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9891 - val_loss: 0.7913\n",
            "Epoch 569/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0062 - val_loss: 0.6788\n",
            "Epoch 570/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0351 - val_loss: 0.7558\n",
            "Epoch 571/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0574 - val_loss: 0.8029\n",
            "Epoch 572/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1203 - val_loss: 0.6813\n",
            "Epoch 573/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9938 - val_loss: 0.8130\n",
            "Epoch 574/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9499 - val_loss: 0.7274\n",
            "Epoch 575/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9492 - val_loss: 0.9093\n",
            "Epoch 576/5000\n",
            "33/33 [==============================] - 0s 12ms/step - loss: 0.9686 - val_loss: 0.8364\n",
            "Epoch 577/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.0286 - val_loss: 0.6761\n",
            "Epoch 578/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9470 - val_loss: 0.8231\n",
            "Epoch 579/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9795 - val_loss: 0.6986\n",
            "Epoch 580/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0077 - val_loss: 0.8259\n",
            "Epoch 581/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0750 - val_loss: 0.6350\n",
            "Epoch 582/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2115 - val_loss: 0.8418\n",
            "Epoch 583/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1119 - val_loss: 0.7185\n",
            "Epoch 584/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0120 - val_loss: 0.8095\n",
            "Epoch 585/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9025 - val_loss: 0.7086\n",
            "Epoch 586/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1108 - val_loss: 0.6924\n",
            "Epoch 587/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0827 - val_loss: 0.8660\n",
            "Epoch 588/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0131 - val_loss: 0.7112\n",
            "Epoch 589/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9253 - val_loss: 0.6155\n",
            "Epoch 590/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9404 - val_loss: 0.6401\n",
            "Epoch 591/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2436 - val_loss: 0.6388\n",
            "Epoch 592/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0294 - val_loss: 0.6617\n",
            "Epoch 593/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2479 - val_loss: 0.8134\n",
            "Epoch 594/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1654 - val_loss: 0.5595\n",
            "Epoch 595/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9602 - val_loss: 0.7798\n",
            "Epoch 596/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1062 - val_loss: 0.5949\n",
            "Epoch 597/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0310 - val_loss: 0.8347\n",
            "Epoch 598/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0188 - val_loss: 0.8067\n",
            "Epoch 599/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9813 - val_loss: 0.9075\n",
            "Epoch 600/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8963 - val_loss: 0.8311\n",
            "Epoch 601/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9893 - val_loss: 0.7573\n",
            "Epoch 602/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1039 - val_loss: 0.6649\n",
            "Epoch 603/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0555 - val_loss: 0.7991\n",
            "Epoch 604/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9570 - val_loss: 0.7383\n",
            "Epoch 605/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9801 - val_loss: 0.6926\n",
            "Epoch 606/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0212 - val_loss: 0.8042\n",
            "Epoch 607/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9368 - val_loss: 0.7902\n",
            "Epoch 608/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9781 - val_loss: 0.7083\n",
            "Epoch 609/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0369 - val_loss: 0.7410\n",
            "Epoch 610/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0636 - val_loss: 0.6658\n",
            "Epoch 611/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9269 - val_loss: 0.6997\n",
            "Epoch 612/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1972 - val_loss: 0.7602\n",
            "Epoch 613/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9282 - val_loss: 0.8337\n",
            "Epoch 614/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0994 - val_loss: 0.8117\n",
            "Epoch 615/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2035 - val_loss: 0.7651\n",
            "Epoch 616/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9573 - val_loss: 0.7844\n",
            "Epoch 617/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9981 - val_loss: 0.9320\n",
            "Epoch 618/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9491 - val_loss: 0.6610\n",
            "Epoch 619/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2040 - val_loss: 0.9259\n",
            "Epoch 620/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9376 - val_loss: 0.8637\n",
            "Epoch 621/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9508 - val_loss: 0.6875\n",
            "Epoch 622/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0585 - val_loss: 0.6366\n",
            "Epoch 623/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9267 - val_loss: 0.7799\n",
            "Epoch 624/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4862 - val_loss: 0.7249\n",
            "Epoch 625/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0317 - val_loss: 0.8868\n",
            "Epoch 626/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0379 - val_loss: 0.6808\n",
            "Epoch 627/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0231 - val_loss: 0.8906\n",
            "Epoch 628/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9413 - val_loss: 0.6181\n",
            "Epoch 629/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0288 - val_loss: 0.6901\n",
            "Epoch 630/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9590 - val_loss: 0.7224\n",
            "Epoch 631/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8970 - val_loss: 0.6511\n",
            "Epoch 632/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9364 - val_loss: 0.8419\n",
            "Epoch 633/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9117 - val_loss: 0.6904\n",
            "Epoch 634/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0934 - val_loss: 0.8876\n",
            "Epoch 635/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0062 - val_loss: 0.8062\n",
            "Epoch 636/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0106 - val_loss: 0.5650\n",
            "Epoch 637/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9999 - val_loss: 0.6390\n",
            "Epoch 638/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9966 - val_loss: 0.8356\n",
            "Epoch 639/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2575 - val_loss: 0.7347\n",
            "Epoch 640/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9305 - val_loss: 0.6272\n",
            "Epoch 641/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9065 - val_loss: 0.7554\n",
            "Epoch 642/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9606 - val_loss: 0.7542\n",
            "Epoch 643/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0154 - val_loss: 0.6779\n",
            "Epoch 644/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9657 - val_loss: 0.6949\n",
            "Epoch 645/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0423 - val_loss: 0.6572\n",
            "Epoch 646/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9381 - val_loss: 0.7095\n",
            "Epoch 647/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0343 - val_loss: 1.1192\n",
            "Epoch 648/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9110 - val_loss: 0.6355\n",
            "Epoch 649/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9531 - val_loss: 0.6518\n",
            "Epoch 650/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0599 - val_loss: 0.7813\n",
            "Epoch 651/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9007 - val_loss: 0.8571\n",
            "Epoch 652/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8799 - val_loss: 0.7796\n",
            "Epoch 653/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9797 - val_loss: 0.6621\n",
            "Epoch 654/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0011 - val_loss: 0.7713\n",
            "Epoch 655/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0948 - val_loss: 0.6885\n",
            "Epoch 656/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9167 - val_loss: 0.7207\n",
            "Epoch 657/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0431 - val_loss: 0.9285\n",
            "Epoch 658/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0947 - val_loss: 0.8620\n",
            "Epoch 659/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0239 - val_loss: 0.9087\n",
            "Epoch 660/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3362 - val_loss: 0.8825\n",
            "Epoch 661/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0274 - val_loss: 1.0491\n",
            "Epoch 662/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9847 - val_loss: 0.8649\n",
            "Epoch 663/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1884 - val_loss: 0.7902\n",
            "Epoch 664/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9841 - val_loss: 0.6410\n",
            "Epoch 665/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0471 - val_loss: 0.7161\n",
            "Epoch 666/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9598 - val_loss: 0.6957\n",
            "Epoch 667/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1516 - val_loss: 0.8821\n",
            "Epoch 668/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0099 - val_loss: 0.7063\n",
            "Epoch 669/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9041 - val_loss: 0.6438\n",
            "Epoch 670/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.2657 - val_loss: 0.7092\n",
            "Epoch 671/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9128 - val_loss: 0.6696\n",
            "Epoch 672/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9681 - val_loss: 0.6373\n",
            "Epoch 673/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1344 - val_loss: 0.6437\n",
            "Epoch 674/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8937 - val_loss: 0.7739\n",
            "Epoch 675/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9276 - val_loss: 0.6443\n",
            "Epoch 676/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0574 - val_loss: 0.8824\n",
            "Epoch 677/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0169 - val_loss: 1.1565\n",
            "Epoch 678/5000\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.9406 - val_loss: 0.7669\n",
            "Epoch 679/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.8999 - val_loss: 0.7169\n",
            "Epoch 680/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1589 - val_loss: 0.7085\n",
            "Epoch 681/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9514 - val_loss: 0.7747\n",
            "Epoch 682/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1199 - val_loss: 0.8795\n",
            "Epoch 683/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9871 - val_loss: 0.7422\n",
            "Epoch 684/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9032 - val_loss: 0.8038\n",
            "Epoch 685/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0043 - val_loss: 0.6877\n",
            "Epoch 686/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9755 - val_loss: 0.6584\n",
            "Epoch 687/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9686 - val_loss: 0.6382\n",
            "Epoch 688/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9338 - val_loss: 0.8622\n",
            "Epoch 689/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8947 - val_loss: 0.9114\n",
            "Epoch 690/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0737 - val_loss: 0.9411\n",
            "Epoch 691/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0168 - val_loss: 0.6461\n",
            "Epoch 692/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1131 - val_loss: 0.7493\n",
            "Epoch 693/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8770 - val_loss: 0.7482\n",
            "Epoch 694/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.8432 - val_loss: 0.5558\n",
            "Epoch 695/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0320 - val_loss: 0.6061\n",
            "Epoch 696/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1212 - val_loss: 0.7269\n",
            "Epoch 697/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9659 - val_loss: 0.6987\n",
            "Epoch 698/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8790 - val_loss: 0.7811\n",
            "Epoch 699/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0323 - val_loss: 0.7478\n",
            "Epoch 700/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9557 - val_loss: 0.6632\n",
            "Epoch 701/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0007 - val_loss: 0.8205\n",
            "Epoch 702/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8965 - val_loss: 0.8125\n",
            "Epoch 703/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0044 - val_loss: 0.7093\n",
            "Epoch 704/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9295 - val_loss: 0.7976\n",
            "Epoch 705/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9691 - val_loss: 0.7245\n",
            "Epoch 706/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1285 - val_loss: 0.9803\n",
            "Epoch 707/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9531 - val_loss: 0.6533\n",
            "Epoch 708/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9515 - val_loss: 0.6723\n",
            "Epoch 709/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9279 - val_loss: 0.6275\n",
            "Epoch 710/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9714 - val_loss: 0.8343\n",
            "Epoch 711/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0275 - val_loss: 0.7748\n",
            "Epoch 712/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9615 - val_loss: 0.6057\n",
            "Epoch 713/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9852 - val_loss: 0.7056\n",
            "Epoch 714/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8786 - val_loss: 0.6238\n",
            "Epoch 715/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9923 - val_loss: 0.7880\n",
            "Epoch 716/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9726 - val_loss: 0.9084\n",
            "Epoch 717/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0981 - val_loss: 0.5972\n",
            "Epoch 718/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9143 - val_loss: 0.6229\n",
            "Epoch 719/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9987 - val_loss: 0.6235\n",
            "Epoch 720/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9107 - val_loss: 0.6906\n",
            "Epoch 721/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0526 - val_loss: 0.7081\n",
            "Epoch 722/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9717 - val_loss: 0.6610\n",
            "Epoch 723/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9541 - val_loss: 0.7525\n",
            "Epoch 724/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9556 - val_loss: 0.6977\n",
            "Epoch 725/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.1149 - val_loss: 0.6581\n",
            "Epoch 726/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9285 - val_loss: 0.6228\n",
            "Epoch 727/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8891 - val_loss: 0.7620\n",
            "Epoch 728/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0238 - val_loss: 0.6863\n",
            "Epoch 729/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9319 - val_loss: 0.7110\n",
            "Epoch 730/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0806 - val_loss: 0.7069\n",
            "Epoch 731/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9807 - val_loss: 0.8662\n",
            "Epoch 732/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0117 - val_loss: 0.7406\n",
            "Epoch 733/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8540 - val_loss: 0.5594\n",
            "Epoch 734/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0669 - val_loss: 0.8462\n",
            "Epoch 735/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8574 - val_loss: 0.8107\n",
            "Epoch 736/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9676 - val_loss: 0.8389\n",
            "Epoch 737/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1244 - val_loss: 0.7055\n",
            "Epoch 738/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0596 - val_loss: 0.6211\n",
            "Epoch 739/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9869 - val_loss: 0.5975\n",
            "Epoch 740/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0055 - val_loss: 0.8477\n",
            "Epoch 741/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9175 - val_loss: 0.7678\n",
            "Epoch 742/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9113 - val_loss: 0.7126\n",
            "Epoch 743/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9525 - val_loss: 0.6900\n",
            "Epoch 744/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9583 - val_loss: 0.5994\n",
            "Epoch 745/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9422 - val_loss: 0.7877\n",
            "Epoch 746/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0032 - val_loss: 0.6659\n",
            "Epoch 747/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2047 - val_loss: 1.0434\n",
            "Epoch 748/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9065 - val_loss: 0.6498\n",
            "Epoch 749/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9625 - val_loss: 0.8076\n",
            "Epoch 750/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9709 - val_loss: 0.7392\n",
            "Epoch 751/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9498 - val_loss: 0.6739\n",
            "Epoch 752/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1270 - val_loss: 0.8077\n",
            "Epoch 753/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0399 - val_loss: 0.7279\n",
            "Epoch 754/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0427 - val_loss: 0.6214\n",
            "Epoch 755/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8642 - val_loss: 0.8089\n",
            "Epoch 756/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1534 - val_loss: 0.9266\n",
            "Epoch 757/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0786 - val_loss: 0.7697\n",
            "Epoch 758/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9628 - val_loss: 0.6292\n",
            "Epoch 759/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8837 - val_loss: 0.6148\n",
            "Epoch 760/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9978 - val_loss: 0.6374\n",
            "Epoch 761/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9848 - val_loss: 0.7089\n",
            "Epoch 762/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9512 - val_loss: 0.7501\n",
            "Epoch 763/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2380 - val_loss: 0.7003\n",
            "Epoch 764/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0520 - val_loss: 0.5590\n",
            "Epoch 765/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9328 - val_loss: 0.6101\n",
            "Epoch 766/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8967 - val_loss: 0.6360\n",
            "Epoch 767/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9316 - val_loss: 0.7070\n",
            "Epoch 768/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8609 - val_loss: 0.6731\n",
            "Epoch 769/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9977 - val_loss: 0.6369\n",
            "Epoch 770/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0984 - val_loss: 0.7988\n",
            "Epoch 771/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8694 - val_loss: 0.7355\n",
            "Epoch 772/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9711 - val_loss: 0.7953\n",
            "Epoch 773/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9576 - val_loss: 0.5965\n",
            "Epoch 774/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9130 - val_loss: 0.8060\n",
            "Epoch 775/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9542 - val_loss: 0.7763\n",
            "Epoch 776/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9930 - val_loss: 0.7206\n",
            "Epoch 777/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9737 - val_loss: 0.6980\n",
            "Epoch 778/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9456 - val_loss: 0.7612\n",
            "Epoch 779/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9833 - val_loss: 0.6453\n",
            "Epoch 780/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8635 - val_loss: 0.7960\n",
            "Epoch 781/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0081 - val_loss: 0.6482\n",
            "Epoch 782/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9297 - val_loss: 0.7894\n",
            "Epoch 783/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8961 - val_loss: 0.7076\n",
            "Epoch 784/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0326 - val_loss: 0.7127\n",
            "Epoch 785/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9387 - val_loss: 0.6383\n",
            "Epoch 786/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0018 - val_loss: 0.7048\n",
            "Epoch 787/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2261 - val_loss: 0.7268\n",
            "Epoch 788/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0159 - val_loss: 0.6341\n",
            "Epoch 789/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9596 - val_loss: 0.8354\n",
            "Epoch 790/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9314 - val_loss: 0.8170\n",
            "Epoch 791/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8997 - val_loss: 0.7031\n",
            "Epoch 792/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9378 - val_loss: 0.7423\n",
            "Epoch 793/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9289 - val_loss: 0.7709\n",
            "Epoch 794/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9033 - val_loss: 0.9450\n",
            "Epoch 795/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9543 - val_loss: 0.7388\n",
            "Epoch 796/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1184 - val_loss: 0.8107\n",
            "Epoch 797/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0275 - val_loss: 0.6621\n",
            "Epoch 798/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9327 - val_loss: 0.6848\n",
            "Epoch 799/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8844 - val_loss: 0.6449\n",
            "Epoch 800/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9876 - val_loss: 0.5181\n",
            "Epoch 801/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9281 - val_loss: 0.6585\n",
            "Epoch 802/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9337 - val_loss: 0.7032\n",
            "Epoch 803/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9889 - val_loss: 0.6870\n",
            "Epoch 804/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8998 - val_loss: 0.8753\n",
            "Epoch 805/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0125 - val_loss: 0.7314\n",
            "Epoch 806/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9943 - val_loss: 0.6801\n",
            "Epoch 807/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9352 - val_loss: 0.6504\n",
            "Epoch 808/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8910 - val_loss: 1.0235\n",
            "Epoch 809/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0533 - val_loss: 0.7059\n",
            "Epoch 810/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9423 - val_loss: 0.5563\n",
            "Epoch 811/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9627 - val_loss: 0.7722\n",
            "Epoch 812/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0025 - val_loss: 0.5555\n",
            "Epoch 813/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9877 - val_loss: 0.7392\n",
            "Epoch 814/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9249 - val_loss: 0.7546\n",
            "Epoch 815/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9798 - val_loss: 0.7090\n",
            "Epoch 816/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9272 - val_loss: 0.7331\n",
            "Epoch 817/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0423 - val_loss: 0.6470\n",
            "Epoch 818/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9458 - val_loss: 0.5711\n",
            "Epoch 819/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9664 - val_loss: 0.6525\n",
            "Epoch 820/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9307 - val_loss: 0.7810\n",
            "Epoch 821/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8764 - val_loss: 0.9321\n",
            "Epoch 822/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9836 - val_loss: 0.6551\n",
            "Epoch 823/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0142 - val_loss: 0.7947\n",
            "Epoch 824/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9904 - val_loss: 0.6374\n",
            "Epoch 825/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9068 - val_loss: 0.7815\n",
            "Epoch 826/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0621 - val_loss: 0.7134\n",
            "Epoch 827/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9344 - val_loss: 0.5812\n",
            "Epoch 828/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1117 - val_loss: 0.6776\n",
            "Epoch 829/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9324 - val_loss: 0.6605\n",
            "Epoch 830/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0285 - val_loss: 0.6865\n",
            "Epoch 831/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9698 - val_loss: 0.6878\n",
            "Epoch 832/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9017 - val_loss: 0.7529\n",
            "Epoch 833/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9646 - val_loss: 0.8357\n",
            "Epoch 834/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0250 - val_loss: 0.8280\n",
            "Epoch 835/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9183 - val_loss: 0.6848\n",
            "Epoch 836/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9636 - val_loss: 0.8126\n",
            "Epoch 837/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9977 - val_loss: 0.7429\n",
            "Epoch 838/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9024 - val_loss: 0.6111\n",
            "Epoch 839/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9217 - val_loss: 1.0048\n",
            "Epoch 840/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9810 - val_loss: 0.6374\n",
            "Epoch 841/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9429 - val_loss: 0.6661\n",
            "Epoch 842/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9109 - val_loss: 0.7787\n",
            "Epoch 843/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9251 - val_loss: 0.7374\n",
            "Epoch 844/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9960 - val_loss: 0.6712\n",
            "Epoch 845/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0273 - val_loss: 0.7620\n",
            "Epoch 846/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9759 - val_loss: 0.8165\n",
            "Epoch 847/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0614 - val_loss: 0.8128\n",
            "Epoch 848/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9432 - val_loss: 0.8384\n",
            "Epoch 849/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9060 - val_loss: 0.6988\n",
            "Epoch 850/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9682 - val_loss: 0.6548\n",
            "Epoch 851/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1011 - val_loss: 0.7771\n",
            "Epoch 852/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9109 - val_loss: 0.6272\n",
            "Epoch 853/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8372 - val_loss: 0.7053\n",
            "Epoch 854/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8817 - val_loss: 0.7270\n",
            "Epoch 855/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9994 - val_loss: 0.7094\n",
            "Epoch 856/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0670 - val_loss: 0.8278\n",
            "Epoch 857/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0448 - val_loss: 0.6138\n",
            "Epoch 858/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9224 - val_loss: 0.5993\n",
            "Epoch 859/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0040 - val_loss: 0.6818\n",
            "Epoch 860/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9347 - val_loss: 0.7710\n",
            "Epoch 861/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9534 - val_loss: 0.8814\n",
            "Epoch 862/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9629 - val_loss: 0.7352\n",
            "Epoch 863/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9439 - val_loss: 0.6748\n",
            "Epoch 864/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9355 - val_loss: 0.6023\n",
            "Epoch 865/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9555 - val_loss: 0.7207\n",
            "Epoch 866/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9399 - val_loss: 0.6007\n",
            "Epoch 867/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9851 - val_loss: 0.7995\n",
            "Epoch 868/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8961 - val_loss: 0.8006\n",
            "Epoch 869/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0476 - val_loss: 0.7503\n",
            "Epoch 870/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9600 - val_loss: 0.7421\n",
            "Epoch 871/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0363 - val_loss: 0.6619\n",
            "Epoch 872/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9580 - val_loss: 0.7350\n",
            "Epoch 873/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0772 - val_loss: 0.6804\n",
            "Epoch 874/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9232 - val_loss: 0.6902\n",
            "Epoch 875/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8873 - val_loss: 0.7370\n",
            "Epoch 876/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9062 - val_loss: 0.8857\n",
            "Epoch 877/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9205 - val_loss: 0.6535\n",
            "Epoch 878/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.8663 - val_loss: 0.6683\n",
            "Epoch 879/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8974 - val_loss: 0.7189\n",
            "Epoch 880/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9240 - val_loss: 0.6296\n",
            "Epoch 881/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9495 - val_loss: 0.8691\n",
            "Epoch 882/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9386 - val_loss: 0.6540\n",
            "Epoch 883/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9123 - val_loss: 0.7555\n",
            "Epoch 884/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9605 - val_loss: 0.6982\n",
            "Epoch 885/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8489 - val_loss: 0.6801\n",
            "Epoch 886/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8699 - val_loss: 0.6797\n",
            "Epoch 887/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0854 - val_loss: 0.6351\n",
            "Epoch 888/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9876 - val_loss: 0.6476\n",
            "Epoch 889/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9849 - val_loss: 0.6697\n",
            "Epoch 890/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9029 - val_loss: 0.7299\n",
            "Epoch 891/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9650 - val_loss: 0.6690\n",
            "Epoch 892/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0592 - val_loss: 0.6497\n",
            "Epoch 893/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9397 - val_loss: 0.7571\n",
            "Epoch 894/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0129 - val_loss: 0.6085\n",
            "Epoch 895/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8819 - val_loss: 0.5540\n",
            "Epoch 896/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0995 - val_loss: 0.6834\n",
            "Epoch 897/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9250 - val_loss: 0.8175\n",
            "Epoch 898/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9475 - val_loss: 0.7483\n",
            "Epoch 899/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9513 - val_loss: 0.7563\n",
            "Epoch 900/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1453 - val_loss: 0.7289\n",
            "Epoch 901/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0067 - val_loss: 0.7810\n",
            "Epoch 902/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9449 - val_loss: 0.7112\n",
            "Epoch 903/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9743 - val_loss: 0.9142\n",
            "Epoch 904/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9746 - val_loss: 0.5752\n",
            "Epoch 905/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9891 - val_loss: 0.6968\n",
            "Epoch 906/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9316 - val_loss: 0.6679\n",
            "Epoch 907/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0322 - val_loss: 0.6546\n",
            "Epoch 908/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9760 - val_loss: 0.7938\n",
            "Epoch 909/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9220 - val_loss: 0.5725\n",
            "Epoch 910/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9061 - val_loss: 0.5954\n",
            "Epoch 911/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0210 - val_loss: 0.7590\n",
            "Epoch 912/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0170 - val_loss: 0.7043\n",
            "Epoch 913/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9302 - val_loss: 0.7138\n",
            "Epoch 914/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0051 - val_loss: 0.6238\n",
            "Epoch 915/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8818 - val_loss: 0.7524\n",
            "Epoch 916/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8672 - val_loss: 0.7256\n",
            "Epoch 917/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0014 - val_loss: 0.8345\n",
            "Epoch 918/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0161 - val_loss: 0.6354\n",
            "Epoch 919/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9413 - val_loss: 0.9110\n",
            "Epoch 920/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0094 - val_loss: 0.7683\n",
            "Epoch 921/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0524 - val_loss: 1.0775\n",
            "Epoch 922/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9872 - val_loss: 0.9189\n",
            "Epoch 923/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9265 - val_loss: 0.6918\n",
            "Epoch 924/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9017 - val_loss: 0.6555\n",
            "Epoch 925/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9672 - val_loss: 0.6672\n",
            "Epoch 926/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9456 - val_loss: 0.8762\n",
            "Epoch 927/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9783 - val_loss: 0.6787\n",
            "Epoch 928/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9358 - val_loss: 0.8345\n",
            "Epoch 929/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9870 - val_loss: 0.6524\n",
            "Epoch 930/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0410 - val_loss: 0.7177\n",
            "Epoch 931/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0048 - val_loss: 0.8330\n",
            "Epoch 932/5000\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.9587 - val_loss: 0.7241\n",
            "Epoch 933/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9307 - val_loss: 0.6987\n",
            "Epoch 934/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9345 - val_loss: 0.8690\n",
            "Epoch 935/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8405 - val_loss: 0.7623\n",
            "Epoch 936/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8999 - val_loss: 0.6838\n",
            "Epoch 937/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9994 - val_loss: 0.6925\n",
            "Epoch 938/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9616 - val_loss: 0.6134\n",
            "Epoch 939/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8741 - val_loss: 0.6757\n",
            "Epoch 940/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9462 - val_loss: 0.7216\n",
            "Epoch 941/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9522 - val_loss: 0.6666\n",
            "Epoch 942/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9707 - val_loss: 0.6278\n",
            "Epoch 943/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9039 - val_loss: 0.6984\n",
            "Epoch 944/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0354 - val_loss: 0.6170\n",
            "Epoch 945/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9861 - val_loss: 0.7029\n",
            "Epoch 946/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9071 - val_loss: 0.7691\n",
            "Epoch 947/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9899 - val_loss: 0.7603\n",
            "Epoch 948/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9563 - val_loss: 0.6713\n",
            "Epoch 949/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0143 - val_loss: 0.5714\n",
            "Epoch 950/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8959 - val_loss: 0.7842\n",
            "Epoch 951/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9542 - val_loss: 0.7269\n",
            "Epoch 952/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8844 - val_loss: 0.7217\n",
            "Epoch 953/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9188 - val_loss: 0.6592\n",
            "Epoch 954/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9393 - val_loss: 0.7248\n",
            "Epoch 955/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9574 - val_loss: 0.6354\n",
            "Epoch 956/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9003 - val_loss: 0.6660\n",
            "Epoch 957/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9639 - val_loss: 0.6681\n",
            "Epoch 958/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9277 - val_loss: 0.6594\n",
            "Epoch 959/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0167 - val_loss: 0.6368\n",
            "Epoch 960/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9869 - val_loss: 0.6380\n",
            "Epoch 961/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9284 - val_loss: 0.6978\n",
            "Epoch 962/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8871 - val_loss: 0.6746\n",
            "Epoch 963/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9337 - val_loss: 0.5747\n",
            "Epoch 964/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9538 - val_loss: 0.6833\n",
            "Epoch 965/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9771 - val_loss: 0.6001\n",
            "Epoch 966/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9387 - val_loss: 0.6321\n",
            "Epoch 967/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9236 - val_loss: 0.6570\n",
            "Epoch 968/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0110 - val_loss: 0.7003\n",
            "Epoch 969/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8917 - val_loss: 0.6168\n",
            "Epoch 970/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9236 - val_loss: 0.7860\n",
            "Epoch 971/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0725 - val_loss: 0.7475\n",
            "Epoch 972/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8920 - val_loss: 0.7134\n",
            "Epoch 973/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0507 - val_loss: 0.7023\n",
            "Epoch 974/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9849 - val_loss: 0.5776\n",
            "Epoch 975/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0558 - val_loss: 0.7523\n",
            "Epoch 976/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9531 - val_loss: 0.6172\n",
            "Epoch 977/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8847 - val_loss: 0.5678\n",
            "Epoch 978/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9406 - val_loss: 0.6606\n",
            "Epoch 979/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9454 - val_loss: 0.7184\n",
            "Epoch 980/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8669 - val_loss: 0.7340\n",
            "Epoch 981/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9605 - val_loss: 0.6118\n",
            "Epoch 982/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.8560 - val_loss: 0.7915\n",
            "Epoch 983/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0339 - val_loss: 0.7317\n",
            "Epoch 984/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0343 - val_loss: 0.7202\n",
            "Epoch 985/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9076 - val_loss: 0.8134\n",
            "Epoch 986/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8470 - val_loss: 0.6473\n",
            "Epoch 987/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9260 - val_loss: 0.8345\n",
            "Epoch 988/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8676 - val_loss: 0.6411\n",
            "Epoch 989/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9373 - val_loss: 0.6744\n",
            "Epoch 990/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.0284 - val_loss: 0.7308\n",
            "Epoch 991/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9001 - val_loss: 0.6369\n",
            "Epoch 992/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9847 - val_loss: 0.8214\n",
            "Epoch 993/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0887 - val_loss: 0.7438\n",
            "Epoch 994/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8866 - val_loss: 0.6824\n",
            "Epoch 995/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9885 - val_loss: 0.6195\n",
            "Epoch 996/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8970 - val_loss: 0.5611\n",
            "Epoch 997/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0524 - val_loss: 0.6095\n",
            "Epoch 998/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9448 - val_loss: 0.6815\n",
            "Epoch 999/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9854 - val_loss: 0.6844\n",
            "Epoch 1000/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9891 - val_loss: 0.6543\n",
            "Epoch 1001/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1023 - val_loss: 0.9116\n",
            "Epoch 1002/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8841 - val_loss: 0.7694\n",
            "Epoch 1003/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0076 - val_loss: 0.6767\n",
            "Epoch 1004/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8959 - val_loss: 0.6476\n",
            "Epoch 1005/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8909 - val_loss: 0.6104\n",
            "Epoch 1006/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9748 - val_loss: 0.7025\n",
            "Epoch 1007/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9421 - val_loss: 0.7201\n",
            "Epoch 1008/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9865 - val_loss: 0.9397\n",
            "Epoch 1009/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9604 - val_loss: 0.6766\n",
            "Epoch 1010/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0691 - val_loss: 0.7428\n",
            "Epoch 1011/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8553 - val_loss: 0.7524\n",
            "Epoch 1012/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9277 - val_loss: 0.7534\n",
            "Epoch 1013/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0760 - val_loss: 0.6777\n",
            "Epoch 1014/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8854 - val_loss: 0.7302\n",
            "Epoch 1015/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9104 - val_loss: 0.7142\n",
            "Epoch 1016/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9259 - val_loss: 0.8945\n",
            "Epoch 1017/5000\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.9014 - val_loss: 0.6515\n",
            "Epoch 1018/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.0192 - val_loss: 0.6090\n",
            "Epoch 1019/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9453 - val_loss: 0.6532\n",
            "Epoch 1020/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9224 - val_loss: 0.7101\n",
            "Epoch 1021/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9914 - val_loss: 0.5983\n",
            "Epoch 1022/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8629 - val_loss: 0.6756\n",
            "Epoch 1023/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9064 - val_loss: 0.6088\n",
            "Epoch 1024/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9346 - val_loss: 0.6714\n",
            "Epoch 1025/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9594 - val_loss: 0.7992\n",
            "Epoch 1026/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9278 - val_loss: 0.7133\n",
            "Epoch 1027/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8676 - val_loss: 0.6666\n",
            "Epoch 1028/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0095 - val_loss: 0.7336\n",
            "Epoch 1029/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9443 - val_loss: 0.8256\n",
            "Epoch 1030/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9153 - val_loss: 0.6830\n",
            "Epoch 1031/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9442 - val_loss: 0.6161\n",
            "Epoch 1032/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9900 - val_loss: 0.7977\n",
            "Epoch 1033/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0226 - val_loss: 0.6068\n",
            "Epoch 1034/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9147 - val_loss: 0.7190\n",
            "Epoch 1035/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8585 - val_loss: 0.6980\n",
            "Epoch 1036/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9690 - val_loss: 0.6647\n",
            "Epoch 1037/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9392 - val_loss: 0.6286\n",
            "Epoch 1038/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9311 - val_loss: 0.7782\n",
            "Epoch 1039/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9589 - val_loss: 0.7066\n",
            "Epoch 1040/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9707 - val_loss: 0.6074\n",
            "Epoch 1041/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8880 - val_loss: 0.7973\n",
            "Epoch 1042/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9387 - val_loss: 0.6758\n",
            "Epoch 1043/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9879 - val_loss: 0.7896\n",
            "Epoch 1044/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9580 - val_loss: 0.7407\n",
            "Epoch 1045/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9035 - val_loss: 0.7622\n",
            "Epoch 1046/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8923 - val_loss: 0.9939\n",
            "Epoch 1047/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1430 - val_loss: 0.5943\n",
            "Epoch 1048/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9162 - val_loss: 0.6512\n",
            "Epoch 1049/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8462 - val_loss: 0.9750\n",
            "Epoch 1050/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9134 - val_loss: 0.7080\n",
            "Epoch 1051/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0289 - val_loss: 0.7279\n",
            "Epoch 1052/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9163 - val_loss: 0.6364\n",
            "Epoch 1053/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8989 - val_loss: 0.5806\n",
            "Epoch 1054/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9561 - val_loss: 0.9836\n",
            "Epoch 1055/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9422 - val_loss: 0.7551\n",
            "Epoch 1056/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8789 - val_loss: 0.6540\n",
            "Epoch 1057/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8996 - val_loss: 0.6858\n",
            "Epoch 1058/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1052 - val_loss: 0.7107\n",
            "Epoch 1059/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0312 - val_loss: 0.6655\n",
            "Epoch 1060/5000\n",
            "32/33 [============================>.] - ETA: 0s - loss: 0.9457Restoring model weights from the end of the best epoch: 60.\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9451 - val_loss: 0.7524\n",
            "Epoch 1060: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-75-b383c653ffcb>:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.5123336315155029\n",
            "Train loss: 1.7024805545806885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7dd8fa039900> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.313475489616394\n",
            "dO18 RMSE: 0.9823125455460783\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0          26.130            0.19025\n",
            "1          26.984            0.40123\n",
            "2          24.656            0.17728\n",
            "3          26.356            0.03953\n",
            "4          23.962            0.30992\n",
            "5          24.848            0.15842\n",
            "6          25.080            0.05125\n",
            "7          26.546            2.14378\n",
            "8          25.682            0.94472\n",
            "9          24.028            0.45852\n",
            "10         23.944            0.15813\n",
            "11         23.752            0.52437\n",
            "12         26.018            0.96417\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       25.793682           0.753814\n",
            "1       25.793682           0.753814\n",
            "2       25.071218           0.752012\n",
            "3       25.115622           0.750865\n",
            "4       25.128965           0.754161\n",
            "5       25.054588           0.752553\n",
            "6       25.054588           0.752553\n",
            "7       25.128975           0.754161\n",
            "8       25.071230           0.752012\n",
            "9       25.054575           0.752553\n",
            "10      25.455435           0.749170\n",
            "11      25.116690           0.754351\n",
            "12      25.455435           0.749170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/MyDrive/amazon_rainforest_files/variational/model/random_ablated_0809_ensemble.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Grouped, fixed, all columns\n",
        "\n",
        "We can't (easily) generate isoscapes with these because the isoscapes for 'predkrig_br_lat_ISORG', 'Iso_Oxi_Stack_mean_TERZER',\n",
        "                   'isoscape_fullmodel_d18O_prec_REGRESSION' are not easily retrievable... but I'm curious how much better the model is if these columns are included."
      ],
      "metadata": {
        "id": "wPDSSm__DR53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_fixed_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_train_fixed_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_test_fixed_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_validation_fixed_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_passthrough = [\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "columns_to_scale = []\n",
        "columns_to_standardize = [\n",
        "    'lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature','Mean Annual Precipitation',\n",
        "    'Iso_Oxi_Stack_mean_TERZER',\n",
        "    'isoscape_fullmodel_d18O_prec_REGRESSION']\n",
        "\n",
        "data = load_and_scale(grouped_fixed_fileset, columns_to_passthrough, columns_to_scale, columns_to_standardize)\n",
        "model = train_and_evaluate(data, \"fixed_all_0809_ensemble\", training_batch_size=3)\n",
        "model.save(get_model_save_location(\"fixed_all_0809_ensemble.tf\"), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location('fixed_all_0809_ensemble.pkl'))"
      ],
      "metadata": {
        "id": "V9_5iUkDVoCV",
        "outputId": "963ee9a6-90b2-4b0d-b4dc-6193fa5f6fb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: GTiff/GeoTIFF\n",
            "Size is 541 x 467 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.922043, 5.233124)\n",
            "Pixel Size = (0.08333, -0.08333)\n",
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('lat_standardizer', StandardScaler(), ['lat']),\n",
            "                                ('long_standardizer', StandardScaler(),\n",
            "                                 ['long']),\n",
            "                                ('VPD_standardizer', StandardScaler(), ['VPD']),\n",
            "                                ('RH_standardizer', StandardScaler(), ['RH']),\n",
            "                                ('PET_standardizer', StandardScaler(), ['PET']),\n",
            "                                ('DEM_standardizer', StandardScaler(), ['DEM']),\n",
            "                                ('PA_standardizer'...\n",
            "                                ('Mean Annual Temperature_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation']),\n",
            "                                ('Iso_Oxi_Stack_mean_TERZER_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Iso_Oxi_Stack_mean_TERZER']),\n",
            "                                ('isoscape_fullmodel_d18O_prec_REGRESSION_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['isoscape_fullmodel_d18O_prec_REGRESSION'])])\n",
            "==================\n",
            "fixed_all_0809_ensemble\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 14)]         0           []                               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 20)           300         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 20)           420         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " var_output (Dense)             (None, 1)            21          ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " mean_output (Dense)            (None, 1)            21          ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.multiply_1 (TFOpLambda  (None, 1)           0           ['var_output[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.multiply (TFOpLambda)  (None, 1)            0           ['mean_output[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 1)           0           ['tf.math.multiply_1[0][0]']     \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 1)           0           ['tf.math.multiply[0][0]']       \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1)            0           ['tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 2)            0           ['tf.__operators__.add[0][0]',   \n",
            "                                                                  'lambda[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 762\n",
            "Trainable params: 762\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5000\n",
            "23/23 [==============================] - 4s 75ms/step - loss: 2.0234 - val_loss: 7.4425\n",
            "Epoch 2/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.6286 - val_loss: 4.8898\n",
            "Epoch 3/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.3785 - val_loss: 4.4814\n",
            "Epoch 4/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.1335 - val_loss: 3.1618\n",
            "Epoch 5/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.2045 - val_loss: 3.6229\n",
            "Epoch 6/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.9549 - val_loss: 2.7275\n",
            "Epoch 7/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.9774 - val_loss: 2.4481\n",
            "Epoch 8/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.0262 - val_loss: 2.5888\n",
            "Epoch 9/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8828 - val_loss: 2.8397\n",
            "Epoch 10/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.9360 - val_loss: 2.7209\n",
            "Epoch 11/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8605 - val_loss: 2.2360\n",
            "Epoch 12/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.9481 - val_loss: 2.7933\n",
            "Epoch 13/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.9206 - val_loss: 2.7097\n",
            "Epoch 14/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.8946 - val_loss: 2.0909\n",
            "Epoch 15/5000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.9544 - val_loss: 1.8337\n",
            "Epoch 16/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8839 - val_loss: 1.7400\n",
            "Epoch 17/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8414 - val_loss: 1.7363\n",
            "Epoch 18/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8910 - val_loss: 1.7266\n",
            "Epoch 19/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8623 - val_loss: 2.2822\n",
            "Epoch 20/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.9013 - val_loss: 1.5273\n",
            "Epoch 21/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.9557 - val_loss: 1.7782\n",
            "Epoch 22/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8451 - val_loss: 1.6377\n",
            "Epoch 23/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8397 - val_loss: 2.1599\n",
            "Epoch 24/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8445 - val_loss: 1.7253\n",
            "Epoch 25/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9070 - val_loss: 1.9875\n",
            "Epoch 26/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8123 - val_loss: 1.4651\n",
            "Epoch 27/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8333 - val_loss: 2.1327\n",
            "Epoch 28/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8213 - val_loss: 1.8756\n",
            "Epoch 29/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8674 - val_loss: 1.6623\n",
            "Epoch 30/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8346 - val_loss: 2.2237\n",
            "Epoch 31/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8639 - val_loss: 1.6934\n",
            "Epoch 32/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8650 - val_loss: 1.7506\n",
            "Epoch 33/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8377 - val_loss: 1.6289\n",
            "Epoch 34/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8169 - val_loss: 1.8351\n",
            "Epoch 35/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8456 - val_loss: 1.5555\n",
            "Epoch 36/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8699 - val_loss: 1.5751\n",
            "Epoch 37/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8209 - val_loss: 1.7458\n",
            "Epoch 38/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9444 - val_loss: 1.7469\n",
            "Epoch 39/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7985 - val_loss: 1.5937\n",
            "Epoch 40/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8923 - val_loss: 1.6660\n",
            "Epoch 41/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7996 - val_loss: 1.4142\n",
            "Epoch 42/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8670 - val_loss: 1.1960\n",
            "Epoch 43/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8378 - val_loss: 1.6393\n",
            "Epoch 44/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8508 - val_loss: 1.4390\n",
            "Epoch 45/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8299 - val_loss: 1.6431\n",
            "Epoch 46/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8722 - val_loss: 1.4043\n",
            "Epoch 47/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7391 - val_loss: 1.7477\n",
            "Epoch 48/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8131 - val_loss: 1.3500\n",
            "Epoch 49/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7929 - val_loss: 1.5242\n",
            "Epoch 50/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8484 - val_loss: 1.6348\n",
            "Epoch 51/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7877 - val_loss: 1.4289\n",
            "Epoch 52/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8693 - val_loss: 1.5038\n",
            "Epoch 53/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7976 - val_loss: 1.7107\n",
            "Epoch 54/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7825 - val_loss: 1.4062\n",
            "Epoch 55/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8224 - val_loss: 1.7712\n",
            "Epoch 56/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7950 - val_loss: 1.5287\n",
            "Epoch 57/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8433 - val_loss: 1.7471\n",
            "Epoch 58/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8414 - val_loss: 2.0705\n",
            "Epoch 59/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8999 - val_loss: 1.2638\n",
            "Epoch 60/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8768 - val_loss: 1.4570\n",
            "Epoch 61/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8311 - val_loss: 1.2933\n",
            "Epoch 62/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8291 - val_loss: 1.2300\n",
            "Epoch 63/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9537 - val_loss: 1.6989\n",
            "Epoch 64/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8408 - val_loss: 1.0791\n",
            "Epoch 65/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7242 - val_loss: 1.9819\n",
            "Epoch 66/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8336 - val_loss: 2.2436\n",
            "Epoch 67/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7426 - val_loss: 1.9564\n",
            "Epoch 68/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7586 - val_loss: 1.3011\n",
            "Epoch 69/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7525 - val_loss: 1.6454\n",
            "Epoch 70/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7612 - val_loss: 1.4671\n",
            "Epoch 71/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8593 - val_loss: 1.7599\n",
            "Epoch 72/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8357 - val_loss: 1.5218\n",
            "Epoch 73/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8403 - val_loss: 1.3393\n",
            "Epoch 74/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8014 - val_loss: 1.6328\n",
            "Epoch 75/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8437 - val_loss: 1.1701\n",
            "Epoch 76/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8170 - val_loss: 1.5898\n",
            "Epoch 77/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7909 - val_loss: 1.3136\n",
            "Epoch 78/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7829 - val_loss: 1.4086\n",
            "Epoch 79/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8087 - val_loss: 1.3424\n",
            "Epoch 80/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7558 - val_loss: 1.4422\n",
            "Epoch 81/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8326 - val_loss: 1.3597\n",
            "Epoch 82/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7443 - val_loss: 1.2070\n",
            "Epoch 83/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8577 - val_loss: 1.4741\n",
            "Epoch 84/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7933 - val_loss: 1.6076\n",
            "Epoch 85/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8258 - val_loss: 1.4968\n",
            "Epoch 86/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8096 - val_loss: 1.7075\n",
            "Epoch 87/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9144 - val_loss: 1.8042\n",
            "Epoch 88/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7961 - val_loss: 1.4525\n",
            "Epoch 89/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8251 - val_loss: 1.1827\n",
            "Epoch 90/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7848 - val_loss: 1.5976\n",
            "Epoch 91/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8236 - val_loss: 1.3913\n",
            "Epoch 92/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7688 - val_loss: 1.0974\n",
            "Epoch 93/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8411 - val_loss: 1.8877\n",
            "Epoch 94/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8250 - val_loss: 1.7177\n",
            "Epoch 95/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7669 - val_loss: 1.6524\n",
            "Epoch 96/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7570 - val_loss: 1.8102\n",
            "Epoch 97/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8478 - val_loss: 1.3388\n",
            "Epoch 98/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8070 - val_loss: 1.7254\n",
            "Epoch 99/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8606 - val_loss: 1.5855\n",
            "Epoch 100/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7688 - val_loss: 1.2169\n",
            "Epoch 101/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8217 - val_loss: 1.1963\n",
            "Epoch 102/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8256 - val_loss: 1.4048\n",
            "Epoch 103/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7691 - val_loss: 1.4902\n",
            "Epoch 104/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8695 - val_loss: 1.7047\n",
            "Epoch 105/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8441 - val_loss: 1.3547\n",
            "Epoch 106/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8201 - val_loss: 1.4355\n",
            "Epoch 107/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8289 - val_loss: 1.3136\n",
            "Epoch 108/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8177 - val_loss: 1.2347\n",
            "Epoch 109/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8310 - val_loss: 2.0084\n",
            "Epoch 110/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7839 - val_loss: 1.5573\n",
            "Epoch 111/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7675 - val_loss: 1.2417\n",
            "Epoch 112/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7977 - val_loss: 1.1821\n",
            "Epoch 113/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7618 - val_loss: 1.6924\n",
            "Epoch 114/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8169 - val_loss: 1.4310\n",
            "Epoch 115/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8904 - val_loss: 1.6338\n",
            "Epoch 116/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8783 - val_loss: 1.1760\n",
            "Epoch 117/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8276 - val_loss: 1.1106\n",
            "Epoch 118/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8373 - val_loss: 1.8103\n",
            "Epoch 119/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7907 - val_loss: 1.2351\n",
            "Epoch 120/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8791 - val_loss: 1.2453\n",
            "Epoch 121/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8261 - val_loss: 1.1782\n",
            "Epoch 122/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7609 - val_loss: 1.4312\n",
            "Epoch 123/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8087 - val_loss: 1.4524\n",
            "Epoch 124/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7942 - val_loss: 1.5507\n",
            "Epoch 125/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8138 - val_loss: 1.6526\n",
            "Epoch 126/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7734 - val_loss: 1.5081\n",
            "Epoch 127/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8768 - val_loss: 1.7012\n",
            "Epoch 128/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7711 - val_loss: 1.1508\n",
            "Epoch 129/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7395 - val_loss: 1.1140\n",
            "Epoch 130/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8123 - val_loss: 1.4050\n",
            "Epoch 131/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7255 - val_loss: 1.2610\n",
            "Epoch 132/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8018 - val_loss: 1.6105\n",
            "Epoch 133/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7888 - val_loss: 1.1425\n",
            "Epoch 134/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8735 - val_loss: 1.4219\n",
            "Epoch 135/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8284 - val_loss: 1.2945\n",
            "Epoch 136/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8084 - val_loss: 1.2995\n",
            "Epoch 137/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7901 - val_loss: 1.7352\n",
            "Epoch 138/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7661 - val_loss: 1.7798\n",
            "Epoch 139/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7516 - val_loss: 1.5098\n",
            "Epoch 140/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7384 - val_loss: 1.3003\n",
            "Epoch 141/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7420 - val_loss: 1.5527\n",
            "Epoch 142/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8309 - val_loss: 1.6807\n",
            "Epoch 143/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7982 - val_loss: 1.4919\n",
            "Epoch 144/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8347 - val_loss: 1.4294\n",
            "Epoch 145/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8521 - val_loss: 1.4685\n",
            "Epoch 146/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8864 - val_loss: 1.4723\n",
            "Epoch 147/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7329 - val_loss: 1.5226\n",
            "Epoch 148/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7496 - val_loss: 1.8663\n",
            "Epoch 149/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8185 - val_loss: 1.5870\n",
            "Epoch 150/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7564 - val_loss: 1.3203\n",
            "Epoch 151/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7586 - val_loss: 1.4190\n",
            "Epoch 152/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7978 - val_loss: 1.8372\n",
            "Epoch 153/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7787 - val_loss: 1.4030\n",
            "Epoch 154/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7828 - val_loss: 1.8273\n",
            "Epoch 155/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8023 - val_loss: 1.2643\n",
            "Epoch 156/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7978 - val_loss: 1.5366\n",
            "Epoch 157/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7410 - val_loss: 1.3114\n",
            "Epoch 158/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8062 - val_loss: 1.3696\n",
            "Epoch 159/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8213 - val_loss: 1.3231\n",
            "Epoch 160/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7549 - val_loss: 1.4126\n",
            "Epoch 161/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8184 - val_loss: 1.7155\n",
            "Epoch 162/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7649 - val_loss: 1.4489\n",
            "Epoch 163/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8430 - val_loss: 1.1812\n",
            "Epoch 164/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8158 - val_loss: 1.6929\n",
            "Epoch 165/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8149 - val_loss: 1.2045\n",
            "Epoch 166/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7589 - val_loss: 1.2566\n",
            "Epoch 167/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7882 - val_loss: 1.5617\n",
            "Epoch 168/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7322 - val_loss: 1.3732\n",
            "Epoch 169/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8009 - val_loss: 1.4612\n",
            "Epoch 170/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8197 - val_loss: 1.4550\n",
            "Epoch 171/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8021 - val_loss: 1.2324\n",
            "Epoch 172/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7746 - val_loss: 1.4716\n",
            "Epoch 173/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8524 - val_loss: 1.3516\n",
            "Epoch 174/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8229 - val_loss: 1.6962\n",
            "Epoch 175/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7417 - val_loss: 1.4701\n",
            "Epoch 176/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7321 - val_loss: 1.6062\n",
            "Epoch 177/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7789 - val_loss: 1.5263\n",
            "Epoch 178/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7777 - val_loss: 1.3289\n",
            "Epoch 179/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7311 - val_loss: 1.2969\n",
            "Epoch 180/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7861 - val_loss: 1.6489\n",
            "Epoch 181/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7619 - val_loss: 1.5595\n",
            "Epoch 182/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7402 - val_loss: 1.5995\n",
            "Epoch 183/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7478 - val_loss: 1.4996\n",
            "Epoch 184/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7380 - val_loss: 1.6018\n",
            "Epoch 185/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7779 - val_loss: 1.2920\n",
            "Epoch 186/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7893 - val_loss: 1.3567\n",
            "Epoch 187/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7517 - val_loss: 1.5169\n",
            "Epoch 188/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8564 - val_loss: 2.4415\n",
            "Epoch 189/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7774 - val_loss: 1.3090\n",
            "Epoch 190/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8264 - val_loss: 1.2945\n",
            "Epoch 191/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8042 - val_loss: 1.7250\n",
            "Epoch 192/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8465 - val_loss: 1.3830\n",
            "Epoch 193/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7858 - val_loss: 1.3641\n",
            "Epoch 194/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7907 - val_loss: 1.6572\n",
            "Epoch 195/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8068 - val_loss: 1.4379\n",
            "Epoch 196/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8020 - val_loss: 1.3647\n",
            "Epoch 197/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7526 - val_loss: 2.7951\n",
            "Epoch 198/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7661 - val_loss: 1.5231\n",
            "Epoch 199/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7439 - val_loss: 1.9481\n",
            "Epoch 200/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7431 - val_loss: 1.6086\n",
            "Epoch 201/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8255 - val_loss: 1.9355\n",
            "Epoch 202/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7173 - val_loss: 1.5878\n",
            "Epoch 203/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7752 - val_loss: 1.5834\n",
            "Epoch 204/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7745 - val_loss: 1.5946\n",
            "Epoch 205/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8353 - val_loss: 1.7017\n",
            "Epoch 206/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7998 - val_loss: 1.4596\n",
            "Epoch 207/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7361 - val_loss: 1.3187\n",
            "Epoch 208/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7518 - val_loss: 1.3325\n",
            "Epoch 209/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7526 - val_loss: 1.2313\n",
            "Epoch 210/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7397 - val_loss: 1.2242\n",
            "Epoch 211/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8008 - val_loss: 1.2775\n",
            "Epoch 212/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8650 - val_loss: 2.1035\n",
            "Epoch 213/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7778 - val_loss: 1.6515\n",
            "Epoch 214/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7727 - val_loss: 1.3918\n",
            "Epoch 215/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7829 - val_loss: 1.4352\n",
            "Epoch 216/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7383 - val_loss: 2.6625\n",
            "Epoch 217/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8449 - val_loss: 1.8142\n",
            "Epoch 218/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7774 - val_loss: 1.6954\n",
            "Epoch 219/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7453 - val_loss: 1.6384\n",
            "Epoch 220/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7383 - val_loss: 2.2708\n",
            "Epoch 221/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7506 - val_loss: 1.4964\n",
            "Epoch 222/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7955 - val_loss: 1.7670\n",
            "Epoch 223/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8049 - val_loss: 1.5878\n",
            "Epoch 224/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7152 - val_loss: 1.3842\n",
            "Epoch 225/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7193 - val_loss: 1.6874\n",
            "Epoch 226/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7131 - val_loss: 1.6301\n",
            "Epoch 227/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7557 - val_loss: 1.7426\n",
            "Epoch 228/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7277 - val_loss: 1.3974\n",
            "Epoch 229/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8057 - val_loss: 2.4452\n",
            "Epoch 230/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7740 - val_loss: 1.8011\n",
            "Epoch 231/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7688 - val_loss: 1.9721\n",
            "Epoch 232/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8084 - val_loss: 1.7661\n",
            "Epoch 233/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7795 - val_loss: 1.5779\n",
            "Epoch 234/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8447 - val_loss: 2.3573\n",
            "Epoch 235/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7251 - val_loss: 1.6171\n",
            "Epoch 236/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8005 - val_loss: 2.1391\n",
            "Epoch 237/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7964 - val_loss: 1.4825\n",
            "Epoch 238/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7832 - val_loss: 1.5151\n",
            "Epoch 239/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7643 - val_loss: 1.8955\n",
            "Epoch 240/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7742 - val_loss: 1.6071\n",
            "Epoch 241/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7776 - val_loss: 1.4841\n",
            "Epoch 242/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7385 - val_loss: 1.6963\n",
            "Epoch 243/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7859 - val_loss: 1.5089\n",
            "Epoch 244/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7585 - val_loss: 1.9459\n",
            "Epoch 245/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7681 - val_loss: 1.2928\n",
            "Epoch 246/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7730 - val_loss: 1.6424\n",
            "Epoch 247/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7447 - val_loss: 1.6505\n",
            "Epoch 248/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.7300 - val_loss: 2.1615\n",
            "Epoch 249/5000\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.7563 - val_loss: 1.4418\n",
            "Epoch 250/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8100 - val_loss: 1.4278\n",
            "Epoch 251/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7554 - val_loss: 1.4901\n",
            "Epoch 252/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7431 - val_loss: 1.4471\n",
            "Epoch 253/5000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.8504 - val_loss: 1.6218\n",
            "Epoch 254/5000\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.7120 - val_loss: 1.5229\n",
            "Epoch 255/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7242 - val_loss: 1.7905\n",
            "Epoch 256/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8795 - val_loss: 1.8554\n",
            "Epoch 257/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7586 - val_loss: 1.4874\n",
            "Epoch 258/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7714 - val_loss: 1.6421\n",
            "Epoch 259/5000\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.7735 - val_loss: 1.5031\n",
            "Epoch 260/5000\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.7149 - val_loss: 1.3964\n",
            "Epoch 261/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7570 - val_loss: 1.5660\n",
            "Epoch 262/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7604 - val_loss: 1.4667\n",
            "Epoch 263/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7662 - val_loss: 1.9805\n",
            "Epoch 264/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7396 - val_loss: 1.1844\n",
            "Epoch 265/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7826 - val_loss: 1.4907\n",
            "Epoch 266/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7963 - val_loss: 1.4223\n",
            "Epoch 267/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7621 - val_loss: 1.3737\n",
            "Epoch 268/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8275 - val_loss: 1.2648\n",
            "Epoch 269/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7830 - val_loss: 1.2827\n",
            "Epoch 270/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7639 - val_loss: 1.2768\n",
            "Epoch 271/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7518 - val_loss: 1.6454\n",
            "Epoch 272/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7484 - val_loss: 1.4075\n",
            "Epoch 273/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7546 - val_loss: 1.4452\n",
            "Epoch 274/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8089 - val_loss: 1.3283\n",
            "Epoch 275/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7385 - val_loss: 1.1529\n",
            "Epoch 276/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7127 - val_loss: 1.4123\n",
            "Epoch 277/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7859 - val_loss: 1.6391\n",
            "Epoch 278/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7550 - val_loss: 1.1945\n",
            "Epoch 279/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7732 - val_loss: 1.5792\n",
            "Epoch 280/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7776 - val_loss: 1.4834\n",
            "Epoch 281/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7993 - val_loss: 2.0399\n",
            "Epoch 282/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7919 - val_loss: 2.1431\n",
            "Epoch 283/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8229 - val_loss: 1.4739\n",
            "Epoch 284/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7219 - val_loss: 1.6415\n",
            "Epoch 285/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8085 - val_loss: 1.3821\n",
            "Epoch 286/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8064 - val_loss: 1.5216\n",
            "Epoch 287/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7643 - val_loss: 1.3547\n",
            "Epoch 288/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7332 - val_loss: 1.5986\n",
            "Epoch 289/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7579 - val_loss: 2.1693\n",
            "Epoch 290/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7456 - val_loss: 1.7126\n",
            "Epoch 291/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8252 - val_loss: 2.1358\n",
            "Epoch 292/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7547 - val_loss: 1.8774\n",
            "Epoch 293/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7986 - val_loss: 1.6538\n",
            "Epoch 294/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8181 - val_loss: 1.5077\n",
            "Epoch 295/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7245 - val_loss: 1.3585\n",
            "Epoch 296/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7628 - val_loss: 1.5650\n",
            "Epoch 297/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7446 - val_loss: 1.5960\n",
            "Epoch 298/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7475 - val_loss: 1.7520\n",
            "Epoch 299/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7402 - val_loss: 1.8932\n",
            "Epoch 300/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7556 - val_loss: 1.4636\n",
            "Epoch 301/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7644 - val_loss: 1.3825\n",
            "Epoch 302/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7867 - val_loss: 1.3758\n",
            "Epoch 303/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7865 - val_loss: 1.7733\n",
            "Epoch 304/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7221 - val_loss: 1.3149\n",
            "Epoch 305/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7883 - val_loss: 1.4787\n",
            "Epoch 306/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7489 - val_loss: 1.6351\n",
            "Epoch 307/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8143 - val_loss: 1.8615\n",
            "Epoch 308/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7250 - val_loss: 2.0580\n",
            "Epoch 309/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7626 - val_loss: 1.5830\n",
            "Epoch 310/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8439 - val_loss: 1.8164\n",
            "Epoch 311/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7952 - val_loss: 1.4124\n",
            "Epoch 312/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7700 - val_loss: 1.7485\n",
            "Epoch 313/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8460 - val_loss: 1.5258\n",
            "Epoch 314/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7757 - val_loss: 1.5669\n",
            "Epoch 315/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7120 - val_loss: 1.5410\n",
            "Epoch 316/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6939 - val_loss: 1.7722\n",
            "Epoch 317/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7500 - val_loss: 1.6528\n",
            "Epoch 318/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8820 - val_loss: 1.5381\n",
            "Epoch 319/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7681 - val_loss: 1.3294\n",
            "Epoch 320/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7302 - val_loss: 1.4165\n",
            "Epoch 321/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6857 - val_loss: 1.7068\n",
            "Epoch 322/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7632 - val_loss: 1.5205\n",
            "Epoch 323/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7527 - val_loss: 1.5661\n",
            "Epoch 324/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7457 - val_loss: 1.8697\n",
            "Epoch 325/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7408 - val_loss: 1.4589\n",
            "Epoch 326/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7813 - val_loss: 1.4593\n",
            "Epoch 327/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7782 - val_loss: 1.4599\n",
            "Epoch 328/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7365 - val_loss: 1.6955\n",
            "Epoch 329/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7716 - val_loss: 1.7461\n",
            "Epoch 330/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7256 - val_loss: 2.1783\n",
            "Epoch 331/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7821 - val_loss: 2.2486\n",
            "Epoch 332/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7069 - val_loss: 1.6506\n",
            "Epoch 333/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7466 - val_loss: 1.6310\n",
            "Epoch 334/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7736 - val_loss: 1.4618\n",
            "Epoch 335/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7913 - val_loss: 1.9825\n",
            "Epoch 336/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7758 - val_loss: 2.2370\n",
            "Epoch 337/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7707 - val_loss: 1.4820\n",
            "Epoch 338/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7663 - val_loss: 1.6144\n",
            "Epoch 339/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8115 - val_loss: 1.5850\n",
            "Epoch 340/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7596 - val_loss: 2.4828\n",
            "Epoch 341/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7686 - val_loss: 1.5135\n",
            "Epoch 342/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7791 - val_loss: 1.5285\n",
            "Epoch 343/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8871 - val_loss: 1.4245\n",
            "Epoch 344/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8216 - val_loss: 1.5434\n",
            "Epoch 345/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8541 - val_loss: 1.4622\n",
            "Epoch 346/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8062 - val_loss: 2.0326\n",
            "Epoch 347/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7188 - val_loss: 1.4107\n",
            "Epoch 348/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7964 - val_loss: 1.7222\n",
            "Epoch 349/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6948 - val_loss: 1.6273\n",
            "Epoch 350/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7310 - val_loss: 1.8594\n",
            "Epoch 351/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7480 - val_loss: 1.6488\n",
            "Epoch 352/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8602 - val_loss: 1.5188\n",
            "Epoch 353/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7615 - val_loss: 1.8653\n",
            "Epoch 354/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7844 - val_loss: 1.4349\n",
            "Epoch 355/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7811 - val_loss: 1.5795\n",
            "Epoch 356/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7652 - val_loss: 1.7755\n",
            "Epoch 357/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7872 - val_loss: 1.5821\n",
            "Epoch 358/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7971 - val_loss: 1.5369\n",
            "Epoch 359/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7674 - val_loss: 1.6145\n",
            "Epoch 360/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8070 - val_loss: 1.7176\n",
            "Epoch 361/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6995 - val_loss: 1.6219\n",
            "Epoch 362/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7330 - val_loss: 1.4755\n",
            "Epoch 363/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6987 - val_loss: 1.6452\n",
            "Epoch 364/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7827 - val_loss: 1.8929\n",
            "Epoch 365/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7865 - val_loss: 1.6113\n",
            "Epoch 366/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7573 - val_loss: 1.9714\n",
            "Epoch 367/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7566 - val_loss: 1.5953\n",
            "Epoch 368/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7534 - val_loss: 1.6242\n",
            "Epoch 369/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9599 - val_loss: 1.6547\n",
            "Epoch 370/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6895 - val_loss: 2.1682\n",
            "Epoch 371/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8531 - val_loss: 1.7146\n",
            "Epoch 372/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7542 - val_loss: 1.5539\n",
            "Epoch 373/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7473 - val_loss: 1.8363\n",
            "Epoch 374/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8164 - val_loss: 1.5598\n",
            "Epoch 375/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7914 - val_loss: 1.3213\n",
            "Epoch 376/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7572 - val_loss: 1.3543\n",
            "Epoch 377/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7310 - val_loss: 1.9704\n",
            "Epoch 378/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7800 - val_loss: 1.9110\n",
            "Epoch 379/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7038 - val_loss: 1.7565\n",
            "Epoch 380/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7952 - val_loss: 1.5787\n",
            "Epoch 381/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7798 - val_loss: 2.9390\n",
            "Epoch 382/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8590 - val_loss: 1.7005\n",
            "Epoch 383/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7878 - val_loss: 1.5553\n",
            "Epoch 384/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7587 - val_loss: 1.8125\n",
            "Epoch 385/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7339 - val_loss: 1.7888\n",
            "Epoch 386/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8031 - val_loss: 2.0426\n",
            "Epoch 387/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7356 - val_loss: 2.5002\n",
            "Epoch 388/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7641 - val_loss: 1.6380\n",
            "Epoch 389/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7295 - val_loss: 2.0816\n",
            "Epoch 390/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7901 - val_loss: 1.8748\n",
            "Epoch 391/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7618 - val_loss: 1.8099\n",
            "Epoch 392/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7451 - val_loss: 1.5501\n",
            "Epoch 393/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8020 - val_loss: 1.9566\n",
            "Epoch 394/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8549 - val_loss: 1.9385\n",
            "Epoch 395/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7073 - val_loss: 1.8708\n",
            "Epoch 396/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7706 - val_loss: 1.4941\n",
            "Epoch 397/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7150 - val_loss: 1.6253\n",
            "Epoch 398/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7175 - val_loss: 1.7963\n",
            "Epoch 399/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7486 - val_loss: 1.7290\n",
            "Epoch 400/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7409 - val_loss: 1.7526\n",
            "Epoch 401/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7068 - val_loss: 1.9271\n",
            "Epoch 402/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7646 - val_loss: 1.5297\n",
            "Epoch 403/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7335 - val_loss: 1.5198\n",
            "Epoch 404/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7419 - val_loss: 1.8469\n",
            "Epoch 405/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6919 - val_loss: 1.5551\n",
            "Epoch 406/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7924 - val_loss: 1.5617\n",
            "Epoch 407/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8779 - val_loss: 1.9256\n",
            "Epoch 408/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7012 - val_loss: 1.5642\n",
            "Epoch 409/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7159 - val_loss: 1.8138\n",
            "Epoch 410/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7099 - val_loss: 1.3855\n",
            "Epoch 411/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7668 - val_loss: 1.5264\n",
            "Epoch 412/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7120 - val_loss: 1.6220\n",
            "Epoch 413/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7346 - val_loss: 1.7424\n",
            "Epoch 414/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8592 - val_loss: 1.6893\n",
            "Epoch 415/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7834 - val_loss: 2.0239\n",
            "Epoch 416/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7314 - val_loss: 2.0280\n",
            "Epoch 417/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7375 - val_loss: 1.8411\n",
            "Epoch 418/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7403 - val_loss: 2.5926\n",
            "Epoch 419/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6800 - val_loss: 1.4366\n",
            "Epoch 420/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6959 - val_loss: 1.4303\n",
            "Epoch 421/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7587 - val_loss: 1.6239\n",
            "Epoch 422/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8055 - val_loss: 1.4642\n",
            "Epoch 423/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6726 - val_loss: 1.6746\n",
            "Epoch 424/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8595 - val_loss: 1.3974\n",
            "Epoch 425/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7903 - val_loss: 1.7928\n",
            "Epoch 426/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7831 - val_loss: 1.7118\n",
            "Epoch 427/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7952 - val_loss: 1.9649\n",
            "Epoch 428/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7710 - val_loss: 1.4220\n",
            "Epoch 429/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6955 - val_loss: 1.7877\n",
            "Epoch 430/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7264 - val_loss: 2.3899\n",
            "Epoch 431/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8077 - val_loss: 1.6956\n",
            "Epoch 432/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7690 - val_loss: 1.5031\n",
            "Epoch 433/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7572 - val_loss: 1.5550\n",
            "Epoch 434/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7092 - val_loss: 1.4348\n",
            "Epoch 435/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7769 - val_loss: 1.5120\n",
            "Epoch 436/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6972 - val_loss: 1.4545\n",
            "Epoch 437/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6730 - val_loss: 1.9918\n",
            "Epoch 438/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8005 - val_loss: 1.7058\n",
            "Epoch 439/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7640 - val_loss: 1.3292\n",
            "Epoch 440/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7269 - val_loss: 1.8911\n",
            "Epoch 441/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7679 - val_loss: 1.9934\n",
            "Epoch 442/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7728 - val_loss: 1.6762\n",
            "Epoch 443/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7322 - val_loss: 1.8410\n",
            "Epoch 444/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7211 - val_loss: 1.5798\n",
            "Epoch 445/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7321 - val_loss: 1.8996\n",
            "Epoch 446/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7523 - val_loss: 1.8982\n",
            "Epoch 447/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6955 - val_loss: 1.8660\n",
            "Epoch 448/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7783 - val_loss: 1.6968\n",
            "Epoch 449/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7391 - val_loss: 1.6823\n",
            "Epoch 450/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7921 - val_loss: 1.8028\n",
            "Epoch 451/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8038 - val_loss: 2.0038\n",
            "Epoch 452/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7709 - val_loss: 2.0297\n",
            "Epoch 453/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8150 - val_loss: 1.9135\n",
            "Epoch 454/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7830 - val_loss: 1.6348\n",
            "Epoch 455/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7313 - val_loss: 1.7547\n",
            "Epoch 456/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7466 - val_loss: 1.7742\n",
            "Epoch 457/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7906 - val_loss: 1.5768\n",
            "Epoch 458/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7147 - val_loss: 1.6947\n",
            "Epoch 459/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8188 - val_loss: 1.7455\n",
            "Epoch 460/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7587 - val_loss: 1.8576\n",
            "Epoch 461/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7887 - val_loss: 1.9568\n",
            "Epoch 462/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7412 - val_loss: 2.2408\n",
            "Epoch 463/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8389 - val_loss: 1.6771\n",
            "Epoch 464/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7307 - val_loss: 1.9235\n",
            "Epoch 465/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7832 - val_loss: 1.7362\n",
            "Epoch 466/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7877 - val_loss: 1.7777\n",
            "Epoch 467/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7719 - val_loss: 1.9253\n",
            "Epoch 468/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8098 - val_loss: 1.8208\n",
            "Epoch 469/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7574 - val_loss: 1.5878\n",
            "Epoch 470/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7574 - val_loss: 1.7318\n",
            "Epoch 471/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8182 - val_loss: 1.7444\n",
            "Epoch 472/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8047 - val_loss: 1.6319\n",
            "Epoch 473/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7320 - val_loss: 1.6275\n",
            "Epoch 474/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7323 - val_loss: 1.6390\n",
            "Epoch 475/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7199 - val_loss: 1.9414\n",
            "Epoch 476/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7235 - val_loss: 1.5826\n",
            "Epoch 477/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7435 - val_loss: 1.8304\n",
            "Epoch 478/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7448 - val_loss: 1.4849\n",
            "Epoch 479/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7420 - val_loss: 2.0643\n",
            "Epoch 480/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7896 - val_loss: 1.8344\n",
            "Epoch 481/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7186 - val_loss: 1.7824\n",
            "Epoch 482/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6900 - val_loss: 1.8183\n",
            "Epoch 483/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7313 - val_loss: 1.8485\n",
            "Epoch 484/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7386 - val_loss: 1.8961\n",
            "Epoch 485/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7184 - val_loss: 2.1312\n",
            "Epoch 486/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7654 - val_loss: 1.6448\n",
            "Epoch 487/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7689 - val_loss: 1.9213\n",
            "Epoch 488/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7205 - val_loss: 1.6647\n",
            "Epoch 489/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7520 - val_loss: 1.7287\n",
            "Epoch 490/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6813 - val_loss: 1.6378\n",
            "Epoch 491/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7817 - val_loss: 1.8885\n",
            "Epoch 492/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7259 - val_loss: 1.7366\n",
            "Epoch 493/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7417 - val_loss: 2.2771\n",
            "Epoch 494/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7353 - val_loss: 1.4899\n",
            "Epoch 495/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7494 - val_loss: 1.9979\n",
            "Epoch 496/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7152 - val_loss: 1.4445\n",
            "Epoch 497/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6975 - val_loss: 1.6076\n",
            "Epoch 498/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7928 - val_loss: 1.9385\n",
            "Epoch 499/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8071 - val_loss: 1.7677\n",
            "Epoch 500/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7908 - val_loss: 1.6695\n",
            "Epoch 501/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7093 - val_loss: 2.2147\n",
            "Epoch 502/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7306 - val_loss: 1.8542\n",
            "Epoch 503/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7897 - val_loss: 2.5121\n",
            "Epoch 504/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8119 - val_loss: 1.5488\n",
            "Epoch 505/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7398 - val_loss: 2.2609\n",
            "Epoch 506/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7339 - val_loss: 2.0841\n",
            "Epoch 507/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7328 - val_loss: 1.7225\n",
            "Epoch 508/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7078 - val_loss: 1.6110\n",
            "Epoch 509/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8006 - val_loss: 1.7684\n",
            "Epoch 510/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7191 - val_loss: 1.6418\n",
            "Epoch 511/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7439 - val_loss: 1.9631\n",
            "Epoch 512/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8101 - val_loss: 1.6762\n",
            "Epoch 513/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7801 - val_loss: 2.1990\n",
            "Epoch 514/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7782 - val_loss: 1.9466\n",
            "Epoch 515/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.9209 - val_loss: 2.4247\n",
            "Epoch 516/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7048 - val_loss: 1.7163\n",
            "Epoch 517/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7819 - val_loss: 2.1003\n",
            "Epoch 518/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7702 - val_loss: 2.0885\n",
            "Epoch 519/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7711 - val_loss: 1.7459\n",
            "Epoch 520/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7181 - val_loss: 1.7320\n",
            "Epoch 521/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7406 - val_loss: 2.3382\n",
            "Epoch 522/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6865 - val_loss: 1.6762\n",
            "Epoch 523/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7249 - val_loss: 2.3565\n",
            "Epoch 524/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7231 - val_loss: 2.0460\n",
            "Epoch 525/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7916 - val_loss: 1.8204\n",
            "Epoch 526/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7577 - val_loss: 1.7702\n",
            "Epoch 527/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6916 - val_loss: 1.8978\n",
            "Epoch 528/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8337 - val_loss: 1.6472\n",
            "Epoch 529/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8356 - val_loss: 1.7805\n",
            "Epoch 530/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7381 - val_loss: 1.5812\n",
            "Epoch 531/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7360 - val_loss: 1.5630\n",
            "Epoch 532/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7884 - val_loss: 1.7461\n",
            "Epoch 533/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8053 - val_loss: 1.7407\n",
            "Epoch 534/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7866 - val_loss: 1.8539\n",
            "Epoch 535/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6491 - val_loss: 1.5696\n",
            "Epoch 536/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7363 - val_loss: 1.8406\n",
            "Epoch 537/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7240 - val_loss: 2.1318\n",
            "Epoch 538/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7319 - val_loss: 2.8040\n",
            "Epoch 539/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7705 - val_loss: 2.3286\n",
            "Epoch 540/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8045 - val_loss: 1.6791\n",
            "Epoch 541/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7614 - val_loss: 1.6738\n",
            "Epoch 542/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6711 - val_loss: 1.5234\n",
            "Epoch 543/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7481 - val_loss: 3.1396\n",
            "Epoch 544/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7523 - val_loss: 1.7910\n",
            "Epoch 545/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7121 - val_loss: 1.5724\n",
            "Epoch 546/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7794 - val_loss: 1.6056\n",
            "Epoch 547/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6789 - val_loss: 1.5170\n",
            "Epoch 548/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7394 - val_loss: 1.6961\n",
            "Epoch 549/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7675 - val_loss: 1.9224\n",
            "Epoch 550/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6545 - val_loss: 1.6976\n",
            "Epoch 551/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7264 - val_loss: 1.8924\n",
            "Epoch 552/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6808 - val_loss: 1.6691\n",
            "Epoch 553/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7206 - val_loss: 1.6939\n",
            "Epoch 554/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7375 - val_loss: 1.7934\n",
            "Epoch 555/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6845 - val_loss: 1.7095\n",
            "Epoch 556/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7682 - val_loss: 1.7919\n",
            "Epoch 557/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7480 - val_loss: 2.2151\n",
            "Epoch 558/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7607 - val_loss: 1.5395\n",
            "Epoch 559/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7405 - val_loss: 1.6383\n",
            "Epoch 560/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7034 - val_loss: 1.8166\n",
            "Epoch 561/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7374 - val_loss: 1.5289\n",
            "Epoch 562/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7000 - val_loss: 2.0622\n",
            "Epoch 563/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7462 - val_loss: 2.1389\n",
            "Epoch 564/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7169 - val_loss: 2.8673\n",
            "Epoch 565/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7035 - val_loss: 1.6759\n",
            "Epoch 566/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7327 - val_loss: 1.4753\n",
            "Epoch 567/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7185 - val_loss: 2.3502\n",
            "Epoch 568/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7827 - val_loss: 2.1909\n",
            "Epoch 569/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8184 - val_loss: 1.7889\n",
            "Epoch 570/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7367 - val_loss: 1.5771\n",
            "Epoch 571/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7367 - val_loss: 1.5345\n",
            "Epoch 572/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7459 - val_loss: 1.5368\n",
            "Epoch 573/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7225 - val_loss: 1.6230\n",
            "Epoch 574/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7864 - val_loss: 1.7147\n",
            "Epoch 575/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7754 - val_loss: 1.9261\n",
            "Epoch 576/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8053 - val_loss: 1.5546\n",
            "Epoch 577/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7678 - val_loss: 1.7092\n",
            "Epoch 578/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7706 - val_loss: 1.6596\n",
            "Epoch 579/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7268 - val_loss: 1.4982\n",
            "Epoch 580/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7583 - val_loss: 2.1346\n",
            "Epoch 581/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7394 - val_loss: 1.5922\n",
            "Epoch 582/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6944 - val_loss: 1.7700\n",
            "Epoch 583/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6751 - val_loss: 1.6864\n",
            "Epoch 584/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7454 - val_loss: 1.5757\n",
            "Epoch 585/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7519 - val_loss: 1.7503\n",
            "Epoch 586/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7592 - val_loss: 1.6993\n",
            "Epoch 587/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7472 - val_loss: 1.7263\n",
            "Epoch 588/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7793 - val_loss: 1.7503\n",
            "Epoch 589/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6991 - val_loss: 2.0187\n",
            "Epoch 590/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7649 - val_loss: 1.7368\n",
            "Epoch 591/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7195 - val_loss: 1.5416\n",
            "Epoch 592/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8146 - val_loss: 1.5777\n",
            "Epoch 593/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7290 - val_loss: 1.9105\n",
            "Epoch 594/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6779 - val_loss: 1.5497\n",
            "Epoch 595/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7101 - val_loss: 1.8299\n",
            "Epoch 596/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7725 - val_loss: 1.7451\n",
            "Epoch 597/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7510 - val_loss: 2.3468\n",
            "Epoch 598/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6725 - val_loss: 2.0223\n",
            "Epoch 599/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7713 - val_loss: 2.2179\n",
            "Epoch 600/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6708 - val_loss: 1.9801\n",
            "Epoch 601/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6769 - val_loss: 1.7358\n",
            "Epoch 602/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7815 - val_loss: 1.6246\n",
            "Epoch 603/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7215 - val_loss: 2.0842\n",
            "Epoch 604/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6849 - val_loss: 1.5519\n",
            "Epoch 605/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7486 - val_loss: 1.8275\n",
            "Epoch 606/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6904 - val_loss: 1.9713\n",
            "Epoch 607/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8397 - val_loss: 1.6814\n",
            "Epoch 608/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7259 - val_loss: 1.8636\n",
            "Epoch 609/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8215 - val_loss: 1.6934\n",
            "Epoch 610/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7454 - val_loss: 1.6884\n",
            "Epoch 611/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7431 - val_loss: 1.9793\n",
            "Epoch 612/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7754 - val_loss: 1.8085\n",
            "Epoch 613/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6688 - val_loss: 1.9633\n",
            "Epoch 614/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7655 - val_loss: 1.3628\n",
            "Epoch 615/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7811 - val_loss: 1.7809\n",
            "Epoch 616/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7699 - val_loss: 1.7231\n",
            "Epoch 617/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7493 - val_loss: 1.5170\n",
            "Epoch 618/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7904 - val_loss: 1.5797\n",
            "Epoch 619/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8809 - val_loss: 2.2856\n",
            "Epoch 620/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7322 - val_loss: 1.8016\n",
            "Epoch 621/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6959 - val_loss: 2.2583\n",
            "Epoch 622/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7143 - val_loss: 1.7920\n",
            "Epoch 623/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8806 - val_loss: 1.7149\n",
            "Epoch 624/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7135 - val_loss: 1.5180\n",
            "Epoch 625/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7546 - val_loss: 1.9116\n",
            "Epoch 626/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7205 - val_loss: 1.8216\n",
            "Epoch 627/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7344 - val_loss: 1.7372\n",
            "Epoch 628/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7124 - val_loss: 1.5761\n",
            "Epoch 629/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7672 - val_loss: 1.6310\n",
            "Epoch 630/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7474 - val_loss: 1.8742\n",
            "Epoch 631/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7308 - val_loss: 1.7499\n",
            "Epoch 632/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7262 - val_loss: 1.6373\n",
            "Epoch 633/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7866 - val_loss: 2.2048\n",
            "Epoch 634/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7395 - val_loss: 1.8107\n",
            "Epoch 635/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7141 - val_loss: 2.1246\n",
            "Epoch 636/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7400 - val_loss: 2.1120\n",
            "Epoch 637/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7569 - val_loss: 1.5154\n",
            "Epoch 638/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7273 - val_loss: 1.4943\n",
            "Epoch 639/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7186 - val_loss: 1.8346\n",
            "Epoch 640/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7422 - val_loss: 1.5133\n",
            "Epoch 641/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7351 - val_loss: 1.7373\n",
            "Epoch 642/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8279 - val_loss: 1.5222\n",
            "Epoch 643/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7285 - val_loss: 1.7011\n",
            "Epoch 644/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7121 - val_loss: 1.9793\n",
            "Epoch 645/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6718 - val_loss: 1.9802\n",
            "Epoch 646/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8144 - val_loss: 1.8287\n",
            "Epoch 647/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7329 - val_loss: 1.7415\n",
            "Epoch 648/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6807 - val_loss: 2.2605\n",
            "Epoch 649/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8007 - val_loss: 1.8065\n",
            "Epoch 650/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7235 - val_loss: 1.5935\n",
            "Epoch 651/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7775 - val_loss: 1.6855\n",
            "Epoch 652/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7256 - val_loss: 1.8360\n",
            "Epoch 653/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7347 - val_loss: 1.6659\n",
            "Epoch 654/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7496 - val_loss: 1.6657\n",
            "Epoch 655/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7154 - val_loss: 1.5897\n",
            "Epoch 656/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7039 - val_loss: 1.7058\n",
            "Epoch 657/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7425 - val_loss: 2.1724\n",
            "Epoch 658/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7440 - val_loss: 1.6861\n",
            "Epoch 659/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7747 - val_loss: 1.8535\n",
            "Epoch 660/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6648 - val_loss: 2.3377\n",
            "Epoch 661/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7281 - val_loss: 1.9485\n",
            "Epoch 662/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7958 - val_loss: 2.0920\n",
            "Epoch 663/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7316 - val_loss: 1.8529\n",
            "Epoch 664/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7252 - val_loss: 1.9384\n",
            "Epoch 665/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7180 - val_loss: 1.6917\n",
            "Epoch 666/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6898 - val_loss: 1.7547\n",
            "Epoch 667/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7140 - val_loss: 2.0909\n",
            "Epoch 668/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7123 - val_loss: 1.8755\n",
            "Epoch 669/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7529 - val_loss: 1.5587\n",
            "Epoch 670/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7021 - val_loss: 1.7174\n",
            "Epoch 671/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6903 - val_loss: 2.6462\n",
            "Epoch 672/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6620 - val_loss: 1.9096\n",
            "Epoch 673/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7486 - val_loss: 1.9407\n",
            "Epoch 674/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7757 - val_loss: 1.7917\n",
            "Epoch 675/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7093 - val_loss: 1.7003\n",
            "Epoch 676/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7491 - val_loss: 1.6305\n",
            "Epoch 677/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6526 - val_loss: 1.7323\n",
            "Epoch 678/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6913 - val_loss: 2.2789\n",
            "Epoch 679/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7934 - val_loss: 1.5767\n",
            "Epoch 680/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7356 - val_loss: 1.5993\n",
            "Epoch 681/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7187 - val_loss: 1.7236\n",
            "Epoch 682/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7509 - val_loss: 1.5852\n",
            "Epoch 683/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7430 - val_loss: 1.6627\n",
            "Epoch 684/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8004 - val_loss: 1.7740\n",
            "Epoch 685/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6935 - val_loss: 1.9599\n",
            "Epoch 686/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7751 - val_loss: 2.1337\n",
            "Epoch 687/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7118 - val_loss: 2.1512\n",
            "Epoch 688/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8035 - val_loss: 1.6207\n",
            "Epoch 689/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6718 - val_loss: 1.7236\n",
            "Epoch 690/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7300 - val_loss: 1.7537\n",
            "Epoch 691/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7132 - val_loss: 1.9147\n",
            "Epoch 692/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7502 - val_loss: 1.7638\n",
            "Epoch 693/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6738 - val_loss: 1.9398\n",
            "Epoch 694/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7312 - val_loss: 2.3226\n",
            "Epoch 695/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7406 - val_loss: 1.9231\n",
            "Epoch 696/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7176 - val_loss: 1.7992\n",
            "Epoch 697/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7441 - val_loss: 2.1977\n",
            "Epoch 698/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8417 - val_loss: 1.8266\n",
            "Epoch 699/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7351 - val_loss: 1.7705\n",
            "Epoch 700/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7896 - val_loss: 1.5645\n",
            "Epoch 701/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7831 - val_loss: 1.7971\n",
            "Epoch 702/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7613 - val_loss: 2.1672\n",
            "Epoch 703/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7421 - val_loss: 1.3677\n",
            "Epoch 704/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7730 - val_loss: 1.6747\n",
            "Epoch 705/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7401 - val_loss: 1.9326\n",
            "Epoch 706/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7171 - val_loss: 1.9578\n",
            "Epoch 707/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7243 - val_loss: 1.9416\n",
            "Epoch 708/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7415 - val_loss: 1.7344\n",
            "Epoch 709/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7266 - val_loss: 1.8206\n",
            "Epoch 710/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7253 - val_loss: 1.7698\n",
            "Epoch 711/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7350 - val_loss: 1.5527\n",
            "Epoch 712/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8083 - val_loss: 1.6223\n",
            "Epoch 713/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8175 - val_loss: 2.0309\n",
            "Epoch 714/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6888 - val_loss: 1.6165\n",
            "Epoch 715/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7403 - val_loss: 1.9305\n",
            "Epoch 716/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7772 - val_loss: 1.7021\n",
            "Epoch 717/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7580 - val_loss: 1.5847\n",
            "Epoch 718/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6933 - val_loss: 1.6598\n",
            "Epoch 719/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6948 - val_loss: 1.6558\n",
            "Epoch 720/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7167 - val_loss: 2.0103\n",
            "Epoch 721/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7332 - val_loss: 1.4330\n",
            "Epoch 722/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7934 - val_loss: 1.8945\n",
            "Epoch 723/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7057 - val_loss: 1.9146\n",
            "Epoch 724/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7833 - val_loss: 1.8289\n",
            "Epoch 725/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7528 - val_loss: 1.9595\n",
            "Epoch 726/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7760 - val_loss: 2.0132\n",
            "Epoch 727/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7004 - val_loss: 1.9966\n",
            "Epoch 728/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7393 - val_loss: 1.7007\n",
            "Epoch 729/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7576 - val_loss: 1.7803\n",
            "Epoch 730/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7477 - val_loss: 1.7754\n",
            "Epoch 731/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7379 - val_loss: 1.5834\n",
            "Epoch 732/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8275 - val_loss: 1.8706\n",
            "Epoch 733/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7763 - val_loss: 2.3809\n",
            "Epoch 734/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7354 - val_loss: 1.7848\n",
            "Epoch 735/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7155 - val_loss: 1.7180\n",
            "Epoch 736/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8189 - val_loss: 1.5456\n",
            "Epoch 737/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7554 - val_loss: 1.7185\n",
            "Epoch 738/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7656 - val_loss: 1.7813\n",
            "Epoch 739/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7592 - val_loss: 1.8091\n",
            "Epoch 740/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6917 - val_loss: 1.6964\n",
            "Epoch 741/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7440 - val_loss: 1.8889\n",
            "Epoch 742/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7050 - val_loss: 1.6046\n",
            "Epoch 743/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7227 - val_loss: 2.1195\n",
            "Epoch 744/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7145 - val_loss: 1.7890\n",
            "Epoch 745/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6837 - val_loss: 1.9247\n",
            "Epoch 746/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8468 - val_loss: 2.4455\n",
            "Epoch 747/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6848 - val_loss: 1.9598\n",
            "Epoch 748/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7403 - val_loss: 2.5100\n",
            "Epoch 749/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7536 - val_loss: 1.6344\n",
            "Epoch 750/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7006 - val_loss: 1.6183\n",
            "Epoch 751/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7370 - val_loss: 2.1338\n",
            "Epoch 752/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7298 - val_loss: 1.8490\n",
            "Epoch 753/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7620 - val_loss: 2.0098\n",
            "Epoch 754/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8001 - val_loss: 1.6875\n",
            "Epoch 755/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7391 - val_loss: 1.4916\n",
            "Epoch 756/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7238 - val_loss: 1.5819\n",
            "Epoch 757/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7183 - val_loss: 1.6555\n",
            "Epoch 758/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7379 - val_loss: 1.5637\n",
            "Epoch 759/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7542 - val_loss: 1.9505\n",
            "Epoch 760/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7160 - val_loss: 1.7766\n",
            "Epoch 761/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7453 - val_loss: 1.9609\n",
            "Epoch 762/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6683 - val_loss: 2.0943\n",
            "Epoch 763/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7525 - val_loss: 1.7884\n",
            "Epoch 764/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7370 - val_loss: 2.0975\n",
            "Epoch 765/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7822 - val_loss: 1.6364\n",
            "Epoch 766/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7113 - val_loss: 1.7645\n",
            "Epoch 767/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7281 - val_loss: 1.9426\n",
            "Epoch 768/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7344 - val_loss: 1.8551\n",
            "Epoch 769/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7635 - val_loss: 1.5495\n",
            "Epoch 770/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7552 - val_loss: 1.9510\n",
            "Epoch 771/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8282 - val_loss: 1.5567\n",
            "Epoch 772/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7541 - val_loss: 2.0414\n",
            "Epoch 773/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7193 - val_loss: 1.7094\n",
            "Epoch 774/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6882 - val_loss: 1.5694\n",
            "Epoch 775/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7370 - val_loss: 2.0275\n",
            "Epoch 776/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7549 - val_loss: 1.7121\n",
            "Epoch 777/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7345 - val_loss: 1.6100\n",
            "Epoch 778/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7760 - val_loss: 1.3991\n",
            "Epoch 779/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7656 - val_loss: 1.5472\n",
            "Epoch 780/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7220 - val_loss: 1.9296\n",
            "Epoch 781/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7486 - val_loss: 1.5931\n",
            "Epoch 782/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7460 - val_loss: 1.9009\n",
            "Epoch 783/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7334 - val_loss: 1.5037\n",
            "Epoch 784/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7276 - val_loss: 1.6190\n",
            "Epoch 785/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7460 - val_loss: 1.4134\n",
            "Epoch 786/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7266 - val_loss: 1.8111\n",
            "Epoch 787/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7738 - val_loss: 1.6027\n",
            "Epoch 788/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7599 - val_loss: 1.5248\n",
            "Epoch 789/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8202 - val_loss: 1.7403\n",
            "Epoch 790/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7023 - val_loss: 1.8575\n",
            "Epoch 791/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8492 - val_loss: 1.7147\n",
            "Epoch 792/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7735 - val_loss: 1.9343\n",
            "Epoch 793/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7405 - val_loss: 1.8623\n",
            "Epoch 794/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7172 - val_loss: 1.4530\n",
            "Epoch 795/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7300 - val_loss: 1.7786\n",
            "Epoch 796/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7917 - val_loss: 1.4496\n",
            "Epoch 797/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7042 - val_loss: 1.7330\n",
            "Epoch 798/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7485 - val_loss: 1.6072\n",
            "Epoch 799/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7381 - val_loss: 1.6753\n",
            "Epoch 800/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7351 - val_loss: 1.6986\n",
            "Epoch 801/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7438 - val_loss: 1.8272\n",
            "Epoch 802/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7404 - val_loss: 1.8097\n",
            "Epoch 803/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7841 - val_loss: 1.7348\n",
            "Epoch 804/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6907 - val_loss: 2.6052\n",
            "Epoch 805/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7356 - val_loss: 1.8344\n",
            "Epoch 806/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7500 - val_loss: 1.6293\n",
            "Epoch 807/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7704 - val_loss: 1.5647\n",
            "Epoch 808/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6982 - val_loss: 1.9723\n",
            "Epoch 809/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7518 - val_loss: 1.5660\n",
            "Epoch 810/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7095 - val_loss: 1.5497\n",
            "Epoch 811/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7547 - val_loss: 1.6293\n",
            "Epoch 812/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7402 - val_loss: 1.6177\n",
            "Epoch 813/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7175 - val_loss: 1.4885\n",
            "Epoch 814/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7402 - val_loss: 1.8422\n",
            "Epoch 815/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7580 - val_loss: 1.6351\n",
            "Epoch 816/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7038 - val_loss: 2.1429\n",
            "Epoch 817/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7434 - val_loss: 1.7342\n",
            "Epoch 818/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6918 - val_loss: 1.5084\n",
            "Epoch 819/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7658 - val_loss: 1.4202\n",
            "Epoch 820/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7658 - val_loss: 1.7597\n",
            "Epoch 821/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8017 - val_loss: 1.6838\n",
            "Epoch 822/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7535 - val_loss: 1.8635\n",
            "Epoch 823/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7026 - val_loss: 2.3991\n",
            "Epoch 824/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7810 - val_loss: 2.1119\n",
            "Epoch 825/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6927 - val_loss: 1.6855\n",
            "Epoch 826/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7036 - val_loss: 1.8199\n",
            "Epoch 827/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7170 - val_loss: 1.5863\n",
            "Epoch 828/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6896 - val_loss: 1.5439\n",
            "Epoch 829/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7441 - val_loss: 1.7629\n",
            "Epoch 830/5000\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.7414 - val_loss: 1.5857\n",
            "Epoch 831/5000\n",
            "23/23 [==============================] - 1s 25ms/step - loss: 0.7794 - val_loss: 1.4237\n",
            "Epoch 832/5000\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.8176 - val_loss: 1.4604\n",
            "Epoch 833/5000\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.7326 - val_loss: 1.8991\n",
            "Epoch 834/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7195 - val_loss: 1.4896\n",
            "Epoch 835/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7838 - val_loss: 1.4811\n",
            "Epoch 836/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8146 - val_loss: 1.4825\n",
            "Epoch 837/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7301 - val_loss: 1.5859\n",
            "Epoch 838/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7127 - val_loss: 1.3462\n",
            "Epoch 839/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7083 - val_loss: 1.3184\n",
            "Epoch 840/5000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.6801 - val_loss: 2.0797\n",
            "Epoch 841/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7587 - val_loss: 1.7537\n",
            "Epoch 842/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7523 - val_loss: 1.6957\n",
            "Epoch 843/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7774 - val_loss: 2.4907\n",
            "Epoch 844/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6558 - val_loss: 2.1000\n",
            "Epoch 845/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6907 - val_loss: 2.2229\n",
            "Epoch 846/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7698 - val_loss: 1.5030\n",
            "Epoch 847/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7468 - val_loss: 1.7047\n",
            "Epoch 848/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8230 - val_loss: 1.9443\n",
            "Epoch 849/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7245 - val_loss: 1.8324\n",
            "Epoch 850/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6886 - val_loss: 1.5524\n",
            "Epoch 851/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7103 - val_loss: 1.7946\n",
            "Epoch 852/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7416 - val_loss: 1.6290\n",
            "Epoch 853/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7104 - val_loss: 1.8769\n",
            "Epoch 854/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6950 - val_loss: 1.6453\n",
            "Epoch 855/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7177 - val_loss: 1.8846\n",
            "Epoch 856/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7005 - val_loss: 1.6218\n",
            "Epoch 857/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7521 - val_loss: 1.6522\n",
            "Epoch 858/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7833 - val_loss: 1.6327\n",
            "Epoch 859/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7890 - val_loss: 1.6890\n",
            "Epoch 860/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7282 - val_loss: 1.4501\n",
            "Epoch 861/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7028 - val_loss: 1.8910\n",
            "Epoch 862/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7561 - val_loss: 1.8666\n",
            "Epoch 863/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7231 - val_loss: 1.9148\n",
            "Epoch 864/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7350 - val_loss: 1.8477\n",
            "Epoch 865/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7921 - val_loss: 2.2307\n",
            "Epoch 866/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7893 - val_loss: 1.8487\n",
            "Epoch 867/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7076 - val_loss: 2.0730\n",
            "Epoch 868/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7112 - val_loss: 1.8457\n",
            "Epoch 869/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7690 - val_loss: 1.6148\n",
            "Epoch 870/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7890 - val_loss: 1.9058\n",
            "Epoch 871/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7405 - val_loss: 1.9891\n",
            "Epoch 872/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7035 - val_loss: 1.5307\n",
            "Epoch 873/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7488 - val_loss: 1.7000\n",
            "Epoch 874/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7362 - val_loss: 1.6420\n",
            "Epoch 875/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7480 - val_loss: 1.6790\n",
            "Epoch 876/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6797 - val_loss: 1.9975\n",
            "Epoch 877/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6889 - val_loss: 2.0688\n",
            "Epoch 878/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8461 - val_loss: 1.8986\n",
            "Epoch 879/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6650 - val_loss: 1.9185\n",
            "Epoch 880/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7146 - val_loss: 1.6427\n",
            "Epoch 881/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6801 - val_loss: 1.5586\n",
            "Epoch 882/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7638 - val_loss: 1.7165\n",
            "Epoch 883/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7226 - val_loss: 1.8051\n",
            "Epoch 884/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7255 - val_loss: 1.7138\n",
            "Epoch 885/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6749 - val_loss: 1.9767\n",
            "Epoch 886/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7228 - val_loss: 1.8223\n",
            "Epoch 887/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7328 - val_loss: 1.8198\n",
            "Epoch 888/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7214 - val_loss: 1.7095\n",
            "Epoch 889/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6969 - val_loss: 3.3249\n",
            "Epoch 890/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7856 - val_loss: 1.9447\n",
            "Epoch 891/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7221 - val_loss: 1.6383\n",
            "Epoch 892/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6837 - val_loss: 2.0420\n",
            "Epoch 893/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7992 - val_loss: 1.8167\n",
            "Epoch 894/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6884 - val_loss: 1.7172\n",
            "Epoch 895/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7074 - val_loss: 1.8144\n",
            "Epoch 896/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7013 - val_loss: 2.2145\n",
            "Epoch 897/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7354 - val_loss: 1.6111\n",
            "Epoch 898/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6922 - val_loss: 1.6452\n",
            "Epoch 899/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7581 - val_loss: 2.9148\n",
            "Epoch 900/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7327 - val_loss: 1.6704\n",
            "Epoch 901/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8311 - val_loss: 1.6812\n",
            "Epoch 902/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7453 - val_loss: 2.3724\n",
            "Epoch 903/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7360 - val_loss: 1.7312\n",
            "Epoch 904/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7111 - val_loss: 1.9002\n",
            "Epoch 905/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6671 - val_loss: 3.1966\n",
            "Epoch 906/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7325 - val_loss: 2.7454\n",
            "Epoch 907/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8140 - val_loss: 1.7122\n",
            "Epoch 908/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7965 - val_loss: 1.8215\n",
            "Epoch 909/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6519 - val_loss: 1.5659\n",
            "Epoch 910/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7241 - val_loss: 1.4830\n",
            "Epoch 911/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6879 - val_loss: 1.5727\n",
            "Epoch 912/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8090 - val_loss: 1.5269\n",
            "Epoch 913/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6930 - val_loss: 1.6716\n",
            "Epoch 914/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7352 - val_loss: 1.7932\n",
            "Epoch 915/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7156 - val_loss: 2.2808\n",
            "Epoch 916/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6904 - val_loss: 1.6624\n",
            "Epoch 917/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7350 - val_loss: 1.5748\n",
            "Epoch 918/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6863 - val_loss: 1.7239\n",
            "Epoch 919/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6999 - val_loss: 1.7082\n",
            "Epoch 920/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6400 - val_loss: 1.8704\n",
            "Epoch 921/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6980 - val_loss: 1.8287\n",
            "Epoch 922/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7604 - val_loss: 1.7640\n",
            "Epoch 923/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7072 - val_loss: 2.0764\n",
            "Epoch 924/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7074 - val_loss: 1.9687\n",
            "Epoch 925/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7984 - val_loss: 1.5161\n",
            "Epoch 926/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7378 - val_loss: 1.8715\n",
            "Epoch 927/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8099 - val_loss: 1.4715\n",
            "Epoch 928/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6607 - val_loss: 1.6128\n",
            "Epoch 929/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7284 - val_loss: 1.8737\n",
            "Epoch 930/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7230 - val_loss: 2.0036\n",
            "Epoch 931/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7529 - val_loss: 1.8010\n",
            "Epoch 932/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7613 - val_loss: 1.5986\n",
            "Epoch 933/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6600 - val_loss: 1.6918\n",
            "Epoch 934/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7492 - val_loss: 2.0640\n",
            "Epoch 935/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7471 - val_loss: 2.3249\n",
            "Epoch 936/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7128 - val_loss: 1.6880\n",
            "Epoch 937/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7206 - val_loss: 1.8298\n",
            "Epoch 938/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8032 - val_loss: 1.8024\n",
            "Epoch 939/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8029 - val_loss: 1.8750\n",
            "Epoch 940/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7799 - val_loss: 2.2892\n",
            "Epoch 941/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6632 - val_loss: 1.9189\n",
            "Epoch 942/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7099 - val_loss: 1.6361\n",
            "Epoch 943/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6734 - val_loss: 1.9564\n",
            "Epoch 944/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7095 - val_loss: 1.6818\n",
            "Epoch 945/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7215 - val_loss: 1.6721\n",
            "Epoch 946/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7087 - val_loss: 2.0769\n",
            "Epoch 947/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7524 - val_loss: 1.9137\n",
            "Epoch 948/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7706 - val_loss: 2.4123\n",
            "Epoch 949/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7666 - val_loss: 1.8737\n",
            "Epoch 950/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6984 - val_loss: 1.9117\n",
            "Epoch 951/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7154 - val_loss: 1.6854\n",
            "Epoch 952/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6744 - val_loss: 1.8289\n",
            "Epoch 953/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7111 - val_loss: 1.9991\n",
            "Epoch 954/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7576 - val_loss: 1.7090\n",
            "Epoch 955/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7619 - val_loss: 1.3908\n",
            "Epoch 956/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8375 - val_loss: 1.7439\n",
            "Epoch 957/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6627 - val_loss: 2.3341\n",
            "Epoch 958/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7438 - val_loss: 1.5748\n",
            "Epoch 959/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7576 - val_loss: 1.7359\n",
            "Epoch 960/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7793 - val_loss: 1.6380\n",
            "Epoch 961/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7200 - val_loss: 1.3920\n",
            "Epoch 962/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7343 - val_loss: 1.7654\n",
            "Epoch 963/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6918 - val_loss: 2.1732\n",
            "Epoch 964/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8528 - val_loss: 1.7702\n",
            "Epoch 965/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7319 - val_loss: 1.8310\n",
            "Epoch 966/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7334 - val_loss: 1.5953\n",
            "Epoch 967/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7216 - val_loss: 1.8222\n",
            "Epoch 968/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7938 - val_loss: 1.9547\n",
            "Epoch 969/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7506 - val_loss: 1.7070\n",
            "Epoch 970/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7121 - val_loss: 1.8958\n",
            "Epoch 971/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7082 - val_loss: 2.2924\n",
            "Epoch 972/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7473 - val_loss: 1.6411\n",
            "Epoch 973/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7583 - val_loss: 1.5592\n",
            "Epoch 974/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7271 - val_loss: 1.6109\n",
            "Epoch 975/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7394 - val_loss: 1.7392\n",
            "Epoch 976/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7466 - val_loss: 1.6357\n",
            "Epoch 977/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7337 - val_loss: 1.7766\n",
            "Epoch 978/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7459 - val_loss: 2.0929\n",
            "Epoch 979/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7269 - val_loss: 1.9943\n",
            "Epoch 980/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7177 - val_loss: 2.0198\n",
            "Epoch 981/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7093 - val_loss: 1.9397\n",
            "Epoch 982/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6969 - val_loss: 1.8153\n",
            "Epoch 983/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7743 - val_loss: 1.4963\n",
            "Epoch 984/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7405 - val_loss: 1.6558\n",
            "Epoch 985/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7400 - val_loss: 1.6329\n",
            "Epoch 986/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7167 - val_loss: 1.6297\n",
            "Epoch 987/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7394 - val_loss: 2.6701\n",
            "Epoch 988/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7406 - val_loss: 1.6060\n",
            "Epoch 989/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7108 - val_loss: 1.6196\n",
            "Epoch 990/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7215 - val_loss: 2.1128\n",
            "Epoch 991/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6615 - val_loss: 1.8781\n",
            "Epoch 992/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7328 - val_loss: 2.2654\n",
            "Epoch 993/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7721 - val_loss: 1.6004\n",
            "Epoch 994/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7469 - val_loss: 1.6471\n",
            "Epoch 995/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7341 - val_loss: 1.6762\n",
            "Epoch 996/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6985 - val_loss: 1.4555\n",
            "Epoch 997/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7511 - val_loss: 1.7294\n",
            "Epoch 998/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7664 - val_loss: 2.0269\n",
            "Epoch 999/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7270 - val_loss: 2.1216\n",
            "Epoch 1000/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6625 - val_loss: 2.0514\n",
            "Epoch 1001/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6551 - val_loss: 1.7706\n",
            "Epoch 1002/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7502 - val_loss: 1.6235\n",
            "Epoch 1003/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7632 - val_loss: 1.7654\n",
            "Epoch 1004/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7053 - val_loss: 2.1734\n",
            "Epoch 1005/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6652 - val_loss: 1.3579\n",
            "Epoch 1006/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7120 - val_loss: 1.4858\n",
            "Epoch 1007/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6839 - val_loss: 2.1941\n",
            "Epoch 1008/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7300 - val_loss: 1.7667\n",
            "Epoch 1009/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7519 - val_loss: 1.6461\n",
            "Epoch 1010/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6916 - val_loss: 1.6356\n",
            "Epoch 1011/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6930 - val_loss: 1.5155\n",
            "Epoch 1012/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7187 - val_loss: 1.7857\n",
            "Epoch 1013/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7148 - val_loss: 1.9112\n",
            "Epoch 1014/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7214 - val_loss: 1.5596\n",
            "Epoch 1015/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7408 - val_loss: 1.8163\n",
            "Epoch 1016/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7674 - val_loss: 1.6460\n",
            "Epoch 1017/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6778 - val_loss: 1.8080\n",
            "Epoch 1018/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7496 - val_loss: 1.7561\n",
            "Epoch 1019/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7482 - val_loss: 1.3719\n",
            "Epoch 1020/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6974 - val_loss: 1.5321\n",
            "Epoch 1021/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7280 - val_loss: 2.0016\n",
            "Epoch 1022/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6917 - val_loss: 2.6094\n",
            "Epoch 1023/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7128 - val_loss: 1.6868\n",
            "Epoch 1024/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7676 - val_loss: 2.1094\n",
            "Epoch 1025/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7542 - val_loss: 2.1265\n",
            "Epoch 1026/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6796 - val_loss: 2.2239\n",
            "Epoch 1027/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7111 - val_loss: 1.8372\n",
            "Epoch 1028/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7382 - val_loss: 1.2308\n",
            "Epoch 1029/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7477 - val_loss: 1.5235\n",
            "Epoch 1030/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7239 - val_loss: 1.4838\n",
            "Epoch 1031/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7098 - val_loss: 1.9136\n",
            "Epoch 1032/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6832 - val_loss: 1.7382\n",
            "Epoch 1033/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7308 - val_loss: 1.5798\n",
            "Epoch 1034/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7200 - val_loss: 1.6957\n",
            "Epoch 1035/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7627 - val_loss: 1.5064\n",
            "Epoch 1036/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7017 - val_loss: 1.4772\n",
            "Epoch 1037/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7502 - val_loss: 1.5443\n",
            "Epoch 1038/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7030 - val_loss: 1.6798\n",
            "Epoch 1039/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7318 - val_loss: 1.6420\n",
            "Epoch 1040/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7358 - val_loss: 1.7822\n",
            "Epoch 1041/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7583 - val_loss: 2.3212\n",
            "Epoch 1042/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7211 - val_loss: 1.6574\n",
            "Epoch 1043/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6897 - val_loss: 1.5930\n",
            "Epoch 1044/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7346 - val_loss: 1.7517\n",
            "Epoch 1045/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8047 - val_loss: 1.6576\n",
            "Epoch 1046/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7177 - val_loss: 1.8892\n",
            "Epoch 1047/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6923 - val_loss: 1.8015\n",
            "Epoch 1048/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7353 - val_loss: 2.0316\n",
            "Epoch 1049/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8335 - val_loss: 1.9182\n",
            "Epoch 1050/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7688 - val_loss: 1.7879\n",
            "Epoch 1051/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6955 - val_loss: 1.5436\n",
            "Epoch 1052/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7119 - val_loss: 1.8239\n",
            "Epoch 1053/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7353 - val_loss: 1.6238\n",
            "Epoch 1054/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7527 - val_loss: 2.0182\n",
            "Epoch 1055/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7570 - val_loss: 1.7272\n",
            "Epoch 1056/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7577 - val_loss: 1.3132\n",
            "Epoch 1057/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7268 - val_loss: 1.4841\n",
            "Epoch 1058/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7547 - val_loss: 1.9603\n",
            "Epoch 1059/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6641 - val_loss: 1.6503\n",
            "Epoch 1060/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8983 - val_loss: 1.4539\n",
            "Epoch 1061/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7076 - val_loss: 1.5378\n",
            "Epoch 1062/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6952 - val_loss: 1.9550\n",
            "Epoch 1063/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7249 - val_loss: 1.6858\n",
            "Epoch 1064/5000\n",
            "17/23 [=====================>........] - ETA: 0s - loss: 0.8475Restoring model weights from the end of the best epoch: 64.\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7973 - val_loss: 1.7784\n",
            "Epoch 1064: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-b383c653ffcb>:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 1.0791453123092651\n",
            "Train loss: 0.840813934803009\n",
            "Test loss: 1.4079772233963013\n",
            "dO18 RMSE: 1.6365471565283325\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       24.042000           0.345970\n",
            "1       25.240000           0.035950\n",
            "2       25.782000           0.372220\n",
            "3       25.076000           0.230280\n",
            "4       25.966000           0.172480\n",
            "5       27.434000           0.501030\n",
            "6       28.156000           0.999030\n",
            "7       26.836000           0.120880\n",
            "8       28.180000           0.778250\n",
            "9       26.834000           0.094930\n",
            "10      26.644000           0.488430\n",
            "11      26.772000           0.373370\n",
            "12      27.684280           1.216389\n",
            "13      27.403235           0.892280\n",
            "14      24.777670           0.736571\n",
            "15      25.551850           0.312355\n",
            "16      25.115885           0.033519\n",
            "17      25.987815           5.280825\n",
            "18      24.132031           0.389042\n",
            "19      24.898000           0.236070\n",
            "20      23.944000           0.158130\n",
            "21      26.018000           0.964170\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       25.885023           2.056505\n",
            "1       25.885023           2.056505\n",
            "2       25.885023           2.056505\n",
            "3       25.885023           2.056505\n",
            "4       25.885023           2.056505\n",
            "5       25.055304           1.553453\n",
            "6       25.117865           1.591347\n",
            "7       25.117865           1.591347\n",
            "8       25.117865           1.591347\n",
            "9       25.117865           1.591347\n",
            "10      25.117865           1.591347\n",
            "11      25.117865           1.591347\n",
            "12      25.055304           1.553453\n",
            "13      25.117865           1.591347\n",
            "14      24.837622           1.950924\n",
            "15      24.998631           1.989461\n",
            "16      25.060457           2.004933\n",
            "17      24.889725           1.963212\n",
            "18      25.025286           1.995988\n",
            "19      25.885023           2.056505\n",
            "20      25.885023           2.056505\n",
            "21      25.885023           2.056505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/MyDrive/amazon_rainforest_files/variational/model/fixed_all_0809_ensemble.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Grouped fixed, ablating other columns besides kriging"
      ],
      "metadata": {
        "id": "h1uuaygyDvXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_fixed_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_train_fixed_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_test_fixed_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_validation_fixed_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_passthrough = [\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "columns_to_scale = []\n",
        "columns_to_standardize =  ['lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature','Mean Annual Precipitation']\n",
        "\n",
        "data = load_and_scale(grouped_fixed_fileset, columns_to_passthrough, columns_to_scale, columns_to_standardize)\n",
        "model = train_and_evaluate(data, 'fixed_ablated_0809_ensemble', training_batch_size=3)\n",
        "model.save(get_model_save_location('fixed_ablated_0809_ensemble.tf'), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location('fixed_ablated_0809_ensemble.pkl'))"
      ],
      "metadata": {
        "id": "WQ60MVzlD5DJ",
        "outputId": "a9926f10-a967-46dc-9412-daa6474d5412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: GTiff/GeoTIFF\n",
            "Size is 541 x 467 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.922043, 5.233124)\n",
            "Pixel Size = (0.08333, -0.08333)\n",
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('lat_standardizer', StandardScaler(), ['lat']),\n",
            "                                ('long_standardizer', StandardScaler(),\n",
            "                                 ['long']),\n",
            "                                ('VPD_standardizer', StandardScaler(), ['VPD']),\n",
            "                                ('RH_standardizer', StandardScaler(), ['RH']),\n",
            "                                ('PET_standardizer', StandardScaler(), ['PET']),\n",
            "                                ('DEM_standardizer', StandardScaler(), ['DEM']),\n",
            "                                ('PA_standardizer', StandardScaler(), ['PA']),\n",
            "                                ('Mean Annual Temperature_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation'])])\n",
            "==================\n",
            "fixed_ablated_0809_ensemble\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 12)]         0           []                               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 20)           260         ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 20)           420         ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " var_output (Dense)             (None, 1)            21          ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " mean_output (Dense)            (None, 1)            21          ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.multiply_3 (TFOpLambda  (None, 1)           0           ['var_output[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.multiply_2 (TFOpLambda  (None, 1)           0           ['mean_output[0][0]']            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TFOpLa  (None, 1)           0           ['tf.math.multiply_3[0][0]']     \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 1)           0           ['tf.math.multiply_2[0][0]']     \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1)            0           ['tf.__operators__.add_3[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 2)            0           ['tf.__operators__.add_2[0][0]', \n",
            "                                                                  'lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 722\n",
            "Trainable params: 722\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5000\n",
            "23/23 [==============================] - 3s 48ms/step - loss: 4.7392 - val_loss: 11.0338\n",
            "Epoch 2/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 3.5638 - val_loss: 7.0464\n",
            "Epoch 3/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 2.6808 - val_loss: 6.6707\n",
            "Epoch 4/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 2.0053 - val_loss: 3.8388\n",
            "Epoch 5/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1453 - val_loss: 5.3931\n",
            "Epoch 6/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 1.5012 - val_loss: 4.0975\n",
            "Epoch 7/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 1.4439 - val_loss: 3.0280\n",
            "Epoch 8/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.5400 - val_loss: 3.5306\n",
            "Epoch 9/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2333 - val_loss: 4.7076\n",
            "Epoch 10/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 1.3120 - val_loss: 4.0954\n",
            "Epoch 11/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 1.1949 - val_loss: 2.8039\n",
            "Epoch 12/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 1.2830 - val_loss: 4.3126\n",
            "Epoch 13/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.1743 - val_loss: 4.5538\n",
            "Epoch 14/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 1.0777 - val_loss: 3.1661\n",
            "Epoch 15/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 1.2264 - val_loss: 2.2506\n",
            "Epoch 16/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 1.0953 - val_loss: 1.8837\n",
            "Epoch 17/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 1.0024 - val_loss: 2.1231\n",
            "Epoch 18/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.0759 - val_loss: 2.0359\n",
            "Epoch 19/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.9842 - val_loss: 3.2269\n",
            "Epoch 20/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 1.0641 - val_loss: 1.6337\n",
            "Epoch 21/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.1017 - val_loss: 2.1662\n",
            "Epoch 22/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9011 - val_loss: 2.0637\n",
            "Epoch 23/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9396 - val_loss: 2.4578\n",
            "Epoch 24/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9461 - val_loss: 1.8544\n",
            "Epoch 25/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 1.0513 - val_loss: 2.4433\n",
            "Epoch 26/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8716 - val_loss: 1.5424\n",
            "Epoch 27/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9208 - val_loss: 2.8596\n",
            "Epoch 28/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8631 - val_loss: 2.3644\n",
            "Epoch 29/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9301 - val_loss: 1.8584\n",
            "Epoch 30/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8967 - val_loss: 2.7241\n",
            "Epoch 31/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9149 - val_loss: 1.8958\n",
            "Epoch 32/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9164 - val_loss: 1.8031\n",
            "Epoch 33/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9181 - val_loss: 1.6633\n",
            "Epoch 34/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8672 - val_loss: 1.9439\n",
            "Epoch 35/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9019 - val_loss: 1.5923\n",
            "Epoch 36/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8914 - val_loss: 1.6326\n",
            "Epoch 37/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8447 - val_loss: 1.9693\n",
            "Epoch 38/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9975 - val_loss: 1.8136\n",
            "Epoch 39/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8194 - val_loss: 1.7118\n",
            "Epoch 40/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9306 - val_loss: 1.7841\n",
            "Epoch 41/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8078 - val_loss: 1.4116\n",
            "Epoch 42/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.9057 - val_loss: 1.2546\n",
            "Epoch 43/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8704 - val_loss: 1.7719\n",
            "Epoch 44/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8796 - val_loss: 1.5251\n",
            "Epoch 45/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8600 - val_loss: 1.9035\n",
            "Epoch 46/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8880 - val_loss: 1.4561\n",
            "Epoch 47/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7531 - val_loss: 1.9240\n",
            "Epoch 48/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8331 - val_loss: 1.3333\n",
            "Epoch 49/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8026 - val_loss: 1.6183\n",
            "Epoch 50/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8724 - val_loss: 1.7894\n",
            "Epoch 51/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7997 - val_loss: 1.4865\n",
            "Epoch 52/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9148 - val_loss: 1.5915\n",
            "Epoch 53/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8103 - val_loss: 1.8247\n",
            "Epoch 54/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8025 - val_loss: 1.5033\n",
            "Epoch 55/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8339 - val_loss: 1.8468\n",
            "Epoch 56/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8228 - val_loss: 1.6269\n",
            "Epoch 57/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8658 - val_loss: 1.8980\n",
            "Epoch 58/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8584 - val_loss: 2.2483\n",
            "Epoch 59/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9278 - val_loss: 1.3396\n",
            "Epoch 60/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9160 - val_loss: 1.5826\n",
            "Epoch 61/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8425 - val_loss: 1.3759\n",
            "Epoch 62/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8497 - val_loss: 1.2728\n",
            "Epoch 63/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.0020 - val_loss: 1.8351\n",
            "Epoch 64/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8723 - val_loss: 1.1088\n",
            "Epoch 65/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7313 - val_loss: 2.2052\n",
            "Epoch 66/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8325 - val_loss: 2.5436\n",
            "Epoch 67/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7493 - val_loss: 2.1850\n",
            "Epoch 68/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7744 - val_loss: 1.3710\n",
            "Epoch 69/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7696 - val_loss: 1.7584\n",
            "Epoch 70/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7766 - val_loss: 1.5795\n",
            "Epoch 71/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8741 - val_loss: 1.8670\n",
            "Epoch 72/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8478 - val_loss: 1.6196\n",
            "Epoch 73/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8548 - val_loss: 1.4230\n",
            "Epoch 74/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8126 - val_loss: 1.7780\n",
            "Epoch 75/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8522 - val_loss: 1.2275\n",
            "Epoch 76/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8300 - val_loss: 1.6936\n",
            "Epoch 77/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8056 - val_loss: 1.3825\n",
            "Epoch 78/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8068 - val_loss: 1.5033\n",
            "Epoch 79/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8110 - val_loss: 1.4070\n",
            "Epoch 80/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7713 - val_loss: 1.5121\n",
            "Epoch 81/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8544 - val_loss: 1.4447\n",
            "Epoch 82/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7516 - val_loss: 1.2772\n",
            "Epoch 83/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8839 - val_loss: 1.5824\n",
            "Epoch 84/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8244 - val_loss: 1.7082\n",
            "Epoch 85/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8415 - val_loss: 1.5862\n",
            "Epoch 86/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8276 - val_loss: 1.8561\n",
            "Epoch 87/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9280 - val_loss: 1.9285\n",
            "Epoch 88/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8050 - val_loss: 1.5521\n",
            "Epoch 89/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8326 - val_loss: 1.2547\n",
            "Epoch 90/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8042 - val_loss: 1.7288\n",
            "Epoch 91/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8427 - val_loss: 1.5216\n",
            "Epoch 92/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7921 - val_loss: 1.1344\n",
            "Epoch 93/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8452 - val_loss: 2.0286\n",
            "Epoch 94/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8378 - val_loss: 1.8272\n",
            "Epoch 95/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7674 - val_loss: 1.7288\n",
            "Epoch 96/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7697 - val_loss: 1.9055\n",
            "Epoch 97/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8752 - val_loss: 1.4369\n",
            "Epoch 98/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8359 - val_loss: 1.7789\n",
            "Epoch 99/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8929 - val_loss: 1.6890\n",
            "Epoch 100/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7806 - val_loss: 1.2350\n",
            "Epoch 101/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8426 - val_loss: 1.2447\n",
            "Epoch 102/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8435 - val_loss: 1.4777\n",
            "Epoch 103/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7746 - val_loss: 1.5597\n",
            "Epoch 104/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8847 - val_loss: 1.7594\n",
            "Epoch 105/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8591 - val_loss: 1.4294\n",
            "Epoch 106/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8317 - val_loss: 1.4743\n",
            "Epoch 107/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8382 - val_loss: 1.3654\n",
            "Epoch 108/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8331 - val_loss: 1.2936\n",
            "Epoch 109/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8470 - val_loss: 2.1264\n",
            "Epoch 110/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7914 - val_loss: 1.5905\n",
            "Epoch 111/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7939 - val_loss: 1.3014\n",
            "Epoch 112/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8235 - val_loss: 1.1890\n",
            "Epoch 113/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7702 - val_loss: 1.7896\n",
            "Epoch 114/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8275 - val_loss: 1.5379\n",
            "Epoch 115/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9149 - val_loss: 1.6607\n",
            "Epoch 116/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8890 - val_loss: 1.2352\n",
            "Epoch 117/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8546 - val_loss: 1.1906\n",
            "Epoch 118/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8506 - val_loss: 1.8884\n",
            "Epoch 119/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8063 - val_loss: 1.2827\n",
            "Epoch 120/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.9024 - val_loss: 1.3372\n",
            "Epoch 121/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8234 - val_loss: 1.2317\n",
            "Epoch 122/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7841 - val_loss: 1.4948\n",
            "Epoch 123/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8167 - val_loss: 1.4687\n",
            "Epoch 124/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8009 - val_loss: 1.6168\n",
            "Epoch 125/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8194 - val_loss: 1.7332\n",
            "Epoch 126/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7840 - val_loss: 1.5695\n",
            "Epoch 127/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8904 - val_loss: 1.7305\n",
            "Epoch 128/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7827 - val_loss: 1.1412\n",
            "Epoch 129/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7586 - val_loss: 1.1158\n",
            "Epoch 130/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8359 - val_loss: 1.4448\n",
            "Epoch 131/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7458 - val_loss: 1.2491\n",
            "Epoch 132/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8147 - val_loss: 1.6650\n",
            "Epoch 133/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8056 - val_loss: 1.1953\n",
            "Epoch 134/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8815 - val_loss: 1.4324\n",
            "Epoch 135/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8314 - val_loss: 1.2944\n",
            "Epoch 136/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8218 - val_loss: 1.3514\n",
            "Epoch 137/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8068 - val_loss: 1.7589\n",
            "Epoch 138/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7836 - val_loss: 1.7987\n",
            "Epoch 139/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7784 - val_loss: 1.5429\n",
            "Epoch 140/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7714 - val_loss: 1.3095\n",
            "Epoch 141/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7667 - val_loss: 1.5201\n",
            "Epoch 142/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8538 - val_loss: 1.6447\n",
            "Epoch 143/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8095 - val_loss: 1.4948\n",
            "Epoch 144/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8471 - val_loss: 1.3914\n",
            "Epoch 145/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8563 - val_loss: 1.4748\n",
            "Epoch 146/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.9040 - val_loss: 1.5527\n",
            "Epoch 147/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7602 - val_loss: 1.5505\n",
            "Epoch 148/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7718 - val_loss: 1.8297\n",
            "Epoch 149/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8376 - val_loss: 1.4697\n",
            "Epoch 150/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7617 - val_loss: 1.3136\n",
            "Epoch 151/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7669 - val_loss: 1.3739\n",
            "Epoch 152/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8289 - val_loss: 1.8100\n",
            "Epoch 153/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7936 - val_loss: 1.3397\n",
            "Epoch 154/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8016 - val_loss: 1.8342\n",
            "Epoch 155/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8331 - val_loss: 1.2247\n",
            "Epoch 156/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8269 - val_loss: 1.5540\n",
            "Epoch 157/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7642 - val_loss: 1.2922\n",
            "Epoch 158/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8204 - val_loss: 1.2831\n",
            "Epoch 159/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8162 - val_loss: 1.2491\n",
            "Epoch 160/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7664 - val_loss: 1.3586\n",
            "Epoch 161/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8264 - val_loss: 1.7795\n",
            "Epoch 162/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7817 - val_loss: 1.4060\n",
            "Epoch 163/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8619 - val_loss: 1.1618\n",
            "Epoch 164/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8424 - val_loss: 1.5722\n",
            "Epoch 165/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8242 - val_loss: 1.2193\n",
            "Epoch 166/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7776 - val_loss: 1.1610\n",
            "Epoch 167/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7926 - val_loss: 1.5522\n",
            "Epoch 168/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7521 - val_loss: 1.3817\n",
            "Epoch 169/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8125 - val_loss: 1.5091\n",
            "Epoch 170/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8234 - val_loss: 1.4571\n",
            "Epoch 171/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8340 - val_loss: 1.1918\n",
            "Epoch 172/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7983 - val_loss: 1.4165\n",
            "Epoch 173/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8598 - val_loss: 1.2740\n",
            "Epoch 174/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8440 - val_loss: 1.7186\n",
            "Epoch 175/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7656 - val_loss: 1.4497\n",
            "Epoch 176/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7396 - val_loss: 1.5167\n",
            "Epoch 177/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7898 - val_loss: 1.4662\n",
            "Epoch 178/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7978 - val_loss: 1.3314\n",
            "Epoch 179/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7332 - val_loss: 1.2389\n",
            "Epoch 180/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8105 - val_loss: 1.5164\n",
            "Epoch 181/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7614 - val_loss: 1.5520\n",
            "Epoch 182/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7430 - val_loss: 1.6739\n",
            "Epoch 183/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7633 - val_loss: 1.4980\n",
            "Epoch 184/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7581 - val_loss: 1.5991\n",
            "Epoch 185/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7860 - val_loss: 1.2921\n",
            "Epoch 186/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7950 - val_loss: 1.3159\n",
            "Epoch 187/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7638 - val_loss: 1.4567\n",
            "Epoch 188/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8619 - val_loss: 2.4973\n",
            "Epoch 189/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7923 - val_loss: 1.2644\n",
            "Epoch 190/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8482 - val_loss: 1.2518\n",
            "Epoch 191/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8267 - val_loss: 1.6394\n",
            "Epoch 192/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8508 - val_loss: 1.3238\n",
            "Epoch 193/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8020 - val_loss: 1.2730\n",
            "Epoch 194/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8084 - val_loss: 1.6716\n",
            "Epoch 195/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8206 - val_loss: 1.3663\n",
            "Epoch 196/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8098 - val_loss: 1.3098\n",
            "Epoch 197/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7618 - val_loss: 2.9021\n",
            "Epoch 198/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7781 - val_loss: 1.5080\n",
            "Epoch 199/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7749 - val_loss: 1.9625\n",
            "Epoch 200/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7512 - val_loss: 1.5289\n",
            "Epoch 201/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8382 - val_loss: 1.8976\n",
            "Epoch 202/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7196 - val_loss: 1.5759\n",
            "Epoch 203/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7821 - val_loss: 1.5441\n",
            "Epoch 204/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7966 - val_loss: 1.6443\n",
            "Epoch 205/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8603 - val_loss: 1.6740\n",
            "Epoch 206/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8148 - val_loss: 1.4693\n",
            "Epoch 207/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7301 - val_loss: 1.3055\n",
            "Epoch 208/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7667 - val_loss: 1.3175\n",
            "Epoch 209/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7669 - val_loss: 1.2435\n",
            "Epoch 210/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7571 - val_loss: 1.2278\n",
            "Epoch 211/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8232 - val_loss: 1.3031\n",
            "Epoch 212/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8691 - val_loss: 2.1235\n",
            "Epoch 213/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7999 - val_loss: 1.4951\n",
            "Epoch 214/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7784 - val_loss: 1.3198\n",
            "Epoch 215/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7896 - val_loss: 1.3631\n",
            "Epoch 216/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7676 - val_loss: 2.8109\n",
            "Epoch 217/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8589 - val_loss: 1.8927\n",
            "Epoch 218/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7856 - val_loss: 1.5979\n",
            "Epoch 219/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7769 - val_loss: 1.5333\n",
            "Epoch 220/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7440 - val_loss: 2.3613\n",
            "Epoch 221/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7684 - val_loss: 1.3621\n",
            "Epoch 222/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8015 - val_loss: 1.7228\n",
            "Epoch 223/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8154 - val_loss: 1.6132\n",
            "Epoch 224/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7360 - val_loss: 1.3221\n",
            "Epoch 225/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7323 - val_loss: 1.6954\n",
            "Epoch 226/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7281 - val_loss: 1.6488\n",
            "Epoch 227/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7651 - val_loss: 1.6986\n",
            "Epoch 228/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7468 - val_loss: 1.3243\n",
            "Epoch 229/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8282 - val_loss: 2.5646\n",
            "Epoch 230/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7851 - val_loss: 1.7555\n",
            "Epoch 231/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7766 - val_loss: 1.9425\n",
            "Epoch 232/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8146 - val_loss: 1.7057\n",
            "Epoch 233/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7918 - val_loss: 1.4053\n",
            "Epoch 234/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8494 - val_loss: 2.4271\n",
            "Epoch 235/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7424 - val_loss: 1.6087\n",
            "Epoch 236/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8136 - val_loss: 2.2036\n",
            "Epoch 237/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8040 - val_loss: 1.4517\n",
            "Epoch 238/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7998 - val_loss: 1.4974\n",
            "Epoch 239/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7787 - val_loss: 1.6745\n",
            "Epoch 240/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7936 - val_loss: 1.4782\n",
            "Epoch 241/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7853 - val_loss: 1.4150\n",
            "Epoch 242/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7661 - val_loss: 1.6258\n",
            "Epoch 243/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8114 - val_loss: 1.5443\n",
            "Epoch 244/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7795 - val_loss: 1.9188\n",
            "Epoch 245/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7840 - val_loss: 1.3025\n",
            "Epoch 246/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7870 - val_loss: 1.5805\n",
            "Epoch 247/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7610 - val_loss: 1.6007\n",
            "Epoch 248/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7429 - val_loss: 2.2400\n",
            "Epoch 249/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7700 - val_loss: 1.4505\n",
            "Epoch 250/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8220 - val_loss: 1.3568\n",
            "Epoch 251/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7578 - val_loss: 1.5255\n",
            "Epoch 252/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7578 - val_loss: 1.2493\n",
            "Epoch 253/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8658 - val_loss: 1.5588\n",
            "Epoch 254/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7386 - val_loss: 1.4783\n",
            "Epoch 255/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7324 - val_loss: 1.7528\n",
            "Epoch 256/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8939 - val_loss: 1.9494\n",
            "Epoch 257/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7694 - val_loss: 1.4494\n",
            "Epoch 258/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7753 - val_loss: 1.6136\n",
            "Epoch 259/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7816 - val_loss: 1.5250\n",
            "Epoch 260/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7358 - val_loss: 1.3399\n",
            "Epoch 261/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7653 - val_loss: 1.4384\n",
            "Epoch 262/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7788 - val_loss: 1.2582\n",
            "Epoch 263/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7881 - val_loss: 2.0888\n",
            "Epoch 264/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7363 - val_loss: 1.1703\n",
            "Epoch 265/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8043 - val_loss: 1.3891\n",
            "Epoch 266/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8001 - val_loss: 1.3122\n",
            "Epoch 267/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7612 - val_loss: 1.3588\n",
            "Epoch 268/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8474 - val_loss: 1.2054\n",
            "Epoch 269/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8051 - val_loss: 1.3103\n",
            "Epoch 270/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7675 - val_loss: 1.2053\n",
            "Epoch 271/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7568 - val_loss: 1.6451\n",
            "Epoch 272/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7620 - val_loss: 1.2022\n",
            "Epoch 273/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7657 - val_loss: 1.3472\n",
            "Epoch 274/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8276 - val_loss: 1.2387\n",
            "Epoch 275/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7550 - val_loss: 1.1641\n",
            "Epoch 276/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7245 - val_loss: 1.4133\n",
            "Epoch 277/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7902 - val_loss: 1.6028\n",
            "Epoch 278/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7652 - val_loss: 1.1242\n",
            "Epoch 279/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7962 - val_loss: 1.5899\n",
            "Epoch 280/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7837 - val_loss: 1.4938\n",
            "Epoch 281/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8010 - val_loss: 2.2316\n",
            "Epoch 282/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7990 - val_loss: 2.2417\n",
            "Epoch 283/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8255 - val_loss: 1.4035\n",
            "Epoch 284/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7400 - val_loss: 1.5483\n",
            "Epoch 285/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8327 - val_loss: 1.3957\n",
            "Epoch 286/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8295 - val_loss: 1.5688\n",
            "Epoch 287/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7703 - val_loss: 1.4540\n",
            "Epoch 288/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7499 - val_loss: 1.4284\n",
            "Epoch 289/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7614 - val_loss: 2.2435\n",
            "Epoch 290/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7715 - val_loss: 1.7718\n",
            "Epoch 291/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8473 - val_loss: 2.2335\n",
            "Epoch 292/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7771 - val_loss: 1.7913\n",
            "Epoch 293/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8143 - val_loss: 1.4779\n",
            "Epoch 294/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8374 - val_loss: 1.5294\n",
            "Epoch 295/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7370 - val_loss: 1.2529\n",
            "Epoch 296/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7752 - val_loss: 1.5845\n",
            "Epoch 297/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7606 - val_loss: 1.6573\n",
            "Epoch 298/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7596 - val_loss: 1.6489\n",
            "Epoch 299/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7455 - val_loss: 1.9021\n",
            "Epoch 300/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7861 - val_loss: 1.3930\n",
            "Epoch 301/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7750 - val_loss: 1.3206\n",
            "Epoch 302/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8025 - val_loss: 1.2810\n",
            "Epoch 303/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8052 - val_loss: 1.7612\n",
            "Epoch 304/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7246 - val_loss: 1.3077\n",
            "Epoch 305/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8199 - val_loss: 1.3656\n",
            "Epoch 306/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7616 - val_loss: 1.5690\n",
            "Epoch 307/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8171 - val_loss: 1.8277\n",
            "Epoch 308/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7401 - val_loss: 2.0597\n",
            "Epoch 309/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7880 - val_loss: 1.3608\n",
            "Epoch 310/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8623 - val_loss: 1.6678\n",
            "Epoch 311/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7974 - val_loss: 1.2742\n",
            "Epoch 312/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7961 - val_loss: 1.7497\n",
            "Epoch 313/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8693 - val_loss: 1.3750\n",
            "Epoch 314/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7741 - val_loss: 1.3543\n",
            "Epoch 315/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7224 - val_loss: 1.3385\n",
            "Epoch 316/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7087 - val_loss: 1.6440\n",
            "Epoch 317/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7656 - val_loss: 1.3552\n",
            "Epoch 318/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8971 - val_loss: 1.4221\n",
            "Epoch 319/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7763 - val_loss: 1.2011\n",
            "Epoch 320/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7443 - val_loss: 1.2358\n",
            "Epoch 321/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7015 - val_loss: 1.7550\n",
            "Epoch 322/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7708 - val_loss: 1.4737\n",
            "Epoch 323/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7635 - val_loss: 1.4371\n",
            "Epoch 324/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7474 - val_loss: 1.8288\n",
            "Epoch 325/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7627 - val_loss: 1.5010\n",
            "Epoch 326/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7858 - val_loss: 1.2327\n",
            "Epoch 327/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7991 - val_loss: 1.2889\n",
            "Epoch 328/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7415 - val_loss: 1.4840\n",
            "Epoch 329/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7943 - val_loss: 1.7855\n",
            "Epoch 330/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7441 - val_loss: 2.1716\n",
            "Epoch 331/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7944 - val_loss: 2.4422\n",
            "Epoch 332/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7326 - val_loss: 1.6579\n",
            "Epoch 333/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7676 - val_loss: 1.4783\n",
            "Epoch 334/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7803 - val_loss: 1.2998\n",
            "Epoch 335/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7930 - val_loss: 1.9183\n",
            "Epoch 336/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7856 - val_loss: 2.4434\n",
            "Epoch 337/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7631 - val_loss: 1.4218\n",
            "Epoch 338/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7710 - val_loss: 1.2522\n",
            "Epoch 339/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8315 - val_loss: 1.5479\n",
            "Epoch 340/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7907 - val_loss: 2.5015\n",
            "Epoch 341/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7761 - val_loss: 1.3797\n",
            "Epoch 342/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8068 - val_loss: 1.3903\n",
            "Epoch 343/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8886 - val_loss: 1.3000\n",
            "Epoch 344/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8343 - val_loss: 1.3674\n",
            "Epoch 345/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8526 - val_loss: 1.4781\n",
            "Epoch 346/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8189 - val_loss: 2.0438\n",
            "Epoch 347/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7336 - val_loss: 1.2272\n",
            "Epoch 348/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7979 - val_loss: 1.4660\n",
            "Epoch 349/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7106 - val_loss: 1.5757\n",
            "Epoch 350/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7294 - val_loss: 1.6513\n",
            "Epoch 351/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7492 - val_loss: 1.4504\n",
            "Epoch 352/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8928 - val_loss: 1.3575\n",
            "Epoch 353/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7752 - val_loss: 1.9435\n",
            "Epoch 354/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7999 - val_loss: 1.3868\n",
            "Epoch 355/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7941 - val_loss: 1.3946\n",
            "Epoch 356/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7826 - val_loss: 1.9577\n",
            "Epoch 357/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8192 - val_loss: 1.4332\n",
            "Epoch 358/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8392 - val_loss: 1.4210\n",
            "Epoch 359/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7862 - val_loss: 1.4774\n",
            "Epoch 360/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8271 - val_loss: 1.3901\n",
            "Epoch 361/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7100 - val_loss: 1.4262\n",
            "Epoch 362/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7450 - val_loss: 1.1826\n",
            "Epoch 363/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7163 - val_loss: 1.5698\n",
            "Epoch 364/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7921 - val_loss: 1.4566\n",
            "Epoch 365/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8178 - val_loss: 1.5324\n",
            "Epoch 366/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7907 - val_loss: 2.0413\n",
            "Epoch 367/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7672 - val_loss: 1.5221\n",
            "Epoch 368/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7800 - val_loss: 1.7002\n",
            "Epoch 369/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9504 - val_loss: 1.3798\n",
            "Epoch 370/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7024 - val_loss: 1.9427\n",
            "Epoch 371/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8406 - val_loss: 1.5148\n",
            "Epoch 372/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7632 - val_loss: 1.4536\n",
            "Epoch 373/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7523 - val_loss: 1.7236\n",
            "Epoch 374/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8236 - val_loss: 1.4209\n",
            "Epoch 375/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8123 - val_loss: 1.1917\n",
            "Epoch 376/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7648 - val_loss: 1.1972\n",
            "Epoch 377/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7350 - val_loss: 2.0827\n",
            "Epoch 378/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8000 - val_loss: 1.7934\n",
            "Epoch 379/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7059 - val_loss: 1.7145\n",
            "Epoch 380/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8130 - val_loss: 1.3021\n",
            "Epoch 381/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7942 - val_loss: 3.2732\n",
            "Epoch 382/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8592 - val_loss: 1.6015\n",
            "Epoch 383/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8004 - val_loss: 1.3467\n",
            "Epoch 384/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7681 - val_loss: 1.8520\n",
            "Epoch 385/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7493 - val_loss: 1.7328\n",
            "Epoch 386/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8191 - val_loss: 2.0046\n",
            "Epoch 387/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7165 - val_loss: 2.7736\n",
            "Epoch 388/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7905 - val_loss: 1.4491\n",
            "Epoch 389/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7454 - val_loss: 2.1468\n",
            "Epoch 390/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8070 - val_loss: 1.5640\n",
            "Epoch 391/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7693 - val_loss: 1.6569\n",
            "Epoch 392/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7631 - val_loss: 1.3679\n",
            "Epoch 393/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8346 - val_loss: 2.0077\n",
            "Epoch 394/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8537 - val_loss: 1.9876\n",
            "Epoch 395/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7087 - val_loss: 1.8584\n",
            "Epoch 396/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7884 - val_loss: 1.4406\n",
            "Epoch 397/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7151 - val_loss: 1.4801\n",
            "Epoch 398/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7464 - val_loss: 1.7430\n",
            "Epoch 399/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7566 - val_loss: 1.5609\n",
            "Epoch 400/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7473 - val_loss: 1.6630\n",
            "Epoch 401/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7245 - val_loss: 1.6688\n",
            "Epoch 402/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7786 - val_loss: 1.2468\n",
            "Epoch 403/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7434 - val_loss: 1.3985\n",
            "Epoch 404/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7570 - val_loss: 1.8215\n",
            "Epoch 405/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7198 - val_loss: 1.5263\n",
            "Epoch 406/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8133 - val_loss: 1.3552\n",
            "Epoch 407/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8879 - val_loss: 1.8540\n",
            "Epoch 408/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7020 - val_loss: 1.4046\n",
            "Epoch 409/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7292 - val_loss: 1.3361\n",
            "Epoch 410/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7181 - val_loss: 1.2154\n",
            "Epoch 411/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7790 - val_loss: 1.3510\n",
            "Epoch 412/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7264 - val_loss: 1.5783\n",
            "Epoch 413/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7428 - val_loss: 1.6112\n",
            "Epoch 414/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8582 - val_loss: 1.3664\n",
            "Epoch 415/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7940 - val_loss: 1.8426\n",
            "Epoch 416/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7547 - val_loss: 2.0435\n",
            "Epoch 417/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7508 - val_loss: 1.7928\n",
            "Epoch 418/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7542 - val_loss: 2.6855\n",
            "Epoch 419/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7025 - val_loss: 1.2884\n",
            "Epoch 420/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7075 - val_loss: 1.1767\n",
            "Epoch 421/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7893 - val_loss: 1.5074\n",
            "Epoch 422/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8298 - val_loss: 1.1305\n",
            "Epoch 423/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6939 - val_loss: 1.4125\n",
            "Epoch 424/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8898 - val_loss: 1.1589\n",
            "Epoch 425/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8017 - val_loss: 1.7050\n",
            "Epoch 426/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7876 - val_loss: 1.4747\n",
            "Epoch 427/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.8007 - val_loss: 1.8277\n",
            "Epoch 428/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7925 - val_loss: 1.1881\n",
            "Epoch 429/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7142 - val_loss: 1.7026\n",
            "Epoch 430/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7316 - val_loss: 2.7248\n",
            "Epoch 431/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8361 - val_loss: 1.5926\n",
            "Epoch 432/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7748 - val_loss: 1.2984\n",
            "Epoch 433/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7685 - val_loss: 1.3583\n",
            "Epoch 434/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7276 - val_loss: 1.2068\n",
            "Epoch 435/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7877 - val_loss: 1.2283\n",
            "Epoch 436/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7214 - val_loss: 1.2299\n",
            "Epoch 437/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7043 - val_loss: 1.8892\n",
            "Epoch 438/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8248 - val_loss: 1.5984\n",
            "Epoch 439/5000\n",
            "23/23 [==============================] - 1s 29ms/step - loss: 0.7720 - val_loss: 1.0805\n",
            "Epoch 440/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7415 - val_loss: 1.7687\n",
            "Epoch 441/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7818 - val_loss: 1.8282\n",
            "Epoch 442/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7810 - val_loss: 1.5259\n",
            "Epoch 443/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7557 - val_loss: 1.7735\n",
            "Epoch 444/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7342 - val_loss: 1.2722\n",
            "Epoch 445/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7419 - val_loss: 1.8464\n",
            "Epoch 446/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7793 - val_loss: 1.5639\n",
            "Epoch 447/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7147 - val_loss: 1.6571\n",
            "Epoch 448/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7840 - val_loss: 1.3548\n",
            "Epoch 449/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7515 - val_loss: 1.5747\n",
            "Epoch 450/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8001 - val_loss: 1.6297\n",
            "Epoch 451/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8085 - val_loss: 1.8752\n",
            "Epoch 452/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7794 - val_loss: 1.9745\n",
            "Epoch 453/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8446 - val_loss: 1.7304\n",
            "Epoch 454/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7951 - val_loss: 1.3634\n",
            "Epoch 455/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7422 - val_loss: 1.5264\n",
            "Epoch 456/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7505 - val_loss: 1.5885\n",
            "Epoch 457/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8211 - val_loss: 1.3418\n",
            "Epoch 458/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7198 - val_loss: 1.5962\n",
            "Epoch 459/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8201 - val_loss: 1.6688\n",
            "Epoch 460/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7759 - val_loss: 1.7076\n",
            "Epoch 461/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8175 - val_loss: 1.8460\n",
            "Epoch 462/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7402 - val_loss: 2.1290\n",
            "Epoch 463/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8712 - val_loss: 1.4031\n",
            "Epoch 464/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7402 - val_loss: 1.6867\n",
            "Epoch 465/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8175 - val_loss: 1.3349\n",
            "Epoch 466/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8024 - val_loss: 1.7003\n",
            "Epoch 467/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7916 - val_loss: 1.5994\n",
            "Epoch 468/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8009 - val_loss: 1.7186\n",
            "Epoch 469/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7561 - val_loss: 1.3984\n",
            "Epoch 470/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7711 - val_loss: 1.3152\n",
            "Epoch 471/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8344 - val_loss: 1.6531\n",
            "Epoch 472/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7896 - val_loss: 1.4681\n",
            "Epoch 473/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7523 - val_loss: 1.4525\n",
            "Epoch 474/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7463 - val_loss: 1.3539\n",
            "Epoch 475/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7410 - val_loss: 1.7853\n",
            "Epoch 476/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7236 - val_loss: 1.3468\n",
            "Epoch 477/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7723 - val_loss: 1.7107\n",
            "Epoch 478/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7555 - val_loss: 1.1966\n",
            "Epoch 479/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7609 - val_loss: 1.8734\n",
            "Epoch 480/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7844 - val_loss: 1.6662\n",
            "Epoch 481/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7344 - val_loss: 1.6171\n",
            "Epoch 482/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7092 - val_loss: 1.7422\n",
            "Epoch 483/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7490 - val_loss: 1.6691\n",
            "Epoch 484/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7621 - val_loss: 1.8161\n",
            "Epoch 485/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7328 - val_loss: 2.1645\n",
            "Epoch 486/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7580 - val_loss: 1.2909\n",
            "Epoch 487/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7817 - val_loss: 1.7268\n",
            "Epoch 488/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7276 - val_loss: 1.3988\n",
            "Epoch 489/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7782 - val_loss: 1.6920\n",
            "Epoch 490/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6964 - val_loss: 1.5181\n",
            "Epoch 491/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8079 - val_loss: 1.8939\n",
            "Epoch 492/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7462 - val_loss: 1.6961\n",
            "Epoch 493/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7462 - val_loss: 2.4208\n",
            "Epoch 494/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7561 - val_loss: 1.1956\n",
            "Epoch 495/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7658 - val_loss: 1.7992\n",
            "Epoch 496/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7272 - val_loss: 1.3195\n",
            "Epoch 497/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7032 - val_loss: 1.2615\n",
            "Epoch 498/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8277 - val_loss: 1.8696\n",
            "Epoch 499/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8137 - val_loss: 1.5426\n",
            "Epoch 500/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8217 - val_loss: 1.3604\n",
            "Epoch 501/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7291 - val_loss: 2.1821\n",
            "Epoch 502/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7317 - val_loss: 1.4801\n",
            "Epoch 503/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8051 - val_loss: 2.6441\n",
            "Epoch 504/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8276 - val_loss: 1.3885\n",
            "Epoch 505/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7458 - val_loss: 2.3136\n",
            "Epoch 506/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7187 - val_loss: 1.7800\n",
            "Epoch 507/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7265 - val_loss: 1.3951\n",
            "Epoch 508/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7062 - val_loss: 1.3083\n",
            "Epoch 509/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8002 - val_loss: 1.4424\n",
            "Epoch 510/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7352 - val_loss: 1.5216\n",
            "Epoch 511/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7519 - val_loss: 1.4739\n",
            "Epoch 512/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8587 - val_loss: 1.3925\n",
            "Epoch 513/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8226 - val_loss: 2.3235\n",
            "Epoch 514/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8035 - val_loss: 1.5750\n",
            "Epoch 515/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9475 - val_loss: 2.5438\n",
            "Epoch 516/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6949 - val_loss: 1.4889\n",
            "Epoch 517/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8150 - val_loss: 2.1578\n",
            "Epoch 518/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7852 - val_loss: 1.8657\n",
            "Epoch 519/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.7900 - val_loss: 1.5773\n",
            "Epoch 520/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7256 - val_loss: 1.5470\n",
            "Epoch 521/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7669 - val_loss: 2.5374\n",
            "Epoch 522/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7011 - val_loss: 1.3634\n",
            "Epoch 523/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7401 - val_loss: 2.2889\n",
            "Epoch 524/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7476 - val_loss: 1.7883\n",
            "Epoch 525/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8106 - val_loss: 1.4762\n",
            "Epoch 526/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7850 - val_loss: 1.4915\n",
            "Epoch 527/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7216 - val_loss: 1.5072\n",
            "Epoch 528/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8393 - val_loss: 1.5464\n",
            "Epoch 529/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8331 - val_loss: 1.5641\n",
            "Epoch 530/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7490 - val_loss: 1.3530\n",
            "Epoch 531/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7386 - val_loss: 1.3397\n",
            "Epoch 532/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7873 - val_loss: 1.5055\n",
            "Epoch 533/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8419 - val_loss: 1.3992\n",
            "Epoch 534/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8114 - val_loss: 1.5337\n",
            "Epoch 535/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6714 - val_loss: 1.4905\n",
            "Epoch 536/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7549 - val_loss: 1.6160\n",
            "Epoch 537/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7489 - val_loss: 1.7270\n",
            "Epoch 538/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7349 - val_loss: 3.2226\n",
            "Epoch 539/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7963 - val_loss: 2.4320\n",
            "Epoch 540/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8241 - val_loss: 1.4383\n",
            "Epoch 541/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7835 - val_loss: 1.5465\n",
            "Epoch 542/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6794 - val_loss: 1.2369\n",
            "Epoch 543/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7575 - val_loss: 3.6074\n",
            "Epoch 544/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7591 - val_loss: 1.4724\n",
            "Epoch 545/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7341 - val_loss: 1.2786\n",
            "Epoch 546/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7998 - val_loss: 1.4344\n",
            "Epoch 547/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7004 - val_loss: 1.3174\n",
            "Epoch 548/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7612 - val_loss: 1.5258\n",
            "Epoch 549/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7690 - val_loss: 1.5419\n",
            "Epoch 550/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6600 - val_loss: 1.2698\n",
            "Epoch 551/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7177 - val_loss: 1.8546\n",
            "Epoch 552/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6870 - val_loss: 1.3746\n",
            "Epoch 553/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7408 - val_loss: 1.5186\n",
            "Epoch 554/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7468 - val_loss: 1.4597\n",
            "Epoch 555/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6896 - val_loss: 1.2697\n",
            "Epoch 556/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7865 - val_loss: 1.8322\n",
            "Epoch 557/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7593 - val_loss: 2.2911\n",
            "Epoch 558/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7957 - val_loss: 1.2741\n",
            "Epoch 559/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7582 - val_loss: 1.4792\n",
            "Epoch 560/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7050 - val_loss: 1.5985\n",
            "Epoch 561/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7518 - val_loss: 1.3971\n",
            "Epoch 562/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6956 - val_loss: 1.9817\n",
            "Epoch 563/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7711 - val_loss: 1.7791\n",
            "Epoch 564/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7052 - val_loss: 2.7242\n",
            "Epoch 565/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7159 - val_loss: 1.5658\n",
            "Epoch 566/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7686 - val_loss: 1.3220\n",
            "Epoch 567/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7064 - val_loss: 2.7023\n",
            "Epoch 568/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7964 - val_loss: 2.3495\n",
            "Epoch 569/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8330 - val_loss: 1.5706\n",
            "Epoch 570/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7541 - val_loss: 1.4901\n",
            "Epoch 571/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7431 - val_loss: 1.2645\n",
            "Epoch 572/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7496 - val_loss: 1.2217\n",
            "Epoch 573/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7457 - val_loss: 1.2891\n",
            "Epoch 574/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7911 - val_loss: 1.6102\n",
            "Epoch 575/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7791 - val_loss: 1.5050\n",
            "Epoch 576/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8132 - val_loss: 1.4574\n",
            "Epoch 577/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7672 - val_loss: 1.4461\n",
            "Epoch 578/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7865 - val_loss: 1.2512\n",
            "Epoch 579/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7401 - val_loss: 1.3105\n",
            "Epoch 580/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7613 - val_loss: 2.2371\n",
            "Epoch 581/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7521 - val_loss: 1.6463\n",
            "Epoch 582/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7063 - val_loss: 1.6666\n",
            "Epoch 583/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6823 - val_loss: 1.5865\n",
            "Epoch 584/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7525 - val_loss: 1.2158\n",
            "Epoch 585/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7731 - val_loss: 1.3960\n",
            "Epoch 586/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7776 - val_loss: 1.5983\n",
            "Epoch 587/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7530 - val_loss: 1.2864\n",
            "Epoch 588/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7813 - val_loss: 1.3713\n",
            "Epoch 589/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6902 - val_loss: 2.0457\n",
            "Epoch 590/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7727 - val_loss: 1.4559\n",
            "Epoch 591/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7369 - val_loss: 1.3772\n",
            "Epoch 592/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8170 - val_loss: 1.5203\n",
            "Epoch 593/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7431 - val_loss: 1.8113\n",
            "Epoch 594/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6909 - val_loss: 1.3097\n",
            "Epoch 595/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7260 - val_loss: 1.7052\n",
            "Epoch 596/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7829 - val_loss: 1.6598\n",
            "Epoch 597/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7523 - val_loss: 2.4556\n",
            "Epoch 598/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6961 - val_loss: 1.9237\n",
            "Epoch 599/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7783 - val_loss: 2.2343\n",
            "Epoch 600/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6787 - val_loss: 1.8412\n",
            "Epoch 601/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6866 - val_loss: 1.7037\n",
            "Epoch 602/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8015 - val_loss: 1.5494\n",
            "Epoch 603/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7333 - val_loss: 1.8274\n",
            "Epoch 604/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7176 - val_loss: 1.4967\n",
            "Epoch 605/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7547 - val_loss: 1.6390\n",
            "Epoch 606/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7003 - val_loss: 1.8422\n",
            "Epoch 607/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8673 - val_loss: 1.4173\n",
            "Epoch 608/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7251 - val_loss: 1.6562\n",
            "Epoch 609/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8339 - val_loss: 1.4967\n",
            "Epoch 610/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7546 - val_loss: 1.4514\n",
            "Epoch 611/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7509 - val_loss: 1.7864\n",
            "Epoch 612/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7875 - val_loss: 1.7846\n",
            "Epoch 613/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6746 - val_loss: 1.8659\n",
            "Epoch 614/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7437 - val_loss: 1.3219\n",
            "Epoch 615/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7968 - val_loss: 1.7666\n",
            "Epoch 616/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7787 - val_loss: 1.6472\n",
            "Epoch 617/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7457 - val_loss: 1.3707\n",
            "Epoch 618/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8133 - val_loss: 1.3695\n",
            "Epoch 619/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.9116 - val_loss: 2.5010\n",
            "Epoch 620/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7334 - val_loss: 1.6638\n",
            "Epoch 621/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7069 - val_loss: 2.3005\n",
            "Epoch 622/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7290 - val_loss: 1.7242\n",
            "Epoch 623/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.9197 - val_loss: 1.4283\n",
            "Epoch 624/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7242 - val_loss: 1.3960\n",
            "Epoch 625/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7706 - val_loss: 2.0335\n",
            "Epoch 626/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7341 - val_loss: 1.7578\n",
            "Epoch 627/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7553 - val_loss: 1.5682\n",
            "Epoch 628/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7191 - val_loss: 1.4306\n",
            "Epoch 629/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7893 - val_loss: 1.6571\n",
            "Epoch 630/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7444 - val_loss: 1.6007\n",
            "Epoch 631/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7417 - val_loss: 1.8737\n",
            "Epoch 632/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7351 - val_loss: 1.2955\n",
            "Epoch 633/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7799 - val_loss: 2.2769\n",
            "Epoch 634/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7555 - val_loss: 1.6772\n",
            "Epoch 635/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7064 - val_loss: 1.8147\n",
            "Epoch 636/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7513 - val_loss: 2.2845\n",
            "Epoch 637/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7821 - val_loss: 1.3454\n",
            "Epoch 638/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7318 - val_loss: 1.3606\n",
            "Epoch 639/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7402 - val_loss: 1.6189\n",
            "Epoch 640/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7656 - val_loss: 1.4691\n",
            "Epoch 641/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7469 - val_loss: 1.5344\n",
            "Epoch 642/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8266 - val_loss: 1.3720\n",
            "Epoch 643/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7233 - val_loss: 1.5798\n",
            "Epoch 644/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7230 - val_loss: 1.9102\n",
            "Epoch 645/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6815 - val_loss: 1.7716\n",
            "Epoch 646/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8106 - val_loss: 1.8742\n",
            "Epoch 647/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7257 - val_loss: 1.7208\n",
            "Epoch 648/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6777 - val_loss: 2.5821\n",
            "Epoch 649/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8089 - val_loss: 1.5789\n",
            "Epoch 650/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7288 - val_loss: 1.4955\n",
            "Epoch 651/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7731 - val_loss: 1.6317\n",
            "Epoch 652/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7347 - val_loss: 1.8259\n",
            "Epoch 653/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7488 - val_loss: 1.3591\n",
            "Epoch 654/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7581 - val_loss: 1.5621\n",
            "Epoch 655/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7456 - val_loss: 1.2446\n",
            "Epoch 656/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7113 - val_loss: 1.3285\n",
            "Epoch 657/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7460 - val_loss: 2.1593\n",
            "Epoch 658/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7504 - val_loss: 1.3510\n",
            "Epoch 659/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7803 - val_loss: 1.5494\n",
            "Epoch 660/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6650 - val_loss: 2.2645\n",
            "Epoch 661/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7414 - val_loss: 1.8212\n",
            "Epoch 662/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8066 - val_loss: 2.1183\n",
            "Epoch 663/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7318 - val_loss: 1.9064\n",
            "Epoch 664/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7139 - val_loss: 2.0652\n",
            "Epoch 665/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7203 - val_loss: 1.5843\n",
            "Epoch 666/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7093 - val_loss: 1.7997\n",
            "Epoch 667/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7202 - val_loss: 2.3886\n",
            "Epoch 668/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7210 - val_loss: 1.7999\n",
            "Epoch 669/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7633 - val_loss: 1.5235\n",
            "Epoch 670/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7250 - val_loss: 1.6223\n",
            "Epoch 671/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7007 - val_loss: 2.7463\n",
            "Epoch 672/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6879 - val_loss: 1.6286\n",
            "Epoch 673/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7554 - val_loss: 1.8437\n",
            "Epoch 674/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7704 - val_loss: 1.7913\n",
            "Epoch 675/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7249 - val_loss: 1.5715\n",
            "Epoch 676/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7580 - val_loss: 1.5146\n",
            "Epoch 677/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6574 - val_loss: 1.6034\n",
            "Epoch 678/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6991 - val_loss: 2.0219\n",
            "Epoch 679/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7916 - val_loss: 1.4007\n",
            "Epoch 680/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7391 - val_loss: 1.5287\n",
            "Epoch 681/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7497 - val_loss: 1.4689\n",
            "Epoch 682/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7549 - val_loss: 1.3383\n",
            "Epoch 683/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7486 - val_loss: 1.4747\n",
            "Epoch 684/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7885 - val_loss: 1.3811\n",
            "Epoch 685/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6866 - val_loss: 1.3666\n",
            "Epoch 686/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7808 - val_loss: 2.1711\n",
            "Epoch 687/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7155 - val_loss: 2.1448\n",
            "Epoch 688/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8212 - val_loss: 1.5977\n",
            "Epoch 689/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6918 - val_loss: 1.5800\n",
            "Epoch 690/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7482 - val_loss: 1.6826\n",
            "Epoch 691/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7243 - val_loss: 1.9270\n",
            "Epoch 692/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7539 - val_loss: 1.5841\n",
            "Epoch 693/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6807 - val_loss: 1.9853\n",
            "Epoch 694/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7404 - val_loss: 2.4626\n",
            "Epoch 695/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7463 - val_loss: 1.8080\n",
            "Epoch 696/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7398 - val_loss: 1.3997\n",
            "Epoch 697/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7480 - val_loss: 2.3229\n",
            "Epoch 698/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8476 - val_loss: 1.7210\n",
            "Epoch 699/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7439 - val_loss: 1.6961\n",
            "Epoch 700/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8087 - val_loss: 1.4367\n",
            "Epoch 701/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8055 - val_loss: 1.7757\n",
            "Epoch 702/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7824 - val_loss: 1.9013\n",
            "Epoch 703/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7497 - val_loss: 1.3230\n",
            "Epoch 704/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7640 - val_loss: 1.5650\n",
            "Epoch 705/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7537 - val_loss: 2.0064\n",
            "Epoch 706/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7272 - val_loss: 1.8389\n",
            "Epoch 707/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7151 - val_loss: 1.7464\n",
            "Epoch 708/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7522 - val_loss: 1.7871\n",
            "Epoch 709/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7345 - val_loss: 1.6183\n",
            "Epoch 710/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7328 - val_loss: 1.6764\n",
            "Epoch 711/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7496 - val_loss: 1.4507\n",
            "Epoch 712/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8312 - val_loss: 1.4981\n",
            "Epoch 713/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8395 - val_loss: 2.3317\n",
            "Epoch 714/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7067 - val_loss: 1.5417\n",
            "Epoch 715/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7587 - val_loss: 1.7262\n",
            "Epoch 716/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7864 - val_loss: 1.5731\n",
            "Epoch 717/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7593 - val_loss: 1.3951\n",
            "Epoch 718/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6984 - val_loss: 1.5356\n",
            "Epoch 719/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7043 - val_loss: 1.4238\n",
            "Epoch 720/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7285 - val_loss: 1.9526\n",
            "Epoch 721/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7341 - val_loss: 1.2296\n",
            "Epoch 722/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8147 - val_loss: 1.3436\n",
            "Epoch 723/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7123 - val_loss: 1.6038\n",
            "Epoch 724/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8065 - val_loss: 1.5103\n",
            "Epoch 725/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7532 - val_loss: 1.8437\n",
            "Epoch 726/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7802 - val_loss: 1.9645\n",
            "Epoch 727/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7281 - val_loss: 1.8443\n",
            "Epoch 728/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7430 - val_loss: 1.3979\n",
            "Epoch 729/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7588 - val_loss: 1.8212\n",
            "Epoch 730/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7754 - val_loss: 1.8230\n",
            "Epoch 731/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7551 - val_loss: 1.3674\n",
            "Epoch 732/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8441 - val_loss: 1.6336\n",
            "Epoch 733/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7668 - val_loss: 2.3433\n",
            "Epoch 734/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7360 - val_loss: 1.7526\n",
            "Epoch 735/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7164 - val_loss: 1.3561\n",
            "Epoch 736/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8246 - val_loss: 1.4394\n",
            "Epoch 737/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7239 - val_loss: 1.5538\n",
            "Epoch 738/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7495 - val_loss: 1.6087\n",
            "Epoch 739/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7565 - val_loss: 1.6847\n",
            "Epoch 740/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7047 - val_loss: 1.7183\n",
            "Epoch 741/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7556 - val_loss: 1.8921\n",
            "Epoch 742/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7073 - val_loss: 1.6000\n",
            "Epoch 743/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7516 - val_loss: 2.0850\n",
            "Epoch 744/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7271 - val_loss: 1.6262\n",
            "Epoch 745/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7016 - val_loss: 1.8092\n",
            "Epoch 746/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8521 - val_loss: 2.7482\n",
            "Epoch 747/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6836 - val_loss: 1.9983\n",
            "Epoch 748/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7369 - val_loss: 2.6730\n",
            "Epoch 749/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7625 - val_loss: 1.4911\n",
            "Epoch 750/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7078 - val_loss: 1.4170\n",
            "Epoch 751/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7367 - val_loss: 2.1563\n",
            "Epoch 752/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7210 - val_loss: 1.8405\n",
            "Epoch 753/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7664 - val_loss: 2.3409\n",
            "Epoch 754/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8135 - val_loss: 1.6749\n",
            "Epoch 755/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7504 - val_loss: 1.2814\n",
            "Epoch 756/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7322 - val_loss: 1.5326\n",
            "Epoch 757/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7131 - val_loss: 1.5939\n",
            "Epoch 758/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7508 - val_loss: 1.5711\n",
            "Epoch 759/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7461 - val_loss: 2.0562\n",
            "Epoch 760/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7198 - val_loss: 1.6805\n",
            "Epoch 761/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7529 - val_loss: 1.7111\n",
            "Epoch 762/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6663 - val_loss: 1.9974\n",
            "Epoch 763/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7733 - val_loss: 1.6819\n",
            "Epoch 764/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7344 - val_loss: 1.9415\n",
            "Epoch 765/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7980 - val_loss: 1.6358\n",
            "Epoch 766/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7012 - val_loss: 1.5274\n",
            "Epoch 767/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7455 - val_loss: 1.8337\n",
            "Epoch 768/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7417 - val_loss: 1.6053\n",
            "Epoch 769/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7601 - val_loss: 1.4810\n",
            "Epoch 770/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7563 - val_loss: 1.7815\n",
            "Epoch 771/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8473 - val_loss: 1.2888\n",
            "Epoch 772/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7538 - val_loss: 2.1045\n",
            "Epoch 773/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7290 - val_loss: 1.8671\n",
            "Epoch 774/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6975 - val_loss: 1.3845\n",
            "Epoch 775/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7386 - val_loss: 1.8691\n",
            "Epoch 776/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7565 - val_loss: 1.8221\n",
            "Epoch 777/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7180 - val_loss: 1.5422\n",
            "Epoch 778/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7793 - val_loss: 1.4690\n",
            "Epoch 779/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7714 - val_loss: 1.4175\n",
            "Epoch 780/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7235 - val_loss: 1.8960\n",
            "Epoch 781/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7464 - val_loss: 1.2904\n",
            "Epoch 782/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7315 - val_loss: 1.7840\n",
            "Epoch 783/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7270 - val_loss: 1.3143\n",
            "Epoch 784/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7287 - val_loss: 1.4622\n",
            "Epoch 785/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7472 - val_loss: 1.3113\n",
            "Epoch 786/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7362 - val_loss: 1.7580\n",
            "Epoch 787/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7838 - val_loss: 1.3560\n",
            "Epoch 788/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7549 - val_loss: 1.3953\n",
            "Epoch 789/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8192 - val_loss: 1.4528\n",
            "Epoch 790/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7140 - val_loss: 1.5844\n",
            "Epoch 791/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8308 - val_loss: 1.7056\n",
            "Epoch 792/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7783 - val_loss: 1.7226\n",
            "Epoch 793/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7371 - val_loss: 1.9109\n",
            "Epoch 794/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7155 - val_loss: 1.2865\n",
            "Epoch 795/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7345 - val_loss: 1.5843\n",
            "Epoch 796/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8024 - val_loss: 1.3889\n",
            "Epoch 797/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7126 - val_loss: 1.6379\n",
            "Epoch 798/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7491 - val_loss: 1.3463\n",
            "Epoch 799/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7258 - val_loss: 1.5084\n",
            "Epoch 800/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7413 - val_loss: 1.6316\n",
            "Epoch 801/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7628 - val_loss: 1.5958\n",
            "Epoch 802/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7473 - val_loss: 1.7785\n",
            "Epoch 803/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7806 - val_loss: 1.7605\n",
            "Epoch 804/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6878 - val_loss: 2.9253\n",
            "Epoch 805/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7469 - val_loss: 1.8854\n",
            "Epoch 806/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7444 - val_loss: 1.8440\n",
            "Epoch 807/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7961 - val_loss: 1.3730\n",
            "Epoch 808/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6987 - val_loss: 1.9643\n",
            "Epoch 809/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7493 - val_loss: 1.3768\n",
            "Epoch 810/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7035 - val_loss: 1.4685\n",
            "Epoch 811/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7632 - val_loss: 1.6384\n",
            "Epoch 812/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7413 - val_loss: 1.5672\n",
            "Epoch 813/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7197 - val_loss: 1.4758\n",
            "Epoch 814/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7276 - val_loss: 1.9399\n",
            "Epoch 815/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7543 - val_loss: 1.5870\n",
            "Epoch 816/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7010 - val_loss: 2.2094\n",
            "Epoch 817/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7622 - val_loss: 1.6338\n",
            "Epoch 818/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6771 - val_loss: 1.4572\n",
            "Epoch 819/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7841 - val_loss: 1.4398\n",
            "Epoch 820/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7630 - val_loss: 1.4467\n",
            "Epoch 821/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7708 - val_loss: 1.5517\n",
            "Epoch 822/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7491 - val_loss: 1.8972\n",
            "Epoch 823/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7059 - val_loss: 2.7979\n",
            "Epoch 824/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7917 - val_loss: 2.5238\n",
            "Epoch 825/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7017 - val_loss: 1.5709\n",
            "Epoch 826/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7185 - val_loss: 1.9312\n",
            "Epoch 827/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7212 - val_loss: 1.5475\n",
            "Epoch 828/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6772 - val_loss: 1.6029\n",
            "Epoch 829/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7464 - val_loss: 1.7744\n",
            "Epoch 830/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7422 - val_loss: 1.4928\n",
            "Epoch 831/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7691 - val_loss: 1.3669\n",
            "Epoch 832/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8197 - val_loss: 1.3713\n",
            "Epoch 833/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7208 - val_loss: 1.9630\n",
            "Epoch 834/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7371 - val_loss: 1.4233\n",
            "Epoch 835/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7938 - val_loss: 1.4425\n",
            "Epoch 836/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8021 - val_loss: 1.4753\n",
            "Epoch 837/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7350 - val_loss: 1.4380\n",
            "Epoch 838/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7122 - val_loss: 1.2707\n",
            "Epoch 839/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7116 - val_loss: 1.3785\n",
            "Epoch 840/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6797 - val_loss: 2.0453\n",
            "Epoch 841/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7550 - val_loss: 1.5868\n",
            "Epoch 842/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7530 - val_loss: 1.5940\n",
            "Epoch 843/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7787 - val_loss: 2.6661\n",
            "Epoch 844/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6565 - val_loss: 2.2817\n",
            "Epoch 845/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6903 - val_loss: 2.5776\n",
            "Epoch 846/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7647 - val_loss: 1.2352\n",
            "Epoch 847/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7431 - val_loss: 1.6454\n",
            "Epoch 848/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8134 - val_loss: 1.9540\n",
            "Epoch 849/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7267 - val_loss: 1.9766\n",
            "Epoch 850/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6982 - val_loss: 1.3619\n",
            "Epoch 851/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7213 - val_loss: 1.9143\n",
            "Epoch 852/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7337 - val_loss: 1.4593\n",
            "Epoch 853/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7028 - val_loss: 1.7212\n",
            "Epoch 854/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6846 - val_loss: 1.5764\n",
            "Epoch 855/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7354 - val_loss: 1.6875\n",
            "Epoch 856/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7014 - val_loss: 1.2483\n",
            "Epoch 857/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7665 - val_loss: 1.7206\n",
            "Epoch 858/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7717 - val_loss: 1.5790\n",
            "Epoch 859/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7927 - val_loss: 1.4620\n",
            "Epoch 860/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7205 - val_loss: 1.2315\n",
            "Epoch 861/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6874 - val_loss: 1.9254\n",
            "Epoch 862/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7531 - val_loss: 1.4763\n",
            "Epoch 863/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7235 - val_loss: 1.7331\n",
            "Epoch 864/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7540 - val_loss: 1.6788\n",
            "Epoch 865/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8071 - val_loss: 2.4496\n",
            "Epoch 866/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7865 - val_loss: 1.8505\n",
            "Epoch 867/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7080 - val_loss: 1.8275\n",
            "Epoch 868/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7060 - val_loss: 1.7975\n",
            "Epoch 869/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7822 - val_loss: 1.4815\n",
            "Epoch 870/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7979 - val_loss: 1.7347\n",
            "Epoch 871/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7450 - val_loss: 1.9923\n",
            "Epoch 872/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6958 - val_loss: 1.3779\n",
            "Epoch 873/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7361 - val_loss: 1.4006\n",
            "Epoch 874/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7275 - val_loss: 1.2973\n",
            "Epoch 875/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7471 - val_loss: 1.7146\n",
            "Epoch 876/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6727 - val_loss: 1.6010\n",
            "Epoch 877/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6963 - val_loss: 2.1749\n",
            "Epoch 878/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8036 - val_loss: 1.8042\n",
            "Epoch 879/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6414 - val_loss: 1.5269\n",
            "Epoch 880/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7172 - val_loss: 1.6409\n",
            "Epoch 881/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6754 - val_loss: 1.5714\n",
            "Epoch 882/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7552 - val_loss: 1.7839\n",
            "Epoch 883/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7263 - val_loss: 1.8497\n",
            "Epoch 884/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7386 - val_loss: 1.8336\n",
            "Epoch 885/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6739 - val_loss: 2.2550\n",
            "Epoch 886/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7247 - val_loss: 1.9592\n",
            "Epoch 887/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7503 - val_loss: 1.7545\n",
            "Epoch 888/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7147 - val_loss: 1.8744\n",
            "Epoch 889/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6904 - val_loss: 4.0803\n",
            "Epoch 890/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7770 - val_loss: 1.7103\n",
            "Epoch 891/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7211 - val_loss: 1.6894\n",
            "Epoch 892/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6896 - val_loss: 2.0773\n",
            "Epoch 893/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8169 - val_loss: 2.0369\n",
            "Epoch 894/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6904 - val_loss: 1.6348\n",
            "Epoch 895/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7125 - val_loss: 1.8628\n",
            "Epoch 896/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7212 - val_loss: 2.2808\n",
            "Epoch 897/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7353 - val_loss: 1.3297\n",
            "Epoch 898/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6893 - val_loss: 1.4675\n",
            "Epoch 899/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.7336 - val_loss: 3.8835\n",
            "Epoch 900/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7170 - val_loss: 1.6789\n",
            "Epoch 901/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8104 - val_loss: 1.5702\n",
            "Epoch 902/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7221 - val_loss: 2.7922\n",
            "Epoch 903/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7451 - val_loss: 1.5870\n",
            "Epoch 904/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7037 - val_loss: 1.9569\n",
            "Epoch 905/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6659 - val_loss: 4.1854\n",
            "Epoch 906/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7351 - val_loss: 3.4343\n",
            "Epoch 907/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7988 - val_loss: 1.7704\n",
            "Epoch 908/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7942 - val_loss: 1.7293\n",
            "Epoch 909/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6639 - val_loss: 1.5228\n",
            "Epoch 910/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7220 - val_loss: 1.2644\n",
            "Epoch 911/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6787 - val_loss: 1.4322\n",
            "Epoch 912/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8032 - val_loss: 1.4469\n",
            "Epoch 913/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6794 - val_loss: 1.3702\n",
            "Epoch 914/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7265 - val_loss: 1.6583\n",
            "Epoch 915/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6924 - val_loss: 2.2486\n",
            "Epoch 916/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6852 - val_loss: 1.4936\n",
            "Epoch 917/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7388 - val_loss: 1.4346\n",
            "Epoch 918/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6829 - val_loss: 1.6120\n",
            "Epoch 919/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6939 - val_loss: 1.6980\n",
            "Epoch 920/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6318 - val_loss: 2.0711\n",
            "Epoch 921/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7040 - val_loss: 1.7159\n",
            "Epoch 922/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7551 - val_loss: 1.8714\n",
            "Epoch 923/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6989 - val_loss: 1.9051\n",
            "Epoch 924/5000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.7108 - val_loss: 1.9618\n",
            "Epoch 925/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7653 - val_loss: 1.2746\n",
            "Epoch 926/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7457 - val_loss: 1.8404\n",
            "Epoch 927/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.8247 - val_loss: 1.6448\n",
            "Epoch 928/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6524 - val_loss: 1.4215\n",
            "Epoch 929/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7218 - val_loss: 1.9332\n",
            "Epoch 930/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7175 - val_loss: 1.8832\n",
            "Epoch 931/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7197 - val_loss: 1.7319\n",
            "Epoch 932/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7486 - val_loss: 1.4688\n",
            "Epoch 933/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6741 - val_loss: 1.5653\n",
            "Epoch 934/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7319 - val_loss: 2.0915\n",
            "Epoch 935/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.7470 - val_loss: 2.1754\n",
            "Epoch 936/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6916 - val_loss: 1.6675\n",
            "Epoch 937/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.7172 - val_loss: 1.7682\n",
            "Epoch 938/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7748 - val_loss: 1.6520\n",
            "Epoch 939/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8093 - val_loss: 1.6321\n",
            "Epoch 940/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7812 - val_loss: 1.5688\n",
            "Epoch 941/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6689 - val_loss: 2.1301\n",
            "Epoch 942/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6961 - val_loss: 1.5404\n",
            "Epoch 943/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6462 - val_loss: 1.8963\n",
            "Epoch 944/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6992 - val_loss: 1.5011\n",
            "Epoch 945/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7263 - val_loss: 1.7272\n",
            "Epoch 946/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7056 - val_loss: 1.9819\n",
            "Epoch 947/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7522 - val_loss: 2.2982\n",
            "Epoch 948/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7717 - val_loss: 2.7962\n",
            "Epoch 949/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7604 - val_loss: 2.0121\n",
            "Epoch 950/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6874 - val_loss: 1.8197\n",
            "Epoch 951/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7348 - val_loss: 1.4705\n",
            "Epoch 952/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6674 - val_loss: 1.7516\n",
            "Epoch 953/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7060 - val_loss: 2.2577\n",
            "Epoch 954/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7564 - val_loss: 1.7063\n",
            "Epoch 955/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7698 - val_loss: 1.3498\n",
            "Epoch 956/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8225 - val_loss: 1.5592\n",
            "Epoch 957/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6598 - val_loss: 2.7070\n",
            "Epoch 958/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7537 - val_loss: 1.6381\n",
            "Epoch 959/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7512 - val_loss: 1.7934\n",
            "Epoch 960/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7589 - val_loss: 1.5121\n",
            "Epoch 961/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7051 - val_loss: 1.2731\n",
            "Epoch 962/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7130 - val_loss: 1.5582\n",
            "Epoch 963/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6805 - val_loss: 2.4721\n",
            "Epoch 964/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8746 - val_loss: 1.5848\n",
            "Epoch 965/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7202 - val_loss: 1.6158\n",
            "Epoch 966/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7391 - val_loss: 1.7822\n",
            "Epoch 967/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7175 - val_loss: 1.6144\n",
            "Epoch 968/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7663 - val_loss: 2.2731\n",
            "Epoch 969/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7617 - val_loss: 1.4811\n",
            "Epoch 970/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6976 - val_loss: 1.7698\n",
            "Epoch 971/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7179 - val_loss: 2.4132\n",
            "Epoch 972/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7373 - val_loss: 1.4337\n",
            "Epoch 973/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7460 - val_loss: 1.4206\n",
            "Epoch 974/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7277 - val_loss: 1.5021\n",
            "Epoch 975/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7252 - val_loss: 1.5809\n",
            "Epoch 976/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7611 - val_loss: 1.4540\n",
            "Epoch 977/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7298 - val_loss: 1.7603\n",
            "Epoch 978/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7300 - val_loss: 2.3079\n",
            "Epoch 979/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7308 - val_loss: 2.0613\n",
            "Epoch 980/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7211 - val_loss: 2.2202\n",
            "Epoch 981/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7015 - val_loss: 2.0989\n",
            "Epoch 982/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6885 - val_loss: 1.7968\n",
            "Epoch 983/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7569 - val_loss: 1.3739\n",
            "Epoch 984/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7421 - val_loss: 1.5729\n",
            "Epoch 985/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7277 - val_loss: 1.5507\n",
            "Epoch 986/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7149 - val_loss: 1.4508\n",
            "Epoch 987/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7323 - val_loss: 2.9014\n",
            "Epoch 988/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7456 - val_loss: 1.3855\n",
            "Epoch 989/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7079 - val_loss: 1.5170\n",
            "Epoch 990/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7124 - val_loss: 2.4015\n",
            "Epoch 991/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6519 - val_loss: 1.7217\n",
            "Epoch 992/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7214 - val_loss: 1.9538\n",
            "Epoch 993/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7686 - val_loss: 1.6954\n",
            "Epoch 994/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7188 - val_loss: 1.6951\n",
            "Epoch 995/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7135 - val_loss: 1.4951\n",
            "Epoch 996/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6857 - val_loss: 1.2710\n",
            "Epoch 997/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7404 - val_loss: 1.5411\n",
            "Epoch 998/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7516 - val_loss: 2.2564\n",
            "Epoch 999/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7244 - val_loss: 2.4961\n",
            "Epoch 1000/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6508 - val_loss: 2.5816\n",
            "Epoch 1001/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6567 - val_loss: 1.7042\n",
            "Epoch 1002/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7441 - val_loss: 1.3473\n",
            "Epoch 1003/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7447 - val_loss: 2.0689\n",
            "Epoch 1004/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7025 - val_loss: 2.4844\n",
            "Epoch 1005/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6415 - val_loss: 1.4135\n",
            "Epoch 1006/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7012 - val_loss: 1.4286\n",
            "Epoch 1007/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6761 - val_loss: 2.4994\n",
            "Epoch 1008/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7119 - val_loss: 2.0439\n",
            "Epoch 1009/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7706 - val_loss: 1.5844\n",
            "Epoch 1010/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6789 - val_loss: 1.7177\n",
            "Epoch 1011/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6874 - val_loss: 1.5464\n",
            "Epoch 1012/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7071 - val_loss: 1.5819\n",
            "Epoch 1013/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6926 - val_loss: 2.0676\n",
            "Epoch 1014/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7181 - val_loss: 1.4562\n",
            "Epoch 1015/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7231 - val_loss: 1.8891\n",
            "Epoch 1016/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7399 - val_loss: 1.5936\n",
            "Epoch 1017/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6613 - val_loss: 1.7637\n",
            "Epoch 1018/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7378 - val_loss: 1.6547\n",
            "Epoch 1019/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7451 - val_loss: 1.3904\n",
            "Epoch 1020/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6857 - val_loss: 1.4724\n",
            "Epoch 1021/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7444 - val_loss: 2.4906\n",
            "Epoch 1022/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6798 - val_loss: 3.1313\n",
            "Epoch 1023/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7123 - val_loss: 1.7802\n",
            "Epoch 1024/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7550 - val_loss: 2.3436\n",
            "Epoch 1025/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7306 - val_loss: 2.3413\n",
            "Epoch 1026/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6785 - val_loss: 2.3857\n",
            "Epoch 1027/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7021 - val_loss: 1.9334\n",
            "Epoch 1028/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7256 - val_loss: 1.3433\n",
            "Epoch 1029/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7315 - val_loss: 1.7050\n",
            "Epoch 1030/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7079 - val_loss: 1.4201\n",
            "Epoch 1031/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6852 - val_loss: 2.1875\n",
            "Epoch 1032/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6732 - val_loss: 1.7719\n",
            "Epoch 1033/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7288 - val_loss: 1.8272\n",
            "Epoch 1034/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7111 - val_loss: 1.8693\n",
            "Epoch 1035/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7632 - val_loss: 1.4511\n",
            "Epoch 1036/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6807 - val_loss: 1.4080\n",
            "Epoch 1037/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7220 - val_loss: 1.5907\n",
            "Epoch 1038/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6897 - val_loss: 1.8673\n",
            "Epoch 1039/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7220 - val_loss: 1.7298\n",
            "Epoch 1040/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7420 - val_loss: 1.9850\n",
            "Epoch 1041/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7456 - val_loss: 2.6942\n",
            "Epoch 1042/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7228 - val_loss: 1.9779\n",
            "Epoch 1043/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6907 - val_loss: 1.6519\n",
            "Epoch 1044/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7241 - val_loss: 2.0274\n",
            "Epoch 1045/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8245 - val_loss: 1.5011\n",
            "Epoch 1046/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7017 - val_loss: 1.8031\n",
            "Epoch 1047/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6851 - val_loss: 2.0734\n",
            "Epoch 1048/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7327 - val_loss: 1.8835\n",
            "Epoch 1049/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8030 - val_loss: 2.1950\n",
            "Epoch 1050/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7594 - val_loss: 1.6807\n",
            "Epoch 1051/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6850 - val_loss: 1.5740\n",
            "Epoch 1052/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6958 - val_loss: 1.7389\n",
            "Epoch 1053/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7077 - val_loss: 1.7449\n",
            "Epoch 1054/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7223 - val_loss: 2.3966\n",
            "Epoch 1055/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7540 - val_loss: 1.7468\n",
            "Epoch 1056/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7358 - val_loss: 1.2568\n",
            "Epoch 1057/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7183 - val_loss: 1.5085\n",
            "Epoch 1058/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7411 - val_loss: 2.2980\n",
            "Epoch 1059/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6545 - val_loss: 1.8905\n",
            "Epoch 1060/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9247 - val_loss: 1.5230\n",
            "Epoch 1061/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6928 - val_loss: 1.5150\n",
            "Epoch 1062/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6789 - val_loss: 1.8922\n",
            "Epoch 1063/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7315 - val_loss: 1.7943\n",
            "Epoch 1064/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7831 - val_loss: 1.5357\n",
            "Epoch 1065/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6829 - val_loss: 3.1809\n",
            "Epoch 1066/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6924 - val_loss: 1.8664\n",
            "Epoch 1067/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7329 - val_loss: 1.3689\n",
            "Epoch 1068/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6817 - val_loss: 1.6200\n",
            "Epoch 1069/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7103 - val_loss: 1.7365\n",
            "Epoch 1070/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6719 - val_loss: 1.6729\n",
            "Epoch 1071/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6996 - val_loss: 1.5770\n",
            "Epoch 1072/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7287 - val_loss: 1.7428\n",
            "Epoch 1073/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6975 - val_loss: 1.5262\n",
            "Epoch 1074/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7248 - val_loss: 1.4116\n",
            "Epoch 1075/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7466 - val_loss: 1.3665\n",
            "Epoch 1076/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6763 - val_loss: 1.5969\n",
            "Epoch 1077/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6981 - val_loss: 2.0847\n",
            "Epoch 1078/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8379 - val_loss: 1.5676\n",
            "Epoch 1079/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6828 - val_loss: 1.5230\n",
            "Epoch 1080/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7575 - val_loss: 1.3610\n",
            "Epoch 1081/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7327 - val_loss: 2.3460\n",
            "Epoch 1082/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7160 - val_loss: 1.3958\n",
            "Epoch 1083/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7833 - val_loss: 1.2814\n",
            "Epoch 1084/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7082 - val_loss: 1.5348\n",
            "Epoch 1085/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7175 - val_loss: 1.8239\n",
            "Epoch 1086/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6468 - val_loss: 1.4461\n",
            "Epoch 1087/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7108 - val_loss: 1.8350\n",
            "Epoch 1088/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7098 - val_loss: 1.7580\n",
            "Epoch 1089/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6681 - val_loss: 2.0991\n",
            "Epoch 1090/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7651 - val_loss: 1.5273\n",
            "Epoch 1091/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7272 - val_loss: 2.4308\n",
            "Epoch 1092/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7057 - val_loss: 1.8498\n",
            "Epoch 1093/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8443 - val_loss: 1.7894\n",
            "Epoch 1094/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7293 - val_loss: 1.3653\n",
            "Epoch 1095/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7205 - val_loss: 2.6144\n",
            "Epoch 1096/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7811 - val_loss: 1.6667\n",
            "Epoch 1097/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6512 - val_loss: 1.6074\n",
            "Epoch 1098/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6873 - val_loss: 1.7628\n",
            "Epoch 1099/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6651 - val_loss: 1.3547\n",
            "Epoch 1100/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6658 - val_loss: 1.7007\n",
            "Epoch 1101/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6803 - val_loss: 1.7920\n",
            "Epoch 1102/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7438 - val_loss: 2.0288\n",
            "Epoch 1103/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6870 - val_loss: 1.7741\n",
            "Epoch 1104/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6895 - val_loss: 2.2109\n",
            "Epoch 1105/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7201 - val_loss: 1.4114\n",
            "Epoch 1106/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7798 - val_loss: 1.3495\n",
            "Epoch 1107/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7218 - val_loss: 1.5794\n",
            "Epoch 1108/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6691 - val_loss: 1.5974\n",
            "Epoch 1109/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6850 - val_loss: 1.7100\n",
            "Epoch 1110/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6776 - val_loss: 2.7366\n",
            "Epoch 1111/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7481 - val_loss: 1.7826\n",
            "Epoch 1112/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6710 - val_loss: 1.9170\n",
            "Epoch 1113/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6751 - val_loss: 1.3485\n",
            "Epoch 1114/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7526 - val_loss: 1.9287\n",
            "Epoch 1115/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7321 - val_loss: 2.8937\n",
            "Epoch 1116/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7190 - val_loss: 1.5933\n",
            "Epoch 1117/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6600 - val_loss: 2.0415\n",
            "Epoch 1118/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7098 - val_loss: 1.7681\n",
            "Epoch 1119/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6826 - val_loss: 1.5508\n",
            "Epoch 1120/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7017 - val_loss: 2.8710\n",
            "Epoch 1121/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6923 - val_loss: 2.5307\n",
            "Epoch 1122/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6834 - val_loss: 2.4544\n",
            "Epoch 1123/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7275 - val_loss: 1.5606\n",
            "Epoch 1124/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7316 - val_loss: 1.3295\n",
            "Epoch 1125/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7014 - val_loss: 1.5888\n",
            "Epoch 1126/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6862 - val_loss: 1.6878\n",
            "Epoch 1127/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7402 - val_loss: 1.8766\n",
            "Epoch 1128/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6966 - val_loss: 1.6711\n",
            "Epoch 1129/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6978 - val_loss: 1.7748\n",
            "Epoch 1130/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6921 - val_loss: 1.9145\n",
            "Epoch 1131/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7300 - val_loss: 1.9727\n",
            "Epoch 1132/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6901 - val_loss: 1.6599\n",
            "Epoch 1133/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7308 - val_loss: 2.1361\n",
            "Epoch 1134/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6703 - val_loss: 1.6011\n",
            "Epoch 1135/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6980 - val_loss: 1.9530\n",
            "Epoch 1136/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6922 - val_loss: 1.3937\n",
            "Epoch 1137/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6885 - val_loss: 2.0335\n",
            "Epoch 1138/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6799 - val_loss: 1.4661\n",
            "Epoch 1139/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6727 - val_loss: 1.6151\n",
            "Epoch 1140/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6731 - val_loss: 1.9339\n",
            "Epoch 1141/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7475 - val_loss: 1.6938\n",
            "Epoch 1142/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6988 - val_loss: 3.0793\n",
            "Epoch 1143/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7434 - val_loss: 1.4809\n",
            "Epoch 1144/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6916 - val_loss: 2.4409\n",
            "Epoch 1145/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7797 - val_loss: 1.5405\n",
            "Epoch 1146/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8239 - val_loss: 1.8507\n",
            "Epoch 1147/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6851 - val_loss: 2.2459\n",
            "Epoch 1148/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7005 - val_loss: 1.7400\n",
            "Epoch 1149/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6980 - val_loss: 1.6464\n",
            "Epoch 1150/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6668 - val_loss: 2.4755\n",
            "Epoch 1151/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7410 - val_loss: 1.7974\n",
            "Epoch 1152/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6758 - val_loss: 1.9561\n",
            "Epoch 1153/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7305 - val_loss: 1.8431\n",
            "Epoch 1154/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6749 - val_loss: 2.0863\n",
            "Epoch 1155/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6796 - val_loss: 1.5889\n",
            "Epoch 1156/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6828 - val_loss: 2.2410\n",
            "Epoch 1157/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7473 - val_loss: 2.2675\n",
            "Epoch 1158/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7062 - val_loss: 3.8068\n",
            "Epoch 1159/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6963 - val_loss: 1.2749\n",
            "Epoch 1160/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6937 - val_loss: 1.8655\n",
            "Epoch 1161/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7269 - val_loss: 2.0835\n",
            "Epoch 1162/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7098 - val_loss: 1.9431\n",
            "Epoch 1163/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7699 - val_loss: 1.7306\n",
            "Epoch 1164/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7676 - val_loss: 2.8255\n",
            "Epoch 1165/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6549 - val_loss: 3.1706\n",
            "Epoch 1166/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7122 - val_loss: 1.7254\n",
            "Epoch 1167/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7262 - val_loss: 1.6739\n",
            "Epoch 1168/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7240 - val_loss: 1.6613\n",
            "Epoch 1169/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7309 - val_loss: 1.8094\n",
            "Epoch 1170/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7085 - val_loss: 2.1963\n",
            "Epoch 1171/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7098 - val_loss: 1.9813\n",
            "Epoch 1172/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7156 - val_loss: 1.9231\n",
            "Epoch 1173/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6332 - val_loss: 1.5242\n",
            "Epoch 1174/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8581 - val_loss: 2.2448\n",
            "Epoch 1175/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7329 - val_loss: 1.5479\n",
            "Epoch 1176/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6766 - val_loss: 1.8136\n",
            "Epoch 1177/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7823 - val_loss: 2.0433\n",
            "Epoch 1178/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7562 - val_loss: 1.8581\n",
            "Epoch 1179/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8037 - val_loss: 1.3595\n",
            "Epoch 1180/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6914 - val_loss: 1.9330\n",
            "Epoch 1181/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6424 - val_loss: 1.5914\n",
            "Epoch 1182/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6813 - val_loss: 1.8624\n",
            "Epoch 1183/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7397 - val_loss: 1.9523\n",
            "Epoch 1184/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7036 - val_loss: 2.1690\n",
            "Epoch 1185/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7327 - val_loss: 1.8232\n",
            "Epoch 1186/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7166 - val_loss: 1.4008\n",
            "Epoch 1187/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6910 - val_loss: 1.3470\n",
            "Epoch 1188/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7603 - val_loss: 2.0917\n",
            "Epoch 1189/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6869 - val_loss: 2.6520\n",
            "Epoch 1190/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7182 - val_loss: 2.3367\n",
            "Epoch 1191/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7399 - val_loss: 1.7796\n",
            "Epoch 1192/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7262 - val_loss: 1.9680\n",
            "Epoch 1193/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6839 - val_loss: 1.5951\n",
            "Epoch 1194/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7139 - val_loss: 1.8729\n",
            "Epoch 1195/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7694 - val_loss: 1.5875\n",
            "Epoch 1196/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7075 - val_loss: 1.5717\n",
            "Epoch 1197/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6711 - val_loss: 4.1439\n",
            "Epoch 1198/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6879 - val_loss: 2.1646\n",
            "Epoch 1199/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7164 - val_loss: 2.4207\n",
            "Epoch 1200/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7723 - val_loss: 1.5977\n",
            "Epoch 1201/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7421 - val_loss: 1.7139\n",
            "Epoch 1202/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7143 - val_loss: 1.4124\n",
            "Epoch 1203/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7003 - val_loss: 2.0353\n",
            "Epoch 1204/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7217 - val_loss: 1.4500\n",
            "Epoch 1205/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7412 - val_loss: 1.8682\n",
            "Epoch 1206/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7550 - val_loss: 1.5220\n",
            "Epoch 1207/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7006 - val_loss: 1.6599\n",
            "Epoch 1208/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6606 - val_loss: 1.4090\n",
            "Epoch 1209/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8064 - val_loss: 2.1514\n",
            "Epoch 1210/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7250 - val_loss: 1.6540\n",
            "Epoch 1211/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6660 - val_loss: 1.8255\n",
            "Epoch 1212/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7271 - val_loss: 1.5193\n",
            "Epoch 1213/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7244 - val_loss: 1.9220\n",
            "Epoch 1214/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7577 - val_loss: 1.9060\n",
            "Epoch 1215/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7484 - val_loss: 1.6784\n",
            "Epoch 1216/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.7474 - val_loss: 1.5171\n",
            "Epoch 1217/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7380 - val_loss: 1.5999\n",
            "Epoch 1218/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7097 - val_loss: 1.8123\n",
            "Epoch 1219/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8442 - val_loss: 1.5746\n",
            "Epoch 1220/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6720 - val_loss: 1.4343\n",
            "Epoch 1221/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7542 - val_loss: 1.5930\n",
            "Epoch 1222/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7776 - val_loss: 1.7441\n",
            "Epoch 1223/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6955 - val_loss: 1.5479\n",
            "Epoch 1224/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7142 - val_loss: 2.3984\n",
            "Epoch 1225/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6880 - val_loss: 1.5300\n",
            "Epoch 1226/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6871 - val_loss: 1.2993\n",
            "Epoch 1227/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7115 - val_loss: 2.2128\n",
            "Epoch 1228/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7813 - val_loss: 1.7249\n",
            "Epoch 1229/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6804 - val_loss: 1.7319\n",
            "Epoch 1230/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7011 - val_loss: 1.4152\n",
            "Epoch 1231/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6478 - val_loss: 1.7236\n",
            "Epoch 1232/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7651 - val_loss: 1.6822\n",
            "Epoch 1233/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7391 - val_loss: 2.1711\n",
            "Epoch 1234/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7819 - val_loss: 1.3727\n",
            "Epoch 1235/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7247 - val_loss: 1.7230\n",
            "Epoch 1236/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7521 - val_loss: 2.0494\n",
            "Epoch 1237/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6776 - val_loss: 1.9376\n",
            "Epoch 1238/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6719 - val_loss: 1.5777\n",
            "Epoch 1239/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7659 - val_loss: 1.5212\n",
            "Epoch 1240/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6926 - val_loss: 1.6270\n",
            "Epoch 1241/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7764 - val_loss: 1.5031\n",
            "Epoch 1242/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7157 - val_loss: 1.6852\n",
            "Epoch 1243/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7428 - val_loss: 2.3223\n",
            "Epoch 1244/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6928 - val_loss: 2.0102\n",
            "Epoch 1245/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7444 - val_loss: 1.5486\n",
            "Epoch 1246/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7123 - val_loss: 1.9434\n",
            "Epoch 1247/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7387 - val_loss: 1.9129\n",
            "Epoch 1248/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7048 - val_loss: 2.5215\n",
            "Epoch 1249/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7246 - val_loss: 1.3026\n",
            "Epoch 1250/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6885 - val_loss: 1.4180\n",
            "Epoch 1251/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6620 - val_loss: 1.6965\n",
            "Epoch 1252/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7194 - val_loss: 1.6042\n",
            "Epoch 1253/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6816 - val_loss: 1.5857\n",
            "Epoch 1254/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6976 - val_loss: 1.5350\n",
            "Epoch 1255/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7499 - val_loss: 1.9062\n",
            "Epoch 1256/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7181 - val_loss: 2.1418\n",
            "Epoch 1257/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7136 - val_loss: 1.5342\n",
            "Epoch 1258/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6606 - val_loss: 1.8221\n",
            "Epoch 1259/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6764 - val_loss: 2.1829\n",
            "Epoch 1260/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6879 - val_loss: 2.0759\n",
            "Epoch 1261/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7460 - val_loss: 2.1763\n",
            "Epoch 1262/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6834 - val_loss: 1.9644\n",
            "Epoch 1263/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7408 - val_loss: 1.8784\n",
            "Epoch 1264/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7399 - val_loss: 1.9220\n",
            "Epoch 1265/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7439 - val_loss: 2.5616\n",
            "Epoch 1266/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7437 - val_loss: 2.1719\n",
            "Epoch 1267/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8476 - val_loss: 2.3972\n",
            "Epoch 1268/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6928 - val_loss: 2.3237\n",
            "Epoch 1269/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7197 - val_loss: 1.7517\n",
            "Epoch 1270/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6965 - val_loss: 1.5709\n",
            "Epoch 1271/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6611 - val_loss: 2.4049\n",
            "Epoch 1272/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7133 - val_loss: 2.0179\n",
            "Epoch 1273/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7097 - val_loss: 2.0825\n",
            "Epoch 1274/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7080 - val_loss: 1.8751\n",
            "Epoch 1275/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7884 - val_loss: 2.0665\n",
            "Epoch 1276/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7219 - val_loss: 1.4214\n",
            "Epoch 1277/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6742 - val_loss: 2.2277\n",
            "Epoch 1278/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7000 - val_loss: 1.3933\n",
            "Epoch 1279/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7694 - val_loss: 2.5611\n",
            "Epoch 1280/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7610 - val_loss: 1.4198\n",
            "Epoch 1281/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6737 - val_loss: 1.8185\n",
            "Epoch 1282/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7118 - val_loss: 1.7486\n",
            "Epoch 1283/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7075 - val_loss: 1.8178\n",
            "Epoch 1284/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6778 - val_loss: 1.7621\n",
            "Epoch 1285/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7025 - val_loss: 1.5311\n",
            "Epoch 1286/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7077 - val_loss: 1.9395\n",
            "Epoch 1287/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8145 - val_loss: 1.7870\n",
            "Epoch 1288/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6655 - val_loss: 1.4043\n",
            "Epoch 1289/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6429 - val_loss: 1.8818\n",
            "Epoch 1290/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7120 - val_loss: 2.8767\n",
            "Epoch 1291/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7310 - val_loss: 2.2194\n",
            "Epoch 1292/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6930 - val_loss: 1.6812\n",
            "Epoch 1293/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6898 - val_loss: 1.2062\n",
            "Epoch 1294/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7249 - val_loss: 1.4979\n",
            "Epoch 1295/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6157 - val_loss: 1.6506\n",
            "Epoch 1296/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7208 - val_loss: 1.8575\n",
            "Epoch 1297/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7013 - val_loss: 2.7520\n",
            "Epoch 1298/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7912 - val_loss: 2.4153\n",
            "Epoch 1299/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6904 - val_loss: 1.7724\n",
            "Epoch 1300/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6614 - val_loss: 1.6581\n",
            "Epoch 1301/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6809 - val_loss: 1.4781\n",
            "Epoch 1302/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7172 - val_loss: 2.7678\n",
            "Epoch 1303/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7934 - val_loss: 1.8751\n",
            "Epoch 1304/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7396 - val_loss: 2.8762\n",
            "Epoch 1305/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6766 - val_loss: 2.7925\n",
            "Epoch 1306/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6541 - val_loss: 1.7794\n",
            "Epoch 1307/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7601 - val_loss: 1.5962\n",
            "Epoch 1308/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6387 - val_loss: 1.8872\n",
            "Epoch 1309/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6963 - val_loss: 1.5344\n",
            "Epoch 1310/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6148 - val_loss: 2.3063\n",
            "Epoch 1311/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6978 - val_loss: 1.9864\n",
            "Epoch 1312/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7028 - val_loss: 1.4474\n",
            "Epoch 1313/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6960 - val_loss: 1.7156\n",
            "Epoch 1314/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6417 - val_loss: 1.4304\n",
            "Epoch 1315/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7482 - val_loss: 1.5426\n",
            "Epoch 1316/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7784 - val_loss: 1.8500\n",
            "Epoch 1317/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6761 - val_loss: 1.5073\n",
            "Epoch 1318/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6260 - val_loss: 2.1589\n",
            "Epoch 1319/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6878 - val_loss: 2.4441\n",
            "Epoch 1320/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.6724 - val_loss: 1.9984\n",
            "Epoch 1321/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6760 - val_loss: 1.6811\n",
            "Epoch 1322/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6408 - val_loss: 1.5856\n",
            "Epoch 1323/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6870 - val_loss: 2.6735\n",
            "Epoch 1324/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7529 - val_loss: 1.6413\n",
            "Epoch 1325/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6738 - val_loss: 1.3974\n",
            "Epoch 1326/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7401 - val_loss: 1.7609\n",
            "Epoch 1327/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7073 - val_loss: 1.7144\n",
            "Epoch 1328/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7574 - val_loss: 1.7218\n",
            "Epoch 1329/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7206 - val_loss: 1.5538\n",
            "Epoch 1330/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7113 - val_loss: 1.7827\n",
            "Epoch 1331/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7260 - val_loss: 1.7627\n",
            "Epoch 1332/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.7179 - val_loss: 2.0059\n",
            "Epoch 1333/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6749 - val_loss: 2.3206\n",
            "Epoch 1334/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.6667 - val_loss: 1.9644\n",
            "Epoch 1335/5000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.7209 - val_loss: 1.5417\n",
            "Epoch 1336/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.6752 - val_loss: 1.8085\n",
            "Epoch 1337/5000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.7470 - val_loss: 1.7989\n",
            "Epoch 1338/5000\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.6497 - val_loss: 1.7294\n",
            "Epoch 1339/5000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.7516 - val_loss: 1.6710\n",
            "Epoch 1340/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6690 - val_loss: 1.7333\n",
            "Epoch 1341/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.8226 - val_loss: 1.9893\n",
            "Epoch 1342/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7606 - val_loss: 1.8903\n",
            "Epoch 1343/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.7456 - val_loss: 1.5288\n",
            "Epoch 1344/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6969 - val_loss: 1.3419\n",
            "Epoch 1345/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.6804 - val_loss: 2.2056\n",
            "Epoch 1346/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6288 - val_loss: 1.7989\n",
            "Epoch 1347/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7632 - val_loss: 1.6537\n",
            "Epoch 1348/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7425 - val_loss: 2.2522\n",
            "Epoch 1349/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7353 - val_loss: 1.3893\n",
            "Epoch 1350/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6600 - val_loss: 1.7856\n",
            "Epoch 1351/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7341 - val_loss: 1.5730\n",
            "Epoch 1352/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7186 - val_loss: 1.8179\n",
            "Epoch 1353/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7265 - val_loss: 1.6105\n",
            "Epoch 1354/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6912 - val_loss: 1.6353\n",
            "Epoch 1355/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6779 - val_loss: 1.5562\n",
            "Epoch 1356/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7081 - val_loss: 2.2582\n",
            "Epoch 1357/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6993 - val_loss: 1.3607\n",
            "Epoch 1358/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7235 - val_loss: 2.1451\n",
            "Epoch 1359/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7161 - val_loss: 1.7409\n",
            "Epoch 1360/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6799 - val_loss: 1.9889\n",
            "Epoch 1361/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6890 - val_loss: 1.8796\n",
            "Epoch 1362/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7040 - val_loss: 2.3638\n",
            "Epoch 1363/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7838 - val_loss: 1.8349\n",
            "Epoch 1364/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7618 - val_loss: 1.8440\n",
            "Epoch 1365/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6174 - val_loss: 1.9249\n",
            "Epoch 1366/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6917 - val_loss: 1.5011\n",
            "Epoch 1367/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6743 - val_loss: 1.7492\n",
            "Epoch 1368/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8197 - val_loss: 2.0065\n",
            "Epoch 1369/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7109 - val_loss: 2.2063\n",
            "Epoch 1370/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6758 - val_loss: 1.8124\n",
            "Epoch 1371/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7255 - val_loss: 1.4863\n",
            "Epoch 1372/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7180 - val_loss: 1.4269\n",
            "Epoch 1373/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6924 - val_loss: 2.3330\n",
            "Epoch 1374/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7476 - val_loss: 1.5460\n",
            "Epoch 1375/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6723 - val_loss: 2.0447\n",
            "Epoch 1376/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6898 - val_loss: 1.8310\n",
            "Epoch 1377/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7049 - val_loss: 3.0775\n",
            "Epoch 1378/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7428 - val_loss: 2.2245\n",
            "Epoch 1379/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6979 - val_loss: 1.7712\n",
            "Epoch 1380/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6448 - val_loss: 1.2927\n",
            "Epoch 1381/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7131 - val_loss: 1.8977\n",
            "Epoch 1382/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7393 - val_loss: 1.6318\n",
            "Epoch 1383/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6982 - val_loss: 1.9954\n",
            "Epoch 1384/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6829 - val_loss: 1.5352\n",
            "Epoch 1385/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8150 - val_loss: 1.6184\n",
            "Epoch 1386/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7210 - val_loss: 1.4563\n",
            "Epoch 1387/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6801 - val_loss: 3.6656\n",
            "Epoch 1388/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6707 - val_loss: 1.4352\n",
            "Epoch 1389/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6607 - val_loss: 2.4388\n",
            "Epoch 1390/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7411 - val_loss: 2.2263\n",
            "Epoch 1391/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6811 - val_loss: 1.6943\n",
            "Epoch 1392/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7071 - val_loss: 2.1840\n",
            "Epoch 1393/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6694 - val_loss: 2.9018\n",
            "Epoch 1394/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6522 - val_loss: 2.3652\n",
            "Epoch 1395/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7233 - val_loss: 1.6591\n",
            "Epoch 1396/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6641 - val_loss: 5.7330\n",
            "Epoch 1397/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7027 - val_loss: 2.2376\n",
            "Epoch 1398/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7181 - val_loss: 2.4319\n",
            "Epoch 1399/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7334 - val_loss: 3.3483\n",
            "Epoch 1400/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6775 - val_loss: 1.8750\n",
            "Epoch 1401/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7052 - val_loss: 2.3115\n",
            "Epoch 1402/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7296 - val_loss: 1.9673\n",
            "Epoch 1403/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6440 - val_loss: 1.4915\n",
            "Epoch 1404/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7311 - val_loss: 2.6452\n",
            "Epoch 1405/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6694 - val_loss: 2.5071\n",
            "Epoch 1406/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7101 - val_loss: 1.6756\n",
            "Epoch 1407/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6725 - val_loss: 1.3785\n",
            "Epoch 1408/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6372 - val_loss: 1.6439\n",
            "Epoch 1409/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6727 - val_loss: 1.5847\n",
            "Epoch 1410/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6989 - val_loss: 2.3725\n",
            "Epoch 1411/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6936 - val_loss: 1.8536\n",
            "Epoch 1412/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7352 - val_loss: 2.1587\n",
            "Epoch 1413/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6843 - val_loss: 2.0361\n",
            "Epoch 1414/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7276 - val_loss: 2.0459\n",
            "Epoch 1415/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7333 - val_loss: 1.9931\n",
            "Epoch 1416/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6658 - val_loss: 1.9129\n",
            "Epoch 1417/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6876 - val_loss: 1.9074\n",
            "Epoch 1418/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6578 - val_loss: 2.4194\n",
            "Epoch 1419/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6812 - val_loss: 2.1204\n",
            "Epoch 1420/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7147 - val_loss: 2.0453\n",
            "Epoch 1421/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6977 - val_loss: 1.9017\n",
            "Epoch 1422/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6703 - val_loss: 1.9116\n",
            "Epoch 1423/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6626 - val_loss: 1.6269\n",
            "Epoch 1424/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6862 - val_loss: 2.1257\n",
            "Epoch 1425/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7951 - val_loss: 1.6963\n",
            "Epoch 1426/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6148 - val_loss: 2.8853\n",
            "Epoch 1427/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6564 - val_loss: 1.5897\n",
            "Epoch 1428/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6642 - val_loss: 2.5171\n",
            "Epoch 1429/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6849 - val_loss: 2.4510\n",
            "Epoch 1430/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6955 - val_loss: 1.5904\n",
            "Epoch 1431/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7269 - val_loss: 2.1284\n",
            "Epoch 1432/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6850 - val_loss: 3.0638\n",
            "Epoch 1433/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.7045 - val_loss: 2.2000\n",
            "Epoch 1434/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6734 - val_loss: 1.6139\n",
            "Epoch 1435/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7410 - val_loss: 1.8663\n",
            "Epoch 1436/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7116 - val_loss: 1.4079\n",
            "Epoch 1437/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7322 - val_loss: 1.5417\n",
            "Epoch 1438/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7227 - val_loss: 1.8205\n",
            "Epoch 1439/5000\n",
            "11/23 [=============>................] - ETA: 0s - loss: 0.5635Restoring model weights from the end of the best epoch: 439.\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7124 - val_loss: 1.8255\n",
            "Epoch 1439: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-b383c653ffcb>:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 1.080522894859314\n",
            "Train loss: 0.7719659209251404\n",
            "Test loss: 4.472787380218506\n",
            "dO18 RMSE: 2.3271043504060485\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       24.042000           0.345970\n",
            "1       25.240000           0.035950\n",
            "2       25.782000           0.372220\n",
            "3       25.076000           0.230280\n",
            "4       25.966000           0.172480\n",
            "5       27.434000           0.501030\n",
            "6       28.156000           0.999030\n",
            "7       26.836000           0.120880\n",
            "8       28.180000           0.778250\n",
            "9       26.834000           0.094930\n",
            "10      26.644000           0.488430\n",
            "11      26.772000           0.373370\n",
            "12      27.684280           1.216389\n",
            "13      27.403235           0.892280\n",
            "14      24.777670           0.736571\n",
            "15      25.551850           0.312355\n",
            "16      25.115885           0.033519\n",
            "17      25.987815           5.280825\n",
            "18      24.132031           0.389042\n",
            "19      24.898000           0.236070\n",
            "20      23.944000           0.158130\n",
            "21      26.018000           0.964170\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       25.617920           1.490253\n",
            "1       25.617920           1.490253\n",
            "2       25.617920           1.490253\n",
            "3       25.617920           1.490253\n",
            "4       25.617920           1.490253\n",
            "5       30.731934           0.820746\n",
            "6       30.683950           0.840528\n",
            "7       30.683950           0.840528\n",
            "8       30.683950           0.840528\n",
            "9       30.683950           0.840528\n",
            "10      30.683950           0.840528\n",
            "11      30.683950           0.840528\n",
            "12      30.731934           0.820746\n",
            "13      30.683950           0.840528\n",
            "14      26.267229           1.044660\n",
            "15      26.151340           1.102831\n",
            "16      26.153357           1.113458\n",
            "17      26.147745           1.083874\n",
            "18      26.149260           1.107550\n",
            "19      25.617920           1.490253\n",
            "20      25.617920           1.490253\n",
            "21      25.617920           1.490253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/MyDrive/amazon_rainforest_files/variational/model/fixed_ablated_0809_ensemble.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}