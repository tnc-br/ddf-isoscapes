{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnc-br/ddf-isoscapes/blob/new_kl/dnn/briso_d13C_boosted_residuals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational model\n",
        "\n",
        "Find the mean/variance of O18 ratios (as well as N15 and C13 in the future) at a particular lat/lon across Brazil. At the bottom of the colab, train and evaluate 4 different versions of the model with different data partitioning strategies."
      ],
      "metadata": {
        "id": "-0IfT3kGwgK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "henIPlAPCb4i",
        "outputId": "df8ced67-223c-42de-8743-bfffb77cc64c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import os\n",
        "from typing import List, Tuple, Dict\n",
        "from dataclasses import dataclass\n",
        "from joblib import dump\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.python.ops import math_ops\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "#@title Debugging\n",
        "# See https://zohaib.me/debugging-in-google-collab-notebook/ for tips,\n",
        "# as well as docs for pdb and ipdb.\n",
        "DEBUG = False #@param {type:\"boolean\"}\n",
        "USE_LOCAL_DRIVE = False #@param {type:\"boolean\"}\n",
        "LOCAL_DIR = \"/usr/local/google/home/ruru/Downloads/amazon_sample_data-20230712T203059Z-001\" #@param\n",
        "GDRIVE_DIR = \"MyDrive/amazon_rainforest_files/\" #@param\n",
        "FP_ROOT = LOCAL_DIR\n",
        "\n",
        "MODEL_SAVE_LOCATION = \"MyDrive/amazon_rainforest_files/variational/model\" #@param\n",
        "\n",
        "def get_model_save_location(filename) -> str:\n",
        "  root = '' if USE_LOCAL_DRIVE else '/content/gdrive'\n",
        "  return os.path.join(root, MODEL_SAVE_LOCATION, filename)\n",
        "\n",
        "# Access data stored on Google Drive if not reading data locally.\n",
        "if not USE_LOCAL_DRIVE:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  global FP_ROOT\n",
        "  FP_ROOT = os.path.join('/content/gdrive', GDRIVE_DIR)\n",
        "\n",
        "if DEBUG:\n",
        "    %pip install -Uqq ipdb\n",
        "    import ipdb\n",
        "    %pdb on"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "!if [ ! -d \"/content/ddf_common_stub\" ] ; then git clone -b test https://github.com/tnc-br/ddf_common_stub.git; fi\n",
        "sys.path.append(\"/content/ddf_common_stub/\")\n",
        "import ddfimport\n",
        "ddfimport.ddf_source_control_pane()\n",
        "\n"
      ],
      "metadata": {
        "id": "AXh86HFwXiax",
        "outputId": "c1666efa-90dc-46ee-d074-74d46cbb2b49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470,
          "referenced_widgets": [
            "70bfdc7ee2bd490181b70fd4abd2508f",
            "8d6e76aa1506472e9ac0025470e508c8",
            "ecad0b67a823441ea9e2bfaf1040014c",
            "a811ba449e4645eca6f055881e5d5ee4",
            "b41f3416523c4fdeb6715a3bd0800297",
            "8f4c97c8a317402e8cd87e2a9f68b110",
            "79d54b7b7d9544bc88b83876a9f90b76",
            "65e718a7efcf4110825afcc58175451b",
            "459ebaa22b5045d38717346cf92c50bd",
            "9effba8769064b20b43b0f8b71bbae53",
            "9a2f3f4e1bd84bb18e478c45f75aa1dc",
            "6ec745afa16e44d18c6267ee871875a9",
            "f8d09ea1a1b845128b6a2dc8f146ff87",
            "7c8783981c9c4d599c743f4accfd6e4b",
            "6da1e760f1c14114a3a8f19875afdd12",
            "0b9bbac4365f410dabb92987ee6f49e4",
            "919e5f838acf480f8c6a478cf434ddcd",
            "37e61ed537774c75abce882b8fb642cf",
            "5c3b3d66a2c549568086dc5b4520e375",
            "b136df15d48f4833a62f847901a2d306",
            "c4f3815be1f14075a159b6b76e98676e",
            "aaa5ed5f345948fc91619f49fabf3573",
            "7b212a12d61547978eb7ab925447fa59"
          ]
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ddf_common_stub'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 11 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (11/11), 5.50 KiB | 5.50 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "interactive(children=(Text(value='', description='Email', placeholder='Enter email'), Text(value='', descripti…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70bfdc7ee2bd490181b70fd4abd2508f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import raster\n",
        "import importlib\n",
        "importlib.reload(raster)"
      ],
      "metadata": {
        "id": "0mUB0y0AXivp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bede6e2-896e-4016-e92a-ffaf9097b354"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'raster' from '/content/gdrive/MyDrive/gen_isoscape/ddf_common/raster.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQrEv57zCb4q"
      },
      "source": [
        "# Data preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path: str, columns_to_keep: List[str], side_raster_input):\n",
        "  df = pd.read_csv(path, encoding=\"ISO-8859-1\", sep=',')\n",
        "  df = df[df['d18O_variance'].notna()]\n",
        "  X = df\n",
        "  X = X.drop(df.columns.difference(columns_to_keep), axis=1)\n",
        "\n",
        "  for name, geotiff in side_raster_input.items():\n",
        "    X[name] = X.apply(lambda row: geotiff.value_at(row['long'], row['lat']), axis=1)\n",
        "    # The last two kriging columns need to remain last. Move new columns forward.\n",
        "\n",
        "  print(X.columns)\n",
        "\n",
        "  print(X['d13C_cel_inferred'])\n",
        "\n",
        "  X['d13C_cel'] = X['d13C_cel'].fillna(X['d13C_cel_inferred'])\n",
        "  X.drop(columns=['d13C_cel_inferred'], inplace=True)\n",
        "\n",
        "  print(X.columns)\n",
        "\n",
        "  Y = df[[\"d18O_mean\", \"d18O_variance\"]]\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "6XMee1aHfcik"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardization"
      ],
      "metadata": {
        "id": "DtkKhMOtb6cS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class FeaturesToLabels:\n",
        "  def __init__(self, X: pd.DataFrame, Y: pd.DataFrame):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "\n",
        "  def as_tuple(self):\n",
        "    return (self.X, self.Y)\n",
        "\n",
        "\n",
        "def create_feature_scaler(X: pd.DataFrame,\n",
        "                          columns_to_passthrough,\n",
        "                          columns_to_scale,\n",
        "                          columns_to_standardize) -> ColumnTransformer:\n",
        "  columns_to_standardize = columns_to_standardize + ['Mean Annual Temperature', 'Mean Annual Precipitation']\n",
        "  feature_scaler = ColumnTransformer(\n",
        "      [(column+'_normalizer', MinMaxScaler(), [column]) for column in columns_to_scale] +\n",
        "      [(column+'_standardizer', StandardScaler(), [column]) for column in columns_to_standardize],\n",
        "      remainder='passthrough')\n",
        "  feature_scaler.fit(X)\n",
        "  print(feature_scaler)\n",
        "  return feature_scaler\n",
        "\n",
        "def create_label_scaler(Y: pd.DataFrame) -> ColumnTransformer:\n",
        "  label_scaler = ColumnTransformer([\n",
        "      ('mean_std_scaler', StandardScaler(), ['d18O_mean']),\n",
        "      ('var_minmax_scaler', MinMaxScaler(), ['d18O_variance'])],\n",
        "      remainder='passthrough')\n",
        "  label_scaler.fit(Y)\n",
        "  return label_scaler\n",
        "\n",
        "def scale(X: pd.DataFrame, feature_scaler):\n",
        "  # transform() outputs numpy arrays :(  need to convert back to DataFrame.\n",
        "  X_standardized = pd.DataFrame(feature_scaler.transform(X),\n",
        "                        index=X.index, columns=X.columns)\n",
        "  return X_standardized"
      ],
      "metadata": {
        "id": "XSDwdvMkb7w8"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just a class organization, holds each scaled dataset and the scaler used.\n",
        "# Useful for unscaling predictions.\n",
        "@dataclass\n",
        "class ScaledPartitions():\n",
        "  def __init__(self,\n",
        "               feature_scaler: ColumnTransformer,\n",
        "               label_scaler: ColumnTransformer,\n",
        "               train: FeaturesToLabels, val: FeaturesToLabels,\n",
        "               test: FeaturesToLabels):\n",
        "    self.feature_scaler = feature_scaler\n",
        "    self.label_scaler = label_scaler\n",
        "    self.train = train\n",
        "    self.val = val\n",
        "    self.test = test\n",
        "\n",
        "\n",
        "def load_and_scale(config: Dict,\n",
        "                   columns_to_passthrough: List[str],\n",
        "                   columns_to_scale: List[str],\n",
        "                   columns_to_standardize: List[str]) -> ScaledPartitions:\n",
        "\n",
        "  geotiff_side_input = {\n",
        "      \"d13C_cel_inferred\" : raster.load_raster(raster.get_raster_path(\"d13C_cel_map_BRAZIL_stack.tiff\"), use_only_band_index=0),\n",
        "      'Mean Annual Temperature': raster.temperature_geotiff(),\n",
        "      'Mean Annual Precipitation': raster.brazil_map_geotiff(),\n",
        "      \"ordinary_kriging_linear_d18O_predicted_mean\" : raster.load_raster(raster.get_raster_path(\"canonical/kriging_overall_means.tiff\")),\n",
        "      \"ordinary_kriging_linear_d18O_predicted_variance\" : raster.load_raster(raster.get_raster_path(\"canonical/kriging_overall_vars.tiff\"))\n",
        "  }\n",
        "  columns_to_keep = columns_to_passthrough + columns_to_scale + columns_to_standardize\n",
        "  X_train, Y_train = load_dataset(config['TRAIN'], columns_to_keep, geotiff_side_input)\n",
        "  X_val, Y_val = load_dataset(config['VALIDATION'], columns_to_keep, geotiff_side_input)\n",
        "  X_test, Y_test = load_dataset(config['TEST'], columns_to_keep, geotiff_side_input)\n",
        "\n",
        "  # Fit the scaler:\n",
        "  feature_scaler = create_feature_scaler(\n",
        "      X_train,\n",
        "      columns_to_passthrough,\n",
        "      columns_to_scale,\n",
        "      columns_to_standardize)\n",
        "\n",
        "  # Apply the scaler:\n",
        "  label_scaler = create_label_scaler(Y_train)\n",
        "  train = FeaturesToLabels(scale(X_train, feature_scaler), Y_train)\n",
        "  val = FeaturesToLabels(scale(X_val, feature_scaler), Y_val)\n",
        "  test = FeaturesToLabels(scale(X_test, feature_scaler), Y_test)\n",
        "  return ScaledPartitions(feature_scaler, label_scaler, train, val, test)\n"
      ],
      "metadata": {
        "id": "_kf2e_fKon2P"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition\n",
        "\n"
      ],
      "metadata": {
        "id": "usGznR593LZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KL Loss function:"
      ],
      "metadata": {
        "id": "khK7C8WvU8ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_normal_distribution(\n",
        "    mean: tf.Tensor,\n",
        "    stdev: tf.Tensor,\n",
        "    n: int) -> tf.Tensor:\n",
        "    '''\n",
        "    Given a batch of normal distributions described by a mean and stdev in\n",
        "    a tf.Tensor, sample n elements from each distribution and return the mean\n",
        "    and standard deviation per sample.\n",
        "    '''\n",
        "    batch_size = tf.shape(mean)[0]\n",
        "\n",
        "    # Output tensor is (n, batch_size, 1)\n",
        "    sample_values = tfp.distributions.Normal(\n",
        "        loc=mean,\n",
        "        scale=stdev).sample(\n",
        "            sample_shape=n)\n",
        "    # Reshaped tensor will be (batch_size, n)\n",
        "    sample_values = tf.transpose(sample_values)\n",
        "    # Get the mean per sample in the batch.\n",
        "    sample_mean = tf.transpose(tf.math.reduce_mean(sample_values, 2))\n",
        "    sample_stdev = tf.transpose(tf.math.reduce_std(sample_values, 2))\n",
        "\n",
        "    return sample_mean, sample_stdev\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# log(σ2/σ1) + ( σ1^2+(μ1−μ2)^2 ) / 2* σ^2   − 1/2\n",
        "def kl_divergence_helper(real, predicted, sample):\n",
        "    '''\n",
        "    real: tf.Tensor of the real mean and standard deviation of sample to compare\n",
        "    predicted: tf.Tensor of the predicted mean and standard deviation to compare\n",
        "    sample: Whether or not to sample the predicted distribution to get a new\n",
        "            mean and standard deviation.\n",
        "    '''\n",
        "    if real.shape != predicted.shape:\n",
        "      raise ValueError(\n",
        "          f\"real.shape {real.shape} != predicted.shape {predicted.shape}\")\n",
        "\n",
        "    real_value = tf.gather(real, [0], axis=1)\n",
        "    real_std = tf.math.sqrt(tf.gather(real, [1], axis=1))\n",
        "\n",
        "\n",
        "    predicted_value = tf.gather(predicted, [0], axis=1)\n",
        "    predicted_std = tf.math.sqrt(tf.gather(predicted, [1], axis=1))\n",
        "    # If true, sample from the distribution defined by the predicted mean and\n",
        "    # standard deviation to use for mean and stdev used in KL divergence loss.\n",
        "    if sample:\n",
        "      predicted_value, predicted_std = sample_normal_distribution(\n",
        "          mean=predicted_value, stdev=predicted_std, n=15)\n",
        "\n",
        "    kl_loss = -0.5 + tf.math.log(predicted_std/real_std) + \\\n",
        "     (tf.square(real_std) + tf.square(real_value - predicted_value))/ \\\n",
        "     (2*tf.square(predicted_std))\n",
        "\n",
        "    if tf.math.is_nan(tf.math.reduce_mean(kl_loss)):\n",
        "       tf.print(predicted)\n",
        "       sess = tf.compat.v1.Session()\n",
        "       sess.close()\n",
        "\n",
        "    return tf.math.reduce_mean(kl_loss)\n",
        "\n",
        "def kl_divergence(real, predicted):\n",
        "  return kl_divergence_helper(real, predicted, True)"
      ],
      "metadata": {
        "id": "urGjYNNnemX6"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the loss function:"
      ],
      "metadata": {
        "id": "fJzBFWQVeqNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model definition"
      ],
      "metadata": {
        "id": "8rI6qPRh7oO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "tf.keras.utils.set_random_seed(18731)\n",
        "\n",
        "def get_early_stopping_callback():\n",
        "  return EarlyStopping(monitor='val_loss', patience=1000, min_delta=0.001,\n",
        "                       verbose=1, restore_best_weights=True, start_from_epoch=0)\n",
        "\n",
        "# I was experimenting with models that took longer to train, and used this\n",
        "# checkpointing callback to periodically save the model. It's optional.\n",
        "def get_checkpoint_callback(model_file):\n",
        "  return ModelCheckpoint(\n",
        "      get_model_save_location(model_file),\n",
        "      monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
        "\n",
        "def train_or_update_variational_model(\n",
        "        sp: ScaledPartitions,\n",
        "        hidden_layers: List[int],\n",
        "        epochs: int,\n",
        "        batch_size: int,\n",
        "        lr: float,\n",
        "        model_file=None,\n",
        "        use_checkpoint=False):\n",
        "  callbacks_list = [get_early_stopping_callback(),\n",
        "                    get_checkpoint_callback(model_file)]\n",
        "  if not use_checkpoint:\n",
        "\n",
        "    # Find the kriging columns and make sure they are at the end of the dataframe.\n",
        "    krig_mean_index = sp.train.X.columns.get_loc('ordinary_kriging_linear_d18O_predicted_mean')\n",
        "    krig_var_index = sp.train.X.columns.get_loc('ordinary_kriging_linear_d18O_predicted_variance')\n",
        "    if (krig_mean_index != sp.train.X.shape[1]-2 and krig_var_index != sp.train.X.shape[1]-1):\n",
        "      raise ValueError(\"ordinary_kriging_linear_d18O_predicted_mean and\"\n",
        "      \"ordinary_kriging_linear_d18O_predicted_variance must be\"\n",
        "      \"located in the last two columns of dataframe\")\n",
        "\n",
        "    inputs = keras.Input(shape=(sp.train.X.shape[1],))\n",
        "    nn = inputs[:,0:-2]\n",
        "    krig_mean = tf.expand_dims(inputs[:,-2], 1)\n",
        "    krig_variance = tf.expand_dims(inputs[:, -1], 1)\n",
        "\n",
        "    for layer_size in hidden_layers:\n",
        "      nn = keras.layers.Dense(layer_size, activation='relu')(nn)\n",
        "\n",
        "    # Invert the normalization on our outputs, and add kriging predictions as\n",
        "    # constants so the network only predicts the residuals.\n",
        "    pred_mean_residual = keras.layers.Dense(1, name='pred_mean_residual')(nn)\n",
        "    pred_var_residual = keras.layers.Dense(1, name='pred_var_output')(nn)\n",
        "\n",
        "    pred_mean = krig_mean + pred_mean_residual\n",
        "    pred_var = keras.layers.Lambda(lambda t: tf.math.log(1 + tf.math.exp(t[0]+t[1])))([krig_variance, pred_var_residual])\n",
        "\n",
        "    # Output mean, variance tuples.\n",
        "    outputs = keras.layers.concatenate([pred_mean, pred_var])\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss=kl_divergence)\n",
        "    model.summary()\n",
        "  else:\n",
        "    model = keras.models.load_model(\n",
        "        get_model_save_location(model_file),\n",
        "        custom_objects={\"kl_divergence\": kl_divergence})\n",
        "  history = model.fit(sp.train.X, sp.train.Y, verbose=1, epochs=epochs,\n",
        "                      batch_size=batch_size,\n",
        "                      validation_data=sp.val.as_tuple(),\n",
        "                      shuffle=True, callbacks=callbacks_list)\n",
        "  return history, model"
      ],
      "metadata": {
        "id": "HCkGSPUo3KqY"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def render_plot_loss(history, name):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title(name + ' model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.yscale(\"log\")\n",
        "  plt.ylim((0, 10))\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['loss', 'val_loss'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def destandardize(sd: ScaledPartitions, df: pd.DataFrame):\n",
        "  means = pd.DataFrame(\n",
        "      sd.label_scaler.named_transformers_['var_std_scaler'].inverse_transform(df[['d18O_mean']]),\n",
        "      index=df.index, columns=['d18O_mean'])\n",
        "  vars = df['d18O_cel_variance']\n",
        "  return means.join(vars)\n",
        "\n",
        "def train_and_evaluate(sp: ScaledPartitions, run_id: str, training_batch_size=5):\n",
        "  print(\"==================\")\n",
        "  print(run_id)\n",
        "  history, model = train_or_update_variational_model(\n",
        "      sp, hidden_layers=[20, 20], epochs=5000, batch_size=training_batch_size,\n",
        "      lr=0.0001, model_file=run_id+\".h5\", use_checkpoint=False)\n",
        "  render_plot_loss(history, run_id+\" kl_loss\")\n",
        "  model.save(get_model_save_location(run_id+\".h5\"), save_format=\"h5\")\n",
        "\n",
        "  best_epoch_index = history.history['val_loss'].index(min(history.history['val_loss']))\n",
        "  print('Val loss:', history.history['val_loss'][best_epoch_index])\n",
        "  print('Train loss:', history.history['loss'][best_epoch_index])\n",
        "  print('Test loss:', model.evaluate(x=sp.test.X, y=sp.test.Y, verbose=0))\n",
        "\n",
        "  predictions = model.predict_on_batch(sp.test.X)\n",
        "  predictions = pd.DataFrame(predictions, columns=['d18O_mean', 'd18O_variance'])\n",
        "  rmse = np.sqrt(mean_squared_error(sp.test.Y['d18O_mean'], predictions['d18O_mean']))\n",
        "  print(\"dO18 RMSE: \"+ str(rmse))\n",
        "  print(\"EXPECTED:\")\n",
        "  print(sp.test.Y.to_string())\n",
        "  print()\n",
        "  print(\"PREDICTED:\")\n",
        "  print(predictions.to_string())\n",
        "  return model"
      ],
      "metadata": {
        "id": "DALuUm8UOgNu"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Grouped, random (jupyter crashed halfway so I reloaded a checkpoint)\n",
        "\n",
        "We can't (easily) generate isoscapes with these because the isoscapes for 'predkrig_br_lat_ISORG', 'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION' are not easily retrievable... but I'm curious how much better the model is if these columns are included."
      ],
      "metadata": {
        "id": "n8pNR1CrvF2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_random_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_train_random_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_test_random_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_validation_random_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_passthrough = []\n",
        "columns_to_scale = []\n",
        "columns_to_standardize = [\n",
        "    'lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA', 'd13C_cel', 'isorix',\n",
        "    'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION']\n",
        "\n",
        "data = load_and_scale(grouped_random_fileset, columns_to_passthrough, columns_to_scale, columns_to_standardize)\n",
        "model = train_and_evaluate(data, \"random_all_isorix_carbon_boosted\", training_batch_size=3)\n",
        "model.save(get_model_save_location(\"random_all_isorix_carbon_boosted.tf\"), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location('random_all_isorix_carbon_boosted.pkl'))\n"
      ],
      "metadata": {
        "id": "rPONfgkjvJWz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "09916640-63b9-4cb7-bd53-e0b0eca2c831"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 218 x 2\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-74.0, 4.500000000659528)\n",
            "Pixel Size = (0.166666666667993, -0.16666666666799657)\n",
            "[[[-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  ...\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]]\n",
            "\n",
            " [[-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  ...\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]]\n",
            "\n",
            " [[-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  ...\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  ...\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]]\n",
            "\n",
            " [[-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  ...\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]]\n",
            "\n",
            " [[-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  ...\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]]]\n",
            "[[-- -- -- ... -- -- --]\n",
            " [-- -- -- ... -- -- --]\n",
            " [-- -- -- ... -- -- --]\n",
            " ...\n",
            " [-- -- -- ... -- -- --]\n",
            " [-- -- -- ... -- -- --]\n",
            " [-- -- -- ... -- -- --]]\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 234 x 1\n",
            "Projection is \n",
            "Origin = (-74.0, 5.333333333999995)\n",
            "Pixel Size = (0.16666666666808508, -0.16666666666808505)\n",
            "[[[-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  ...\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]]\n",
            "\n",
            " [[-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  ...\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]]\n",
            "\n",
            " [[-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  ...\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  ...\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]]\n",
            "\n",
            " [[-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  ...\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]]\n",
            "\n",
            " [[-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  ...\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]]]\n",
            "[[-- -- -- ... -- -- --]\n",
            " [-- -- -- ... -- -- --]\n",
            " [-- -- -- ... -- -- --]\n",
            " ...\n",
            " [-- -- -- ... -- -- --]\n",
            " [-- -- -- ... -- -- --]\n",
            " [-- -- -- ... -- -- --]]\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 234 x 1\n",
            "Projection is GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n",
            "Origin = (-74.0, 5.333333333999995)\n",
            "Pixel Size = (0.16666666666808508, -0.16666666666808505)\n",
            "[[[24.88016384 24.88016384 24.88016384 ... 24.88016384 24.88016384\n",
            "   24.88016384]\n",
            "  [24.89114627 24.89114627 24.89114627 ... 24.89114627 24.89114627\n",
            "   24.89114627]\n",
            "  [24.9024049  24.9024049  24.9024049  ... 24.9024049  24.9024049\n",
            "   24.9024049 ]\n",
            "  ...\n",
            "  [27.13953135 27.13953135 27.13953135 ... 27.13953135 27.13953135\n",
            "   27.13953135]\n",
            "  [27.14290807 27.14290807 27.14290807 ... 27.14290807 27.14290807\n",
            "   27.14290807]\n",
            "  [27.14624707 27.14624707 27.14624707 ... 27.14624707 27.14624707\n",
            "   27.14624707]]\n",
            "\n",
            " [[24.86881208 24.86881208 24.86881208 ... 24.86881208 24.86881208\n",
            "   24.86881208]\n",
            "  [24.87973391 24.87973391 24.87973391 ... 24.87973391 24.87973391\n",
            "   24.87973391]\n",
            "  [24.8909363  24.8909363  24.8909363  ... 24.8909363  24.8909363\n",
            "   24.8909363 ]\n",
            "  ...\n",
            "  [27.14229805 27.14229805 27.14229805 ... 27.14229805 27.14229805\n",
            "   27.14229805]\n",
            "  [27.14566742 27.14566742 27.14566742 ... 27.14566742 27.14566742\n",
            "   27.14566742]\n",
            "  [27.14899881 27.14899881 27.14899881 ... 27.14899881 27.14899881\n",
            "   27.14899881]]\n",
            "\n",
            " [[24.85730438 24.85730438 24.85730438 ... 24.85730438 24.85730438\n",
            "   24.85730438]\n",
            "  [24.86815929 24.86815929 24.86815929 ... 24.86815929 24.86815929\n",
            "   24.86815929]\n",
            "  [24.87929901 24.87929901 24.87929901 ... 24.87929901 24.87929901\n",
            "   24.87929901]\n",
            "  ...\n",
            "  [27.14506948 27.14506948 27.14506948 ... 27.14506948 27.14506948\n",
            "   27.14506948]\n",
            "  [27.14843113 27.14843113 27.14843113 ... 27.14843113 27.14843113\n",
            "   27.14843113]\n",
            "  [27.15175452 27.15175452 27.15175452 ... 27.15175452 27.15175452\n",
            "   27.15175452]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[25.38982147 25.38982147 25.38982147 ... 25.38982147 25.38982147\n",
            "   25.38982147]\n",
            "  [25.39660471 25.39660471 25.39660471 ... 25.39660471 25.39660471\n",
            "   25.39660471]\n",
            "  [25.40343295 25.40343295 25.40343295 ... 25.40343295 25.40343295\n",
            "   25.40343295]\n",
            "  ...\n",
            "  [27.00955313 27.00955313 27.00955313 ... 27.00955313 27.00955313\n",
            "   27.00955313]\n",
            "  [27.01257618 27.01257618 27.01257618 ... 27.01257618 27.01257618\n",
            "   27.01257618]\n",
            "  [27.01556757 27.01556757 27.01556757 ... 27.01556757 27.01556757\n",
            "   27.01556757]]\n",
            "\n",
            " [[25.39432335 25.39432335 25.39432335 ... 25.39432335 25.39432335\n",
            "   25.39432335]\n",
            "  [25.40107831 25.40107831 25.40107831 ... 25.40107831 25.40107831\n",
            "   25.40107831]\n",
            "  [25.40787761 25.40787761 25.40787761 ... 25.40787761 25.40787761\n",
            "   25.40787761]\n",
            "  ...\n",
            "  [27.0052847  27.0052847  27.0052847  ... 27.0052847  27.0052847\n",
            "   27.0052847 ]\n",
            "  [27.00831832 27.00831832 27.00831832 ... 27.00831832 27.00831832\n",
            "   27.00831832]\n",
            "  [27.01132041 27.01132041 27.01132041 ... 27.01132041 27.01132041\n",
            "   27.01132041]]\n",
            "\n",
            " [[25.39878784 25.39878784 25.39878784 ... 25.39878784 25.39878784\n",
            "   25.39878784]\n",
            "  [25.40551442 25.40551442 25.40551442 ... 25.40551442 25.40551442\n",
            "   25.40551442]\n",
            "  [25.41228468 25.41228468 25.41228468 ... 25.41228468 25.41228468\n",
            "   25.41228468]\n",
            "  ...\n",
            "  [27.00102031 27.00102031 27.00102031 ... 27.00102031 27.00102031\n",
            "   27.00102031]\n",
            "  [27.00406416 27.00406416 27.00406416 ... 27.00406416 27.00406416\n",
            "   27.00406416]\n",
            "  [27.00707661 27.00707661 27.00707661 ... 27.00707661 27.00707661\n",
            "   27.00707661]]]\n",
            "[[24.88016384 24.89114627 24.9024049  ... 27.13953135 27.14290807\n",
            "  27.14624707]\n",
            " [24.86881208 24.87973391 24.8909363  ... 27.14229805 27.14566742\n",
            "  27.14899881]\n",
            " [24.85730438 24.86815929 24.87929901 ... 27.14506948 27.14843113\n",
            "  27.15175452]\n",
            " ...\n",
            " [25.38982147 25.39660471 25.40343295 ... 27.00955313 27.01257618\n",
            "  27.01556757]\n",
            " [25.39432335 25.40107831 25.40787761 ... 27.0052847  27.00831832\n",
            "  27.01132041]\n",
            " [25.39878784 25.40551442 25.41228468 ... 27.00102031 27.00406416\n",
            "  27.00707661]]\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 234 x 1\n",
            "Projection is GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n",
            "Origin = (-74.0, 5.333333333999995)\n",
            "Pixel Size = (0.16666666666808508, -0.16666666666808505)\n",
            "[[[2.97395882 2.97395882 2.97395882 ... 2.97395882 2.97395882 2.97395882]\n",
            "  [2.94223839 2.94223839 2.94223839 ... 2.94223839 2.94223839 2.94223839]\n",
            "  [2.91086151 2.91086151 2.91086151 ... 2.91086151 2.91086151 2.91086151]\n",
            "  ...\n",
            "  [6.57256759 6.57256759 6.57256759 ... 6.57256759 6.57256759 6.57256759]\n",
            "  [6.61470019 6.61470019 6.61470019 ... 6.61470019 6.61470019 6.61470019]\n",
            "  [6.65690594 6.65690594 6.65690594 ... 6.65690594 6.65690594 6.65690594]]\n",
            "\n",
            " [[2.93891623 2.93891623 2.93891623 ... 2.93891623 2.93891623 2.93891623]\n",
            "  [2.90694216 2.90694216 2.90694216 ... 2.90694216 2.90694216 2.90694216]\n",
            "  [2.87530979 2.87530979 2.87530979 ... 2.87530979 2.87530979 2.87530979]\n",
            "  ...\n",
            "  [6.54909537 6.54909537 6.54909537 ... 6.54909537 6.54909537 6.54909537]\n",
            "  [6.59132806 6.59132806 6.59132806 ... 6.59132806 6.59132806 6.59132806]\n",
            "  [6.63363328 6.63363328 6.63363328 ... 6.63363328 6.63363328 6.63363328]]\n",
            "\n",
            " [[2.9041802  2.9041802  2.9041802  ... 2.9041802  2.9041802  2.9041802 ]\n",
            "  [2.87195245 2.87195245 2.87195245 ... 2.87195245 2.87195245 2.87195245]\n",
            "  [2.84006449 2.84006449 2.84006449 ... 2.84006449 2.84006449 2.84006449]\n",
            "  ...\n",
            "  [6.52580722 6.52580722 6.52580722 ... 6.52580722 6.52580722 6.52580722]\n",
            "  [6.56813939 6.56813939 6.56813939 ... 6.56813939 6.56813939 6.56813939]\n",
            "  [6.61054349 6.61054349 6.61054349 ... 6.61054349 6.61054349 6.61054349]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[7.37829616 7.37829616 7.37829616 ... 7.37829616 7.37829616 7.37829616]\n",
            "  [7.35917429 7.35917429 7.35917429 ... 7.35917429 7.35917429 7.35917429]\n",
            "  [7.34019922 7.34019922 7.34019922 ... 7.34019922 7.34019922 7.34019922]\n",
            "  ...\n",
            "  [8.26709692 8.26709692 8.26709692 ... 8.26709692 8.26709692 8.26709692]\n",
            "  [8.29588932 8.29588932 8.29588932 ... 8.29588932 8.29588932 8.29588932]\n",
            "  [8.32482709 8.32482709 8.32482709 ... 8.32482709 8.32482709 8.32482709]]\n",
            "\n",
            " [[7.41956576 7.41956576 7.41956576 ... 7.41956576 7.41956576 7.41956576]\n",
            "  [7.40052933 7.40052933 7.40052933 ... 7.40052933 7.40052933 7.40052933]\n",
            "  [7.38163934 7.38163934 7.38163934 ... 7.38163934 7.38163934 7.38163934]\n",
            "  ...\n",
            "  [8.30363244 8.30363244 8.30363244 ... 8.30363244 8.30363244 8.30363244]\n",
            "  [8.33226524 8.33226524 8.33226524 ... 8.33226524 8.33226524 8.33226524]\n",
            "  [8.36104339 8.36104339 8.36104339 ... 8.36104339 8.36104339 8.36104339]]\n",
            "\n",
            " [[7.4608968  7.4608968  7.4608968  ... 7.4608968  7.4608968  7.4608968 ]\n",
            "  [7.44194573 7.44194573 7.44194573 ... 7.44194573 7.44194573 7.44194573]\n",
            "  [7.42314073 7.42314073 7.42314073 ... 7.42314073 7.42314073 7.42314073]\n",
            "  ...\n",
            "  [8.34028099 8.34028099 8.34028099 ... 8.34028099 8.34028099 8.34028099]\n",
            "  [8.36875505 8.36875505 8.36875505 ... 8.36875505 8.36875505 8.36875505]\n",
            "  [8.39737443 8.39737443 8.39737443 ... 8.39737443 8.39737443 8.39737443]]]\n",
            "[[2.97395882 2.94223839 2.91086151 ... 6.57256759 6.61470019 6.65690594]\n",
            " [2.93891623 2.90694216 2.87530979 ... 6.54909537 6.59132806 6.63363328]\n",
            " [2.9041802  2.87195245 2.84006449 ... 6.52580722 6.56813939 6.61054349]\n",
            " ...\n",
            " [7.37829616 7.35917429 7.34019922 ... 8.26709692 8.29588932 8.32482709]\n",
            " [7.41956576 7.40052933 7.38163934 ... 8.30363244 8.33226524 8.36104339]\n",
            " [7.4608968  7.44194573 7.42314073 ... 8.34028099 8.36875505 8.39737443]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'd18O_variance'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-7f1831e875f9>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION']\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrouped_random_fileset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_passthrough\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_standardize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"random_all_isorix_carbon_boosted\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_model_save_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"random_all_isorix_carbon_boosted.tf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-17fea7e18828>\u001b[0m in \u001b[0;36mload_and_scale\u001b[0;34m(config, columns_to_passthrough, columns_to_scale, columns_to_standardize)\u001b[0m\n\u001b[1;32m     28\u001b[0m   }\n\u001b[1;32m     29\u001b[0m   \u001b[0mcolumns_to_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns_to_passthrough\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcolumns_to_scale\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcolumns_to_standardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TRAIN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_keep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeotiff_side_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'VALIDATION'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_keep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeotiff_side_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TEST'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_keep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeotiff_side_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-7d820ac1beb4>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, columns_to_keep, side_raster_input)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_keep\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside_raster_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd18O_variance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns_to_keep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'd18O_variance'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Grouped, fixed, all columns\n",
        "\n",
        "We can't (easily) generate isoscapes with these because the isoscapes for 'predkrig_br_lat_ISORG', 'Iso_Oxi_Stack_mean_TERZER',\n",
        "                   'isoscape_fullmodel_d18O_prec_REGRESSION' are not easily retrievable... but I'm curious how much better the model is if these columns are included."
      ],
      "metadata": {
        "id": "wPDSSm__DR53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_fixed_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_new_train_fixed_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_new_validation_fixed_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_new_test_fixed_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_passthrough = []\n",
        "columns_to_scale = []\n",
        "columns_to_standardize = [\n",
        "    'lat', 'long', 'vpd', 'rh', 'pet', 'dem', 'pa', 'd13C_cel', 'isorix',\n",
        "    'Iso_Oxi_Stack_mean_TERZER',\n",
        "    'isoscape_fullmodel_d18O_prec_REGRESSION']\n",
        "\n",
        "data = load_and_scale(grouped_fixed_fileset, columns_to_passthrough, columns_to_scale, columns_to_standardize)\n",
        "model = train_and_evaluate(data, \"fixed_all_isorix_carbon_boosted\", training_batch_size=3)\n",
        "model.save(get_model_save_location(\"fixed_all_isorix_carbon_boosted.tf\"), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location('fixed_all_isorix_carbon_boosted.pkl'))"
      ],
      "metadata": {
        "id": "V9_5iUkDVoCV",
        "outputId": "ffeca6a4-035a-4c4c-acfd-c425bb40635a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 218 x 2\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-74.0, 4.500000000659528)\n",
            "Pixel Size = (0.166666666667993, -0.16666666666799657)\n",
            "[[[-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  ...\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]]\n",
            "\n",
            " [[-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  ...\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]]\n",
            "\n",
            " [[-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  ...\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  ...\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]]\n",
            "\n",
            " [[-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  ...\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]]\n",
            "\n",
            " [[-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  ...\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]\n",
            "  [-- -- -- ... -- -- --]]]\n",
            "[[-- -- -- ... -- -- --]\n",
            " [-- -- -- ... -- -- --]\n",
            " [-- -- -- ... -- -- --]\n",
            " ...\n",
            " [-- -- -- ... -- -- --]\n",
            " [-- -- -- ... -- -- --]\n",
            " [-- -- -- ... -- -- --]]\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 234 x 1\n",
            "Projection is GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n",
            "Origin = (-74.0, 5.333333333999995)\n",
            "Pixel Size = (0.16666666666808508, -0.16666666666808505)\n",
            "[[[24.88016384 24.88016384 24.88016384 ... 24.88016384 24.88016384\n",
            "   24.88016384]\n",
            "  [24.89114627 24.89114627 24.89114627 ... 24.89114627 24.89114627\n",
            "   24.89114627]\n",
            "  [24.9024049  24.9024049  24.9024049  ... 24.9024049  24.9024049\n",
            "   24.9024049 ]\n",
            "  ...\n",
            "  [27.13953135 27.13953135 27.13953135 ... 27.13953135 27.13953135\n",
            "   27.13953135]\n",
            "  [27.14290807 27.14290807 27.14290807 ... 27.14290807 27.14290807\n",
            "   27.14290807]\n",
            "  [27.14624707 27.14624707 27.14624707 ... 27.14624707 27.14624707\n",
            "   27.14624707]]\n",
            "\n",
            " [[24.86881208 24.86881208 24.86881208 ... 24.86881208 24.86881208\n",
            "   24.86881208]\n",
            "  [24.87973391 24.87973391 24.87973391 ... 24.87973391 24.87973391\n",
            "   24.87973391]\n",
            "  [24.8909363  24.8909363  24.8909363  ... 24.8909363  24.8909363\n",
            "   24.8909363 ]\n",
            "  ...\n",
            "  [27.14229805 27.14229805 27.14229805 ... 27.14229805 27.14229805\n",
            "   27.14229805]\n",
            "  [27.14566742 27.14566742 27.14566742 ... 27.14566742 27.14566742\n",
            "   27.14566742]\n",
            "  [27.14899881 27.14899881 27.14899881 ... 27.14899881 27.14899881\n",
            "   27.14899881]]\n",
            "\n",
            " [[24.85730438 24.85730438 24.85730438 ... 24.85730438 24.85730438\n",
            "   24.85730438]\n",
            "  [24.86815929 24.86815929 24.86815929 ... 24.86815929 24.86815929\n",
            "   24.86815929]\n",
            "  [24.87929901 24.87929901 24.87929901 ... 24.87929901 24.87929901\n",
            "   24.87929901]\n",
            "  ...\n",
            "  [27.14506948 27.14506948 27.14506948 ... 27.14506948 27.14506948\n",
            "   27.14506948]\n",
            "  [27.14843113 27.14843113 27.14843113 ... 27.14843113 27.14843113\n",
            "   27.14843113]\n",
            "  [27.15175452 27.15175452 27.15175452 ... 27.15175452 27.15175452\n",
            "   27.15175452]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[25.38982147 25.38982147 25.38982147 ... 25.38982147 25.38982147\n",
            "   25.38982147]\n",
            "  [25.39660471 25.39660471 25.39660471 ... 25.39660471 25.39660471\n",
            "   25.39660471]\n",
            "  [25.40343295 25.40343295 25.40343295 ... 25.40343295 25.40343295\n",
            "   25.40343295]\n",
            "  ...\n",
            "  [27.00955313 27.00955313 27.00955313 ... 27.00955313 27.00955313\n",
            "   27.00955313]\n",
            "  [27.01257618 27.01257618 27.01257618 ... 27.01257618 27.01257618\n",
            "   27.01257618]\n",
            "  [27.01556757 27.01556757 27.01556757 ... 27.01556757 27.01556757\n",
            "   27.01556757]]\n",
            "\n",
            " [[25.39432335 25.39432335 25.39432335 ... 25.39432335 25.39432335\n",
            "   25.39432335]\n",
            "  [25.40107831 25.40107831 25.40107831 ... 25.40107831 25.40107831\n",
            "   25.40107831]\n",
            "  [25.40787761 25.40787761 25.40787761 ... 25.40787761 25.40787761\n",
            "   25.40787761]\n",
            "  ...\n",
            "  [27.0052847  27.0052847  27.0052847  ... 27.0052847  27.0052847\n",
            "   27.0052847 ]\n",
            "  [27.00831832 27.00831832 27.00831832 ... 27.00831832 27.00831832\n",
            "   27.00831832]\n",
            "  [27.01132041 27.01132041 27.01132041 ... 27.01132041 27.01132041\n",
            "   27.01132041]]\n",
            "\n",
            " [[25.39878784 25.39878784 25.39878784 ... 25.39878784 25.39878784\n",
            "   25.39878784]\n",
            "  [25.40551442 25.40551442 25.40551442 ... 25.40551442 25.40551442\n",
            "   25.40551442]\n",
            "  [25.41228468 25.41228468 25.41228468 ... 25.41228468 25.41228468\n",
            "   25.41228468]\n",
            "  ...\n",
            "  [27.00102031 27.00102031 27.00102031 ... 27.00102031 27.00102031\n",
            "   27.00102031]\n",
            "  [27.00406416 27.00406416 27.00406416 ... 27.00406416 27.00406416\n",
            "   27.00406416]\n",
            "  [27.00707661 27.00707661 27.00707661 ... 27.00707661 27.00707661\n",
            "   27.00707661]]]\n",
            "[[24.88016384 24.89114627 24.9024049  ... 27.13953135 27.14290807\n",
            "  27.14624707]\n",
            " [24.86881208 24.87973391 24.8909363  ... 27.14229805 27.14566742\n",
            "  27.14899881]\n",
            " [24.85730438 24.86815929 24.87929901 ... 27.14506948 27.14843113\n",
            "  27.15175452]\n",
            " ...\n",
            " [25.38982147 25.39660471 25.40343295 ... 27.00955313 27.01257618\n",
            "  27.01556757]\n",
            " [25.39432335 25.40107831 25.40787761 ... 27.0052847  27.00831832\n",
            "  27.01132041]\n",
            " [25.39878784 25.40551442 25.41228468 ... 27.00102031 27.00406416\n",
            "  27.00707661]]\n",
            "Driver: GTiff/GeoTIFF\n",
            "Size is 235 x 234 x 1\n",
            "Projection is GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n",
            "Origin = (-74.0, 5.333333333999995)\n",
            "Pixel Size = (0.16666666666808508, -0.16666666666808505)\n",
            "[[[2.97395882 2.97395882 2.97395882 ... 2.97395882 2.97395882 2.97395882]\n",
            "  [2.94223839 2.94223839 2.94223839 ... 2.94223839 2.94223839 2.94223839]\n",
            "  [2.91086151 2.91086151 2.91086151 ... 2.91086151 2.91086151 2.91086151]\n",
            "  ...\n",
            "  [6.57256759 6.57256759 6.57256759 ... 6.57256759 6.57256759 6.57256759]\n",
            "  [6.61470019 6.61470019 6.61470019 ... 6.61470019 6.61470019 6.61470019]\n",
            "  [6.65690594 6.65690594 6.65690594 ... 6.65690594 6.65690594 6.65690594]]\n",
            "\n",
            " [[2.93891623 2.93891623 2.93891623 ... 2.93891623 2.93891623 2.93891623]\n",
            "  [2.90694216 2.90694216 2.90694216 ... 2.90694216 2.90694216 2.90694216]\n",
            "  [2.87530979 2.87530979 2.87530979 ... 2.87530979 2.87530979 2.87530979]\n",
            "  ...\n",
            "  [6.54909537 6.54909537 6.54909537 ... 6.54909537 6.54909537 6.54909537]\n",
            "  [6.59132806 6.59132806 6.59132806 ... 6.59132806 6.59132806 6.59132806]\n",
            "  [6.63363328 6.63363328 6.63363328 ... 6.63363328 6.63363328 6.63363328]]\n",
            "\n",
            " [[2.9041802  2.9041802  2.9041802  ... 2.9041802  2.9041802  2.9041802 ]\n",
            "  [2.87195245 2.87195245 2.87195245 ... 2.87195245 2.87195245 2.87195245]\n",
            "  [2.84006449 2.84006449 2.84006449 ... 2.84006449 2.84006449 2.84006449]\n",
            "  ...\n",
            "  [6.52580722 6.52580722 6.52580722 ... 6.52580722 6.52580722 6.52580722]\n",
            "  [6.56813939 6.56813939 6.56813939 ... 6.56813939 6.56813939 6.56813939]\n",
            "  [6.61054349 6.61054349 6.61054349 ... 6.61054349 6.61054349 6.61054349]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[7.37829616 7.37829616 7.37829616 ... 7.37829616 7.37829616 7.37829616]\n",
            "  [7.35917429 7.35917429 7.35917429 ... 7.35917429 7.35917429 7.35917429]\n",
            "  [7.34019922 7.34019922 7.34019922 ... 7.34019922 7.34019922 7.34019922]\n",
            "  ...\n",
            "  [8.26709692 8.26709692 8.26709692 ... 8.26709692 8.26709692 8.26709692]\n",
            "  [8.29588932 8.29588932 8.29588932 ... 8.29588932 8.29588932 8.29588932]\n",
            "  [8.32482709 8.32482709 8.32482709 ... 8.32482709 8.32482709 8.32482709]]\n",
            "\n",
            " [[7.41956576 7.41956576 7.41956576 ... 7.41956576 7.41956576 7.41956576]\n",
            "  [7.40052933 7.40052933 7.40052933 ... 7.40052933 7.40052933 7.40052933]\n",
            "  [7.38163934 7.38163934 7.38163934 ... 7.38163934 7.38163934 7.38163934]\n",
            "  ...\n",
            "  [8.30363244 8.30363244 8.30363244 ... 8.30363244 8.30363244 8.30363244]\n",
            "  [8.33226524 8.33226524 8.33226524 ... 8.33226524 8.33226524 8.33226524]\n",
            "  [8.36104339 8.36104339 8.36104339 ... 8.36104339 8.36104339 8.36104339]]\n",
            "\n",
            " [[7.4608968  7.4608968  7.4608968  ... 7.4608968  7.4608968  7.4608968 ]\n",
            "  [7.44194573 7.44194573 7.44194573 ... 7.44194573 7.44194573 7.44194573]\n",
            "  [7.42314073 7.42314073 7.42314073 ... 7.42314073 7.42314073 7.42314073]\n",
            "  ...\n",
            "  [8.34028099 8.34028099 8.34028099 ... 8.34028099 8.34028099 8.34028099]\n",
            "  [8.36875505 8.36875505 8.36875505 ... 8.36875505 8.36875505 8.36875505]\n",
            "  [8.39737443 8.39737443 8.39737443 ... 8.39737443 8.39737443 8.39737443]]]\n",
            "[[2.97395882 2.94223839 2.91086151 ... 6.57256759 6.61470019 6.65690594]\n",
            " [2.93891623 2.90694216 2.87530979 ... 6.54909537 6.59132806 6.63363328]\n",
            " [2.9041802  2.87195245 2.84006449 ... 6.52580722 6.56813939 6.61054349]\n",
            " ...\n",
            " [7.37829616 7.35917429 7.34019922 ... 8.26709692 8.29588932 8.32482709]\n",
            " [7.41956576 7.40052933 7.38163934 ... 8.30363244 8.33226524 8.36104339]\n",
            " [7.4608968  7.44194573 7.42314073 ... 8.34028099 8.36875505 8.39737443]]\n",
            "-27.921724955240887 -3.995544837 -57.58927257 98 50\n",
            "-26.60095977783203 -3.9923 -54.908 114 50\n",
            "-26.60095977783203 -3.9923 -54.908 114 50\n",
            "-26.60095977783203 -3.9923 -54.908 114 50\n",
            "-26.60095977783203 -3.9923 -54.908 114 50\n",
            "-26.60095977783203 -3.9923 -54.908 114 50\n",
            "-26.60095977783203 -3.9923 -54.908 114 50\n",
            "-26.60095977783203 -3.9923 -54.908 114 50\n",
            "-27.921724955240887 -3.995544837 -57.58927257 98 50\n",
            "-27.921724955240887 -3.995544837 -57.58927257 98 50\n",
            "-27.921724955240887 -3.995544837 -57.58927257 98 50\n",
            "-27.24651336669922 -2.63795 -60.14972 83 42\n",
            "-27.24651336669922 -2.63795 -60.14972 83 42\n",
            "-27.24651336669922 -2.63795 -60.14972 83 42\n",
            "-27.24651336669922 -2.63795 -60.14972 83 42\n",
            "-27.24651336669922 -2.63795 -60.14972 83 42\n",
            "-26.286356608072918 -0.90361 -60.43406 81 32\n",
            "-27.921724955240887 -3.995544837 -57.58927257 98 50\n",
            "-27.24651336669922 -2.63795 -60.14972 83 42\n",
            "-26.286356608072918 -0.90361 -60.43406 81 32\n",
            "-26.286356608072918 -0.90361 -60.43406 81 32\n",
            "-26.286356608072918 -0.90361 -60.43406 81 32\n",
            "-26.286356608072918 -0.90361 -60.43406 81 32\n",
            "-26.286356608072918 -0.90361 -60.43406 81 32\n",
            "-26.286356608072918 -0.90361 -60.43406 81 32\n",
            "-26.38085428873698 -2.499 -59.121 89 41\n",
            "-26.38085428873698 -2.496 -59.126 89 41\n",
            "-26.38085428873698 -2.495 -59.12 89 41\n",
            "-26.38085428873698 -2.497 -59.116 89 41\n",
            "-26.38085428873698 -2.495 -59.124 89 41\n",
            "-26.38085428873698 -2.493 -59.121 89 41\n",
            "-26.38085428873698 -2.498 -59.118 89 41\n",
            "-26.38085428873698 -2.482 -59.126 89 41\n",
            "-26.38085428873698 -2.483 -59.126 89 41\n",
            "-26.38085428873698 -2.496 -59.125 89 41\n",
            "-26.38085428873698 -2.495 -59.123 89 41\n",
            "-26.38085428873698 -2.495 -59.122 89 41\n",
            "-26.38085428873698 -2.496 -59.125 89 41\n",
            "-26.38085428873698 -2.485 -59.125 89 41\n",
            "-26.38085428873698 -2.492 -59.123 89 41\n",
            "-26.38085428873698 -2.495 -59.124 89 41\n",
            "-26.38085428873698 -2.496 -59.121 89 41\n",
            "-27.921724955240887 -3.995544837 -57.58927257 98 50\n",
            "-26.38085428873698 -2.485 -59.124 89 41\n",
            "-26.38085428873698 -2.495 -59.119 89 41\n",
            "-26.38085428873698 -2.494 -59.119 89 41\n",
            "-26.38085428873698 -2.486 -59.126 89 41\n",
            "-26.38085428873698 -2.496 -59.121 89 41\n",
            "-26.38085428873698 -2.495 -59.122 89 41\n",
            "-26.38085428873698 -2.495 -59.121 89 41\n",
            "-26.38085428873698 -2.496 -59.118 89 41\n",
            "-26.38085428873698 -2.496 -59.12 89 41\n",
            "-26.38085428873698 -2.493 -59.121 89 41\n",
            "-26.38085428873698 -2.497 -59.121 89 41\n",
            "-26.38085428873698 -2.483 -59.124 89 41\n",
            "-26.48528798421224 -3.3983972 -54.973982 114 47\n",
            "-26.48528798421224 -3.3983972 -54.973982 114 47\n",
            "-26.48528798421224 -3.3983972 -54.973982 114 47\n",
            "-26.48528798421224 -3.3983972 -54.973982 114 47\n",
            "-26.48528798421224 -3.3983972 -54.973982 114 47\n",
            "-26.48528798421224 -3.3983972 -54.973982 114 47\n",
            "-26.48528798421224 -3.3983972 -54.973982 114 47\n",
            "-26.48528798421224 -3.3983972 -54.973982 114 47\n",
            "-26.48528798421224 -3.3983972 -54.973982 114 47\n",
            "-26.60095977783203 -3.9923 -54.908 114 50\n",
            "-26.60095977783203 -3.9923 -54.908 114 50\n",
            "-26.60095977783203 -3.9923 -54.908 114 50\n",
            "-26.60095977783203 -3.9923 -54.908 114 50\n",
            "27.166664123535156 -3.995544837 -57.58927257 393 222\n",
            "26.29583485921224 -3.9923 -54.908 458 222\n",
            "26.29583485921224 -3.9923 -54.908 458 222\n",
            "26.29583485921224 -3.9923 -54.908 458 222\n",
            "26.29583485921224 -3.9923 -54.908 458 222\n",
            "26.29583485921224 -3.9923 -54.908 458 222\n",
            "26.29583485921224 -3.9923 -54.908 458 222\n",
            "26.29583485921224 -3.9923 -54.908 458 222\n",
            "27.166664123535156 -3.995544837 -57.58927257 393 222\n",
            "27.166664123535156 -3.995544837 -57.58927257 393 222\n",
            "27.166664123535156 -3.995544837 -57.58927257 393 222\n",
            "26.67083231608073 -2.63795 -60.14972 332 190\n",
            "26.67083231608073 -2.63795 -60.14972 332 190\n",
            "26.67083231608073 -2.63795 -60.14972 332 190\n",
            "26.67083231608073 -2.63795 -60.14972 332 190\n",
            "26.67083231608073 -2.63795 -60.14972 332 190\n",
            "26.870834350585938 -0.90361 -60.43406 325 148\n",
            "27.166664123535156 -3.995544837 -57.58927257 393 222\n",
            "26.67083231608073 -2.63795 -60.14972 332 190\n",
            "26.870834350585938 -0.90361 -60.43406 325 148\n",
            "26.870834350585938 -0.90361 -60.43406 325 148\n",
            "26.870834350585938 -0.90361 -60.43406 325 148\n",
            "26.870834350585938 -0.90361 -60.43406 325 148\n",
            "26.870834350585938 -0.90361 -60.43406 325 148\n",
            "26.870834350585938 -0.90361 -60.43406 325 148\n",
            "26.791666666666668 -2.499 -59.121 357 186\n",
            "26.725001017252605 -2.496 -59.126 356 186\n",
            "26.791666666666668 -2.495 -59.12 357 186\n",
            "26.791666666666668 -2.497 -59.116 357 186\n",
            "26.791666666666668 -2.495 -59.124 357 186\n",
            "26.791666666666668 -2.493 -59.121 357 186\n",
            "26.791666666666668 -2.498 -59.118 357 186\n",
            "26.725001017252605 -2.482 -59.126 356 186\n",
            "26.725001017252605 -2.483 -59.126 356 186\n",
            "26.791666666666668 -2.496 -59.125 357 186\n",
            "26.791666666666668 -2.495 -59.123 357 186\n",
            "26.791666666666668 -2.495 -59.122 357 186\n",
            "26.791666666666668 -2.496 -59.125 357 186\n",
            "26.791666666666668 -2.485 -59.125 357 186\n",
            "26.791666666666668 -2.492 -59.123 357 186\n",
            "26.791666666666668 -2.495 -59.124 357 186\n",
            "26.791666666666668 -2.496 -59.121 357 186\n",
            "27.166664123535156 -3.995544837 -57.58927257 393 222\n",
            "26.791666666666668 -2.485 -59.124 357 186\n",
            "26.791666666666668 -2.495 -59.119 357 186\n",
            "26.791666666666668 -2.494 -59.119 357 186\n",
            "26.725001017252605 -2.486 -59.126 356 186\n",
            "26.791666666666668 -2.496 -59.121 357 186\n",
            "26.791666666666668 -2.495 -59.122 357 186\n",
            "26.791666666666668 -2.495 -59.121 357 186\n",
            "26.791666666666668 -2.496 -59.118 357 186\n",
            "26.791666666666668 -2.496 -59.12 357 186\n",
            "26.791666666666668 -2.493 -59.121 357 186\n",
            "26.791666666666668 -2.497 -59.121 357 186\n",
            "26.791666666666668 -2.483 -59.124 357 186\n",
            "26.0 -3.3983972 -54.973982 456 208\n",
            "26.0 -3.3983972 -54.973982 456 208\n",
            "26.0 -3.3983972 -54.973982 456 208\n",
            "26.0 -3.3983972 -54.973982 456 208\n",
            "26.0 -3.3983972 -54.973982 456 208\n",
            "26.0 -3.3983972 -54.973982 456 208\n",
            "26.0 -3.3983972 -54.973982 456 208\n",
            "26.0 -3.3983972 -54.973982 456 208\n",
            "26.0 -3.3983972 -54.973982 456 208\n",
            "26.29583485921224 -3.9923 -54.908 458 222\n",
            "26.29583485921224 -3.9923 -54.908 458 222\n",
            "26.29583485921224 -3.9923 -54.908 458 222\n",
            "26.29583485921224 -3.9923 -54.908 458 222\n",
            "2274.0 -3.995544837 -57.58927257 98 55\n",
            "1883.0 -3.9923 -54.908 114 55\n",
            "1883.0 -3.9923 -54.908 114 55\n",
            "1883.0 -3.9923 -54.908 114 55\n",
            "1883.0 -3.9923 -54.908 114 55\n",
            "1883.0 -3.9923 -54.908 114 55\n",
            "1883.0 -3.9923 -54.908 114 55\n",
            "1883.0 -3.9923 -54.908 114 55\n",
            "2274.0 -3.995544837 -57.58927257 98 55\n",
            "2274.0 -3.995544837 -57.58927257 98 55\n",
            "2274.0 -3.995544837 -57.58927257 98 55\n",
            "2363.0 -2.63795 -60.14972 83 47\n",
            "2363.0 -2.63795 -60.14972 83 47\n",
            "2363.0 -2.63795 -60.14972 83 47\n",
            "2363.0 -2.63795 -60.14972 83 47\n",
            "2363.0 -2.63795 -60.14972 83 47\n",
            "2225.0 -0.90361 -60.43406 81 37\n",
            "2274.0 -3.995544837 -57.58927257 98 55\n",
            "2363.0 -2.63795 -60.14972 83 47\n",
            "2225.0 -0.90361 -60.43406 81 37\n",
            "2225.0 -0.90361 -60.43406 81 37\n",
            "2225.0 -0.90361 -60.43406 81 37\n",
            "2225.0 -0.90361 -60.43406 81 37\n",
            "2225.0 -0.90361 -60.43406 81 37\n",
            "2225.0 -0.90361 -60.43406 81 37\n",
            "2236.0 -2.499 -59.121 89 46\n",
            "2236.0 -2.496 -59.126 89 46\n",
            "2236.0 -2.495 -59.12 89 46\n",
            "2236.0 -2.497 -59.116 89 46\n",
            "2236.0 -2.495 -59.124 89 46\n",
            "2236.0 -2.493 -59.121 89 46\n",
            "2236.0 -2.498 -59.118 89 46\n",
            "2236.0 -2.482 -59.126 89 46\n",
            "2236.0 -2.483 -59.126 89 46\n",
            "2236.0 -2.496 -59.125 89 46\n",
            "2236.0 -2.495 -59.123 89 46\n",
            "2236.0 -2.495 -59.122 89 46\n",
            "2236.0 -2.496 -59.125 89 46\n",
            "2236.0 -2.485 -59.125 89 46\n",
            "2236.0 -2.492 -59.123 89 46\n",
            "2236.0 -2.495 -59.124 89 46\n",
            "2236.0 -2.496 -59.121 89 46\n",
            "2274.0 -3.995544837 -57.58927257 98 55\n",
            "2236.0 -2.485 -59.124 89 46\n",
            "2236.0 -2.495 -59.119 89 46\n",
            "2236.0 -2.494 -59.119 89 46\n",
            "2236.0 -2.486 -59.126 89 46\n",
            "2236.0 -2.496 -59.121 89 46\n",
            "2236.0 -2.495 -59.122 89 46\n",
            "2236.0 -2.495 -59.121 89 46\n",
            "2236.0 -2.496 -59.118 89 46\n",
            "2236.0 -2.496 -59.12 89 46\n",
            "2236.0 -2.493 -59.121 89 46\n",
            "2236.0 -2.497 -59.121 89 46\n",
            "2236.0 -2.483 -59.124 89 46\n",
            "1833.0 -3.3983972 -54.973982 114 52\n",
            "1833.0 -3.3983972 -54.973982 114 52\n",
            "1833.0 -3.3983972 -54.973982 114 52\n",
            "1833.0 -3.3983972 -54.973982 114 52\n",
            "1833.0 -3.3983972 -54.973982 114 52\n",
            "1833.0 -3.3983972 -54.973982 114 52\n",
            "1833.0 -3.3983972 -54.973982 114 52\n",
            "1833.0 -3.3983972 -54.973982 114 52\n",
            "1833.0 -3.3983972 -54.973982 114 52\n",
            "1883.0 -3.9923 -54.908 114 55\n",
            "1883.0 -3.9923 -54.908 114 55\n",
            "1883.0 -3.9923 -54.908 114 55\n",
            "1883.0 -3.9923 -54.908 114 55\n",
            "26.087119713578208 -3.995544837 -57.58927257 98 55\n",
            "26.387516402606604 -3.9923 -54.908 114 55\n",
            "26.387516402606604 -3.9923 -54.908 114 55\n",
            "26.387516402606604 -3.9923 -54.908 114 55\n",
            "26.387516402606604 -3.9923 -54.908 114 55\n",
            "26.387516402606604 -3.9923 -54.908 114 55\n",
            "26.387516402606604 -3.9923 -54.908 114 55\n",
            "26.387516402606604 -3.9923 -54.908 114 55\n",
            "26.087119713578208 -3.995544837 -57.58927257 98 55\n",
            "26.087119713578208 -3.995544837 -57.58927257 98 55\n",
            "26.087119713578208 -3.995544837 -57.58927257 98 55\n",
            "25.873459533520073 -2.63795 -60.14972 83 47\n",
            "25.873459533520073 -2.63795 -60.14972 83 47\n",
            "25.873459533520073 -2.63795 -60.14972 83 47\n",
            "25.873459533520073 -2.63795 -60.14972 83 47\n",
            "25.873459533520073 -2.63795 -60.14972 83 47\n",
            "26.939959041745897 -0.90361 -60.43406 81 37\n",
            "26.087119713578208 -3.995544837 -57.58927257 98 55\n",
            "25.873459533520073 -2.63795 -60.14972 83 47\n",
            "26.939959041745897 -0.90361 -60.43406 81 37\n",
            "26.939959041745897 -0.90361 -60.43406 81 37\n",
            "26.939959041745897 -0.90361 -60.43406 81 37\n",
            "26.939959041745897 -0.90361 -60.43406 81 37\n",
            "26.939959041745897 -0.90361 -60.43406 81 37\n",
            "26.939959041745897 -0.90361 -60.43406 81 37\n",
            "25.49869717314499 -2.499 -59.121 89 46\n",
            "25.49869717314499 -2.496 -59.126 89 46\n",
            "25.49869717314499 -2.495 -59.12 89 46\n",
            "25.49869717314499 -2.497 -59.116 89 46\n",
            "25.49869717314499 -2.495 -59.124 89 46\n",
            "25.49869717314499 -2.493 -59.121 89 46\n",
            "25.49869717314499 -2.498 -59.118 89 46\n",
            "25.49869717314499 -2.482 -59.126 89 46\n",
            "25.49869717314499 -2.483 -59.126 89 46\n",
            "25.49869717314499 -2.496 -59.125 89 46\n",
            "25.49869717314499 -2.495 -59.123 89 46\n",
            "25.49869717314499 -2.495 -59.122 89 46\n",
            "25.49869717314499 -2.496 -59.125 89 46\n",
            "25.49869717314499 -2.485 -59.125 89 46\n",
            "25.49869717314499 -2.492 -59.123 89 46\n",
            "25.49869717314499 -2.495 -59.124 89 46\n",
            "25.49869717314499 -2.496 -59.121 89 46\n",
            "26.087119713578208 -3.995544837 -57.58927257 98 55\n",
            "25.49869717314499 -2.485 -59.124 89 46\n",
            "25.49869717314499 -2.495 -59.119 89 46\n",
            "25.49869717314499 -2.494 -59.119 89 46\n",
            "25.49869717314499 -2.486 -59.126 89 46\n",
            "25.49869717314499 -2.496 -59.121 89 46\n",
            "25.49869717314499 -2.495 -59.122 89 46\n",
            "25.49869717314499 -2.495 -59.121 89 46\n",
            "25.49869717314499 -2.496 -59.118 89 46\n",
            "25.49869717314499 -2.496 -59.12 89 46\n",
            "25.49869717314499 -2.493 -59.121 89 46\n",
            "25.49869717314499 -2.497 -59.121 89 46\n",
            "25.49869717314499 -2.483 -59.124 89 46\n",
            "25.94692621851425 -3.3983972 -54.973982 114 52\n",
            "25.94692621851425 -3.3983972 -54.973982 114 52\n",
            "25.94692621851425 -3.3983972 -54.973982 114 52\n",
            "25.94692621851425 -3.3983972 -54.973982 114 52\n",
            "25.94692621851425 -3.3983972 -54.973982 114 52\n",
            "25.94692621851425 -3.3983972 -54.973982 114 52\n",
            "25.94692621851425 -3.3983972 -54.973982 114 52\n",
            "25.94692621851425 -3.3983972 -54.973982 114 52\n",
            "25.94692621851425 -3.3983972 -54.973982 114 52\n",
            "26.387516402606604 -3.9923 -54.908 114 55\n",
            "26.387516402606604 -3.9923 -54.908 114 55\n",
            "26.387516402606604 -3.9923 -54.908 114 55\n",
            "26.387516402606604 -3.9923 -54.908 114 55\n",
            "0.6578289434977157 -3.995544837 -57.58927257 98 55\n",
            "0.6177113017947357 -3.9923 -54.908 114 55\n",
            "0.6177113017947357 -3.9923 -54.908 114 55\n",
            "0.6177113017947357 -3.9923 -54.908 114 55\n",
            "0.6177113017947357 -3.9923 -54.908 114 55\n",
            "0.6177113017947357 -3.9923 -54.908 114 55\n",
            "0.6177113017947357 -3.9923 -54.908 114 55\n",
            "0.6177113017947357 -3.9923 -54.908 114 55\n",
            "0.6578289434977157 -3.995544837 -57.58927257 98 55\n",
            "0.6578289434977157 -3.995544837 -57.58927257 98 55\n",
            "0.6578289434977157 -3.995544837 -57.58927257 98 55\n",
            "0.6423736834304751 -2.63795 -60.14972 83 47\n",
            "0.6423736834304751 -2.63795 -60.14972 83 47\n",
            "0.6423736834304751 -2.63795 -60.14972 83 47\n",
            "0.6423736834304751 -2.63795 -60.14972 83 47\n",
            "0.6423736834304751 -2.63795 -60.14972 83 47\n",
            "0.6380141218839962 -0.90361 -60.43406 81 37\n",
            "0.6578289434977157 -3.995544837 -57.58927257 98 55\n",
            "0.6423736834304751 -2.63795 -60.14972 83 47\n",
            "0.6380141218839962 -0.90361 -60.43406 81 37\n",
            "0.6380141218839962 -0.90361 -60.43406 81 37\n",
            "0.6380141218839962 -0.90361 -60.43406 81 37\n",
            "0.6380141218839962 -0.90361 -60.43406 81 37\n",
            "0.6380141218839962 -0.90361 -60.43406 81 37\n",
            "0.6380141218839962 -0.90361 -60.43406 81 37\n",
            "0.6090029753476368 -2.499 -59.121 89 46\n",
            "0.6090029753476368 -2.496 -59.126 89 46\n",
            "0.6090029753476368 -2.495 -59.12 89 46\n",
            "0.6090029753476368 -2.497 -59.116 89 46\n",
            "0.6090029753476368 -2.495 -59.124 89 46\n",
            "0.6090029753476368 -2.493 -59.121 89 46\n",
            "0.6090029753476368 -2.498 -59.118 89 46\n",
            "0.6090029753476368 -2.482 -59.126 89 46\n",
            "0.6090029753476368 -2.483 -59.126 89 46\n",
            "0.6090029753476368 -2.496 -59.125 89 46\n",
            "0.6090029753476368 -2.495 -59.123 89 46\n",
            "0.6090029753476368 -2.495 -59.122 89 46\n",
            "0.6090029753476368 -2.496 -59.125 89 46\n",
            "0.6090029753476368 -2.485 -59.125 89 46\n",
            "0.6090029753476368 -2.492 -59.123 89 46\n",
            "0.6090029753476368 -2.495 -59.124 89 46\n",
            "0.6090029753476368 -2.496 -59.121 89 46\n",
            "0.6578289434977157 -3.995544837 -57.58927257 98 55\n",
            "0.6090029753476368 -2.485 -59.124 89 46\n",
            "0.6090029753476368 -2.495 -59.119 89 46\n",
            "0.6090029753476368 -2.494 -59.119 89 46\n",
            "0.6090029753476368 -2.486 -59.126 89 46\n",
            "0.6090029753476368 -2.496 -59.121 89 46\n",
            "0.6090029753476368 -2.495 -59.122 89 46\n",
            "0.6090029753476368 -2.495 -59.121 89 46\n",
            "0.6090029753476368 -2.496 -59.118 89 46\n",
            "0.6090029753476368 -2.496 -59.12 89 46\n",
            "0.6090029753476368 -2.493 -59.121 89 46\n",
            "0.6090029753476368 -2.497 -59.121 89 46\n",
            "0.6090029753476368 -2.483 -59.124 89 46\n",
            "0.6213484853393234 -3.3983972 -54.973982 114 52\n",
            "0.6213484853393234 -3.3983972 -54.973982 114 52\n",
            "0.6213484853393234 -3.3983972 -54.973982 114 52\n",
            "0.6213484853393234 -3.3983972 -54.973982 114 52\n",
            "0.6213484853393234 -3.3983972 -54.973982 114 52\n",
            "0.6213484853393234 -3.3983972 -54.973982 114 52\n",
            "0.6213484853393234 -3.3983972 -54.973982 114 52\n",
            "0.6213484853393234 -3.3983972 -54.973982 114 52\n",
            "0.6213484853393234 -3.3983972 -54.973982 114 52\n",
            "0.6177113017947357 -3.9923 -54.908 114 55\n",
            "0.6177113017947357 -3.9923 -54.908 114 55\n",
            "0.6177113017947357 -3.9923 -54.908 114 55\n",
            "0.6177113017947357 -3.9923 -54.908 114 55\n",
            "Index(['lat', 'long', 'd13C_cel', 'vpd', 'rh', 'pet', 'dem', 'pa', 'isorix',\n",
            "       'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION',\n",
            "       'd13C_cel_inferred', 'Mean Annual Temperature',\n",
            "       'Mean Annual Precipitation',\n",
            "       'ordinary_kriging_linear_d18O_predicted_mean',\n",
            "       'ordinary_kriging_linear_d18O_predicted_variance'],\n",
            "      dtype='object')\n",
            "0    -27.921725\n",
            "1    -26.600960\n",
            "2    -26.600960\n",
            "3    -26.600960\n",
            "4    -26.600960\n",
            "        ...    \n",
            "64   -26.485288\n",
            "65   -26.600960\n",
            "66   -26.600960\n",
            "67   -26.600960\n",
            "68   -26.600960\n",
            "Name: d13C_cel_inferred, Length: 68, dtype: float64\n",
            "Index(['lat', 'long', 'd13C_cel', 'vpd', 'rh', 'pet', 'dem', 'pa', 'isorix',\n",
            "       'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION',\n",
            "       'Mean Annual Temperature', 'Mean Annual Precipitation',\n",
            "       'ordinary_kriging_linear_d18O_predicted_mean',\n",
            "       'ordinary_kriging_linear_d18O_predicted_variance'],\n",
            "      dtype='object')\n",
            "-28.70690155029297 -6.009706576 -61.8686565 72 63\n",
            "-28.70690155029297 -6.009706576 -61.8686565 72 63\n",
            "-28.70690155029297 -6.009706576 -61.8686565 72 63\n",
            "-28.70690155029297 -6.009706576 -61.8686565 72 63\n",
            "-28.70690155029297 -6.009706576 -61.8686565 72 63\n",
            "-26.659779866536457 -13.0813 -52.3771 129 105\n",
            "-26.659779866536457 -13.079 -52.3864 129 105\n",
            "-26.659779866536457 -13.079 -52.3864 129 105\n",
            "-26.659779866536457 -13.079 -52.3864 129 105\n",
            "-26.659779866536457 -13.079 -52.3864 129 105\n",
            "-26.659779866536457 -13.079 -52.3864 129 105\n",
            "-26.659779866536457 -13.079 -52.3864 129 105\n",
            "-26.659779866536457 -13.0813 -52.3771 129 105\n",
            "-26.659779866536457 -13.079 -52.3864 129 105\n",
            "-25.785423278808594 -9.318 -60.978 78 82\n",
            "-26.46899922688802 -9.312 -62.982 66 82\n",
            "-26.46899922688802 -9.311 -62.974 66 82\n",
            "-26.46899922688802 -9.317 -62.981 66 82\n",
            "-26.46899922688802 -9.299 -62.977 66 82\n",
            "-28.70690155029297 -6.009706576 -61.8686565 72 63\n",
            "-28.70690155029297 -6.009706576 -61.8686565 72 63\n",
            "-28.70690155029297 -6.009706576 -61.8686565 72 63\n",
            "27.1999994913737 -6.009706576 -61.8686565 291 271\n",
            "27.1999994913737 -6.009706576 -61.8686565 291 271\n",
            "27.1999994913737 -6.009706576 -61.8686565 291 271\n",
            "27.1999994913737 -6.009706576 -61.8686565 291 271\n",
            "27.1999994913737 -6.009706576 -61.8686565 291 271\n",
            "25.066665649414062 -13.0813 -52.3771 518 440\n",
            "25.066665649414062 -13.079 -52.3864 518 440\n",
            "25.066665649414062 -13.079 -52.3864 518 440\n",
            "25.066665649414062 -13.079 -52.3864 518 440\n",
            "25.066665649414062 -13.079 -52.3864 518 440\n",
            "25.066665649414062 -13.079 -52.3864 518 440\n",
            "25.066665649414062 -13.079 -52.3864 518 440\n",
            "25.066665649414062 -13.0813 -52.3771 518 440\n",
            "25.066665649414062 -13.079 -52.3864 518 440\n",
            "25.458333333333332 -9.318 -60.978 312 350\n",
            "25.37500254313151 -9.312 -62.982 264 350\n",
            "25.37500254313151 -9.311 -62.974 264 350\n",
            "25.37500254313151 -9.317 -62.981 264 350\n",
            "25.37500254313151 -9.299 -62.977 264 350\n",
            "27.1999994913737 -6.009706576 -61.8686565 291 271\n",
            "27.1999994913737 -6.009706576 -61.8686565 291 271\n",
            "27.1999994913737 -6.009706576 -61.8686565 291 271\n",
            "1955.0 -6.009706576 -61.8686565 72 68\n",
            "1955.0 -6.009706576 -61.8686565 72 68\n",
            "1955.0 -6.009706576 -61.8686565 72 68\n",
            "1955.0 -6.009706576 -61.8686565 72 68\n",
            "1955.0 -6.009706576 -61.8686565 72 68\n",
            "1574.0 -13.0813 -52.3771 129 110\n",
            "1574.0 -13.079 -52.3864 129 110\n",
            "1574.0 -13.079 -52.3864 129 110\n",
            "1574.0 -13.079 -52.3864 129 110\n",
            "1574.0 -13.079 -52.3864 129 110\n",
            "1574.0 -13.079 -52.3864 129 110\n",
            "1574.0 -13.079 -52.3864 129 110\n",
            "1574.0 -13.0813 -52.3771 129 110\n",
            "1574.0 -13.079 -52.3864 129 110\n",
            "2199.0 -9.318 -60.978 78 87\n",
            "2391.0 -9.312 -62.982 66 87\n",
            "2391.0 -9.311 -62.974 66 87\n",
            "2391.0 -9.317 -62.981 66 87\n",
            "2391.0 -9.299 -62.977 66 87\n",
            "1955.0 -6.009706576 -61.8686565 72 68\n",
            "1955.0 -6.009706576 -61.8686565 72 68\n",
            "1955.0 -6.009706576 -61.8686565 72 68\n",
            "25.148024333365793 -6.009706576 -61.8686565 72 68\n",
            "25.148024333365793 -6.009706576 -61.8686565 72 68\n",
            "25.148024333365793 -6.009706576 -61.8686565 72 68\n",
            "25.148024333365793 -6.009706576 -61.8686565 72 68\n",
            "25.148024333365793 -6.009706576 -61.8686565 72 68\n",
            "27.280440305142708 -13.0813 -52.3771 129 110\n",
            "27.280440305142708 -13.079 -52.3864 129 110\n",
            "27.280440305142708 -13.079 -52.3864 129 110\n",
            "27.280440305142708 -13.079 -52.3864 129 110\n",
            "27.280440305142708 -13.079 -52.3864 129 110\n",
            "27.280440305142708 -13.079 -52.3864 129 110\n",
            "27.280440305142708 -13.079 -52.3864 129 110\n",
            "27.280440305142708 -13.0813 -52.3771 129 110\n",
            "27.280440305142708 -13.079 -52.3864 129 110\n",
            "25.1917364664239 -9.318 -60.978 78 87\n",
            "25.135426018607863 -9.312 -62.982 66 87\n",
            "25.135426018607863 -9.311 -62.974 66 87\n",
            "25.135426018607863 -9.317 -62.981 66 87\n",
            "25.135426018607863 -9.299 -62.977 66 87\n",
            "25.148024333365793 -6.009706576 -61.8686565 72 68\n",
            "25.148024333365793 -6.009706576 -61.8686565 72 68\n",
            "25.148024333365793 -6.009706576 -61.8686565 72 68\n",
            "0.6418359645045993 -6.009706576 -61.8686565 72 68\n",
            "0.6418359645045993 -6.009706576 -61.8686565 72 68\n",
            "0.6418359645045993 -6.009706576 -61.8686565 72 68\n",
            "0.6418359645045993 -6.009706576 -61.8686565 72 68\n",
            "0.6418359645045993 -6.009706576 -61.8686565 72 68\n",
            "0.6243255938234153 -13.0813 -52.3771 129 110\n",
            "0.6243255938234153 -13.079 -52.3864 129 110\n",
            "0.6243255938234153 -13.079 -52.3864 129 110\n",
            "0.6243255938234153 -13.079 -52.3864 129 110\n",
            "0.6243255938234153 -13.079 -52.3864 129 110\n",
            "0.6243255938234153 -13.079 -52.3864 129 110\n",
            "0.6243255938234153 -13.079 -52.3864 129 110\n",
            "0.6243255938234153 -13.0813 -52.3771 129 110\n",
            "0.6243255938234153 -13.079 -52.3864 129 110\n",
            "0.8288806104778278 -9.318 -60.978 78 87\n",
            "0.688800012104716 -9.312 -62.982 66 87\n",
            "0.688800012104716 -9.311 -62.974 66 87\n",
            "0.688800012104716 -9.317 -62.981 66 87\n",
            "0.688800012104716 -9.299 -62.977 66 87\n",
            "0.6418359645045993 -6.009706576 -61.8686565 72 68\n",
            "0.6418359645045993 -6.009706576 -61.8686565 72 68\n",
            "0.6418359645045993 -6.009706576 -61.8686565 72 68\n",
            "Index(['lat', 'long', 'd13C_cel', 'vpd', 'rh', 'pet', 'dem', 'pa', 'isorix',\n",
            "       'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION',\n",
            "       'd13C_cel_inferred', 'Mean Annual Temperature',\n",
            "       'Mean Annual Precipitation',\n",
            "       'ordinary_kriging_linear_d18O_predicted_mean',\n",
            "       'ordinary_kriging_linear_d18O_predicted_variance'],\n",
            "      dtype='object')\n",
            "0    -28.706902\n",
            "1    -28.706902\n",
            "2    -28.706902\n",
            "3    -28.706902\n",
            "4    -28.706902\n",
            "5    -26.659780\n",
            "6    -26.659780\n",
            "7    -26.659780\n",
            "8    -26.659780\n",
            "9    -26.659780\n",
            "10   -26.659780\n",
            "11   -26.659780\n",
            "12   -26.659780\n",
            "13   -26.659780\n",
            "14   -25.785423\n",
            "15   -26.468999\n",
            "16   -26.468999\n",
            "17   -26.468999\n",
            "18   -26.468999\n",
            "19   -28.706902\n",
            "20   -28.706902\n",
            "21   -28.706902\n",
            "Name: d13C_cel_inferred, dtype: float64\n",
            "Index(['lat', 'long', 'd13C_cel', 'vpd', 'rh', 'pet', 'dem', 'pa', 'isorix',\n",
            "       'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION',\n",
            "       'Mean Annual Temperature', 'Mean Annual Precipitation',\n",
            "       'ordinary_kriging_linear_d18O_predicted_mean',\n",
            "       'ordinary_kriging_linear_d18O_predicted_variance'],\n",
            "      dtype='object')\n",
            "-28.60016632080078 -0.121 -67.013 41 27\n",
            "-28.472923278808594 -0.041 -66.872 42 27\n",
            "-28.60016632080078 -0.121229892 -67.01310259 41 27\n",
            "-29.709925333658855 -3.907183314 -66.05514576 47 50\n",
            "-29.38927968343099 -4.303698078 -70.29106779 23 53\n",
            "-28.60016632080078 -0.121229892 -67.01310259 41 27\n",
            "-29.38927968343099 -4.303698078 -70.29106779 23 53\n",
            "-28.60016632080078 -0.121229892 -67.01310259 41 27\n",
            "-29.38927968343099 -4.303698078 -70.29106779 23 53\n",
            "-28.60016632080078 -0.121229892 -67.01310259 41 27\n",
            "-29.38927968343099 -4.303698078 -70.29106779 23 53\n",
            "-29.709925333658855 -3.907 -66.055 47 50\n",
            "-28.60016632080078 -0.121229892 -67.01310259 41 27\n",
            "-29.38927968343099 -4.303698078 -70.29106779 23 53\n",
            "-28.60016632080078 -0.121229892 -67.01310259 41 27\n",
            "-28.60016632080078 -0.121229892 -67.01310259 41 27\n",
            "-28.60016632080078 -0.121229892 -67.01310259 41 27\n",
            "-29.709925333658855 -3.907 -66.055 47 50\n",
            "-29.709925333658855 -3.907183314 -66.05514576 47 50\n",
            "-29.709925333658855 -3.907183314 -66.05514576 47 50\n",
            "-29.709925333658855 -3.907183314 -66.05514576 47 50\n",
            "-29.709925333658855 -3.907183314 -66.05514576 47 50\n",
            "-29.38927968343099 -4.303698078 -70.29106779 23 53\n",
            "-29.38927968343099 -4.303698078 -70.29106779 23 53\n",
            "-29.709925333658855 -3.907183314 -66.05514576 47 50\n",
            "-28.60016632080078 -0.121 -67.013 41 27\n",
            "-28.472923278808594 -0.041 -66.872 42 27\n",
            "-29.709925333658855 -3.907 -66.055 47 50\n",
            "-29.38927968343099 -4.304 -70.291 23 53\n",
            "-29.38927968343099 -4.304 -70.291 23 53\n",
            "-29.709925333658855 -3.907183314 -66.05514576 47 50\n",
            "-28.60016632080078 -0.121229892 -67.01310259 41 27\n",
            "-29.38927968343099 -4.303698078 -70.29106779 23 53\n",
            "-29.837618509928387 -3.758533554 -66.0737537 47 49\n",
            "26.208333333333332 -0.121 -67.013 167 129\n",
            "26.375 -0.041 -66.872 171 127\n",
            "26.208333333333332 -0.121229892 -67.01310259 167 129\n",
            "26.71666717529297 -3.907183314 -66.05514576 190 220\n",
            "26.645833333333332 -4.303698078 -70.29106779 89 230\n",
            "26.208333333333332 -0.121229892 -67.01310259 167 129\n",
            "26.645833333333332 -4.303698078 -70.29106779 89 230\n",
            "26.208333333333332 -0.121229892 -67.01310259 167 129\n",
            "26.645833333333332 -4.303698078 -70.29106779 89 230\n",
            "26.208333333333332 -0.121229892 -67.01310259 167 129\n",
            "26.645833333333332 -4.303698078 -70.29106779 89 230\n",
            "26.71666717529297 -3.907 -66.055 190 220\n",
            "26.208333333333332 -0.121229892 -67.01310259 167 129\n",
            "26.645833333333332 -4.303698078 -70.29106779 89 230\n",
            "26.208333333333332 -0.121229892 -67.01310259 167 129\n",
            "26.208333333333332 -0.121229892 -67.01310259 167 129\n",
            "26.208333333333332 -0.121229892 -67.01310259 167 129\n",
            "26.71666717529297 -3.907 -66.055 190 220\n",
            "26.71666717529297 -3.907183314 -66.05514576 190 220\n",
            "26.71666717529297 -3.907183314 -66.05514576 190 220\n",
            "26.71666717529297 -3.907183314 -66.05514576 190 220\n",
            "26.71666717529297 -3.907183314 -66.05514576 190 220\n",
            "26.645833333333332 -4.303698078 -70.29106779 89 230\n",
            "26.645833333333332 -4.303698078 -70.29106779 89 230\n",
            "26.71666717529297 -3.907183314 -66.05514576 190 220\n",
            "26.208333333333332 -0.121 -67.013 167 129\n",
            "26.375 -0.041 -66.872 171 127\n",
            "26.71666717529297 -3.907 -66.055 190 220\n",
            "26.645833333333332 -4.304 -70.291 89 230\n",
            "26.645833333333332 -4.304 -70.291 89 230\n",
            "26.71666717529297 -3.907183314 -66.05514576 190 220\n",
            "26.208333333333332 -0.121229892 -67.01310259 167 129\n",
            "26.645833333333332 -4.303698078 -70.29106779 89 230\n",
            "26.71666717529297 -3.758533554 -66.0737537 190 217\n",
            "2841.0 -0.121 -67.013 41 32\n",
            "2771.0 -0.041 -66.872 42 32\n",
            "2841.0 -0.121229892 -67.01310259 41 32\n",
            "2787.0 -3.907183314 -66.05514576 47 55\n",
            "-- -4.303698078 -70.29106779 55 23\n",
            "2841.0 -0.121229892 -67.01310259 41 32\n",
            "-- -4.303698078 -70.29106779 55 23\n",
            "2841.0 -0.121229892 -67.01310259 41 32\n",
            "-- -4.303698078 -70.29106779 55 23\n",
            "2841.0 -0.121229892 -67.01310259 41 32\n",
            "-- -4.303698078 -70.29106779 55 23\n",
            "2787.0 -3.907 -66.055 47 55\n",
            "2841.0 -0.121229892 -67.01310259 41 32\n",
            "-- -4.303698078 -70.29106779 55 23\n",
            "2841.0 -0.121229892 -67.01310259 41 32\n",
            "2841.0 -0.121229892 -67.01310259 41 32\n",
            "2841.0 -0.121229892 -67.01310259 41 32\n",
            "2787.0 -3.907 -66.055 47 55\n",
            "2787.0 -3.907183314 -66.05514576 47 55\n",
            "2787.0 -3.907183314 -66.05514576 47 55\n",
            "2787.0 -3.907183314 -66.05514576 47 55\n",
            "2787.0 -3.907183314 -66.05514576 47 55\n",
            "-- -4.303698078 -70.29106779 55 23\n",
            "-- -4.303698078 -70.29106779 55 23\n",
            "2787.0 -3.907183314 -66.05514576 47 55\n",
            "2841.0 -0.121 -67.013 41 32\n",
            "2771.0 -0.041 -66.872 42 32\n",
            "2787.0 -3.907 -66.055 47 55\n",
            "-- -4.304 -70.291 55 23\n",
            "-- -4.304 -70.291 55 23\n",
            "2787.0 -3.907183314 -66.05514576 47 55\n",
            "2841.0 -0.121229892 -67.01310259 41 32\n",
            "-- -4.303698078 -70.29106779 55 23\n",
            "2873.0 -3.758533554 -66.0737537 47 54\n",
            "25.113644902709286 -0.121 -67.013 41 32\n",
            "25.191728310002834 -0.041 -66.872 42 32\n",
            "25.113644902709286 -0.121229892 -67.01310259 41 32\n",
            "24.890322162938237 -3.907183314 -66.05514576 47 55\n",
            "25.857736792649984 -4.303698078 -70.29106779 55 23\n",
            "25.113644902709286 -0.121229892 -67.01310259 41 32\n",
            "25.857736792649984 -4.303698078 -70.29106779 55 23\n",
            "25.113644902709286 -0.121229892 -67.01310259 41 32\n",
            "25.857736792649984 -4.303698078 -70.29106779 55 23\n",
            "25.113644902709286 -0.121229892 -67.01310259 41 32\n",
            "25.857736792649984 -4.303698078 -70.29106779 55 23\n",
            "24.890322162938237 -3.907 -66.055 47 55\n",
            "25.113644902709286 -0.121229892 -67.01310259 41 32\n",
            "25.857736792649984 -4.303698078 -70.29106779 55 23\n",
            "25.113644902709286 -0.121229892 -67.01310259 41 32\n",
            "25.113644902709286 -0.121229892 -67.01310259 41 32\n",
            "25.113644902709286 -0.121229892 -67.01310259 41 32\n",
            "24.890322162938237 -3.907 -66.055 47 55\n",
            "24.890322162938237 -3.907183314 -66.05514576 47 55\n",
            "24.890322162938237 -3.907183314 -66.05514576 47 55\n",
            "24.890322162938237 -3.907183314 -66.05514576 47 55\n",
            "24.890322162938237 -3.907183314 -66.05514576 47 55\n",
            "25.857736792649984 -4.303698078 -70.29106779 55 23\n",
            "25.857736792649984 -4.303698078 -70.29106779 55 23\n",
            "24.890322162938237 -3.907183314 -66.05514576 47 55\n",
            "25.113644902709286 -0.121 -67.013 41 32\n",
            "25.191728310002834 -0.041 -66.872 42 32\n",
            "24.890322162938237 -3.907 -66.055 47 55\n",
            "25.857736792649984 -4.304 -70.291 55 23\n",
            "25.857736792649984 -4.304 -70.291 55 23\n",
            "24.890322162938237 -3.907183314 -66.05514576 47 55\n",
            "25.113644902709286 -0.121229892 -67.01310259 41 32\n",
            "25.857736792649984 -4.303698078 -70.29106779 55 23\n",
            "24.858199309283673 -3.758533554 -66.0737537 47 54\n",
            "0.6438317116725394 -0.121 -67.013 41 32\n",
            "0.623212637892986 -0.041 -66.872 42 32\n",
            "0.6438317116725394 -0.121229892 -67.01310259 41 32\n",
            "0.6250570530803327 -3.907183314 -66.05514576 47 55\n",
            "1.1759524122666536 -4.303698078 -70.29106779 55 23\n",
            "0.6438317116725394 -0.121229892 -67.01310259 41 32\n",
            "1.1759524122666536 -4.303698078 -70.29106779 55 23\n",
            "0.6438317116725394 -0.121229892 -67.01310259 41 32\n",
            "1.1759524122666536 -4.303698078 -70.29106779 55 23\n",
            "0.6438317116725394 -0.121229892 -67.01310259 41 32\n",
            "1.1759524122666536 -4.303698078 -70.29106779 55 23\n",
            "0.6250570530803327 -3.907 -66.055 47 55\n",
            "0.6438317116725394 -0.121229892 -67.01310259 41 32\n",
            "1.1759524122666536 -4.303698078 -70.29106779 55 23\n",
            "0.6438317116725394 -0.121229892 -67.01310259 41 32\n",
            "0.6438317116725394 -0.121229892 -67.01310259 41 32\n",
            "0.6438317116725394 -0.121229892 -67.01310259 41 32\n",
            "0.6250570530803327 -3.907 -66.055 47 55\n",
            "0.6250570530803327 -3.907183314 -66.05514576 47 55\n",
            "0.6250570530803327 -3.907183314 -66.05514576 47 55\n",
            "0.6250570530803327 -3.907183314 -66.05514576 47 55\n",
            "0.6250570530803327 -3.907183314 -66.05514576 47 55\n",
            "1.1759524122666536 -4.303698078 -70.29106779 55 23\n",
            "1.1759524122666536 -4.303698078 -70.29106779 55 23\n",
            "0.6250570530803327 -3.907183314 -66.05514576 47 55\n",
            "0.6438317116725394 -0.121 -67.013 41 32\n",
            "0.623212637892986 -0.041 -66.872 42 32\n",
            "0.6250570530803327 -3.907 -66.055 47 55\n",
            "1.1759524122666536 -4.304 -70.291 55 23\n",
            "1.1759524122666536 -4.304 -70.291 55 23\n",
            "0.6250570530803327 -3.907183314 -66.05514576 47 55\n",
            "0.6438317116725394 -0.121229892 -67.01310259 41 32\n",
            "1.1759524122666536 -4.303698078 -70.29106779 55 23\n",
            "0.6520904340478871 -3.758533554 -66.0737537 47 54\n",
            "Index(['lat', 'long', 'd13C_cel', 'vpd', 'rh', 'pet', 'dem', 'pa', 'isorix',\n",
            "       'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION',\n",
            "       'd13C_cel_inferred', 'Mean Annual Temperature',\n",
            "       'Mean Annual Precipitation',\n",
            "       'ordinary_kriging_linear_d18O_predicted_mean',\n",
            "       'ordinary_kriging_linear_d18O_predicted_variance'],\n",
            "      dtype='object')\n",
            "0    -28.600166\n",
            "1    -28.472923\n",
            "2    -28.600166\n",
            "3    -29.709925\n",
            "4    -29.389280\n",
            "5    -28.600166\n",
            "6    -29.389280\n",
            "7    -28.600166\n",
            "8    -29.389280\n",
            "9    -28.600166\n",
            "10   -29.389280\n",
            "11   -29.709925\n",
            "12   -28.600166\n",
            "13   -29.389280\n",
            "14   -28.600166\n",
            "15   -28.600166\n",
            "16   -28.600166\n",
            "17   -29.709925\n",
            "18   -29.709925\n",
            "19   -29.709925\n",
            "20   -29.709925\n",
            "21   -29.709925\n",
            "22   -29.389280\n",
            "23   -29.389280\n",
            "24   -29.709925\n",
            "25   -28.600166\n",
            "26   -28.472923\n",
            "27   -29.709925\n",
            "28   -29.389280\n",
            "29   -29.389280\n",
            "30   -29.709925\n",
            "31   -28.600166\n",
            "32   -29.389280\n",
            "33   -29.837619\n",
            "Name: d13C_cel_inferred, dtype: float64\n",
            "Index(['lat', 'long', 'd13C_cel', 'vpd', 'rh', 'pet', 'dem', 'pa', 'isorix',\n",
            "       'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION',\n",
            "       'Mean Annual Temperature', 'Mean Annual Precipitation',\n",
            "       'ordinary_kriging_linear_d18O_predicted_mean',\n",
            "       'ordinary_kriging_linear_d18O_predicted_variance'],\n",
            "      dtype='object')\n",
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('lat_standardizer', StandardScaler(), ['lat']),\n",
            "                                ('long_standardizer', StandardScaler(),\n",
            "                                 ['long']),\n",
            "                                ('vpd_standardizer', StandardScaler(), ['vpd']),\n",
            "                                ('rh_standardizer', StandardScaler(), ['rh']),\n",
            "                                ('pet_standardizer', StandardScaler(), ['pet']),\n",
            "                                ('dem_standardizer', StandardScaler(), ['dem']),\n",
            "                                ('pa_standardizer'...\n",
            "                                ('Iso_Oxi_Stack_mean_TERZER_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Iso_Oxi_Stack_mean_TERZER']),\n",
            "                                ('isoscape_fullmodel_d18O_prec_REGRESSION_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['isoscape_fullmodel_d18O_prec_REGRESSION']),\n",
            "                                ('Mean Annual Temperature_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation'])])\n",
            "==================\n",
            "fixed_all_isorix_carbon_boosted\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 15)]         0           []                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_3 (Sl  (None, 13)          0           ['input_4[0][0]']                \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 20)           280         ['tf.__operators__.getitem_3[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_4 (Sl  (None,)             0           ['input_4[0][0]']                \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 20)           420         ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_5 (Sl  (None,)             0           ['input_4[0][0]']                \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " tf.expand_dims_2 (TFOpLambda)  (None, 1)            0           ['tf.__operators__.getitem_4[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " pred_mean_residual (Dense)     (None, 1)            21          ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " tf.expand_dims_3 (TFOpLambda)  (None, 1)            0           ['tf.__operators__.getitem_5[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " pred_var_output (Dense)        (None, 1)            21          ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TFOpLa  (None, 1)           0           ['tf.expand_dims_2[0][0]',       \n",
            " mbda)                                                            'pred_mean_residual[0][0]']     \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1)            0           ['tf.expand_dims_3[0][0]',       \n",
            "                                                                  'pred_var_output[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 2)            0           ['tf.__operators__.add_5[0][0]', \n",
            "                                                                  'lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 742\n",
            "Trainable params: 742\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5000\n",
            "23/23 [==============================] - 3s 43ms/step - loss: 1.1459 - val_loss: 43.9760\n",
            "Epoch 2/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 1.2207 - val_loss: 20.0037\n",
            "Epoch 3/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.0487 - val_loss: 16.6438\n",
            "Epoch 4/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 1.0536 - val_loss: 17.1709\n",
            "Epoch 5/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.1630 - val_loss: 14.2676\n",
            "Epoch 6/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.9123 - val_loss: 15.2568\n",
            "Epoch 7/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.9667 - val_loss: 12.6039\n",
            "Epoch 8/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.0774 - val_loss: 11.1323\n",
            "Epoch 9/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.9368 - val_loss: 8.7061\n",
            "Epoch 10/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.9929 - val_loss: 8.3327\n",
            "Epoch 11/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8692 - val_loss: 9.2643\n",
            "Epoch 12/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.9497 - val_loss: 7.1956\n",
            "Epoch 13/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8910 - val_loss: 6.0755\n",
            "Epoch 14/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8386 - val_loss: 6.9297\n",
            "Epoch 15/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.9616 - val_loss: 4.9313\n",
            "Epoch 16/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8704 - val_loss: 5.7602\n",
            "Epoch 17/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8771 - val_loss: 5.8658\n",
            "Epoch 18/5000\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.8344 - val_loss: 6.7739\n",
            "Epoch 19/5000\n",
            "23/23 [==============================] - 1s 30ms/step - loss: 0.8184 - val_loss: 4.9676\n",
            "Epoch 20/5000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.8729 - val_loss: 4.4747\n",
            "Epoch 21/5000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.9657 - val_loss: 4.4437\n",
            "Epoch 22/5000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.7773 - val_loss: 5.0276\n",
            "Epoch 23/5000\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.7566 - val_loss: 3.8860\n",
            "Epoch 24/5000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.8238 - val_loss: 3.8527\n",
            "Epoch 25/5000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.8703 - val_loss: 3.7546\n",
            "Epoch 26/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7114 - val_loss: 3.7764\n",
            "Epoch 27/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7882 - val_loss: 3.7760\n",
            "Epoch 28/5000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.7540 - val_loss: 3.2090\n",
            "Epoch 29/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.7869 - val_loss: 4.1900\n",
            "Epoch 30/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7881 - val_loss: 3.3304\n",
            "Epoch 31/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7647 - val_loss: 4.6063\n",
            "Epoch 32/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8230 - val_loss: 2.9911\n",
            "Epoch 33/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.8246 - val_loss: 3.3121\n",
            "Epoch 34/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7528 - val_loss: 3.1103\n",
            "Epoch 35/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7653 - val_loss: 3.4681\n",
            "Epoch 36/5000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.7999 - val_loss: 3.0408\n",
            "Epoch 37/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7400 - val_loss: 4.1767\n",
            "Epoch 38/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.9122 - val_loss: 3.3676\n",
            "Epoch 39/5000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.7404 - val_loss: 2.6592\n",
            "Epoch 40/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8278 - val_loss: 2.4270\n",
            "Epoch 41/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7315 - val_loss: 3.1646\n",
            "Epoch 42/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7916 - val_loss: 3.1070\n",
            "Epoch 43/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8459 - val_loss: 2.8396\n",
            "Epoch 44/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7912 - val_loss: 2.5821\n",
            "Epoch 45/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7573 - val_loss: 2.8767\n",
            "Epoch 46/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7834 - val_loss: 2.7999\n",
            "Epoch 47/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6708 - val_loss: 2.6425\n",
            "Epoch 48/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7194 - val_loss: 2.2556\n",
            "Epoch 49/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6895 - val_loss: 3.0800\n",
            "Epoch 50/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7808 - val_loss: 2.5603\n",
            "Epoch 51/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7080 - val_loss: 2.5946\n",
            "Epoch 52/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7689 - val_loss: 2.4545\n",
            "Epoch 53/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7356 - val_loss: 2.4596\n",
            "Epoch 54/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6929 - val_loss: 2.1661\n",
            "Epoch 55/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7541 - val_loss: 2.7140\n",
            "Epoch 56/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7330 - val_loss: 2.1381\n",
            "Epoch 57/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7617 - val_loss: 2.5661\n",
            "Epoch 58/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7591 - val_loss: 2.1501\n",
            "Epoch 59/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8455 - val_loss: 2.4376\n",
            "Epoch 60/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7897 - val_loss: 2.2870\n",
            "Epoch 61/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7433 - val_loss: 2.3147\n",
            "Epoch 62/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7779 - val_loss: 2.3973\n",
            "Epoch 63/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9047 - val_loss: 2.3493\n",
            "Epoch 64/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7525 - val_loss: 2.2676\n",
            "Epoch 65/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6500 - val_loss: 2.2529\n",
            "Epoch 66/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7434 - val_loss: 2.2745\n",
            "Epoch 67/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6560 - val_loss: 2.2382\n",
            "Epoch 68/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6889 - val_loss: 2.1194\n",
            "Epoch 69/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6613 - val_loss: 2.5685\n",
            "Epoch 70/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6877 - val_loss: 2.2799\n",
            "Epoch 71/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7791 - val_loss: 2.1782\n",
            "Epoch 72/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7709 - val_loss: 2.0115\n",
            "Epoch 73/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7743 - val_loss: 1.9146\n",
            "Epoch 74/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7229 - val_loss: 2.1339\n",
            "Epoch 75/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7594 - val_loss: 2.2727\n",
            "Epoch 76/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7278 - val_loss: 2.0062\n",
            "Epoch 77/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6950 - val_loss: 2.0256\n",
            "Epoch 78/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6884 - val_loss: 2.1321\n",
            "Epoch 79/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7293 - val_loss: 1.9585\n",
            "Epoch 80/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6515 - val_loss: 1.9500\n",
            "Epoch 81/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7448 - val_loss: 1.9942\n",
            "Epoch 82/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6722 - val_loss: 1.8320\n",
            "Epoch 83/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7943 - val_loss: 1.9938\n",
            "Epoch 84/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7383 - val_loss: 2.2700\n",
            "Epoch 85/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7446 - val_loss: 2.0148\n",
            "Epoch 86/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7119 - val_loss: 1.9868\n",
            "Epoch 87/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8242 - val_loss: 1.9066\n",
            "Epoch 88/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7122 - val_loss: 1.7415\n",
            "Epoch 89/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7353 - val_loss: 1.7896\n",
            "Epoch 90/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7067 - val_loss: 2.1995\n",
            "Epoch 91/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7409 - val_loss: 1.9435\n",
            "Epoch 92/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7070 - val_loss: 2.0308\n",
            "Epoch 93/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7531 - val_loss: 2.0985\n",
            "Epoch 94/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7248 - val_loss: 1.8330\n",
            "Epoch 95/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6767 - val_loss: 1.7669\n",
            "Epoch 96/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6737 - val_loss: 1.8002\n",
            "Epoch 97/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7630 - val_loss: 1.9194\n",
            "Epoch 98/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7240 - val_loss: 1.8754\n",
            "Epoch 99/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7705 - val_loss: 2.2407\n",
            "Epoch 100/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6845 - val_loss: 1.9302\n",
            "Epoch 101/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7210 - val_loss: 1.9161\n",
            "Epoch 102/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7290 - val_loss: 1.9660\n",
            "Epoch 103/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6740 - val_loss: 1.8039\n",
            "Epoch 104/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7836 - val_loss: 1.7605\n",
            "Epoch 105/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7708 - val_loss: 1.8229\n",
            "Epoch 106/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7048 - val_loss: 1.9425\n",
            "Epoch 107/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7392 - val_loss: 1.7198\n",
            "Epoch 108/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7061 - val_loss: 1.7779\n",
            "Epoch 109/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7602 - val_loss: 1.7233\n",
            "Epoch 110/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.6986 - val_loss: 1.9209\n",
            "Epoch 111/5000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.6820 - val_loss: 1.6746\n",
            "Epoch 112/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7043 - val_loss: 1.8446\n",
            "Epoch 113/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6848 - val_loss: 1.6293\n",
            "Epoch 114/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7403 - val_loss: 1.6031\n",
            "Epoch 115/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8077 - val_loss: 1.8196\n",
            "Epoch 116/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7918 - val_loss: 1.9744\n",
            "Epoch 117/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7437 - val_loss: 1.8539\n",
            "Epoch 118/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7486 - val_loss: 1.7898\n",
            "Epoch 119/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6775 - val_loss: 1.8450\n",
            "Epoch 120/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7752 - val_loss: 1.8262\n",
            "Epoch 121/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7438 - val_loss: 1.7793\n",
            "Epoch 122/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6815 - val_loss: 1.9770\n",
            "Epoch 123/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7168 - val_loss: 2.2004\n",
            "Epoch 124/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6983 - val_loss: 1.7507\n",
            "Epoch 125/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7183 - val_loss: 1.8752\n",
            "Epoch 126/5000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.6840 - val_loss: 1.5867\n",
            "Epoch 127/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7886 - val_loss: 1.5917\n",
            "Epoch 128/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6949 - val_loss: 1.6183\n",
            "Epoch 129/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6614 - val_loss: 1.7365\n",
            "Epoch 130/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7401 - val_loss: 1.9415\n",
            "Epoch 131/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6371 - val_loss: 1.6797\n",
            "Epoch 132/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7152 - val_loss: 1.8447\n",
            "Epoch 133/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6911 - val_loss: 1.7511\n",
            "Epoch 134/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7771 - val_loss: 1.6567\n",
            "Epoch 135/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7345 - val_loss: 1.8236\n",
            "Epoch 136/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7426 - val_loss: 1.6411\n",
            "Epoch 137/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6918 - val_loss: 1.8951\n",
            "Epoch 138/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6829 - val_loss: 1.9551\n",
            "Epoch 139/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6572 - val_loss: 1.8514\n",
            "Epoch 140/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6593 - val_loss: 1.5390\n",
            "Epoch 141/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6829 - val_loss: 1.6164\n",
            "Epoch 142/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7434 - val_loss: 1.6721\n",
            "Epoch 143/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7088 - val_loss: 2.1066\n",
            "Epoch 144/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7419 - val_loss: 1.9511\n",
            "Epoch 145/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7392 - val_loss: 1.6322\n",
            "Epoch 146/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7792 - val_loss: 1.7470\n",
            "Epoch 147/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6504 - val_loss: 1.5908\n",
            "Epoch 148/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6804 - val_loss: 1.8694\n",
            "Epoch 149/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7306 - val_loss: 1.7852\n",
            "Epoch 150/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6976 - val_loss: 1.7504\n",
            "Epoch 151/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6607 - val_loss: 1.4237\n",
            "Epoch 152/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7179 - val_loss: 1.8146\n",
            "Epoch 153/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6881 - val_loss: 1.7793\n",
            "Epoch 154/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6984 - val_loss: 1.4504\n",
            "Epoch 155/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7095 - val_loss: 1.4824\n",
            "Epoch 156/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7253 - val_loss: 2.0634\n",
            "Epoch 157/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6574 - val_loss: 1.8023\n",
            "Epoch 158/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7157 - val_loss: 1.8634\n",
            "Epoch 159/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7157 - val_loss: 1.7152\n",
            "Epoch 160/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6826 - val_loss: 1.7986\n",
            "Epoch 161/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7047 - val_loss: 1.9757\n",
            "Epoch 162/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6795 - val_loss: 1.5425\n",
            "Epoch 163/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7826 - val_loss: 1.7305\n",
            "Epoch 164/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7485 - val_loss: 1.6366\n",
            "Epoch 165/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7305 - val_loss: 1.7394\n",
            "Epoch 166/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6616 - val_loss: 1.8937\n",
            "Epoch 167/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7270 - val_loss: 1.7999\n",
            "Epoch 168/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6467 - val_loss: 1.6245\n",
            "Epoch 169/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7243 - val_loss: 1.8751\n",
            "Epoch 170/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7725 - val_loss: 1.5604\n",
            "Epoch 171/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7386 - val_loss: 1.7082\n",
            "Epoch 172/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6845 - val_loss: 1.5662\n",
            "Epoch 173/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7345 - val_loss: 1.8311\n",
            "Epoch 174/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7253 - val_loss: 1.4596\n",
            "Epoch 175/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6615 - val_loss: 1.6571\n",
            "Epoch 176/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6602 - val_loss: 1.7541\n",
            "Epoch 177/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7077 - val_loss: 1.6608\n",
            "Epoch 178/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6832 - val_loss: 1.7808\n",
            "Epoch 179/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6486 - val_loss: 1.4801\n",
            "Epoch 180/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6946 - val_loss: 1.8790\n",
            "Epoch 181/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6719 - val_loss: 1.7876\n",
            "Epoch 182/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6501 - val_loss: 1.5502\n",
            "Epoch 183/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6832 - val_loss: 1.5916\n",
            "Epoch 184/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6680 - val_loss: 1.6321\n",
            "Epoch 185/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7034 - val_loss: 1.5489\n",
            "Epoch 186/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7209 - val_loss: 1.6569\n",
            "Epoch 187/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6676 - val_loss: 1.6718\n",
            "Epoch 188/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7553 - val_loss: 1.7208\n",
            "Epoch 189/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7110 - val_loss: 1.7454\n",
            "Epoch 190/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7498 - val_loss: 1.5074\n",
            "Epoch 191/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7284 - val_loss: 1.8962\n",
            "Epoch 192/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7481 - val_loss: 1.7940\n",
            "Epoch 193/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6917 - val_loss: 1.5693\n",
            "Epoch 194/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6891 - val_loss: 2.0114\n",
            "Epoch 195/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7080 - val_loss: 1.9286\n",
            "Epoch 196/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6936 - val_loss: 1.7907\n",
            "Epoch 197/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6710 - val_loss: 1.5347\n",
            "Epoch 198/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6897 - val_loss: 1.7709\n",
            "Epoch 199/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6709 - val_loss: 1.5209\n",
            "Epoch 200/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6760 - val_loss: 1.9505\n",
            "Epoch 201/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7066 - val_loss: 1.5121\n",
            "Epoch 202/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6550 - val_loss: 1.6566\n",
            "Epoch 203/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6977 - val_loss: 1.6426\n",
            "Epoch 204/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6910 - val_loss: 1.7724\n",
            "Epoch 205/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7444 - val_loss: 1.7139\n",
            "Epoch 206/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6934 - val_loss: 1.8265\n",
            "Epoch 207/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6778 - val_loss: 2.0238\n",
            "Epoch 208/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6659 - val_loss: 1.6179\n",
            "Epoch 209/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6818 - val_loss: 1.7910\n",
            "Epoch 210/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6642 - val_loss: 1.8268\n",
            "Epoch 211/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7076 - val_loss: 2.0626\n",
            "Epoch 212/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7779 - val_loss: 1.6556\n",
            "Epoch 213/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6821 - val_loss: 1.5683\n",
            "Epoch 214/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6898 - val_loss: 1.6910\n",
            "Epoch 215/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7063 - val_loss: 1.7206\n",
            "Epoch 216/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6612 - val_loss: 1.5210\n",
            "Epoch 217/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7555 - val_loss: 1.5799\n",
            "Epoch 218/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6901 - val_loss: 2.2486\n",
            "Epoch 219/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6716 - val_loss: 1.8875\n",
            "Epoch 220/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6630 - val_loss: 1.9530\n",
            "Epoch 221/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6658 - val_loss: 1.8641\n",
            "Epoch 222/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7141 - val_loss: 1.4852\n",
            "Epoch 223/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7206 - val_loss: 2.2232\n",
            "Epoch 224/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6355 - val_loss: 1.5721\n",
            "Epoch 225/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6525 - val_loss: 1.7622\n",
            "Epoch 226/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6276 - val_loss: 1.4902\n",
            "Epoch 227/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6924 - val_loss: 1.5111\n",
            "Epoch 228/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6509 - val_loss: 1.5262\n",
            "Epoch 229/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7288 - val_loss: 1.8681\n",
            "Epoch 230/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6885 - val_loss: 1.9614\n",
            "Epoch 231/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6639 - val_loss: 1.6431\n",
            "Epoch 232/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7378 - val_loss: 1.6966\n",
            "Epoch 233/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6906 - val_loss: 1.8777\n",
            "Epoch 234/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7568 - val_loss: 1.6005\n",
            "Epoch 235/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6685 - val_loss: 1.7472\n",
            "Epoch 236/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7165 - val_loss: 2.2180\n",
            "Epoch 237/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7020 - val_loss: 1.6221\n",
            "Epoch 238/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6932 - val_loss: 1.7949\n",
            "Epoch 239/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6601 - val_loss: 1.7868\n",
            "Epoch 240/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6828 - val_loss: 1.9399\n",
            "Epoch 241/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7091 - val_loss: 1.7973\n",
            "Epoch 242/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6601 - val_loss: 1.5672\n",
            "Epoch 243/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6959 - val_loss: 1.4821\n",
            "Epoch 244/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6721 - val_loss: 1.6298\n",
            "Epoch 245/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6737 - val_loss: 1.9980\n",
            "Epoch 246/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6863 - val_loss: 1.5673\n",
            "Epoch 247/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6543 - val_loss: 1.7292\n",
            "Epoch 248/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6613 - val_loss: 1.5924\n",
            "Epoch 249/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6759 - val_loss: 1.9826\n",
            "Epoch 250/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7133 - val_loss: 1.4806\n",
            "Epoch 251/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6521 - val_loss: 1.7633\n",
            "Epoch 252/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6631 - val_loss: 1.4626\n",
            "Epoch 253/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7724 - val_loss: 1.5848\n",
            "Epoch 254/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6516 - val_loss: 1.7169\n",
            "Epoch 255/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6776 - val_loss: 1.4442\n",
            "Epoch 256/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7856 - val_loss: 1.4928\n",
            "Epoch 257/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6944 - val_loss: 1.9044\n",
            "Epoch 258/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6726 - val_loss: 1.6386\n",
            "Epoch 259/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7012 - val_loss: 1.6534\n",
            "Epoch 260/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6435 - val_loss: 1.6057\n",
            "Epoch 261/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6944 - val_loss: 1.6296\n",
            "Epoch 262/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6768 - val_loss: 1.5071\n",
            "Epoch 263/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6901 - val_loss: 1.5790\n",
            "Epoch 264/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6562 - val_loss: 1.6211\n",
            "Epoch 265/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7110 - val_loss: 1.8288\n",
            "Epoch 266/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7092 - val_loss: 1.8330\n",
            "Epoch 267/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6726 - val_loss: 1.6029\n",
            "Epoch 268/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7309 - val_loss: 1.7621\n",
            "Epoch 269/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7168 - val_loss: 1.8089\n",
            "Epoch 270/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6757 - val_loss: 1.6597\n",
            "Epoch 271/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6659 - val_loss: 1.7723\n",
            "Epoch 272/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6818 - val_loss: 1.6688\n",
            "Epoch 273/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6878 - val_loss: 1.9319\n",
            "Epoch 274/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7340 - val_loss: 1.7713\n",
            "Epoch 275/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6572 - val_loss: 1.7836\n",
            "Epoch 276/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6495 - val_loss: 1.6602\n",
            "Epoch 277/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7027 - val_loss: 1.7416\n",
            "Epoch 278/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6787 - val_loss: 1.8908\n",
            "Epoch 279/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6965 - val_loss: 1.5601\n",
            "Epoch 280/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6907 - val_loss: 1.4440\n",
            "Epoch 281/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7007 - val_loss: 1.9116\n",
            "Epoch 282/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6824 - val_loss: 2.0190\n",
            "Epoch 283/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7164 - val_loss: 1.5081\n",
            "Epoch 284/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6530 - val_loss: 1.5334\n",
            "Epoch 285/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7235 - val_loss: 1.6918\n",
            "Epoch 286/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7021 - val_loss: 1.9016\n",
            "Epoch 287/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6842 - val_loss: 1.7554\n",
            "Epoch 288/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6681 - val_loss: 1.8600\n",
            "Epoch 289/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6596 - val_loss: 1.7811\n",
            "Epoch 290/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6790 - val_loss: 1.5467\n",
            "Epoch 291/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7606 - val_loss: 1.5949\n",
            "Epoch 292/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6779 - val_loss: 1.8898\n",
            "Epoch 293/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6936 - val_loss: 2.2236\n",
            "Epoch 294/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7291 - val_loss: 1.8142\n",
            "Epoch 295/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6523 - val_loss: 1.8056\n",
            "Epoch 296/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6672 - val_loss: 1.6935\n",
            "Epoch 297/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6585 - val_loss: 1.7642\n",
            "Epoch 298/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6818 - val_loss: 1.8193\n",
            "Epoch 299/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6631 - val_loss: 1.6631\n",
            "Epoch 300/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6891 - val_loss: 1.6599\n",
            "Epoch 301/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6694 - val_loss: 1.9253\n",
            "Epoch 302/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7168 - val_loss: 1.7272\n",
            "Epoch 303/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6921 - val_loss: 1.9388\n",
            "Epoch 304/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6381 - val_loss: 1.8560\n",
            "Epoch 305/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7316 - val_loss: 1.8188\n",
            "Epoch 306/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6629 - val_loss: 1.7374\n",
            "Epoch 307/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7324 - val_loss: 1.6234\n",
            "Epoch 308/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6452 - val_loss: 1.9916\n",
            "Epoch 309/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6932 - val_loss: 1.7921\n",
            "Epoch 310/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7698 - val_loss: 1.5823\n",
            "Epoch 311/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6981 - val_loss: 1.6347\n",
            "Epoch 312/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6923 - val_loss: 1.7187\n",
            "Epoch 313/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7897 - val_loss: 1.5862\n",
            "Epoch 314/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6858 - val_loss: 1.7721\n",
            "Epoch 315/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6215 - val_loss: 1.7164\n",
            "Epoch 316/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6177 - val_loss: 1.7909\n",
            "Epoch 317/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6553 - val_loss: 1.7286\n",
            "Epoch 318/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7925 - val_loss: 2.1438\n",
            "Epoch 319/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6794 - val_loss: 1.7013\n",
            "Epoch 320/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6497 - val_loss: 1.8774\n",
            "Epoch 321/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5926 - val_loss: 1.6194\n",
            "Epoch 322/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6916 - val_loss: 1.6819\n",
            "Epoch 323/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6753 - val_loss: 1.6220\n",
            "Epoch 324/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6630 - val_loss: 1.7326\n",
            "Epoch 325/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6744 - val_loss: 1.7277\n",
            "Epoch 326/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7069 - val_loss: 1.5987\n",
            "Epoch 327/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6902 - val_loss: 1.6352\n",
            "Epoch 328/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6791 - val_loss: 1.7352\n",
            "Epoch 329/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6842 - val_loss: 1.6986\n",
            "Epoch 330/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6514 - val_loss: 1.8340\n",
            "Epoch 331/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6741 - val_loss: 1.8871\n",
            "Epoch 332/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6470 - val_loss: 1.6800\n",
            "Epoch 333/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6712 - val_loss: 1.7613\n",
            "Epoch 334/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6498 - val_loss: 1.4824\n",
            "Epoch 335/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6994 - val_loss: 1.7711\n",
            "Epoch 336/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6860 - val_loss: 2.0040\n",
            "Epoch 337/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6883 - val_loss: 2.4206\n",
            "Epoch 338/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6756 - val_loss: 1.9588\n",
            "Epoch 339/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7381 - val_loss: 1.8849\n",
            "Epoch 340/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6942 - val_loss: 2.0182\n",
            "Epoch 341/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6901 - val_loss: 2.3169\n",
            "Epoch 342/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6912 - val_loss: 2.0344\n",
            "Epoch 343/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7803 - val_loss: 1.4719\n",
            "Epoch 344/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7459 - val_loss: 1.5123\n",
            "Epoch 345/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7816 - val_loss: 1.7535\n",
            "Epoch 346/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7249 - val_loss: 1.8765\n",
            "Epoch 347/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6528 - val_loss: 1.6100\n",
            "Epoch 348/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6976 - val_loss: 1.7130\n",
            "Epoch 349/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6071 - val_loss: 1.9047\n",
            "Epoch 350/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6594 - val_loss: 1.6786\n",
            "Epoch 351/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6887 - val_loss: 1.6470\n",
            "Epoch 352/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7826 - val_loss: 1.9100\n",
            "Epoch 353/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6719 - val_loss: 1.7073\n",
            "Epoch 354/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7140 - val_loss: 1.7831\n",
            "Epoch 355/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6704 - val_loss: 1.6615\n",
            "Epoch 356/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6592 - val_loss: 1.9094\n",
            "Epoch 357/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6989 - val_loss: 1.8869\n",
            "Epoch 358/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7188 - val_loss: 1.9835\n",
            "Epoch 359/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6928 - val_loss: 2.1397\n",
            "Epoch 360/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7068 - val_loss: 2.1772\n",
            "Epoch 361/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6205 - val_loss: 2.0066\n",
            "Epoch 362/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6593 - val_loss: 1.7772\n",
            "Epoch 363/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6220 - val_loss: 1.6948\n",
            "Epoch 364/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7033 - val_loss: 1.6614\n",
            "Epoch 365/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7019 - val_loss: 1.8102\n",
            "Epoch 366/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6928 - val_loss: 1.5906\n",
            "Epoch 367/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6871 - val_loss: 1.7060\n",
            "Epoch 368/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6820 - val_loss: 1.7386\n",
            "Epoch 369/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8990 - val_loss: 1.5813\n",
            "Epoch 370/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6100 - val_loss: 1.5348\n",
            "Epoch 371/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7629 - val_loss: 2.2781\n",
            "Epoch 372/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6729 - val_loss: 1.8504\n",
            "Epoch 373/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6736 - val_loss: 1.6650\n",
            "Epoch 374/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7239 - val_loss: 1.7419\n",
            "Epoch 375/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7266 - val_loss: 2.1717\n",
            "Epoch 376/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6615 - val_loss: 1.5463\n",
            "Epoch 377/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6594 - val_loss: 2.0536\n",
            "Epoch 378/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7038 - val_loss: 1.7621\n",
            "Epoch 379/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6140 - val_loss: 1.7163\n",
            "Epoch 380/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7057 - val_loss: 1.8242\n",
            "Epoch 381/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7061 - val_loss: 1.7236\n",
            "Epoch 382/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7253 - val_loss: 2.0190\n",
            "Epoch 383/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7097 - val_loss: 1.8241\n",
            "Epoch 384/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6595 - val_loss: 1.5781\n",
            "Epoch 385/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6506 - val_loss: 1.9704\n",
            "Epoch 386/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7208 - val_loss: 1.5889\n",
            "Epoch 387/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6737 - val_loss: 1.7327\n",
            "Epoch 388/5000\n",
            "23/23 [==============================] - 1s 27ms/step - loss: 0.6844 - val_loss: 1.4120\n",
            "Epoch 389/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6515 - val_loss: 1.7215\n",
            "Epoch 390/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6855 - val_loss: 1.8032\n",
            "Epoch 391/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6769 - val_loss: 1.9821\n",
            "Epoch 392/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6616 - val_loss: 1.7914\n",
            "Epoch 393/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7337 - val_loss: 1.4714\n",
            "Epoch 394/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7603 - val_loss: 1.5930\n",
            "Epoch 395/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6378 - val_loss: 1.8412\n",
            "Epoch 396/5000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6778 - val_loss: 1.8735\n",
            "Epoch 397/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6271 - val_loss: 1.7229\n",
            "Epoch 398/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6413 - val_loss: 1.9523\n",
            "Epoch 399/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6587 - val_loss: 1.6487\n",
            "Epoch 400/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6630 - val_loss: 1.8213\n",
            "Epoch 401/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6270 - val_loss: 1.8993\n",
            "Epoch 402/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6958 - val_loss: 1.8238\n",
            "Epoch 403/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6716 - val_loss: 1.5499\n",
            "Epoch 404/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6681 - val_loss: 2.2791\n",
            "Epoch 405/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6192 - val_loss: 1.9398\n",
            "Epoch 406/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7051 - val_loss: 2.0396\n",
            "Epoch 407/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7679 - val_loss: 1.7700\n",
            "Epoch 408/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6474 - val_loss: 1.6369\n",
            "Epoch 409/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6705 - val_loss: 1.5814\n",
            "Epoch 410/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6263 - val_loss: 1.9385\n",
            "Epoch 411/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7080 - val_loss: 1.8050\n",
            "Epoch 412/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6223 - val_loss: 1.6979\n",
            "Epoch 413/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6660 - val_loss: 1.5037\n",
            "Epoch 414/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7612 - val_loss: 1.8815\n",
            "Epoch 415/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7075 - val_loss: 1.6317\n",
            "Epoch 416/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6733 - val_loss: 1.9393\n",
            "Epoch 417/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6648 - val_loss: 1.8879\n",
            "Epoch 418/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6607 - val_loss: 2.1196\n",
            "Epoch 419/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5931 - val_loss: 1.9103\n",
            "Epoch 420/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6117 - val_loss: 1.9072\n",
            "Epoch 421/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7073 - val_loss: 1.9894\n",
            "Epoch 422/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7254 - val_loss: 1.9728\n",
            "Epoch 423/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6167 - val_loss: 1.8575\n",
            "Epoch 424/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7695 - val_loss: 2.0000\n",
            "Epoch 425/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7041 - val_loss: 2.2478\n",
            "Epoch 426/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7241 - val_loss: 1.6357\n",
            "Epoch 427/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7063 - val_loss: 2.1398\n",
            "Epoch 428/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6797 - val_loss: 1.8978\n",
            "Epoch 429/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6290 - val_loss: 1.8681\n",
            "Epoch 430/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6384 - val_loss: 2.3680\n",
            "Epoch 431/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7281 - val_loss: 1.6098\n",
            "Epoch 432/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7001 - val_loss: 1.6363\n",
            "Epoch 433/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6635 - val_loss: 2.5714\n",
            "Epoch 434/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6392 - val_loss: 2.2750\n",
            "Epoch 435/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6974 - val_loss: 1.6555\n",
            "Epoch 436/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6496 - val_loss: 1.8181\n",
            "Epoch 437/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6136 - val_loss: 2.0945\n",
            "Epoch 438/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7254 - val_loss: 2.1237\n",
            "Epoch 439/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6796 - val_loss: 1.8893\n",
            "Epoch 440/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6694 - val_loss: 1.9731\n",
            "Epoch 441/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6746 - val_loss: 1.7818\n",
            "Epoch 442/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6961 - val_loss: 1.7451\n",
            "Epoch 443/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6553 - val_loss: 2.2208\n",
            "Epoch 444/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6508 - val_loss: 1.7142\n",
            "Epoch 445/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6678 - val_loss: 1.8588\n",
            "Epoch 446/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6866 - val_loss: 2.0329\n",
            "Epoch 447/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6230 - val_loss: 1.6722\n",
            "Epoch 448/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7041 - val_loss: 2.1804\n",
            "Epoch 449/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6538 - val_loss: 1.8616\n",
            "Epoch 450/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7129 - val_loss: 2.2559\n",
            "Epoch 451/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7197 - val_loss: 2.2246\n",
            "Epoch 452/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6814 - val_loss: 1.8752\n",
            "Epoch 453/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7392 - val_loss: 1.8410\n",
            "Epoch 454/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6891 - val_loss: 1.8741\n",
            "Epoch 455/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6511 - val_loss: 1.7783\n",
            "Epoch 456/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6730 - val_loss: 1.6998\n",
            "Epoch 457/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7121 - val_loss: 1.7677\n",
            "Epoch 458/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6284 - val_loss: 1.5802\n",
            "Epoch 459/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7109 - val_loss: 1.9584\n",
            "Epoch 460/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6983 - val_loss: 2.0231\n",
            "Epoch 461/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7356 - val_loss: 1.8460\n",
            "Epoch 462/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6734 - val_loss: 2.2246\n",
            "Epoch 463/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7301 - val_loss: 2.0673\n",
            "Epoch 464/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6418 - val_loss: 1.7924\n",
            "Epoch 465/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7014 - val_loss: 1.9055\n",
            "Epoch 466/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7153 - val_loss: 1.8637\n",
            "Epoch 467/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6934 - val_loss: 1.9052\n",
            "Epoch 468/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7108 - val_loss: 2.3447\n",
            "Epoch 469/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6692 - val_loss: 1.7823\n",
            "Epoch 470/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6732 - val_loss: 1.8118\n",
            "Epoch 471/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6941 - val_loss: 2.3452\n",
            "Epoch 472/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7565 - val_loss: 1.9177\n",
            "Epoch 473/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6434 - val_loss: 1.7309\n",
            "Epoch 474/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6523 - val_loss: 1.8740\n",
            "Epoch 475/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6575 - val_loss: 2.6690\n",
            "Epoch 476/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6679 - val_loss: 1.7110\n",
            "Epoch 477/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6858 - val_loss: 2.0134\n",
            "Epoch 478/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6886 - val_loss: 1.7120\n",
            "Epoch 479/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6515 - val_loss: 2.2935\n",
            "Epoch 480/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7008 - val_loss: 2.1997\n",
            "Epoch 481/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6552 - val_loss: 2.1054\n",
            "Epoch 482/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6180 - val_loss: 1.9138\n",
            "Epoch 483/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6667 - val_loss: 2.2463\n",
            "Epoch 484/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6520 - val_loss: 2.0883\n",
            "Epoch 485/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6484 - val_loss: 2.1921\n",
            "Epoch 486/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6875 - val_loss: 2.1767\n",
            "Epoch 487/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6896 - val_loss: 1.6635\n",
            "Epoch 488/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6555 - val_loss: 2.2479\n",
            "Epoch 489/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6732 - val_loss: 1.7010\n",
            "Epoch 490/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6149 - val_loss: 1.6886\n",
            "Epoch 491/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7171 - val_loss: 2.0447\n",
            "Epoch 492/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6454 - val_loss: 1.8107\n",
            "Epoch 493/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6566 - val_loss: 2.1752\n",
            "Epoch 494/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6580 - val_loss: 2.0828\n",
            "Epoch 495/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6798 - val_loss: 1.9516\n",
            "Epoch 496/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6535 - val_loss: 1.8533\n",
            "Epoch 497/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6378 - val_loss: 2.0964\n",
            "Epoch 498/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7155 - val_loss: 1.8323\n",
            "Epoch 499/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7239 - val_loss: 1.9931\n",
            "Epoch 500/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7225 - val_loss: 1.6978\n",
            "Epoch 501/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6475 - val_loss: 1.8268\n",
            "Epoch 502/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6499 - val_loss: 1.9061\n",
            "Epoch 503/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7310 - val_loss: 2.3276\n",
            "Epoch 504/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7127 - val_loss: 2.0934\n",
            "Epoch 505/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6538 - val_loss: 1.6600\n",
            "Epoch 506/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6520 - val_loss: 1.7420\n",
            "Epoch 507/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6370 - val_loss: 2.1146\n",
            "Epoch 508/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6380 - val_loss: 2.2949\n",
            "Epoch 509/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7179 - val_loss: 2.1545\n",
            "Epoch 510/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6540 - val_loss: 2.0347\n",
            "Epoch 511/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6745 - val_loss: 2.0390\n",
            "Epoch 512/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7300 - val_loss: 2.2165\n",
            "Epoch 513/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6850 - val_loss: 1.5665\n",
            "Epoch 514/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7012 - val_loss: 2.0707\n",
            "Epoch 515/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8522 - val_loss: 1.7605\n",
            "Epoch 516/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6363 - val_loss: 2.1295\n",
            "Epoch 517/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7174 - val_loss: 2.0327\n",
            "Epoch 518/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6648 - val_loss: 1.9114\n",
            "Epoch 519/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6869 - val_loss: 2.4236\n",
            "Epoch 520/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6477 - val_loss: 2.0096\n",
            "Epoch 521/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6702 - val_loss: 2.0762\n",
            "Epoch 522/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6502 - val_loss: 1.7947\n",
            "Epoch 523/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6595 - val_loss: 1.9902\n",
            "Epoch 524/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6662 - val_loss: 1.9330\n",
            "Epoch 525/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6990 - val_loss: 1.6213\n",
            "Epoch 526/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6775 - val_loss: 2.4455\n",
            "Epoch 527/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6241 - val_loss: 2.0915\n",
            "Epoch 528/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7718 - val_loss: 2.2478\n",
            "Epoch 529/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7659 - val_loss: 1.6957\n",
            "Epoch 530/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6632 - val_loss: 2.0489\n",
            "Epoch 531/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6441 - val_loss: 1.8500\n",
            "Epoch 532/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6958 - val_loss: 1.8299\n",
            "Epoch 533/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7330 - val_loss: 1.9012\n",
            "Epoch 534/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7212 - val_loss: 1.7885\n",
            "Epoch 535/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5724 - val_loss: 1.7649\n",
            "Epoch 536/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6728 - val_loss: 2.0014\n",
            "Epoch 537/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6606 - val_loss: 2.2378\n",
            "Epoch 538/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6637 - val_loss: 1.8299\n",
            "Epoch 539/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7054 - val_loss: 1.8588\n",
            "Epoch 540/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7017 - val_loss: 2.2145\n",
            "Epoch 541/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6936 - val_loss: 1.8747\n",
            "Epoch 542/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5902 - val_loss: 2.5170\n",
            "Epoch 543/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6774 - val_loss: 1.8023\n",
            "Epoch 544/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6603 - val_loss: 2.0709\n",
            "Epoch 545/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6171 - val_loss: 1.8994\n",
            "Epoch 546/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7059 - val_loss: 1.9775\n",
            "Epoch 547/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6042 - val_loss: 1.9986\n",
            "Epoch 548/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6554 - val_loss: 2.0979\n",
            "Epoch 549/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6918 - val_loss: 1.8755\n",
            "Epoch 550/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5997 - val_loss: 1.7491\n",
            "Epoch 551/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6568 - val_loss: 1.7933\n",
            "Epoch 552/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6133 - val_loss: 2.3920\n",
            "Epoch 553/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6396 - val_loss: 1.9286\n",
            "Epoch 554/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6577 - val_loss: 1.8699\n",
            "Epoch 555/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.5945 - val_loss: 2.4426\n",
            "Epoch 556/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6878 - val_loss: 1.8459\n",
            "Epoch 557/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6817 - val_loss: 1.9733\n",
            "Epoch 558/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7151 - val_loss: 1.6908\n",
            "Epoch 559/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6698 - val_loss: 1.7600\n",
            "Epoch 560/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6391 - val_loss: 1.8194\n",
            "Epoch 561/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6809 - val_loss: 2.0446\n",
            "Epoch 562/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6026 - val_loss: 3.0460\n",
            "Epoch 563/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6887 - val_loss: 2.0143\n",
            "Epoch 564/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6413 - val_loss: 2.0010\n",
            "Epoch 565/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6174 - val_loss: 1.8574\n",
            "Epoch 566/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6745 - val_loss: 1.8729\n",
            "Epoch 567/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6553 - val_loss: 2.1434\n",
            "Epoch 568/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7108 - val_loss: 2.2575\n",
            "Epoch 569/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7273 - val_loss: 1.9539\n",
            "Epoch 570/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6622 - val_loss: 2.1549\n",
            "Epoch 571/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6437 - val_loss: 2.2406\n",
            "Epoch 572/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6907 - val_loss: 1.7092\n",
            "Epoch 573/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6296 - val_loss: 1.9033\n",
            "Epoch 574/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6959 - val_loss: 2.3005\n",
            "Epoch 575/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6953 - val_loss: 1.8586\n",
            "Epoch 576/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7287 - val_loss: 1.8859\n",
            "Epoch 577/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6977 - val_loss: 2.1445\n",
            "Epoch 578/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6987 - val_loss: 2.0994\n",
            "Epoch 579/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6387 - val_loss: 1.7477\n",
            "Epoch 580/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6765 - val_loss: 2.4429\n",
            "Epoch 581/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6602 - val_loss: 2.2100\n",
            "Epoch 582/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6189 - val_loss: 2.1197\n",
            "Epoch 583/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6041 - val_loss: 1.9785\n",
            "Epoch 584/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6494 - val_loss: 1.8171\n",
            "Epoch 585/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6956 - val_loss: 1.8999\n",
            "Epoch 586/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6794 - val_loss: 2.1879\n",
            "Epoch 587/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6796 - val_loss: 1.9661\n",
            "Epoch 588/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7073 - val_loss: 2.2386\n",
            "Epoch 589/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6320 - val_loss: 2.1024\n",
            "Epoch 590/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6875 - val_loss: 2.3792\n",
            "Epoch 591/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6431 - val_loss: 3.5926\n",
            "Epoch 592/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7315 - val_loss: 2.1030\n",
            "Epoch 593/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6701 - val_loss: 1.8439\n",
            "Epoch 594/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6104 - val_loss: 2.3964\n",
            "Epoch 595/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6329 - val_loss: 2.1364\n",
            "Epoch 596/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7009 - val_loss: 1.8983\n",
            "Epoch 597/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6741 - val_loss: 1.8816\n",
            "Epoch 598/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5959 - val_loss: 2.6618\n",
            "Epoch 599/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6766 - val_loss: 2.3094\n",
            "Epoch 600/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6155 - val_loss: 1.9412\n",
            "Epoch 601/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6024 - val_loss: 1.7812\n",
            "Epoch 602/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6963 - val_loss: 1.9788\n",
            "Epoch 603/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6486 - val_loss: 2.2573\n",
            "Epoch 604/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6150 - val_loss: 2.2909\n",
            "Epoch 605/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6868 - val_loss: 2.0689\n",
            "Epoch 606/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6241 - val_loss: 2.0728\n",
            "Epoch 607/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7781 - val_loss: 1.9403\n",
            "Epoch 608/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6688 - val_loss: 2.1840\n",
            "Epoch 609/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7397 - val_loss: 2.2192\n",
            "Epoch 610/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6661 - val_loss: 1.9267\n",
            "Epoch 611/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6866 - val_loss: 1.8845\n",
            "Epoch 612/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6941 - val_loss: 1.7448\n",
            "Epoch 613/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5899 - val_loss: 2.5624\n",
            "Epoch 614/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6840 - val_loss: 1.8476\n",
            "Epoch 615/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7066 - val_loss: 1.9979\n",
            "Epoch 616/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7048 - val_loss: 2.1695\n",
            "Epoch 617/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6688 - val_loss: 2.0944\n",
            "Epoch 618/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6981 - val_loss: 2.1828\n",
            "Epoch 619/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8173 - val_loss: 1.8352\n",
            "Epoch 620/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6506 - val_loss: 3.0146\n",
            "Epoch 621/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6258 - val_loss: 1.6733\n",
            "Epoch 622/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6605 - val_loss: 2.4346\n",
            "Epoch 623/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7780 - val_loss: 2.4998\n",
            "Epoch 624/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6404 - val_loss: 1.8143\n",
            "Epoch 625/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6668 - val_loss: 1.9821\n",
            "Epoch 626/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6268 - val_loss: 2.1873\n",
            "Epoch 627/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6411 - val_loss: 1.6177\n",
            "Epoch 628/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6385 - val_loss: 2.1171\n",
            "Epoch 629/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6872 - val_loss: 2.0523\n",
            "Epoch 630/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6840 - val_loss: 2.3624\n",
            "Epoch 631/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6413 - val_loss: 2.1762\n",
            "Epoch 632/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6531 - val_loss: 1.9865\n",
            "Epoch 633/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7073 - val_loss: 2.2801\n",
            "Epoch 634/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6719 - val_loss: 2.2070\n",
            "Epoch 635/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6419 - val_loss: 2.2266\n",
            "Epoch 636/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6644 - val_loss: 1.9620\n",
            "Epoch 637/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6930 - val_loss: 2.1838\n",
            "Epoch 638/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6504 - val_loss: 1.9360\n",
            "Epoch 639/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6355 - val_loss: 2.0551\n",
            "Epoch 640/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6610 - val_loss: 2.2114\n",
            "Epoch 641/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6611 - val_loss: 2.2161\n",
            "Epoch 642/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7396 - val_loss: 2.0616\n",
            "Epoch 643/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6493 - val_loss: 2.2240\n",
            "Epoch 644/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6400 - val_loss: 2.1579\n",
            "Epoch 645/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5911 - val_loss: 2.5868\n",
            "Epoch 646/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7155 - val_loss: 2.1721\n",
            "Epoch 647/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6459 - val_loss: 1.8933\n",
            "Epoch 648/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6024 - val_loss: 2.8796\n",
            "Epoch 649/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6974 - val_loss: 2.4817\n",
            "Epoch 650/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6479 - val_loss: 2.1880\n",
            "Epoch 651/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6614 - val_loss: 1.8942\n",
            "Epoch 652/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6603 - val_loss: 1.7370\n",
            "Epoch 653/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6822 - val_loss: 2.6060\n",
            "Epoch 654/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6631 - val_loss: 2.2541\n",
            "Epoch 655/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6508 - val_loss: 1.9140\n",
            "Epoch 656/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6376 - val_loss: 1.7288\n",
            "Epoch 657/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6506 - val_loss: 2.5369\n",
            "Epoch 658/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6729 - val_loss: 2.2091\n",
            "Epoch 659/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6821 - val_loss: 2.2029\n",
            "Epoch 660/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5979 - val_loss: 2.2242\n",
            "Epoch 661/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6554 - val_loss: 2.1647\n",
            "Epoch 662/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.7012 - val_loss: 2.3105\n",
            "Epoch 663/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6534 - val_loss: 1.9629\n",
            "Epoch 664/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6460 - val_loss: 1.9730\n",
            "Epoch 665/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6260 - val_loss: 2.2404\n",
            "Epoch 666/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6164 - val_loss: 2.4609\n",
            "Epoch 667/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6389 - val_loss: 2.1747\n",
            "Epoch 668/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6241 - val_loss: 2.7755\n",
            "Epoch 669/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6710 - val_loss: 2.5984\n",
            "Epoch 670/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6287 - val_loss: 2.2726\n",
            "Epoch 671/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6136 - val_loss: 1.8406\n",
            "Epoch 672/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.5904 - val_loss: 2.0946\n",
            "Epoch 673/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6548 - val_loss: 2.2472\n",
            "Epoch 674/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6926 - val_loss: 2.2979\n",
            "Epoch 675/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6445 - val_loss: 2.2004\n",
            "Epoch 676/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6713 - val_loss: 1.8503\n",
            "Epoch 677/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.5739 - val_loss: 2.5795\n",
            "Epoch 678/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6174 - val_loss: 2.4725\n",
            "Epoch 679/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7025 - val_loss: 2.6442\n",
            "Epoch 680/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6562 - val_loss: 2.7341\n",
            "Epoch 681/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6336 - val_loss: 2.6838\n",
            "Epoch 682/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6777 - val_loss: 2.4173\n",
            "Epoch 683/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6742 - val_loss: 2.0623\n",
            "Epoch 684/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7167 - val_loss: 2.1344\n",
            "Epoch 685/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6204 - val_loss: 2.4288\n",
            "Epoch 686/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6781 - val_loss: 1.9746\n",
            "Epoch 687/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6309 - val_loss: 2.3870\n",
            "Epoch 688/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7195 - val_loss: 2.0195\n",
            "Epoch 689/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.5874 - val_loss: 2.6629\n",
            "Epoch 690/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6541 - val_loss: 2.5561\n",
            "Epoch 691/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6323 - val_loss: 2.6387\n",
            "Epoch 692/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6896 - val_loss: 2.1287\n",
            "Epoch 693/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5928 - val_loss: 2.2545\n",
            "Epoch 694/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6614 - val_loss: 2.6548\n",
            "Epoch 695/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6728 - val_loss: 2.3858\n",
            "Epoch 696/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6537 - val_loss: 2.1571\n",
            "Epoch 697/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6571 - val_loss: 1.8048\n",
            "Epoch 698/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7495 - val_loss: 2.0414\n",
            "Epoch 699/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6550 - val_loss: 2.2412\n",
            "Epoch 700/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7157 - val_loss: 2.1724\n",
            "Epoch 701/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7170 - val_loss: 2.7834\n",
            "Epoch 702/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6738 - val_loss: 2.3856\n",
            "Epoch 703/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6442 - val_loss: 2.7732\n",
            "Epoch 704/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7051 - val_loss: 2.3026\n",
            "Epoch 705/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6702 - val_loss: 2.3162\n",
            "Epoch 706/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6162 - val_loss: 2.1784\n",
            "Epoch 707/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6504 - val_loss: 2.0199\n",
            "Epoch 708/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6591 - val_loss: 2.4335\n",
            "Epoch 709/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6557 - val_loss: 2.1464\n",
            "Epoch 710/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6343 - val_loss: 2.1243\n",
            "Epoch 711/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6523 - val_loss: 2.1307\n",
            "Epoch 712/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7185 - val_loss: 2.3687\n",
            "Epoch 713/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7396 - val_loss: 1.9865\n",
            "Epoch 714/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6153 - val_loss: 1.8858\n",
            "Epoch 715/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6648 - val_loss: 2.4510\n",
            "Epoch 716/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6835 - val_loss: 2.8178\n",
            "Epoch 717/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6909 - val_loss: 2.3262\n",
            "Epoch 718/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6088 - val_loss: 2.4721\n",
            "Epoch 719/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6204 - val_loss: 2.9615\n",
            "Epoch 720/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6512 - val_loss: 2.2559\n",
            "Epoch 721/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6573 - val_loss: 2.0916\n",
            "Epoch 722/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7292 - val_loss: 2.3014\n",
            "Epoch 723/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6352 - val_loss: 2.4119\n",
            "Epoch 724/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7005 - val_loss: 2.6766\n",
            "Epoch 725/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6781 - val_loss: 2.6042\n",
            "Epoch 726/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6886 - val_loss: 2.8607\n",
            "Epoch 727/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6195 - val_loss: 2.3391\n",
            "Epoch 728/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6527 - val_loss: 2.6240\n",
            "Epoch 729/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6750 - val_loss: 2.1641\n",
            "Epoch 730/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6866 - val_loss: 2.2147\n",
            "Epoch 731/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6583 - val_loss: 2.3164\n",
            "Epoch 732/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7304 - val_loss: 2.4870\n",
            "Epoch 733/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6922 - val_loss: 2.3669\n",
            "Epoch 734/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6477 - val_loss: 2.8512\n",
            "Epoch 735/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6147 - val_loss: 2.2720\n",
            "Epoch 736/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7260 - val_loss: 2.4511\n",
            "Epoch 737/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6508 - val_loss: 2.1854\n",
            "Epoch 738/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6807 - val_loss: 2.2292\n",
            "Epoch 739/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6687 - val_loss: 2.8069\n",
            "Epoch 740/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6134 - val_loss: 2.5461\n",
            "Epoch 741/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6587 - val_loss: 3.1470\n",
            "Epoch 742/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6247 - val_loss: 2.5126\n",
            "Epoch 743/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6677 - val_loss: 2.4818\n",
            "Epoch 744/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6305 - val_loss: 3.3311\n",
            "Epoch 745/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6129 - val_loss: 2.1967\n",
            "Epoch 746/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7369 - val_loss: 2.5064\n",
            "Epoch 747/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5988 - val_loss: 2.4245\n",
            "Epoch 748/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6676 - val_loss: 2.9780\n",
            "Epoch 749/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6692 - val_loss: 2.2730\n",
            "Epoch 750/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6252 - val_loss: 2.2637\n",
            "Epoch 751/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6479 - val_loss: 2.0162\n",
            "Epoch 752/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6399 - val_loss: 2.9955\n",
            "Epoch 753/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6952 - val_loss: 2.8372\n",
            "Epoch 754/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7040 - val_loss: 2.6641\n",
            "Epoch 755/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6637 - val_loss: 2.6571\n",
            "Epoch 756/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6833 - val_loss: 3.1629\n",
            "Epoch 757/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6469 - val_loss: 2.3974\n",
            "Epoch 758/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6742 - val_loss: 2.3988\n",
            "Epoch 759/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6640 - val_loss: 2.6389\n",
            "Epoch 760/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6447 - val_loss: 2.5663\n",
            "Epoch 761/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6775 - val_loss: 2.3658\n",
            "Epoch 762/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5967 - val_loss: 2.2608\n",
            "Epoch 763/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6856 - val_loss: 2.3646\n",
            "Epoch 764/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6643 - val_loss: 2.3519\n",
            "Epoch 765/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7082 - val_loss: 2.3299\n",
            "Epoch 766/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6313 - val_loss: 2.3157\n",
            "Epoch 767/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6603 - val_loss: 2.3244\n",
            "Epoch 768/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6548 - val_loss: 2.5605\n",
            "Epoch 769/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6754 - val_loss: 2.3145\n",
            "Epoch 770/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6810 - val_loss: 3.3114\n",
            "Epoch 771/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7398 - val_loss: 3.3103\n",
            "Epoch 772/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6708 - val_loss: 2.5468\n",
            "Epoch 773/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6442 - val_loss: 2.3656\n",
            "Epoch 774/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6057 - val_loss: 3.1089\n",
            "Epoch 775/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6695 - val_loss: 2.3604\n",
            "Epoch 776/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6751 - val_loss: 2.4421\n",
            "Epoch 777/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6422 - val_loss: 2.5288\n",
            "Epoch 778/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6751 - val_loss: 2.0782\n",
            "Epoch 779/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6647 - val_loss: 2.1732\n",
            "Epoch 780/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6471 - val_loss: 2.8535\n",
            "Epoch 781/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6596 - val_loss: 3.0510\n",
            "Epoch 782/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6621 - val_loss: 3.2841\n",
            "Epoch 783/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6609 - val_loss: 2.1561\n",
            "Epoch 784/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6426 - val_loss: 3.3617\n",
            "Epoch 785/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6693 - val_loss: 3.1488\n",
            "Epoch 786/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6195 - val_loss: 3.4328\n",
            "Epoch 787/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6870 - val_loss: 2.5018\n",
            "Epoch 788/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6745 - val_loss: 2.7840\n",
            "Epoch 789/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7343 - val_loss: 2.9384\n",
            "Epoch 790/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6402 - val_loss: 2.7683\n",
            "Epoch 791/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7324 - val_loss: 2.3753\n",
            "Epoch 792/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6915 - val_loss: 2.3723\n",
            "Epoch 793/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6485 - val_loss: 2.9624\n",
            "Epoch 794/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6438 - val_loss: 2.5970\n",
            "Epoch 795/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6590 - val_loss: 2.3347\n",
            "Epoch 796/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7129 - val_loss: 2.3209\n",
            "Epoch 797/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6263 - val_loss: 2.5231\n",
            "Epoch 798/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6677 - val_loss: 2.3969\n",
            "Epoch 799/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6521 - val_loss: 2.5581\n",
            "Epoch 800/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6595 - val_loss: 2.8883\n",
            "Epoch 801/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6587 - val_loss: 2.6034\n",
            "Epoch 802/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6591 - val_loss: 2.4720\n",
            "Epoch 803/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7001 - val_loss: 2.4170\n",
            "Epoch 804/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6278 - val_loss: 2.6354\n",
            "Epoch 805/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6638 - val_loss: 2.9461\n",
            "Epoch 806/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6520 - val_loss: 2.6096\n",
            "Epoch 807/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7005 - val_loss: 2.3346\n",
            "Epoch 808/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6490 - val_loss: 3.1230\n",
            "Epoch 809/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6773 - val_loss: 2.4367\n",
            "Epoch 810/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6311 - val_loss: 2.6461\n",
            "Epoch 811/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6802 - val_loss: 2.7939\n",
            "Epoch 812/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6554 - val_loss: 2.3748\n",
            "Epoch 813/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6199 - val_loss: 2.2786\n",
            "Epoch 814/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6396 - val_loss: 2.0566\n",
            "Epoch 815/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6753 - val_loss: 3.0213\n",
            "Epoch 816/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6220 - val_loss: 2.1440\n",
            "Epoch 817/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6673 - val_loss: 2.3508\n",
            "Epoch 818/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6042 - val_loss: 2.5828\n",
            "Epoch 819/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6811 - val_loss: 2.3069\n",
            "Epoch 820/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6725 - val_loss: 3.2422\n",
            "Epoch 821/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6782 - val_loss: 2.7607\n",
            "Epoch 822/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6658 - val_loss: 2.7392\n",
            "Epoch 823/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6166 - val_loss: 2.6470\n",
            "Epoch 824/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7119 - val_loss: 2.6790\n",
            "Epoch 825/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6037 - val_loss: 2.6209\n",
            "Epoch 826/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6324 - val_loss: 3.4960\n",
            "Epoch 827/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6397 - val_loss: 2.1906\n",
            "Epoch 828/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6162 - val_loss: 2.4936\n",
            "Epoch 829/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6580 - val_loss: 2.7107\n",
            "Epoch 830/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6329 - val_loss: 2.7466\n",
            "Epoch 831/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6854 - val_loss: 2.5110\n",
            "Epoch 832/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7297 - val_loss: 4.1463\n",
            "Epoch 833/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6609 - val_loss: 2.4034\n",
            "Epoch 834/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6592 - val_loss: 2.3978\n",
            "Epoch 835/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7035 - val_loss: 2.7275\n",
            "Epoch 836/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7084 - val_loss: 2.9296\n",
            "Epoch 837/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6737 - val_loss: 3.0038\n",
            "Epoch 838/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6413 - val_loss: 2.2461\n",
            "Epoch 839/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6259 - val_loss: 3.0982\n",
            "Epoch 840/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6208 - val_loss: 2.9324\n",
            "Epoch 841/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6660 - val_loss: 2.6781\n",
            "Epoch 842/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6626 - val_loss: 2.4655\n",
            "Epoch 843/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7132 - val_loss: 2.5560\n",
            "Epoch 844/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5652 - val_loss: 2.5308\n",
            "Epoch 845/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5948 - val_loss: 3.2219\n",
            "Epoch 846/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6811 - val_loss: 2.7924\n",
            "Epoch 847/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6666 - val_loss: 2.9550\n",
            "Epoch 848/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7297 - val_loss: 2.7880\n",
            "Epoch 849/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6379 - val_loss: 2.9773\n",
            "Epoch 850/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6213 - val_loss: 3.2761\n",
            "Epoch 851/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6230 - val_loss: 2.5071\n",
            "Epoch 852/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6784 - val_loss: 2.9821\n",
            "Epoch 853/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6532 - val_loss: 3.7758\n",
            "Epoch 854/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6122 - val_loss: 2.4098\n",
            "Epoch 855/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6503 - val_loss: 4.1889\n",
            "Epoch 856/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6280 - val_loss: 2.9452\n",
            "Epoch 857/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6559 - val_loss: 3.2068\n",
            "Epoch 858/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6763 - val_loss: 3.0616\n",
            "Epoch 859/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7031 - val_loss: 3.0177\n",
            "Epoch 860/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6578 - val_loss: 3.8842\n",
            "Epoch 861/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6053 - val_loss: 2.1181\n",
            "Epoch 862/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6792 - val_loss: 3.0957\n",
            "Epoch 863/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6228 - val_loss: 2.5942\n",
            "Epoch 864/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6782 - val_loss: 2.2576\n",
            "Epoch 865/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7131 - val_loss: 3.2782\n",
            "Epoch 866/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6982 - val_loss: 3.5215\n",
            "Epoch 867/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6371 - val_loss: 3.1928\n",
            "Epoch 868/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6336 - val_loss: 2.9465\n",
            "Epoch 869/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6934 - val_loss: 3.4017\n",
            "Epoch 870/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7002 - val_loss: 2.7419\n",
            "Epoch 871/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6419 - val_loss: 2.3926\n",
            "Epoch 872/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6381 - val_loss: 3.0279\n",
            "Epoch 873/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6596 - val_loss: 3.6565\n",
            "Epoch 874/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6726 - val_loss: 3.3391\n",
            "Epoch 875/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6617 - val_loss: 3.3527\n",
            "Epoch 876/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5974 - val_loss: 3.9197\n",
            "Epoch 877/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6311 - val_loss: 3.0087\n",
            "Epoch 878/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7355 - val_loss: 2.9154\n",
            "Epoch 879/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6091 - val_loss: 2.2760\n",
            "Epoch 880/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6354 - val_loss: 3.3914\n",
            "Epoch 881/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5943 - val_loss: 2.9684\n",
            "Epoch 882/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6722 - val_loss: 2.8828\n",
            "Epoch 883/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6406 - val_loss: 3.2726\n",
            "Epoch 884/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6566 - val_loss: 2.2563\n",
            "Epoch 885/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6134 - val_loss: 3.7896\n",
            "Epoch 886/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6327 - val_loss: 2.4821\n",
            "Epoch 887/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6627 - val_loss: 2.9685\n",
            "Epoch 888/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6347 - val_loss: 2.8748\n",
            "Epoch 889/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6123 - val_loss: 2.9363\n",
            "Epoch 890/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7049 - val_loss: 3.6536\n",
            "Epoch 891/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6316 - val_loss: 3.0627\n",
            "Epoch 892/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6405 - val_loss: 3.1923\n",
            "Epoch 893/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7107 - val_loss: 2.6722\n",
            "Epoch 894/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6014 - val_loss: 3.3271\n",
            "Epoch 895/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6359 - val_loss: 3.9363\n",
            "Epoch 896/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6489 - val_loss: 3.3433\n",
            "Epoch 897/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6543 - val_loss: 3.7190\n",
            "Epoch 898/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6027 - val_loss: 2.8562\n",
            "Epoch 899/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6384 - val_loss: 2.8658\n",
            "Epoch 900/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6526 - val_loss: 3.1077\n",
            "Epoch 901/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7460 - val_loss: 2.7551\n",
            "Epoch 902/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6470 - val_loss: 3.3974\n",
            "Epoch 903/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6565 - val_loss: 2.9746\n",
            "Epoch 904/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6305 - val_loss: 2.6342\n",
            "Epoch 905/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5981 - val_loss: 2.4325\n",
            "Epoch 906/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6252 - val_loss: 3.2607\n",
            "Epoch 907/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7332 - val_loss: 2.9974\n",
            "Epoch 908/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6841 - val_loss: 2.9787\n",
            "Epoch 909/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.5957 - val_loss: 3.6337\n",
            "Epoch 910/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6226 - val_loss: 3.4169\n",
            "Epoch 911/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6324 - val_loss: 2.8886\n",
            "Epoch 912/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7279 - val_loss: 3.0807\n",
            "Epoch 913/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.5964 - val_loss: 2.9865\n",
            "Epoch 914/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6698 - val_loss: 3.0670\n",
            "Epoch 915/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6204 - val_loss: 2.7208\n",
            "Epoch 916/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6398 - val_loss: 3.3451\n",
            "Epoch 917/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6414 - val_loss: 2.7071\n",
            "Epoch 918/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6169 - val_loss: 3.5628\n",
            "Epoch 919/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6218 - val_loss: 3.2508\n",
            "Epoch 920/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.5470 - val_loss: 3.5152\n",
            "Epoch 921/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6074 - val_loss: 2.7823\n",
            "Epoch 922/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6795 - val_loss: 2.7604\n",
            "Epoch 923/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6138 - val_loss: 3.3851\n",
            "Epoch 924/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6183 - val_loss: 3.4952\n",
            "Epoch 925/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6746 - val_loss: 3.1895\n",
            "Epoch 926/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6442 - val_loss: 3.4819\n",
            "Epoch 927/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7108 - val_loss: 3.0811\n",
            "Epoch 928/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.5888 - val_loss: 3.2527\n",
            "Epoch 929/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6386 - val_loss: 2.7425\n",
            "Epoch 930/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6382 - val_loss: 2.9635\n",
            "Epoch 931/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.5961 - val_loss: 3.5729\n",
            "Epoch 932/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6780 - val_loss: 3.5869\n",
            "Epoch 933/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6129 - val_loss: 3.4735\n",
            "Epoch 934/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6392 - val_loss: 3.3903\n",
            "Epoch 935/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6667 - val_loss: 2.9965\n",
            "Epoch 936/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6301 - val_loss: 2.9186\n",
            "Epoch 937/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6291 - val_loss: 4.8137\n",
            "Epoch 938/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7078 - val_loss: 3.7293\n",
            "Epoch 939/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7472 - val_loss: 3.4592\n",
            "Epoch 940/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7130 - val_loss: 3.4897\n",
            "Epoch 941/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6109 - val_loss: 2.6487\n",
            "Epoch 942/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6439 - val_loss: 2.8589\n",
            "Epoch 943/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5808 - val_loss: 2.8842\n",
            "Epoch 944/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6337 - val_loss: 3.1265\n",
            "Epoch 945/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6422 - val_loss: 3.4866\n",
            "Epoch 946/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6353 - val_loss: 3.4457\n",
            "Epoch 947/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6930 - val_loss: 2.5058\n",
            "Epoch 948/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6743 - val_loss: 3.0722\n",
            "Epoch 949/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6510 - val_loss: 3.4663\n",
            "Epoch 950/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6212 - val_loss: 4.2827\n",
            "Epoch 951/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6556 - val_loss: 2.6489\n",
            "Epoch 952/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5985 - val_loss: 3.1418\n",
            "Epoch 953/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6309 - val_loss: 3.5250\n",
            "Epoch 954/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6675 - val_loss: 3.1328\n",
            "Epoch 955/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6807 - val_loss: 2.9389\n",
            "Epoch 956/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7004 - val_loss: 3.1853\n",
            "Epoch 957/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5668 - val_loss: 5.4670\n",
            "Epoch 958/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6576 - val_loss: 2.9874\n",
            "Epoch 959/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6629 - val_loss: 2.6780\n",
            "Epoch 960/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6670 - val_loss: 2.9258\n",
            "Epoch 961/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6200 - val_loss: 3.9865\n",
            "Epoch 962/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6385 - val_loss: 2.9106\n",
            "Epoch 963/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6099 - val_loss: 3.4162\n",
            "Epoch 964/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8206 - val_loss: 3.5659\n",
            "Epoch 965/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6215 - val_loss: 2.9896\n",
            "Epoch 966/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6527 - val_loss: 3.5367\n",
            "Epoch 967/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6517 - val_loss: 3.7243\n",
            "Epoch 968/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6968 - val_loss: 3.5197\n",
            "Epoch 969/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6966 - val_loss: 3.4929\n",
            "Epoch 970/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6124 - val_loss: 2.9437\n",
            "Epoch 971/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6197 - val_loss: 2.7657\n",
            "Epoch 972/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6656 - val_loss: 4.0069\n",
            "Epoch 973/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6737 - val_loss: 3.0559\n",
            "Epoch 974/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6589 - val_loss: 3.0799\n",
            "Epoch 975/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6393 - val_loss: 3.9490\n",
            "Epoch 976/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6689 - val_loss: 3.1276\n",
            "Epoch 977/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6619 - val_loss: 3.3011\n",
            "Epoch 978/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6309 - val_loss: 3.8682\n",
            "Epoch 979/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6637 - val_loss: 3.1224\n",
            "Epoch 980/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6277 - val_loss: 2.9779\n",
            "Epoch 981/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6089 - val_loss: 2.8911\n",
            "Epoch 982/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6027 - val_loss: 3.0373\n",
            "Epoch 983/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6746 - val_loss: 3.4961\n",
            "Epoch 984/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6664 - val_loss: 3.3160\n",
            "Epoch 985/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6634 - val_loss: 2.9639\n",
            "Epoch 986/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6372 - val_loss: 2.8951\n",
            "Epoch 987/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6416 - val_loss: 3.0429\n",
            "Epoch 988/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6571 - val_loss: 2.6946\n",
            "Epoch 989/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6398 - val_loss: 3.4978\n",
            "Epoch 990/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6508 - val_loss: 3.0781\n",
            "Epoch 991/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5930 - val_loss: 3.3733\n",
            "Epoch 992/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6484 - val_loss: 3.3411\n",
            "Epoch 993/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6725 - val_loss: 3.0517\n",
            "Epoch 994/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6525 - val_loss: 3.2644\n",
            "Epoch 995/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6458 - val_loss: 3.4015\n",
            "Epoch 996/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6045 - val_loss: 2.6637\n",
            "Epoch 997/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6668 - val_loss: 2.8680\n",
            "Epoch 998/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6829 - val_loss: 4.1957\n",
            "Epoch 999/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6744 - val_loss: 3.0219\n",
            "Epoch 1000/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5755 - val_loss: 3.7639\n",
            "Epoch 1001/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5903 - val_loss: 3.6641\n",
            "Epoch 1002/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6554 - val_loss: 2.9556\n",
            "Epoch 1003/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6698 - val_loss: 3.8094\n",
            "Epoch 1004/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6240 - val_loss: 2.9465\n",
            "Epoch 1005/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.5728 - val_loss: 2.7787\n",
            "Epoch 1006/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6313 - val_loss: 2.7843\n",
            "Epoch 1007/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6015 - val_loss: 3.2664\n",
            "Epoch 1008/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6254 - val_loss: 3.7180\n",
            "Epoch 1009/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6648 - val_loss: 3.7090\n",
            "Epoch 1010/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6055 - val_loss: 5.0717\n",
            "Epoch 1011/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6076 - val_loss: 3.4621\n",
            "Epoch 1012/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6540 - val_loss: 2.6451\n",
            "Epoch 1013/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6122 - val_loss: 3.1125\n",
            "Epoch 1014/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6229 - val_loss: 3.5842\n",
            "Epoch 1015/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6546 - val_loss: 3.0177\n",
            "Epoch 1016/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6918 - val_loss: 3.7837\n",
            "Epoch 1017/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.5922 - val_loss: 2.9063\n",
            "Epoch 1018/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6467 - val_loss: 3.0936\n",
            "Epoch 1019/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6623 - val_loss: 2.9769\n",
            "Epoch 1020/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6210 - val_loss: 3.3290\n",
            "Epoch 1021/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6276 - val_loss: 3.4664\n",
            "Epoch 1022/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6200 - val_loss: 2.8364\n",
            "Epoch 1023/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6351 - val_loss: 2.7922\n",
            "Epoch 1024/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6669 - val_loss: 3.2855\n",
            "Epoch 1025/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6799 - val_loss: 3.0411\n",
            "Epoch 1026/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6149 - val_loss: 2.8914\n",
            "Epoch 1027/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6269 - val_loss: 4.7694\n",
            "Epoch 1028/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6484 - val_loss: 2.4684\n",
            "Epoch 1029/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6578 - val_loss: 3.4136\n",
            "Epoch 1030/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6247 - val_loss: 3.3838\n",
            "Epoch 1031/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6178 - val_loss: 3.3285\n",
            "Epoch 1032/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6008 - val_loss: 3.2137\n",
            "Epoch 1033/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6343 - val_loss: 2.7893\n",
            "Epoch 1034/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6394 - val_loss: 3.6392\n",
            "Epoch 1035/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6786 - val_loss: 3.0634\n",
            "Epoch 1036/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6535 - val_loss: 3.3507\n",
            "Epoch 1037/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6630 - val_loss: 6.4292\n",
            "Epoch 1038/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6362 - val_loss: 3.4426\n",
            "Epoch 1039/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6620 - val_loss: 3.3353\n",
            "Epoch 1040/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6730 - val_loss: 3.4808\n",
            "Epoch 1041/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6995 - val_loss: 3.8343\n",
            "Epoch 1042/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6337 - val_loss: 3.6019\n",
            "Epoch 1043/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.5985 - val_loss: 2.9603\n",
            "Epoch 1044/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6368 - val_loss: 3.5506\n",
            "Epoch 1045/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7122 - val_loss: 3.0903\n",
            "Epoch 1046/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6032 - val_loss: 3.6505\n",
            "Epoch 1047/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6075 - val_loss: 3.4948\n",
            "Epoch 1048/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6470 - val_loss: 3.3998\n",
            "Epoch 1049/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7145 - val_loss: 4.1139\n",
            "Epoch 1050/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6821 - val_loss: 3.4387\n",
            "Epoch 1051/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5957 - val_loss: 3.9160\n",
            "Epoch 1052/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6178 - val_loss: 3.6007\n",
            "Epoch 1053/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6279 - val_loss: 3.6792\n",
            "Epoch 1054/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6514 - val_loss: 3.4526\n",
            "Epoch 1055/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6562 - val_loss: 2.7783\n",
            "Epoch 1056/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6601 - val_loss: 3.3709\n",
            "Epoch 1057/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6492 - val_loss: 3.4603\n",
            "Epoch 1058/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6440 - val_loss: 3.6321\n",
            "Epoch 1059/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5772 - val_loss: 4.0376\n",
            "Epoch 1060/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.8179 - val_loss: 3.7798\n",
            "Epoch 1061/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6388 - val_loss: 2.9720\n",
            "Epoch 1062/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6221 - val_loss: 3.9838\n",
            "Epoch 1063/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6475 - val_loss: 3.0532\n",
            "Epoch 1064/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6983 - val_loss: 4.4709\n",
            "Epoch 1065/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5893 - val_loss: 3.6373\n",
            "Epoch 1066/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6200 - val_loss: 4.1729\n",
            "Epoch 1067/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6694 - val_loss: 3.6909\n",
            "Epoch 1068/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6352 - val_loss: 4.1323\n",
            "Epoch 1069/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6112 - val_loss: 3.4683\n",
            "Epoch 1070/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5796 - val_loss: 3.4649\n",
            "Epoch 1071/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6082 - val_loss: 2.9991\n",
            "Epoch 1072/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6173 - val_loss: 5.0071\n",
            "Epoch 1073/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6339 - val_loss: 3.7977\n",
            "Epoch 1074/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6457 - val_loss: 3.6810\n",
            "Epoch 1075/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6852 - val_loss: 3.4845\n",
            "Epoch 1076/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5979 - val_loss: 3.4425\n",
            "Epoch 1077/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6137 - val_loss: 3.6534\n",
            "Epoch 1078/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7617 - val_loss: 2.8589\n",
            "Epoch 1079/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6040 - val_loss: 3.3647\n",
            "Epoch 1080/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6421 - val_loss: 3.6671\n",
            "Epoch 1081/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6577 - val_loss: 3.0196\n",
            "Epoch 1082/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6419 - val_loss: 3.9385\n",
            "Epoch 1083/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7193 - val_loss: 4.3352\n",
            "Epoch 1084/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6411 - val_loss: 3.3720\n",
            "Epoch 1085/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6741 - val_loss: 3.5761\n",
            "Epoch 1086/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5680 - val_loss: 5.3872\n",
            "Epoch 1087/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6358 - val_loss: 4.0931\n",
            "Epoch 1088/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6341 - val_loss: 3.5050\n",
            "Epoch 1089/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5949 - val_loss: 3.8223\n",
            "Epoch 1090/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6717 - val_loss: 3.2306\n",
            "Epoch 1091/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6653 - val_loss: 3.8954\n",
            "Epoch 1092/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6209 - val_loss: 3.8044\n",
            "Epoch 1093/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7650 - val_loss: 4.0581\n",
            "Epoch 1094/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6473 - val_loss: 4.0673\n",
            "Epoch 1095/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6477 - val_loss: 3.0151\n",
            "Epoch 1096/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7015 - val_loss: 3.6388\n",
            "Epoch 1097/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5905 - val_loss: 3.8479\n",
            "Epoch 1098/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6124 - val_loss: 3.5702\n",
            "Epoch 1099/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6003 - val_loss: 2.8567\n",
            "Epoch 1100/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5973 - val_loss: 4.1846\n",
            "Epoch 1101/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6091 - val_loss: 3.7054\n",
            "Epoch 1102/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6440 - val_loss: 3.5132\n",
            "Epoch 1103/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6170 - val_loss: 3.7576\n",
            "Epoch 1104/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5980 - val_loss: 3.3147\n",
            "Epoch 1105/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6616 - val_loss: 4.1107\n",
            "Epoch 1106/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6997 - val_loss: 4.4205\n",
            "Epoch 1107/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6340 - val_loss: 4.5560\n",
            "Epoch 1108/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6091 - val_loss: 3.0641\n",
            "Epoch 1109/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6173 - val_loss: 3.3921\n",
            "Epoch 1110/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6186 - val_loss: 4.7947\n",
            "Epoch 1111/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6805 - val_loss: 4.2415\n",
            "Epoch 1112/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.5859 - val_loss: 3.3425\n",
            "Epoch 1113/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6159 - val_loss: 3.2458\n",
            "Epoch 1114/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6849 - val_loss: 3.4312\n",
            "Epoch 1115/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6641 - val_loss: 4.5458\n",
            "Epoch 1116/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6372 - val_loss: 3.1805\n",
            "Epoch 1117/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.5832 - val_loss: 3.4220\n",
            "Epoch 1118/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6416 - val_loss: 3.7392\n",
            "Epoch 1119/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6097 - val_loss: 3.6446\n",
            "Epoch 1120/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6413 - val_loss: 3.3831\n",
            "Epoch 1121/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6367 - val_loss: 5.0380\n",
            "Epoch 1122/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6194 - val_loss: 4.5429\n",
            "Epoch 1123/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6667 - val_loss: 4.0116\n",
            "Epoch 1124/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6829 - val_loss: 5.3735\n",
            "Epoch 1125/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6255 - val_loss: 3.3243\n",
            "Epoch 1126/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6021 - val_loss: 3.9876\n",
            "Epoch 1127/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6634 - val_loss: 3.3085\n",
            "Epoch 1128/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6358 - val_loss: 4.5330\n",
            "Epoch 1129/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6445 - val_loss: 3.2667\n",
            "Epoch 1130/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6171 - val_loss: 4.3088\n",
            "Epoch 1131/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6488 - val_loss: 4.0279\n",
            "Epoch 1132/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6285 - val_loss: 6.2684\n",
            "Epoch 1133/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6396 - val_loss: 4.2196\n",
            "Epoch 1134/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6086 - val_loss: 4.3897\n",
            "Epoch 1135/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6202 - val_loss: 3.9409\n",
            "Epoch 1136/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6364 - val_loss: 4.3519\n",
            "Epoch 1137/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6106 - val_loss: 4.2868\n",
            "Epoch 1138/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6236 - val_loss: 3.5180\n",
            "Epoch 1139/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6007 - val_loss: 3.1616\n",
            "Epoch 1140/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.5914 - val_loss: 4.1598\n",
            "Epoch 1141/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6709 - val_loss: 4.3776\n",
            "Epoch 1142/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6226 - val_loss: 5.0535\n",
            "Epoch 1143/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6740 - val_loss: 3.4211\n",
            "Epoch 1144/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6219 - val_loss: 5.2132\n",
            "Epoch 1145/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7191 - val_loss: 3.2811\n",
            "Epoch 1146/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7170 - val_loss: 3.3820\n",
            "Epoch 1147/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6372 - val_loss: 4.6442\n",
            "Epoch 1148/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6128 - val_loss: 4.2205\n",
            "Epoch 1149/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6456 - val_loss: 3.4269\n",
            "Epoch 1150/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.5980 - val_loss: 3.7462\n",
            "Epoch 1151/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6496 - val_loss: 3.7258\n",
            "Epoch 1152/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6164 - val_loss: 3.7904\n",
            "Epoch 1153/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6839 - val_loss: 4.3776\n",
            "Epoch 1154/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6009 - val_loss: 3.2997\n",
            "Epoch 1155/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6275 - val_loss: 6.3759\n",
            "Epoch 1156/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6165 - val_loss: 3.8768\n",
            "Epoch 1157/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6566 - val_loss: 4.1975\n",
            "Epoch 1158/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6385 - val_loss: 3.6793\n",
            "Epoch 1159/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6424 - val_loss: 3.0693\n",
            "Epoch 1160/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6104 - val_loss: 4.6522\n",
            "Epoch 1161/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6562 - val_loss: 3.5197\n",
            "Epoch 1162/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6227 - val_loss: 4.3995\n",
            "Epoch 1163/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.7260 - val_loss: 4.0422\n",
            "Epoch 1164/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7050 - val_loss: 4.3347\n",
            "Epoch 1165/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5900 - val_loss: 3.5711\n",
            "Epoch 1166/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6354 - val_loss: 3.7251\n",
            "Epoch 1167/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6543 - val_loss: 4.1232\n",
            "Epoch 1168/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6608 - val_loss: 5.3811\n",
            "Epoch 1169/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6597 - val_loss: 3.3535\n",
            "Epoch 1170/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6208 - val_loss: 3.8589\n",
            "Epoch 1171/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6374 - val_loss: 4.9628\n",
            "Epoch 1172/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6349 - val_loss: 4.4386\n",
            "Epoch 1173/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5910 - val_loss: 3.5571\n",
            "Epoch 1174/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7596 - val_loss: 3.4466\n",
            "Epoch 1175/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6798 - val_loss: 3.7548\n",
            "Epoch 1176/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6181 - val_loss: 3.4977\n",
            "Epoch 1177/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6884 - val_loss: 2.8346\n",
            "Epoch 1178/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6920 - val_loss: 3.3278\n",
            "Epoch 1179/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7408 - val_loss: 3.4762\n",
            "Epoch 1180/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6151 - val_loss: 3.5578\n",
            "Epoch 1181/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5789 - val_loss: 3.9156\n",
            "Epoch 1182/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5867 - val_loss: 3.4659\n",
            "Epoch 1183/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6678 - val_loss: 3.1411\n",
            "Epoch 1184/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6141 - val_loss: 4.0533\n",
            "Epoch 1185/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6585 - val_loss: 4.0753\n",
            "Epoch 1186/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6235 - val_loss: 4.2256\n",
            "Epoch 1187/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6196 - val_loss: 3.0733\n",
            "Epoch 1188/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6642 - val_loss: 3.8946\n",
            "Epoch 1189/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6141 - val_loss: 4.1909\n",
            "Epoch 1190/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6493 - val_loss: 3.5123\n",
            "Epoch 1191/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6587 - val_loss: 3.7462\n",
            "Epoch 1192/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6393 - val_loss: 3.9266\n",
            "Epoch 1193/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6255 - val_loss: 3.8017\n",
            "Epoch 1194/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6414 - val_loss: 3.3633\n",
            "Epoch 1195/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6678 - val_loss: 4.3105\n",
            "Epoch 1196/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6262 - val_loss: 3.9458\n",
            "Epoch 1197/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6142 - val_loss: 4.7973\n",
            "Epoch 1198/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6303 - val_loss: 4.0992\n",
            "Epoch 1199/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6067 - val_loss: 3.8267\n",
            "Epoch 1200/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6882 - val_loss: 3.5495\n",
            "Epoch 1201/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6549 - val_loss: 4.4696\n",
            "Epoch 1202/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6649 - val_loss: 3.6549\n",
            "Epoch 1203/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6155 - val_loss: 4.2669\n",
            "Epoch 1204/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6478 - val_loss: 3.8448\n",
            "Epoch 1205/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6585 - val_loss: 4.3022\n",
            "Epoch 1206/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6743 - val_loss: 3.9970\n",
            "Epoch 1207/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6096 - val_loss: 3.5812\n",
            "Epoch 1208/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5979 - val_loss: 5.3405\n",
            "Epoch 1209/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6793 - val_loss: 4.1395\n",
            "Epoch 1210/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6316 - val_loss: 3.6228\n",
            "Epoch 1211/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6057 - val_loss: 3.7618\n",
            "Epoch 1212/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6182 - val_loss: 3.1263\n",
            "Epoch 1213/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6818 - val_loss: 4.5236\n",
            "Epoch 1214/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6802 - val_loss: 3.4377\n",
            "Epoch 1215/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6438 - val_loss: 3.7441\n",
            "Epoch 1216/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6885 - val_loss: 3.8236\n",
            "Epoch 1217/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6739 - val_loss: 3.9071\n",
            "Epoch 1218/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6399 - val_loss: 4.8856\n",
            "Epoch 1219/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7662 - val_loss: 4.4758\n",
            "Epoch 1220/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6306 - val_loss: 3.6801\n",
            "Epoch 1221/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6704 - val_loss: 4.4388\n",
            "Epoch 1222/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7025 - val_loss: 3.4348\n",
            "Epoch 1223/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6297 - val_loss: 4.3027\n",
            "Epoch 1224/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6253 - val_loss: 4.1623\n",
            "Epoch 1225/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6268 - val_loss: 3.3256\n",
            "Epoch 1226/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6190 - val_loss: 4.0662\n",
            "Epoch 1227/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6206 - val_loss: 4.7725\n",
            "Epoch 1228/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7120 - val_loss: 4.3540\n",
            "Epoch 1229/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6238 - val_loss: 3.9029\n",
            "Epoch 1230/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6278 - val_loss: 4.7849\n",
            "Epoch 1231/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.5815 - val_loss: 5.1391\n",
            "Epoch 1232/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6775 - val_loss: 4.6737\n",
            "Epoch 1233/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6725 - val_loss: 4.1787\n",
            "Epoch 1234/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6981 - val_loss: 4.2018\n",
            "Epoch 1235/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6606 - val_loss: 3.8658\n",
            "Epoch 1236/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6790 - val_loss: 3.2645\n",
            "Epoch 1237/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6195 - val_loss: 3.3298\n",
            "Epoch 1238/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6041 - val_loss: 3.7033\n",
            "Epoch 1239/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6917 - val_loss: 3.6858\n",
            "Epoch 1240/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6173 - val_loss: 4.5302\n",
            "Epoch 1241/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7002 - val_loss: 5.3042\n",
            "Epoch 1242/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6613 - val_loss: 4.2776\n",
            "Epoch 1243/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6643 - val_loss: 4.0599\n",
            "Epoch 1244/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6079 - val_loss: 3.8106\n",
            "Epoch 1245/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6659 - val_loss: 4.1052\n",
            "Epoch 1246/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6354 - val_loss: 3.2464\n",
            "Epoch 1247/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6306 - val_loss: 3.7086\n",
            "Epoch 1248/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6185 - val_loss: 4.3902\n",
            "Epoch 1249/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6478 - val_loss: 4.4620\n",
            "Epoch 1250/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6377 - val_loss: 3.5881\n",
            "Epoch 1251/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.5955 - val_loss: 4.1456\n",
            "Epoch 1252/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6532 - val_loss: 4.7523\n",
            "Epoch 1253/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6159 - val_loss: 5.6557\n",
            "Epoch 1254/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6174 - val_loss: 5.0099\n",
            "Epoch 1255/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6869 - val_loss: 5.3978\n",
            "Epoch 1256/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6636 - val_loss: 4.3833\n",
            "Epoch 1257/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6372 - val_loss: 4.8167\n",
            "Epoch 1258/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.5769 - val_loss: 4.9932\n",
            "Epoch 1259/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6234 - val_loss: 4.7338\n",
            "Epoch 1260/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6156 - val_loss: 4.0719\n",
            "Epoch 1261/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6422 - val_loss: 5.4125\n",
            "Epoch 1262/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6143 - val_loss: 3.5311\n",
            "Epoch 1263/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6459 - val_loss: 4.1393\n",
            "Epoch 1264/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6788 - val_loss: 4.5464\n",
            "Epoch 1265/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6568 - val_loss: 3.8952\n",
            "Epoch 1266/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6716 - val_loss: 5.2462\n",
            "Epoch 1267/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7467 - val_loss: 4.5346\n",
            "Epoch 1268/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6025 - val_loss: 4.3557\n",
            "Epoch 1269/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6589 - val_loss: 3.4449\n",
            "Epoch 1270/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6197 - val_loss: 4.0981\n",
            "Epoch 1271/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5889 - val_loss: 4.5450\n",
            "Epoch 1272/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6279 - val_loss: 4.1275\n",
            "Epoch 1273/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6058 - val_loss: 4.1902\n",
            "Epoch 1274/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6306 - val_loss: 4.3060\n",
            "Epoch 1275/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7283 - val_loss: 3.7405\n",
            "Epoch 1276/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6261 - val_loss: 3.4114\n",
            "Epoch 1277/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6081 - val_loss: 4.7238\n",
            "Epoch 1278/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6345 - val_loss: 4.6912\n",
            "Epoch 1279/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6823 - val_loss: 4.7895\n",
            "Epoch 1280/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7019 - val_loss: 4.5658\n",
            "Epoch 1281/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5937 - val_loss: 3.9282\n",
            "Epoch 1282/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6496 - val_loss: 4.1938\n",
            "Epoch 1283/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6715 - val_loss: 4.2797\n",
            "Epoch 1284/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6344 - val_loss: 3.5188\n",
            "Epoch 1285/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6440 - val_loss: 3.7848\n",
            "Epoch 1286/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6368 - val_loss: 4.0345\n",
            "Epoch 1287/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7022 - val_loss: 3.5478\n",
            "Epoch 1288/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6032 - val_loss: 3.9320\n",
            "Epoch 1289/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5756 - val_loss: 4.8679\n",
            "Epoch 1290/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6480 - val_loss: 5.1507\n",
            "Epoch 1291/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6658 - val_loss: 4.3000\n",
            "Epoch 1292/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6534 - val_loss: 3.8475\n",
            "Epoch 1293/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6305 - val_loss: 4.6645\n",
            "Epoch 1294/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6486 - val_loss: 4.0211\n",
            "Epoch 1295/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5566 - val_loss: 3.8540\n",
            "Epoch 1296/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6526 - val_loss: 4.0434\n",
            "Epoch 1297/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6215 - val_loss: 4.5232\n",
            "Epoch 1298/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7183 - val_loss: 3.8284\n",
            "Epoch 1299/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6337 - val_loss: 4.1448\n",
            "Epoch 1300/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5881 - val_loss: 4.2364\n",
            "Epoch 1301/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5887 - val_loss: 4.5153\n",
            "Epoch 1302/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6579 - val_loss: 4.1988\n",
            "Epoch 1303/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6772 - val_loss: 4.3323\n",
            "Epoch 1304/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6967 - val_loss: 3.6825\n",
            "Epoch 1305/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5973 - val_loss: 3.7224\n",
            "Epoch 1306/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5842 - val_loss: 3.4744\n",
            "Epoch 1307/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6756 - val_loss: 3.4486\n",
            "Epoch 1308/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5807 - val_loss: 4.7966\n",
            "Epoch 1309/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6202 - val_loss: 3.6290\n",
            "Epoch 1310/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5275 - val_loss: 4.5931\n",
            "Epoch 1311/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6189 - val_loss: 3.9737\n",
            "Epoch 1312/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6202 - val_loss: 4.1916\n",
            "Epoch 1313/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6159 - val_loss: 3.3087\n",
            "Epoch 1314/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5761 - val_loss: 4.9993\n",
            "Epoch 1315/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6521 - val_loss: 4.5836\n",
            "Epoch 1316/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6689 - val_loss: 5.1710\n",
            "Epoch 1317/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5979 - val_loss: 4.1148\n",
            "Epoch 1318/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5473 - val_loss: 4.2675\n",
            "Epoch 1319/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6192 - val_loss: 4.0502\n",
            "Epoch 1320/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5957 - val_loss: 3.7996\n",
            "Epoch 1321/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6090 - val_loss: 4.3541\n",
            "Epoch 1322/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5845 - val_loss: 4.5833\n",
            "Epoch 1323/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6379 - val_loss: 4.1676\n",
            "Epoch 1324/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6611 - val_loss: 4.5557\n",
            "Epoch 1325/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6088 - val_loss: 5.6944\n",
            "Epoch 1326/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6817 - val_loss: 4.9057\n",
            "Epoch 1327/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6469 - val_loss: 5.4400\n",
            "Epoch 1328/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6470 - val_loss: 4.4474\n",
            "Epoch 1329/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6657 - val_loss: 3.5568\n",
            "Epoch 1330/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6311 - val_loss: 4.4719\n",
            "Epoch 1331/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6177 - val_loss: 4.8703\n",
            "Epoch 1332/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6621 - val_loss: 4.4211\n",
            "Epoch 1333/5000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.6172 - val_loss: 4.1158\n",
            "Epoch 1334/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6035 - val_loss: 4.2792\n",
            "Epoch 1335/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6493 - val_loss: 5.2095\n",
            "Epoch 1336/5000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.6196 - val_loss: 5.0117\n",
            "Epoch 1337/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6892 - val_loss: 4.2745\n",
            "Epoch 1338/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.5954 - val_loss: 4.2035\n",
            "Epoch 1339/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6854 - val_loss: 4.1032\n",
            "Epoch 1340/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6128 - val_loss: 4.4768\n",
            "Epoch 1341/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7539 - val_loss: 4.3028\n",
            "Epoch 1342/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7041 - val_loss: 4.5003\n",
            "Epoch 1343/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6523 - val_loss: 4.4338\n",
            "Epoch 1344/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6164 - val_loss: 5.9753\n",
            "Epoch 1345/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6395 - val_loss: 4.0260\n",
            "Epoch 1346/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.5699 - val_loss: 4.7148\n",
            "Epoch 1347/5000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6859 - val_loss: 4.1557\n",
            "Epoch 1348/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6738 - val_loss: 3.9440\n",
            "Epoch 1349/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6819 - val_loss: 4.4633\n",
            "Epoch 1350/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6005 - val_loss: 4.6902\n",
            "Epoch 1351/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6723 - val_loss: 5.0295\n",
            "Epoch 1352/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6388 - val_loss: 4.1483\n",
            "Epoch 1353/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6376 - val_loss: 4.1362\n",
            "Epoch 1354/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6210 - val_loss: 3.9313\n",
            "Epoch 1355/5000\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6154 - val_loss: 4.2809\n",
            "Epoch 1356/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6437 - val_loss: 4.3633\n",
            "Epoch 1357/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6358 - val_loss: 4.3883\n",
            "Epoch 1358/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6077 - val_loss: 4.9087\n",
            "Epoch 1359/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6260 - val_loss: 5.3903\n",
            "Epoch 1360/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6195 - val_loss: 3.9451\n",
            "Epoch 1361/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6071 - val_loss: 4.6698\n",
            "Epoch 1362/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6291 - val_loss: 4.7796\n",
            "Epoch 1363/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7087 - val_loss: 4.3266\n",
            "Epoch 1364/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6948 - val_loss: 3.7740\n",
            "Epoch 1365/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5639 - val_loss: 5.1215\n",
            "Epoch 1366/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6225 - val_loss: 5.1294\n",
            "Epoch 1367/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5953 - val_loss: 5.4250\n",
            "Epoch 1368/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7565 - val_loss: 4.8696\n",
            "Epoch 1369/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6310 - val_loss: 4.9558\n",
            "Epoch 1370/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5929 - val_loss: 4.4507\n",
            "Epoch 1371/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6521 - val_loss: 4.3507\n",
            "Epoch 1372/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6284 - val_loss: 5.2203\n",
            "Epoch 1373/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6154 - val_loss: 4.4427\n",
            "Epoch 1374/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6791 - val_loss: 4.8938\n",
            "Epoch 1375/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6231 - val_loss: 4.9795\n",
            "Epoch 1376/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6125 - val_loss: 5.3254\n",
            "Epoch 1377/5000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6455 - val_loss: 4.8933\n",
            "Epoch 1378/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6404 - val_loss: 4.5943\n",
            "Epoch 1379/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6156 - val_loss: 4.5677\n",
            "Epoch 1380/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.5879 - val_loss: 4.6712\n",
            "Epoch 1381/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5939 - val_loss: 4.5793\n",
            "Epoch 1382/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6594 - val_loss: 5.1278\n",
            "Epoch 1383/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6164 - val_loss: 5.3831\n",
            "Epoch 1384/5000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6248 - val_loss: 4.5052\n",
            "Epoch 1385/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7469 - val_loss: 4.5800\n",
            "Epoch 1386/5000\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6553 - val_loss: 4.3625\n",
            "Epoch 1387/5000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6318 - val_loss: 4.7504\n",
            "Epoch 1388/5000\n",
            "16/23 [===================>..........] - ETA: 0s - loss: 0.5408Restoring model weights from the end of the best epoch: 388.\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5956 - val_loss: 5.2575\n",
            "Epoch 1388: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-81abb35ebaca>:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 1.4119796752929688\n",
            "Train loss: 0.684356689453125\n",
            "[[25.2101955 2.01308393]\n",
            " [25.444416 2.01645303]\n",
            " [25.2737904 1.89819062]\n",
            " ...\n",
            " [nan -nan]\n",
            " [24.3138599 1.26072359]\n",
            " [25.2101612 2.01298237]]\n",
            "[[nan nan]\n",
            " [24.4407177 1.36963642]]\n",
            "Test loss: nan\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-865d278bbc51>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrouped_fixed_fileset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_passthrough\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_standardize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fixed_all_isorix_carbon_boosted\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_model_save_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fixed_all_isorix_carbon_boosted.tf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_scaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_model_save_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fixed_all_isorix_carbon_boosted.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-81abb35ebaca>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(sp, run_id, training_batch_size)\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd18O_mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'd18O_variance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m   \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd18O_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd18O_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dO18 RMSE: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EXPECTED:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;36m0.825\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \"\"\"\n\u001b[0;32m--> 442\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "70bfdc7ee2bd490181b70fd4abd2508f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d6e76aa1506472e9ac0025470e508c8",
              "IPY_MODEL_ecad0b67a823441ea9e2bfaf1040014c",
              "IPY_MODEL_a811ba449e4645eca6f055881e5d5ee4",
              "IPY_MODEL_b41f3416523c4fdeb6715a3bd0800297"
            ],
            "layout": "IPY_MODEL_8f4c97c8a317402e8cd87e2a9f68b110"
          }
        },
        "8d6e76aa1506472e9ac0025470e508c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Email",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_79d54b7b7d9544bc88b83876a9f90b76",
            "placeholder": "Enter email",
            "style": "IPY_MODEL_65e718a7efcf4110825afcc58175451b",
            "value": "tripiace@yahoo.com"
          }
        },
        "ecad0b67a823441ea9e2bfaf1040014c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Branch name",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_459ebaa22b5045d38717346cf92c50bd",
            "placeholder": "Enter branch name",
            "style": "IPY_MODEL_9effba8769064b20b43b0f8b71bbae53",
            "value": "gen_isoscape"
          }
        },
        "a811ba449e4645eca6f055881e5d5ee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Checkout Code",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_9a2f3f4e1bd84bb18e478c45f75aa1dc",
            "style": "IPY_MODEL_6ec745afa16e44d18c6267ee871875a9",
            "tooltip": ""
          }
        },
        "b41f3416523c4fdeb6715a3bd0800297": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f8d09ea1a1b845128b6a2dc8f146ff87",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "executing checkout_branch gen_isoscape...\n",
                  "Branch gen_isosca already checked out.\n",
                  "Remember to reload your imports with `importlib.reload(module)`.\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "b\"fatal: destination path 'ddf_common' already exists and is not an empty directory.\\nRepository already exists.\\n\"\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "b'Already up to date.\\n'\n",
                  "b''\n",
                  "gen_isoscape branch checked out at \"/content/gdrive/MyDrive/gen_isoscape/ddf_common\". You may now use ddf_common imports and change common files.\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "interactive(children=(Text(value='required', description='Commit Msg', placeholder='Enter commit message'), Bu…",
                  "application/vnd.jupyter.widget-view+json": {
                    "version_major": 2,
                    "version_minor": 0,
                    "model_id": "7c8783981c9c4d599c743f4accfd6e4b"
                  }
                },
                "metadata": {}
              }
            ]
          }
        },
        "8f4c97c8a317402e8cd87e2a9f68b110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79d54b7b7d9544bc88b83876a9f90b76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65e718a7efcf4110825afcc58175451b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "459ebaa22b5045d38717346cf92c50bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9effba8769064b20b43b0f8b71bbae53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a2f3f4e1bd84bb18e478c45f75aa1dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ec745afa16e44d18c6267ee871875a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "f8d09ea1a1b845128b6a2dc8f146ff87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c8783981c9c4d599c743f4accfd6e4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6da1e760f1c14114a3a8f19875afdd12",
              "IPY_MODEL_0b9bbac4365f410dabb92987ee6f49e4",
              "IPY_MODEL_919e5f838acf480f8c6a478cf434ddcd"
            ],
            "layout": "IPY_MODEL_37e61ed537774c75abce882b8fb642cf"
          }
        },
        "6da1e760f1c14114a3a8f19875afdd12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Commit Msg",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_5c3b3d66a2c549568086dc5b4520e375",
            "placeholder": "Enter commit message",
            "style": "IPY_MODEL_b136df15d48f4833a62f847901a2d306",
            "value": "required"
          }
        },
        "0b9bbac4365f410dabb92987ee6f49e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Commit All Changes",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_c4f3815be1f14075a159b6b76e98676e",
            "style": "IPY_MODEL_aaa5ed5f345948fc91619f49fabf3573",
            "tooltip": ""
          }
        },
        "919e5f838acf480f8c6a478cf434ddcd": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_7b212a12d61547978eb7ab925447fa59",
            "msg_id": "",
            "outputs": []
          }
        },
        "37e61ed537774c75abce882b8fb642cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c3b3d66a2c549568086dc5b4520e375": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b136df15d48f4833a62f847901a2d306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4f3815be1f14075a159b6b76e98676e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaa5ed5f345948fc91619f49fabf3573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "7b212a12d61547978eb7ab925447fa59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}