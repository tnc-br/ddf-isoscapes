{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnc-br/ddf-isoscapes/blob/new_kl/dnn/briso_d13C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational model\n",
        "\n",
        "Find the mean/variance of O18 ratios (as well as N15 and C13 in the future) at a particular lat/lon across Brazil. At the bottom of the colab, train and evaluate 4 different versions of the model with different data partitioning strategies."
      ],
      "metadata": {
        "id": "-0IfT3kGwgK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "henIPlAPCb4i",
        "outputId": "2f42898d-0f8a-474c-f1b4-2c406fedfe0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import os\n",
        "from typing import List, Tuple, Dict\n",
        "from dataclasses import dataclass\n",
        "from joblib import dump\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.python.ops import math_ops\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "#@title Debugging\n",
        "# See https://zohaib.me/debugging-in-google-collab-notebook/ for tips,\n",
        "# as well as docs for pdb and ipdb.\n",
        "DEBUG = False #@param {type:\"boolean\"}\n",
        "USE_LOCAL_DRIVE = False #@param {type:\"boolean\"}\n",
        "LOCAL_DIR = \"/usr/local/google/home/ruru/Downloads/amazon_sample_data-20230712T203059Z-001\" #@param\n",
        "GDRIVE_DIR = \"MyDrive/amazon_rainforest_files/\" #@param\n",
        "FP_ROOT = LOCAL_DIR\n",
        "\n",
        "MODEL_SAVE_LOCATION = \"MyDrive/amazon_rainforest_files/variational/model\" #@param\n",
        "\n",
        "def get_model_save_location(filename) -> str:\n",
        "  root = '' if USE_LOCAL_DRIVE else '/content/gdrive'\n",
        "  return os.path.join(root, MODEL_SAVE_LOCATION, filename)\n",
        "\n",
        "# Access data stored on Google Drive if not reading data locally.\n",
        "if not USE_LOCAL_DRIVE:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  global FP_ROOT\n",
        "  FP_ROOT = os.path.join('/content/gdrive', GDRIVE_DIR)\n",
        "\n",
        "if DEBUG:\n",
        "    %pip install -Uqq ipdb\n",
        "    import ipdb\n",
        "    %pdb on"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "!if [ ! -d \"/content/ddf_common_stub\" ] ; then git clone -b test https://github.com/tnc-br/ddf_common_stub.git; fi\n",
        "sys.path.append(\"/content/ddf_common_stub/\")\n",
        "import ddfimport\n",
        "ddfimport.ddf_import_common()\n",
        "\n"
      ],
      "metadata": {
        "id": "AXh86HFwXiax",
        "outputId": "0f6f39e4-7a89-4758-f71d-6eefa2aa70e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "executing checkout_branch ...\n",
            "Branch main already checked out.\n",
            "Remember to reload your imports with `importlib.reload(module)`.\n",
            "b''\n",
            "main branch checked out as readonly. You may now use ddf_common imports\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import raster"
      ],
      "metadata": {
        "id": "0mUB0y0AXivp"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQrEv57zCb4q"
      },
      "source": [
        "# Data preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path: str, columns_to_keep: List[str], side_raster_input):\n",
        "  df = pd.read_csv(path, encoding=\"ISO-8859-1\", sep=',')\n",
        "  df = df[df['d18O_cel_variance'].notna()]\n",
        "  X = df\n",
        "  X = X.drop(df.columns.difference(columns_to_keep), axis=1)\n",
        "\n",
        "  for name, geotiff in side_raster_input.items():\n",
        "    X[name] = X.apply(lambda row: geotiff.value_at(row['long'], row['lat']), axis=1)\n",
        "    # The last two kriging columns need to remain last. Move new columns forward.\n",
        "    X.insert(len(X.columns)-3, name, X.pop(name))\n",
        "\n",
        "  Y = df[[\"d18O_cel_mean\", \"d18O_cel_variance\"]]\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "6XMee1aHfcik"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardization"
      ],
      "metadata": {
        "id": "DtkKhMOtb6cS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class FeaturesToLabels:\n",
        "  def __init__(self, X: pd.DataFrame, Y: pd.DataFrame):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "\n",
        "  def as_tuple(self):\n",
        "    return (self.X, self.Y)\n",
        "\n",
        "\n",
        "def create_feature_scaler(X: pd.DataFrame,\n",
        "                          columns_to_passthrough,\n",
        "                          columns_to_scale,\n",
        "                          columns_to_standardize) -> ColumnTransformer:\n",
        "  columns_to_standardize = columns_to_standardize\n",
        "  feature_scaler = ColumnTransformer(\n",
        "      [(column+'_normalizer', MinMaxScaler(), [column]) for column in columns_to_scale] +\n",
        "      [(column+'_standardizer', StandardScaler(), [column]) for column in columns_to_standardize],\n",
        "      remainder='passthrough')\n",
        "  feature_scaler.fit(X)\n",
        "  print(feature_scaler)\n",
        "  return feature_scaler\n",
        "\n",
        "def create_label_scaler(Y: pd.DataFrame) -> ColumnTransformer:\n",
        "  label_scaler = ColumnTransformer([\n",
        "      ('mean_std_scaler', StandardScaler(), ['d18O_cel_mean']),\n",
        "      ('var_minmax_scaler', MinMaxScaler(), ['d18O_cel_variance'])],\n",
        "      remainder='passthrough')\n",
        "  label_scaler.fit(Y)\n",
        "  return label_scaler\n",
        "\n",
        "def scale(X: pd.DataFrame, feature_scaler):\n",
        "  # transform() outputs numpy arrays :(  need to convert back to DataFrame.\n",
        "  X_standardized = pd.DataFrame(feature_scaler.transform(X),\n",
        "                        index=X.index, columns=X.columns)\n",
        "  return X_standardized"
      ],
      "metadata": {
        "id": "XSDwdvMkb7w8"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just a class organization, holds each scaled dataset and the scaler used.\n",
        "# Useful for unscaling predictions.\n",
        "@dataclass\n",
        "class ScaledPartitions():\n",
        "  def __init__(self,\n",
        "               feature_scaler: ColumnTransformer,\n",
        "               label_scaler: ColumnTransformer,\n",
        "               train: FeaturesToLabels, val: FeaturesToLabels,\n",
        "               test: FeaturesToLabels):\n",
        "    self.feature_scaler = feature_scaler\n",
        "    self.label_scaler = label_scaler\n",
        "    self.train = train\n",
        "    self.val = val\n",
        "    self.test = test\n",
        "\n",
        "\n",
        "def load_and_scale(config: Dict,\n",
        "                   columns_to_passthrough: List[str],\n",
        "                   columns_to_scale: List[str],\n",
        "                   columns_to_standardize: List[str]) -> ScaledPartitions:\n",
        "\n",
        "  geotiff_side_input = {\n",
        "      \"brisoscape_mean_ISORIX\" : raster.load_raster(raster.get_raster_path(\"brisoscape_mean_ISORIX.tif\")),\n",
        "      # \"d13C_cel_mean\" : raster.load_raster(raster.get_raster_path(\"d13C_cel_Brazil_stack_terra_null.tiff\"), use_only_band_index=0),\n",
        "      # \"d13C_cel_var\" : raster.load_raster(raster.get_raster_path(\"d13C_cel_Brazil_stack_terra_null.tiff\"), use_only_band_index=1)\n",
        "  }\n",
        "  columns_to_keep = columns_to_passthrough + columns_to_scale + columns_to_standardize\n",
        "  X_train, Y_train = load_dataset(config['TRAIN'], columns_to_keep, geotiff_side_input)\n",
        "  X_val, Y_val = load_dataset(config['VALIDATION'], columns_to_keep, geotiff_side_input)\n",
        "  X_test, Y_test = load_dataset(config['TEST'], columns_to_keep, geotiff_side_input)\n",
        "\n",
        "  # Fit the scaler:\n",
        "  feature_scaler = create_feature_scaler(\n",
        "      X_train,\n",
        "      columns_to_passthrough,\n",
        "      columns_to_scale,\n",
        "      columns_to_standardize)\n",
        "\n",
        "  # Apply the scaler:\n",
        "  label_scaler = create_label_scaler(Y_train)\n",
        "  train = FeaturesToLabels(scale(X_train, feature_scaler), Y_train)\n",
        "  val = FeaturesToLabels(scale(X_val, feature_scaler), Y_val)\n",
        "  test = FeaturesToLabels(scale(X_test, feature_scaler), Y_test)\n",
        "  return ScaledPartitions(feature_scaler, label_scaler, train, val, test)\n"
      ],
      "metadata": {
        "id": "_kf2e_fKon2P"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition\n",
        "\n"
      ],
      "metadata": {
        "id": "usGznR593LZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KL Loss function:"
      ],
      "metadata": {
        "id": "khK7C8WvU8ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_normal_distribution(\n",
        "    mean: tf.Tensor,\n",
        "    stdev: tf.Tensor,\n",
        "    n: int) -> tf.Tensor:\n",
        "    '''\n",
        "    Given a batch of normal distributions described by a mean and stdev in\n",
        "    a tf.Tensor, sample n elements from each distribution and return the mean\n",
        "    and standard deviation per sample.\n",
        "    '''\n",
        "    batch_size = tf.shape(mean)[0]\n",
        "\n",
        "    # Output tensor is (n, batch_size, 1)\n",
        "    sample_values = tfp.distributions.Normal(\n",
        "        loc=mean,\n",
        "        scale=stdev).sample(\n",
        "            sample_shape=n)\n",
        "    # Reshaped tensor will be (batch_size, n)\n",
        "    sample_values = tf.transpose(sample_values)\n",
        "    # Get the mean per sample in the batch.\n",
        "    sample_mean = tf.transpose(tf.math.reduce_mean(sample_values, 2))\n",
        "    sample_stdev = tf.transpose(tf.math.reduce_std(sample_values, 2))\n",
        "\n",
        "    return sample_mean, sample_stdev\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# log(σ2/σ1) + ( σ1^2+(μ1−μ2)^2 ) / 2* σ^2   − 1/2\n",
        "def kl_divergence_helper(real, predicted, sample):\n",
        "    '''\n",
        "    real: tf.Tensor of the real mean and standard deviation of sample to compare\n",
        "    predicted: tf.Tensor of the predicted mean and standard deviation to compare\n",
        "    sample: Whether or not to sample the predicted distribution to get a new\n",
        "            mean and standard deviation.\n",
        "    '''\n",
        "    if real.shape != predicted.shape:\n",
        "      raise ValueError(\n",
        "          f\"real.shape {real.shape} != predicted.shape {predicted.shape}\")\n",
        "\n",
        "    real_value = tf.gather(real, [0], axis=1)\n",
        "    real_std = tf.math.sqrt(tf.gather(real, [1], axis=1))\n",
        "\n",
        "\n",
        "    predicted_value = tf.gather(predicted, [0], axis=1)\n",
        "    predicted_std = tf.math.sqrt(tf.gather(predicted, [1], axis=1))\n",
        "    # If true, sample from the distribution defined by the predicted mean and\n",
        "    # standard deviation to use for mean and stdev used in KL divergence loss.\n",
        "    if sample:\n",
        "      predicted_value, predicted_std = sample_normal_distribution(\n",
        "          mean=predicted_value, stdev=predicted_std, n=15)\n",
        "\n",
        "    kl_loss = -0.5 + tf.math.log(predicted_std/real_std) + \\\n",
        "     (tf.square(real_std) + tf.square(real_value - predicted_value))/ \\\n",
        "     (2*tf.square(predicted_std))\n",
        "\n",
        "    if tf.math.is_nan(tf.math.reduce_mean(kl_loss)):\n",
        "       tf.print(predicted)\n",
        "       sess = tf.compat.v1.Session()\n",
        "       sess.close()\n",
        "\n",
        "    return tf.math.reduce_mean(kl_loss)\n",
        "\n",
        "def kl_divergence(real, predicted):\n",
        "  return kl_divergence_helper(real, predicted, True)"
      ],
      "metadata": {
        "id": "urGjYNNnemX6"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the loss function:"
      ],
      "metadata": {
        "id": "fJzBFWQVeqNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model definition"
      ],
      "metadata": {
        "id": "8rI6qPRh7oO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "def get_early_stopping_callback():\n",
        "  return EarlyStopping(monitor='val_loss', patience=1000, min_delta=0.001,\n",
        "                       verbose=1, restore_best_weights=True, start_from_epoch=0)\n",
        "\n",
        "tf.keras.utils.set_random_seed(18731)\n",
        "\n",
        "# I was experimenting with models that took longer to train, and used this\n",
        "# checkpointing callback to periodically save the model. It's optional.\n",
        "def get_checkpoint_callback(model_file):\n",
        "  return ModelCheckpoint(\n",
        "      get_model_save_location(model_file),\n",
        "      monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
        "\n",
        "def train_or_update_variational_model(\n",
        "        sp: ScaledPartitions,\n",
        "        hidden_layers: List[int],\n",
        "        epochs: int,\n",
        "        batch_size: int,\n",
        "        lr: float,\n",
        "        model_file=None,\n",
        "        use_checkpoint=False):\n",
        "  callbacks_list = [get_early_stopping_callback(),\n",
        "                    get_checkpoint_callback(model_file)]\n",
        "  if not use_checkpoint:\n",
        "    inputs = keras.Input(shape=(sp.train.X.shape[1],))\n",
        "    x = inputs\n",
        "    for layer_size in hidden_layers:\n",
        "      x = keras.layers.Dense(\n",
        "          layer_size, activation='relu')(x)\n",
        "    mean_output = keras.layers.Dense(\n",
        "        1, name='mean_output')(x)\n",
        "\n",
        "    # We can not have negative variance. Apply very little variance.\n",
        "    var_output = keras.layers.Dense(\n",
        "        1, name='var_output')(x)\n",
        "\n",
        "    # Invert the normalization on our outputs\n",
        "    mean_scaler = sp.label_scaler.named_transformers_['mean_std_scaler']\n",
        "    untransformed_mean = mean_output * mean_scaler.var_ + mean_scaler.mean_\n",
        "\n",
        "    var_scaler = sp.label_scaler.named_transformers_['var_minmax_scaler']\n",
        "    unscaled_var = var_output * var_scaler.scale_ + var_scaler.min_\n",
        "    untransformed_var = keras.layers.Lambda(lambda t: tf.math.log(1 + tf.exp(t)))(unscaled_var)\n",
        "\n",
        "    # Output mean,  tuples.\n",
        "    outputs = keras.layers.concatenate([untransformed_mean, untransformed_var])\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss=kl_divergence)\n",
        "    model.summary()\n",
        "  else:\n",
        "    model = keras.models.load_model(\n",
        "        get_model_save_location(model_file),\n",
        "        custom_objects={\"kl_divergence\": kl_divergence})\n",
        "  history = model.fit(sp.train.X, sp.train.Y, verbose=1, validation_data=sp.val.as_tuple(), shuffle=True,\n",
        "                      epochs=epochs, batch_size=batch_size, callbacks=callbacks_list)\n",
        "  return history, model"
      ],
      "metadata": {
        "id": "HCkGSPUo3KqY"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def render_plot_loss(history, name):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title(name + ' model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.yscale(\"log\")\n",
        "  plt.ylim((0, 10))\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['loss', 'val_loss'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def destandardize(sd: ScaledPartitions, df: pd.DataFrame):\n",
        "  means = pd.DataFrame(\n",
        "      sd.label_scaler.named_transformers_['var_std_scaler'].inverse_transform(df[['d18O_cel_mean']]),\n",
        "      index=df.index, columns=['d18O_cel_mean'])\n",
        "  vars = df['d18O_cel_variance']\n",
        "  return means.join(vars)\n",
        "\n",
        "def train_and_evaluate(sp: ScaledPartitions, run_id: str, training_batch_size=5):\n",
        "  print(\"==================\")\n",
        "  print(run_id)\n",
        "  history, model = train_or_update_variational_model(\n",
        "      sp, hidden_layers=[20, 20], epochs=5000, batch_size=training_batch_size,\n",
        "      lr=0.0001, model_file=run_id+\".h5\", use_checkpoint=False)\n",
        "  render_plot_loss(history, run_id+\" kl_loss\")\n",
        "  model.save(get_model_save_location(run_id+\".h5\"), save_format=\"h5\")\n",
        "\n",
        "  best_epoch_index = history.history['val_loss'].index(min(history.history['val_loss']))\n",
        "  print('Val loss:', history.history['val_loss'][best_epoch_index])\n",
        "  print('Train loss:', history.history['loss'][best_epoch_index])\n",
        "  print('Test loss:', model.evaluate(x=sp.test.X, y=sp.test.Y, verbose=0))\n",
        "\n",
        "  predictions = model.predict_on_batch(sp.test.X)\n",
        "  predictions = pd.DataFrame(predictions, columns=['d18O_cel_mean', 'd18O_cel_variance'])\n",
        "  rmse = np.sqrt(mean_squared_error(sp.test.Y['d18O_cel_mean'], predictions['d18O_cel_mean']))\n",
        "  print(\"dO18 RMSE: \"+ str(rmse))\n",
        "  print(\"EXPECTED:\")\n",
        "  print(sp.test.Y.to_string())\n",
        "  print()\n",
        "  print(\"PREDICTED:\")\n",
        "  print(predictions.to_string())\n",
        "  return model"
      ],
      "metadata": {
        "id": "DALuUm8UOgNu"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Grouped, random (jupyter crashed halfway so I reloaded a checkpoint)\n",
        "\n",
        "We can't (easily) generate isoscapes with these because the isoscapes for 'predkrig_br_lat_ISORG', 'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION' are not easily retrievable... but I'm curious how much better the model is if these columns are included."
      ],
      "metadata": {
        "id": "n8pNR1CrvF2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_random_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_train_random_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_test_random_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_validation_random_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_passthrough = [\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "columns_to_scale = []\n",
        "columns_to_standardize = [\n",
        "    'lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature', 'Mean Annual Precipitation',\n",
        "    'Iso_Oxi_Stack_mean_TERZER', 'isoscape_fullmodel_d18O_prec_REGRESSION']\n",
        "\n",
        "data = load_and_scale(grouped_random_fileset, columns_to_passthrough, columns_to_scale, columns_to_standardize)\n",
        "model = train_and_evaluate(data, \"random_all_0809_ensemble\", training_batch_size=3)\n",
        "model.save(get_model_save_location(\"random_all_0809_ensemble.tf\"), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location('random_all_0809_ensemble.pkl'))\n"
      ],
      "metadata": {
        "id": "rPONfgkjvJWz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7839d2b0-2521-411b-c7cf-e4446ac24879"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: GTiff/GeoTIFF\n",
            "Size is 541 x 467 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.922043, 5.233124)\n",
            "Pixel Size = (0.08333, -0.08333)\n",
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('lat_standardizer', StandardScaler(), ['lat']),\n",
            "                                ('long_standardizer', StandardScaler(),\n",
            "                                 ['long']),\n",
            "                                ('VPD_standardizer', StandardScaler(), ['VPD']),\n",
            "                                ('RH_standardizer', StandardScaler(), ['RH']),\n",
            "                                ('PET_standardizer', StandardScaler(), ['PET']),\n",
            "                                ('DEM_standardizer', StandardScaler(), ['DEM']),\n",
            "                                ('PA_standardizer'...\n",
            "                                ('Mean Annual Temperature_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation']),\n",
            "                                ('Iso_Oxi_Stack_mean_TERZER_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Iso_Oxi_Stack_mean_TERZER']),\n",
            "                                ('isoscape_fullmodel_d18O_prec_REGRESSION_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['isoscape_fullmodel_d18O_prec_REGRESSION'])])\n",
            "==================\n",
            "random_all_0809_ensemble\n",
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_10 (InputLayer)          [(None, 14)]         0           []                               \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (None, 20)           300         ['input_10[0][0]']               \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 20)           420         ['dense_18[0][0]']               \n",
            "                                                                                                  \n",
            " var_output (Dense)             (None, 1)            21          ['dense_19[0][0]']               \n",
            "                                                                                                  \n",
            " mean_output (Dense)            (None, 1)            21          ['dense_19[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.multiply_5 (TFOpLambda  (None, 1)           0           ['var_output[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.multiply_4 (TFOpLambda  (None, 1)           0           ['mean_output[0][0]']            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_11 (TFOpL  (None, 1)           0           ['tf.math.multiply_5[0][0]']     \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_10 (TFOpL  (None, 1)           0           ['tf.math.multiply_4[0][0]']     \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1)            0           ['tf.__operators__.add_11[0][0]']\n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate)    (None, 2)            0           ['tf.__operators__.add_10[0][0]',\n",
            "                                                                  'lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 762\n",
            "Trainable params: 762\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5000\n",
            "33/33 [==============================] - 4s 45ms/step - loss: 7.8876 - val_loss: 3.6281\n",
            "Epoch 2/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 5.7319 - val_loss: 1.6371\n",
            "Epoch 3/5000\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 4.6630 - val_loss: 1.3392\n",
            "Epoch 4/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 4.0546 - val_loss: 0.8678\n",
            "Epoch 5/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 3.7195 - val_loss: 0.8960\n",
            "Epoch 6/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 3.0052 - val_loss: 1.0095\n",
            "Epoch 7/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 3.0079 - val_loss: 0.9020\n",
            "Epoch 8/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.8540 - val_loss: 0.9445\n",
            "Epoch 9/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.5353 - val_loss: 0.9385\n",
            "Epoch 10/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.7961 - val_loss: 1.0791\n",
            "Epoch 11/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.7252 - val_loss: 0.8821\n",
            "Epoch 12/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.6755 - val_loss: 1.3462\n",
            "Epoch 13/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.4336 - val_loss: 1.1739\n",
            "Epoch 14/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 2.2553 - val_loss: 0.8496\n",
            "Epoch 15/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.7867 - val_loss: 0.8930\n",
            "Epoch 16/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 2.4044 - val_loss: 0.8044\n",
            "Epoch 17/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.0281 - val_loss: 0.7067\n",
            "Epoch 18/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.4516 - val_loss: 0.7712\n",
            "Epoch 19/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.0579 - val_loss: 0.8815\n",
            "Epoch 20/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 2.2687 - val_loss: 1.2074\n",
            "Epoch 21/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.9173 - val_loss: 0.8743\n",
            "Epoch 22/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 2.1996 - val_loss: 0.8600\n",
            "Epoch 23/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 2.1499 - val_loss: 0.7470\n",
            "Epoch 24/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 2.0294 - val_loss: 0.9825\n",
            "Epoch 25/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.3933 - val_loss: 0.8926\n",
            "Epoch 26/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.9209 - val_loss: 0.9852\n",
            "Epoch 27/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.9105 - val_loss: 1.1425\n",
            "Epoch 28/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 2.0317 - val_loss: 1.0003\n",
            "Epoch 29/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.7324 - val_loss: 1.1007\n",
            "Epoch 30/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.1202 - val_loss: 0.7761\n",
            "Epoch 31/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.7097 - val_loss: 0.7583\n",
            "Epoch 32/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6100 - val_loss: 0.8990\n",
            "Epoch 33/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 2.2548 - val_loss: 0.9648\n",
            "Epoch 34/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.8296 - val_loss: 0.9295\n",
            "Epoch 35/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.0747 - val_loss: 0.7294\n",
            "Epoch 36/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7959 - val_loss: 0.9291\n",
            "Epoch 37/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.8698 - val_loss: 0.7511\n",
            "Epoch 38/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6832 - val_loss: 0.6274\n",
            "Epoch 39/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6388 - val_loss: 0.9064\n",
            "Epoch 40/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7585 - val_loss: 1.1379\n",
            "Epoch 41/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5117 - val_loss: 1.0246\n",
            "Epoch 42/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6588 - val_loss: 1.0792\n",
            "Epoch 43/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2545 - val_loss: 0.9386\n",
            "Epoch 44/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5989 - val_loss: 0.7043\n",
            "Epoch 45/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.8410 - val_loss: 0.8547\n",
            "Epoch 46/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6587 - val_loss: 0.6419\n",
            "Epoch 47/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.7603 - val_loss: 0.8055\n",
            "Epoch 48/5000\n",
            "33/33 [==============================] - 0s 11ms/step - loss: 1.4894 - val_loss: 0.7497\n",
            "Epoch 49/5000\n",
            "33/33 [==============================] - 0s 13ms/step - loss: 1.5719 - val_loss: 0.5738\n",
            "Epoch 50/5000\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 1.6403 - val_loss: 0.6219\n",
            "Epoch 51/5000\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 1.6341 - val_loss: 0.5553\n",
            "Epoch 52/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.5582 - val_loss: 0.8308\n",
            "Epoch 53/5000\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 1.5054 - val_loss: 0.7005\n",
            "Epoch 54/5000\n",
            "33/33 [==============================] - 0s 11ms/step - loss: 1.4136 - val_loss: 0.6641\n",
            "Epoch 55/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.3257 - val_loss: 0.6648\n",
            "Epoch 56/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3594 - val_loss: 0.7631\n",
            "Epoch 57/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2981 - val_loss: 0.7621\n",
            "Epoch 58/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.5218 - val_loss: 0.5331\n",
            "Epoch 59/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.7327 - val_loss: 0.5932\n",
            "Epoch 60/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5042 - val_loss: 0.4934\n",
            "Epoch 61/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.5605 - val_loss: 0.6065\n",
            "Epoch 62/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6299 - val_loss: 0.8640\n",
            "Epoch 63/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3962 - val_loss: 0.6354\n",
            "Epoch 64/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3824 - val_loss: 0.8507\n",
            "Epoch 65/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4488 - val_loss: 0.5472\n",
            "Epoch 66/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4891 - val_loss: 0.7003\n",
            "Epoch 67/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3774 - val_loss: 0.7918\n",
            "Epoch 68/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2186 - val_loss: 0.6595\n",
            "Epoch 69/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.3157 - val_loss: 0.8987\n",
            "Epoch 70/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.6341 - val_loss: 0.7619\n",
            "Epoch 71/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5791 - val_loss: 0.6019\n",
            "Epoch 72/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.4406 - val_loss: 0.6842\n",
            "Epoch 73/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.3755 - val_loss: 0.9311\n",
            "Epoch 74/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.6545 - val_loss: 0.7009\n",
            "Epoch 75/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3438 - val_loss: 0.7423\n",
            "Epoch 76/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.3227 - val_loss: 0.8009\n",
            "Epoch 77/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.8326 - val_loss: 0.7947\n",
            "Epoch 78/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.4079 - val_loss: 0.5234\n",
            "Epoch 79/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6122 - val_loss: 1.0474\n",
            "Epoch 80/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.6191 - val_loss: 0.5378\n",
            "Epoch 81/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.5321 - val_loss: 0.5880\n",
            "Epoch 82/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.7673 - val_loss: 0.6092\n",
            "Epoch 83/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4645 - val_loss: 0.7913\n",
            "Epoch 84/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4445 - val_loss: 0.7011\n",
            "Epoch 85/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.4986 - val_loss: 0.7465\n",
            "Epoch 86/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.3695 - val_loss: 0.7124\n",
            "Epoch 87/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3682 - val_loss: 0.6762\n",
            "Epoch 88/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.6232 - val_loss: 0.9900\n",
            "Epoch 89/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4379 - val_loss: 0.7571\n",
            "Epoch 90/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.7325 - val_loss: 0.6896\n",
            "Epoch 91/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5422 - val_loss: 0.6039\n",
            "Epoch 92/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2770 - val_loss: 0.6510\n",
            "Epoch 93/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.3637 - val_loss: 0.6392\n",
            "Epoch 94/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4719 - val_loss: 0.8336\n",
            "Epoch 95/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.3288 - val_loss: 0.5903\n",
            "Epoch 96/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.3197 - val_loss: 0.7233\n",
            "Epoch 97/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5780 - val_loss: 0.8143\n",
            "Epoch 98/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7934 - val_loss: 0.5704\n",
            "Epoch 99/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2610 - val_loss: 0.5851\n",
            "Epoch 100/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5497 - val_loss: 0.6971\n",
            "Epoch 101/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6031 - val_loss: 1.0244\n",
            "Epoch 102/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2836 - val_loss: 0.7987\n",
            "Epoch 103/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4436 - val_loss: 0.6667\n",
            "Epoch 104/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5141 - val_loss: 0.5912\n",
            "Epoch 105/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3818 - val_loss: 0.8448\n",
            "Epoch 106/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4871 - val_loss: 0.7779\n",
            "Epoch 107/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3382 - val_loss: 0.7505\n",
            "Epoch 108/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3675 - val_loss: 0.6231\n",
            "Epoch 109/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4164 - val_loss: 0.9592\n",
            "Epoch 110/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.8226 - val_loss: 0.5805\n",
            "Epoch 111/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1153 - val_loss: 0.7636\n",
            "Epoch 112/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5714 - val_loss: 0.6525\n",
            "Epoch 113/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4557 - val_loss: 0.7231\n",
            "Epoch 114/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6043 - val_loss: 0.7174\n",
            "Epoch 115/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3314 - val_loss: 0.9541\n",
            "Epoch 116/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5685 - val_loss: 0.7838\n",
            "Epoch 117/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3141 - val_loss: 0.6627\n",
            "Epoch 118/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1835 - val_loss: 0.7055\n",
            "Epoch 119/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2159 - val_loss: 0.8514\n",
            "Epoch 120/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5786 - val_loss: 0.7620\n",
            "Epoch 121/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1972 - val_loss: 0.6184\n",
            "Epoch 122/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2716 - val_loss: 0.8844\n",
            "Epoch 123/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3566 - val_loss: 0.4949\n",
            "Epoch 124/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3225 - val_loss: 0.7668\n",
            "Epoch 125/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 2.2879 - val_loss: 0.6598\n",
            "Epoch 126/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3946 - val_loss: 0.8916\n",
            "Epoch 127/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7591 - val_loss: 0.5244\n",
            "Epoch 128/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3561 - val_loss: 0.8704\n",
            "Epoch 129/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6846 - val_loss: 0.8506\n",
            "Epoch 130/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.8911 - val_loss: 0.6595\n",
            "Epoch 131/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3817 - val_loss: 1.2332\n",
            "Epoch 132/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3084 - val_loss: 0.7615\n",
            "Epoch 133/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3329 - val_loss: 0.8111\n",
            "Epoch 134/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2506 - val_loss: 0.6723\n",
            "Epoch 135/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4761 - val_loss: 0.6483\n",
            "Epoch 136/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2838 - val_loss: 0.7212\n",
            "Epoch 137/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2362 - val_loss: 0.7058\n",
            "Epoch 138/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3815 - val_loss: 0.7876\n",
            "Epoch 139/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4034 - val_loss: 0.5577\n",
            "Epoch 140/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1606 - val_loss: 0.9644\n",
            "Epoch 141/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4444 - val_loss: 0.9264\n",
            "Epoch 142/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6451 - val_loss: 0.8411\n",
            "Epoch 143/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3151 - val_loss: 0.6002\n",
            "Epoch 144/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2171 - val_loss: 0.8473\n",
            "Epoch 145/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2627 - val_loss: 0.9379\n",
            "Epoch 146/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3037 - val_loss: 0.7917\n",
            "Epoch 147/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4435 - val_loss: 0.7262\n",
            "Epoch 148/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2771 - val_loss: 0.8298\n",
            "Epoch 149/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2629 - val_loss: 0.9031\n",
            "Epoch 150/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3954 - val_loss: 0.6475\n",
            "Epoch 151/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3681 - val_loss: 0.7833\n",
            "Epoch 152/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3935 - val_loss: 0.7744\n",
            "Epoch 153/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3361 - val_loss: 0.8315\n",
            "Epoch 154/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3171 - val_loss: 0.7818\n",
            "Epoch 155/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4787 - val_loss: 1.0280\n",
            "Epoch 156/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3249 - val_loss: 0.8094\n",
            "Epoch 157/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6142 - val_loss: 0.9734\n",
            "Epoch 158/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2380 - val_loss: 1.1279\n",
            "Epoch 159/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2163 - val_loss: 0.6408\n",
            "Epoch 160/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.3735 - val_loss: 1.0315\n",
            "Epoch 161/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1432 - val_loss: 0.7244\n",
            "Epoch 162/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2414 - val_loss: 0.8672\n",
            "Epoch 163/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1999 - val_loss: 0.8865\n",
            "Epoch 164/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.3517 - val_loss: 0.6603\n",
            "Epoch 165/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3697 - val_loss: 0.8837\n",
            "Epoch 166/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1679 - val_loss: 0.9225\n",
            "Epoch 167/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3263 - val_loss: 0.7981\n",
            "Epoch 168/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2449 - val_loss: 0.8576\n",
            "Epoch 169/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.3015 - val_loss: 0.5644\n",
            "Epoch 170/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1743 - val_loss: 0.8125\n",
            "Epoch 171/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2037 - val_loss: 0.6532\n",
            "Epoch 172/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1184 - val_loss: 0.6471\n",
            "Epoch 173/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.3000 - val_loss: 0.6151\n",
            "Epoch 174/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6641 - val_loss: 0.6856\n",
            "Epoch 175/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1843 - val_loss: 0.6737\n",
            "Epoch 176/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2736 - val_loss: 0.8506\n",
            "Epoch 177/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2425 - val_loss: 0.6822\n",
            "Epoch 178/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0210 - val_loss: 0.6685\n",
            "Epoch 179/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4138 - val_loss: 0.8144\n",
            "Epoch 180/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3063 - val_loss: 0.8539\n",
            "Epoch 181/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1639 - val_loss: 0.6682\n",
            "Epoch 182/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2294 - val_loss: 0.6298\n",
            "Epoch 183/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.5526 - val_loss: 0.7591\n",
            "Epoch 184/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3981 - val_loss: 0.6337\n",
            "Epoch 185/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3056 - val_loss: 0.7128\n",
            "Epoch 186/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0538 - val_loss: 1.2440\n",
            "Epoch 187/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1188 - val_loss: 1.0173\n",
            "Epoch 188/5000\n",
            "33/33 [==============================] - 1s 29ms/step - loss: 1.1281 - val_loss: 0.4780\n",
            "Epoch 189/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2137 - val_loss: 0.7242\n",
            "Epoch 190/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2200 - val_loss: 0.8717\n",
            "Epoch 191/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2078 - val_loss: 2.0943\n",
            "Epoch 192/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2670 - val_loss: 0.6948\n",
            "Epoch 193/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1215 - val_loss: 1.1447\n",
            "Epoch 194/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2941 - val_loss: 0.7732\n",
            "Epoch 195/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4540 - val_loss: 0.7106\n",
            "Epoch 196/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1826 - val_loss: 0.7966\n",
            "Epoch 197/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0302 - val_loss: 0.7441\n",
            "Epoch 198/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3210 - val_loss: 1.1253\n",
            "Epoch 199/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1970 - val_loss: 0.7832\n",
            "Epoch 200/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1806 - val_loss: 0.9863\n",
            "Epoch 201/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2557 - val_loss: 0.6803\n",
            "Epoch 202/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2130 - val_loss: 0.6960\n",
            "Epoch 203/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2698 - val_loss: 0.6841\n",
            "Epoch 204/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3793 - val_loss: 0.8572\n",
            "Epoch 205/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5085 - val_loss: 0.6758\n",
            "Epoch 206/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2905 - val_loss: 0.7783\n",
            "Epoch 207/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0875 - val_loss: 0.6197\n",
            "Epoch 208/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2084 - val_loss: 0.7086\n",
            "Epoch 209/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0983 - val_loss: 0.7860\n",
            "Epoch 210/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0946 - val_loss: 0.7246\n",
            "Epoch 211/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1542 - val_loss: 0.7599\n",
            "Epoch 212/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1170 - val_loss: 0.7319\n",
            "Epoch 213/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1374 - val_loss: 0.8287\n",
            "Epoch 214/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1519 - val_loss: 1.4345\n",
            "Epoch 215/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3438 - val_loss: 0.6510\n",
            "Epoch 216/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1989 - val_loss: 0.8970\n",
            "Epoch 217/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3386 - val_loss: 1.0777\n",
            "Epoch 218/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1354 - val_loss: 0.6790\n",
            "Epoch 219/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4451 - val_loss: 0.8023\n",
            "Epoch 220/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1317 - val_loss: 0.9053\n",
            "Epoch 221/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0817 - val_loss: 0.7203\n",
            "Epoch 222/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1288 - val_loss: 0.7394\n",
            "Epoch 223/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2354 - val_loss: 1.2127\n",
            "Epoch 224/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2056 - val_loss: 0.8947\n",
            "Epoch 225/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9773 - val_loss: 0.9504\n",
            "Epoch 226/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2859 - val_loss: 0.9202\n",
            "Epoch 227/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1171 - val_loss: 0.5644\n",
            "Epoch 228/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2125 - val_loss: 0.6735\n",
            "Epoch 229/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1266 - val_loss: 0.9751\n",
            "Epoch 230/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2567 - val_loss: 0.8847\n",
            "Epoch 231/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2264 - val_loss: 0.9308\n",
            "Epoch 232/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6725 - val_loss: 0.7297\n",
            "Epoch 233/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1982 - val_loss: 1.3185\n",
            "Epoch 234/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1800 - val_loss: 0.8165\n",
            "Epoch 235/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1753 - val_loss: 1.0723\n",
            "Epoch 236/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.2862 - val_loss: 1.0196\n",
            "Epoch 237/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1315 - val_loss: 0.8376\n",
            "Epoch 238/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1367 - val_loss: 0.7287\n",
            "Epoch 239/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2400 - val_loss: 0.7410\n",
            "Epoch 240/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0449 - val_loss: 0.7312\n",
            "Epoch 241/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0037 - val_loss: 1.0033\n",
            "Epoch 242/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2635 - val_loss: 0.9649\n",
            "Epoch 243/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1190 - val_loss: 1.2724\n",
            "Epoch 244/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0127 - val_loss: 0.8019\n",
            "Epoch 245/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3898 - val_loss: 0.7990\n",
            "Epoch 246/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.3265 - val_loss: 0.6087\n",
            "Epoch 247/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9275 - val_loss: 0.7504\n",
            "Epoch 248/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1040 - val_loss: 0.6451\n",
            "Epoch 249/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4155 - val_loss: 0.7963\n",
            "Epoch 250/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9787 - val_loss: 0.8487\n",
            "Epoch 251/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3630 - val_loss: 0.8920\n",
            "Epoch 252/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1030 - val_loss: 0.7416\n",
            "Epoch 253/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0773 - val_loss: 0.6918\n",
            "Epoch 254/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4891 - val_loss: 0.7260\n",
            "Epoch 255/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0505 - val_loss: 0.8220\n",
            "Epoch 256/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1453 - val_loss: 1.0576\n",
            "Epoch 257/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0278 - val_loss: 0.6854\n",
            "Epoch 258/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9800 - val_loss: 0.9171\n",
            "Epoch 259/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0825 - val_loss: 0.8070\n",
            "Epoch 260/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0694 - val_loss: 0.7291\n",
            "Epoch 261/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3436 - val_loss: 0.7509\n",
            "Epoch 262/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2007 - val_loss: 0.8974\n",
            "Epoch 263/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0893 - val_loss: 1.1613\n",
            "Epoch 264/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2474 - val_loss: 0.7274\n",
            "Epoch 265/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0415 - val_loss: 0.6969\n",
            "Epoch 266/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1187 - val_loss: 0.8075\n",
            "Epoch 267/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1205 - val_loss: 0.8212\n",
            "Epoch 268/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0843 - val_loss: 1.0071\n",
            "Epoch 269/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.3744 - val_loss: 0.9420\n",
            "Epoch 270/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2006 - val_loss: 0.9841\n",
            "Epoch 271/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0867 - val_loss: 0.8870\n",
            "Epoch 272/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1390 - val_loss: 0.7075\n",
            "Epoch 273/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1331 - val_loss: 0.8638\n",
            "Epoch 274/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0824 - val_loss: 0.6800\n",
            "Epoch 275/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1096 - val_loss: 0.9119\n",
            "Epoch 276/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1275 - val_loss: 0.8000\n",
            "Epoch 277/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0288 - val_loss: 0.7220\n",
            "Epoch 278/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.4317 - val_loss: 0.9218\n",
            "Epoch 279/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6386 - val_loss: 0.8416\n",
            "Epoch 280/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1120 - val_loss: 0.9202\n",
            "Epoch 281/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2059 - val_loss: 0.8917\n",
            "Epoch 282/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1497 - val_loss: 0.8255\n",
            "Epoch 283/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0797 - val_loss: 1.2875\n",
            "Epoch 284/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0248 - val_loss: 0.7845\n",
            "Epoch 285/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0637 - val_loss: 0.7180\n",
            "Epoch 286/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0322 - val_loss: 1.0238\n",
            "Epoch 287/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0539 - val_loss: 0.8270\n",
            "Epoch 288/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9791 - val_loss: 0.9952\n",
            "Epoch 289/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0629 - val_loss: 0.8368\n",
            "Epoch 290/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1651 - val_loss: 0.7580\n",
            "Epoch 291/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1419 - val_loss: 0.9012\n",
            "Epoch 292/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0280 - val_loss: 0.8891\n",
            "Epoch 293/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0604 - val_loss: 0.7008\n",
            "Epoch 294/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0360 - val_loss: 0.9316\n",
            "Epoch 295/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0383 - val_loss: 0.8579\n",
            "Epoch 296/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0818 - val_loss: 0.9224\n",
            "Epoch 297/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1829 - val_loss: 0.7775\n",
            "Epoch 298/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1340 - val_loss: 0.6796\n",
            "Epoch 299/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1002 - val_loss: 0.9639\n",
            "Epoch 300/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0810 - val_loss: 0.9353\n",
            "Epoch 301/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0858 - val_loss: 0.8074\n",
            "Epoch 302/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0072 - val_loss: 1.0052\n",
            "Epoch 303/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1086 - val_loss: 0.8640\n",
            "Epoch 304/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9317 - val_loss: 0.8156\n",
            "Epoch 305/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0238 - val_loss: 0.8759\n",
            "Epoch 306/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1276 - val_loss: 0.6575\n",
            "Epoch 307/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1781 - val_loss: 0.8192\n",
            "Epoch 308/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2197 - val_loss: 0.8522\n",
            "Epoch 309/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0346 - val_loss: 0.8490\n",
            "Epoch 310/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9511 - val_loss: 0.7659\n",
            "Epoch 311/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1927 - val_loss: 0.7260\n",
            "Epoch 312/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0150 - val_loss: 0.6429\n",
            "Epoch 313/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1111 - val_loss: 0.9993\n",
            "Epoch 314/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0588 - val_loss: 0.8241\n",
            "Epoch 315/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9928 - val_loss: 0.8811\n",
            "Epoch 316/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9672 - val_loss: 0.9089\n",
            "Epoch 317/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0882 - val_loss: 0.8404\n",
            "Epoch 318/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1306 - val_loss: 0.6768\n",
            "Epoch 319/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1260 - val_loss: 0.9047\n",
            "Epoch 320/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9974 - val_loss: 0.9376\n",
            "Epoch 321/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0811 - val_loss: 0.7652\n",
            "Epoch 322/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1137 - val_loss: 0.9239\n",
            "Epoch 323/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2575 - val_loss: 0.8315\n",
            "Epoch 324/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0136 - val_loss: 0.6750\n",
            "Epoch 325/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0838 - val_loss: 1.0619\n",
            "Epoch 326/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9956 - val_loss: 0.7175\n",
            "Epoch 327/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0027 - val_loss: 0.6767\n",
            "Epoch 328/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9296 - val_loss: 0.8293\n",
            "Epoch 329/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1781 - val_loss: 0.5800\n",
            "Epoch 330/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9746 - val_loss: 0.6428\n",
            "Epoch 331/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9682 - val_loss: 0.9138\n",
            "Epoch 332/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0389 - val_loss: 0.9137\n",
            "Epoch 333/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9889 - val_loss: 0.8324\n",
            "Epoch 334/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1427 - val_loss: 1.0820\n",
            "Epoch 335/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9675 - val_loss: 0.7420\n",
            "Epoch 336/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0593 - val_loss: 0.9473\n",
            "Epoch 337/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0979 - val_loss: 0.7469\n",
            "Epoch 338/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1111 - val_loss: 0.6706\n",
            "Epoch 339/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4859 - val_loss: 0.9042\n",
            "Epoch 340/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0378 - val_loss: 1.1378\n",
            "Epoch 341/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0954 - val_loss: 0.7361\n",
            "Epoch 342/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1331 - val_loss: 0.7153\n",
            "Epoch 343/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1386 - val_loss: 0.8852\n",
            "Epoch 344/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9336 - val_loss: 0.5893\n",
            "Epoch 345/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0445 - val_loss: 0.8573\n",
            "Epoch 346/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0548 - val_loss: 0.8634\n",
            "Epoch 347/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.2534 - val_loss: 0.6666\n",
            "Epoch 348/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9715 - val_loss: 0.8119\n",
            "Epoch 349/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9873 - val_loss: 0.7805\n",
            "Epoch 350/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1699 - val_loss: 0.8158\n",
            "Epoch 351/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0127 - val_loss: 0.9260\n",
            "Epoch 352/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9891 - val_loss: 0.6400\n",
            "Epoch 353/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0509 - val_loss: 0.6478\n",
            "Epoch 354/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9447 - val_loss: 0.7264\n",
            "Epoch 355/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9799 - val_loss: 0.8581\n",
            "Epoch 356/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9737 - val_loss: 0.9764\n",
            "Epoch 357/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0008 - val_loss: 0.8228\n",
            "Epoch 358/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8764 - val_loss: 0.8399\n",
            "Epoch 359/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9415 - val_loss: 0.8899\n",
            "Epoch 360/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9671 - val_loss: 0.8202\n",
            "Epoch 361/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9448 - val_loss: 0.8360\n",
            "Epoch 362/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9888 - val_loss: 0.8297\n",
            "Epoch 363/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9807 - val_loss: 0.9895\n",
            "Epoch 364/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0079 - val_loss: 0.8853\n",
            "Epoch 365/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8835 - val_loss: 0.9937\n",
            "Epoch 366/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9903 - val_loss: 1.0892\n",
            "Epoch 367/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0750 - val_loss: 0.8871\n",
            "Epoch 368/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0570 - val_loss: 0.7190\n",
            "Epoch 369/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1555 - val_loss: 1.0737\n",
            "Epoch 370/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0995 - val_loss: 0.8926\n",
            "Epoch 371/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9950 - val_loss: 1.1452\n",
            "Epoch 372/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8878 - val_loss: 0.8981\n",
            "Epoch 373/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9787 - val_loss: 0.7022\n",
            "Epoch 374/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1893 - val_loss: 0.5679\n",
            "Epoch 375/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0160 - val_loss: 0.8494\n",
            "Epoch 376/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0057 - val_loss: 0.8507\n",
            "Epoch 377/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0176 - val_loss: 0.8081\n",
            "Epoch 378/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1644 - val_loss: 0.7427\n",
            "Epoch 379/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0481 - val_loss: 0.8024\n",
            "Epoch 380/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0629 - val_loss: 0.8069\n",
            "Epoch 381/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3884 - val_loss: 1.0415\n",
            "Epoch 382/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0203 - val_loss: 0.6987\n",
            "Epoch 383/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0090 - val_loss: 0.7718\n",
            "Epoch 384/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1256 - val_loss: 0.8508\n",
            "Epoch 385/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0262 - val_loss: 0.7082\n",
            "Epoch 386/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9664 - val_loss: 0.8086\n",
            "Epoch 387/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0543 - val_loss: 1.0862\n",
            "Epoch 388/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1641 - val_loss: 0.6442\n",
            "Epoch 389/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0755 - val_loss: 0.6956\n",
            "Epoch 390/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0547 - val_loss: 0.7236\n",
            "Epoch 391/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9023 - val_loss: 0.8011\n",
            "Epoch 392/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0198 - val_loss: 0.7520\n",
            "Epoch 393/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9680 - val_loss: 0.7645\n",
            "Epoch 394/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0759 - val_loss: 0.9269\n",
            "Epoch 395/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0149 - val_loss: 0.7884\n",
            "Epoch 396/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9343 - val_loss: 0.8527\n",
            "Epoch 397/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9627 - val_loss: 0.7737\n",
            "Epoch 398/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0430 - val_loss: 0.8984\n",
            "Epoch 399/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9670 - val_loss: 1.3221\n",
            "Epoch 400/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0724 - val_loss: 0.9883\n",
            "Epoch 401/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9619 - val_loss: 0.7731\n",
            "Epoch 402/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9865 - val_loss: 0.7920\n",
            "Epoch 403/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9638 - val_loss: 1.1408\n",
            "Epoch 404/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0305 - val_loss: 0.7352\n",
            "Epoch 405/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0589 - val_loss: 0.6486\n",
            "Epoch 406/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0124 - val_loss: 0.8584\n",
            "Epoch 407/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0152 - val_loss: 0.9351\n",
            "Epoch 408/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0677 - val_loss: 1.0079\n",
            "Epoch 409/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9601 - val_loss: 1.0338\n",
            "Epoch 410/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1812 - val_loss: 1.0105\n",
            "Epoch 411/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9995 - val_loss: 0.7713\n",
            "Epoch 412/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0081 - val_loss: 0.8924\n",
            "Epoch 413/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0026 - val_loss: 0.8833\n",
            "Epoch 414/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1418 - val_loss: 0.8599\n",
            "Epoch 415/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9370 - val_loss: 0.7469\n",
            "Epoch 416/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0376 - val_loss: 0.8592\n",
            "Epoch 417/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0975 - val_loss: 0.6368\n",
            "Epoch 418/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5016 - val_loss: 0.7414\n",
            "Epoch 419/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8521 - val_loss: 0.7921\n",
            "Epoch 420/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0692 - val_loss: 0.6408\n",
            "Epoch 421/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9549 - val_loss: 0.6605\n",
            "Epoch 422/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9409 - val_loss: 0.6447\n",
            "Epoch 423/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.9167 - val_loss: 0.6978\n",
            "Epoch 424/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9809 - val_loss: 0.6231\n",
            "Epoch 425/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9407 - val_loss: 0.7913\n",
            "Epoch 426/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8964 - val_loss: 0.7009\n",
            "Epoch 427/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0745 - val_loss: 0.7847\n",
            "Epoch 428/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8885 - val_loss: 0.8993\n",
            "Epoch 429/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0862 - val_loss: 0.7394\n",
            "Epoch 430/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0861 - val_loss: 0.8712\n",
            "Epoch 431/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0086 - val_loss: 1.0572\n",
            "Epoch 432/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9669 - val_loss: 0.6461\n",
            "Epoch 433/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0219 - val_loss: 0.8255\n",
            "Epoch 434/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0653 - val_loss: 1.1031\n",
            "Epoch 435/5000\n",
            "33/33 [==============================] - 0s 15ms/step - loss: 1.0097 - val_loss: 0.8098\n",
            "Epoch 436/5000\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.9548 - val_loss: 1.0913\n",
            "Epoch 437/5000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.9988 - val_loss: 0.9081\n",
            "Epoch 438/5000\n",
            "33/33 [==============================] - 0s 11ms/step - loss: 0.9769 - val_loss: 0.7718\n",
            "Epoch 439/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9564 - val_loss: 0.8314\n",
            "Epoch 440/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9157 - val_loss: 0.7303\n",
            "Epoch 441/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9315 - val_loss: 0.6981\n",
            "Epoch 442/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9298 - val_loss: 1.0613\n",
            "Epoch 443/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0497 - val_loss: 0.6988\n",
            "Epoch 444/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9931 - val_loss: 0.9822\n",
            "Epoch 445/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0076 - val_loss: 0.8823\n",
            "Epoch 446/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0281 - val_loss: 0.8076\n",
            "Epoch 447/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9145 - val_loss: 0.9313\n",
            "Epoch 448/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0121 - val_loss: 0.7844\n",
            "Epoch 449/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9304 - val_loss: 0.8450\n",
            "Epoch 450/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0347 - val_loss: 0.7979\n",
            "Epoch 451/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9419 - val_loss: 0.7627\n",
            "Epoch 452/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9041 - val_loss: 0.6671\n",
            "Epoch 453/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9654 - val_loss: 0.8422\n",
            "Epoch 454/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9819 - val_loss: 0.7378\n",
            "Epoch 455/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0343 - val_loss: 0.7505\n",
            "Epoch 456/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9531 - val_loss: 0.8755\n",
            "Epoch 457/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0238 - val_loss: 0.8357\n",
            "Epoch 458/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9131 - val_loss: 0.6475\n",
            "Epoch 459/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9153 - val_loss: 0.7204\n",
            "Epoch 460/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9834 - val_loss: 0.8245\n",
            "Epoch 461/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9423 - val_loss: 0.7408\n",
            "Epoch 462/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2057 - val_loss: 0.8496\n",
            "Epoch 463/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5185 - val_loss: 0.6953\n",
            "Epoch 464/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0951 - val_loss: 0.6917\n",
            "Epoch 465/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9784 - val_loss: 0.8263\n",
            "Epoch 466/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9751 - val_loss: 0.8020\n",
            "Epoch 467/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9407 - val_loss: 0.7048\n",
            "Epoch 468/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0243 - val_loss: 0.7564\n",
            "Epoch 469/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8419 - val_loss: 0.7749\n",
            "Epoch 470/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9534 - val_loss: 0.6996\n",
            "Epoch 471/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9765 - val_loss: 0.6917\n",
            "Epoch 472/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8619 - val_loss: 0.9899\n",
            "Epoch 473/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0412 - val_loss: 0.8802\n",
            "Epoch 474/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9072 - val_loss: 0.7501\n",
            "Epoch 475/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9331 - val_loss: 0.9966\n",
            "Epoch 476/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9881 - val_loss: 0.9963\n",
            "Epoch 477/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9641 - val_loss: 1.0416\n",
            "Epoch 478/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9401 - val_loss: 0.7739\n",
            "Epoch 479/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1190 - val_loss: 0.6903\n",
            "Epoch 480/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9021 - val_loss: 0.8566\n",
            "Epoch 481/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9692 - val_loss: 0.8752\n",
            "Epoch 482/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8966 - val_loss: 0.8729\n",
            "Epoch 483/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0257 - val_loss: 0.8762\n",
            "Epoch 484/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9323 - val_loss: 0.7333\n",
            "Epoch 485/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8996 - val_loss: 0.8016\n",
            "Epoch 486/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0024 - val_loss: 0.6486\n",
            "Epoch 487/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0268 - val_loss: 0.8532\n",
            "Epoch 488/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1674 - val_loss: 0.7977\n",
            "Epoch 489/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9138 - val_loss: 0.6412\n",
            "Epoch 490/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8689 - val_loss: 0.8718\n",
            "Epoch 491/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9072 - val_loss: 0.7598\n",
            "Epoch 492/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8992 - val_loss: 0.8267\n",
            "Epoch 493/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9396 - val_loss: 0.7546\n",
            "Epoch 494/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3481 - val_loss: 0.7451\n",
            "Epoch 495/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8677 - val_loss: 0.8268\n",
            "Epoch 496/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9254 - val_loss: 0.7783\n",
            "Epoch 497/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9056 - val_loss: 0.7962\n",
            "Epoch 498/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8968 - val_loss: 0.7835\n",
            "Epoch 499/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9972 - val_loss: 0.6171\n",
            "Epoch 500/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9511 - val_loss: 0.7732\n",
            "Epoch 501/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0000 - val_loss: 0.8479\n",
            "Epoch 502/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9287 - val_loss: 0.8402\n",
            "Epoch 503/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9336 - val_loss: 0.8706\n",
            "Epoch 504/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9698 - val_loss: 0.9971\n",
            "Epoch 505/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9716 - val_loss: 0.9014\n",
            "Epoch 506/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9615 - val_loss: 0.8394\n",
            "Epoch 507/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9351 - val_loss: 0.9052\n",
            "Epoch 508/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9822 - val_loss: 0.7812\n",
            "Epoch 509/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9265 - val_loss: 0.8197\n",
            "Epoch 510/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9687 - val_loss: 0.8936\n",
            "Epoch 511/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8976 - val_loss: 0.6973\n",
            "Epoch 512/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9567 - val_loss: 0.8987\n",
            "Epoch 513/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0320 - val_loss: 0.7458\n",
            "Epoch 514/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0188 - val_loss: 0.9284\n",
            "Epoch 515/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9901 - val_loss: 1.0214\n",
            "Epoch 516/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.8851 - val_loss: 0.9322\n",
            "Epoch 517/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9237 - val_loss: 0.7447\n",
            "Epoch 518/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9724 - val_loss: 0.7909\n",
            "Epoch 519/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8910 - val_loss: 0.8606\n",
            "Epoch 520/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0167 - val_loss: 0.9083\n",
            "Epoch 521/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6403 - val_loss: 0.7932\n",
            "Epoch 522/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9397 - val_loss: 0.7687\n",
            "Epoch 523/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0284 - val_loss: 0.8511\n",
            "Epoch 524/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0114 - val_loss: 0.6094\n",
            "Epoch 525/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0403 - val_loss: 0.8543\n",
            "Epoch 526/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9850 - val_loss: 0.7151\n",
            "Epoch 527/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1642 - val_loss: 0.8297\n",
            "Epoch 528/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9627 - val_loss: 0.8601\n",
            "Epoch 529/5000\n",
            "33/33 [==============================] - 0s 12ms/step - loss: 0.9880 - val_loss: 0.7935\n",
            "Epoch 530/5000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.9804 - val_loss: 0.8049\n",
            "Epoch 531/5000\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 0.9477 - val_loss: 0.8404\n",
            "Epoch 532/5000\n",
            "33/33 [==============================] - 0s 11ms/step - loss: 0.8842 - val_loss: 0.8307\n",
            "Epoch 533/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9609 - val_loss: 0.8159\n",
            "Epoch 534/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0554 - val_loss: 0.6641\n",
            "Epoch 535/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9903 - val_loss: 0.7603\n",
            "Epoch 536/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8593 - val_loss: 0.7900\n",
            "Epoch 537/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9292 - val_loss: 0.7978\n",
            "Epoch 538/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0515 - val_loss: 0.8043\n",
            "Epoch 539/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0225 - val_loss: 0.8074\n",
            "Epoch 540/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9431 - val_loss: 0.9356\n",
            "Epoch 541/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9598 - val_loss: 0.9119\n",
            "Epoch 542/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1340 - val_loss: 0.9011\n",
            "Epoch 543/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0550 - val_loss: 0.7929\n",
            "Epoch 544/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0313 - val_loss: 0.8371\n",
            "Epoch 545/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9518 - val_loss: 0.8050\n",
            "Epoch 546/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9825 - val_loss: 0.9305\n",
            "Epoch 547/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8958 - val_loss: 0.7653\n",
            "Epoch 548/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9501 - val_loss: 1.0713\n",
            "Epoch 549/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8580 - val_loss: 0.7516\n",
            "Epoch 550/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8925 - val_loss: 0.8577\n",
            "Epoch 551/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9361 - val_loss: 0.9364\n",
            "Epoch 552/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9573 - val_loss: 0.7114\n",
            "Epoch 553/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0527 - val_loss: 0.8024\n",
            "Epoch 554/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9500 - val_loss: 0.7968\n",
            "Epoch 555/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8001 - val_loss: 0.7500\n",
            "Epoch 556/5000\n",
            "33/33 [==============================] - 0s 13ms/step - loss: 1.1656 - val_loss: 0.9611\n",
            "Epoch 557/5000\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.9601 - val_loss: 0.9267\n",
            "Epoch 558/5000\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 0.9168 - val_loss: 0.8065\n",
            "Epoch 559/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9066 - val_loss: 0.8865\n",
            "Epoch 560/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9412 - val_loss: 0.7407\n",
            "Epoch 561/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9827 - val_loss: 0.7310\n",
            "Epoch 562/5000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.9878 - val_loss: 0.7319\n",
            "Epoch 563/5000\n",
            "33/33 [==============================] - 0s 13ms/step - loss: 1.0608 - val_loss: 0.8155\n",
            "Epoch 564/5000\n",
            "33/33 [==============================] - 0s 11ms/step - loss: 0.9273 - val_loss: 0.7785\n",
            "Epoch 565/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0139 - val_loss: 0.8250\n",
            "Epoch 566/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9245 - val_loss: 0.7128\n",
            "Epoch 567/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9826 - val_loss: 1.0017\n",
            "Epoch 568/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9287 - val_loss: 0.8401\n",
            "Epoch 569/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9466 - val_loss: 0.6836\n",
            "Epoch 570/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9550 - val_loss: 0.7764\n",
            "Epoch 571/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9719 - val_loss: 0.8222\n",
            "Epoch 572/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0156 - val_loss: 0.7317\n",
            "Epoch 573/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9286 - val_loss: 0.8258\n",
            "Epoch 574/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9012 - val_loss: 0.7503\n",
            "Epoch 575/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9041 - val_loss: 0.9052\n",
            "Epoch 576/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9160 - val_loss: 0.8703\n",
            "Epoch 577/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9682 - val_loss: 0.7041\n",
            "Epoch 578/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9013 - val_loss: 0.8675\n",
            "Epoch 579/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9239 - val_loss: 0.7370\n",
            "Epoch 580/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9404 - val_loss: 0.8601\n",
            "Epoch 581/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0018 - val_loss: 0.6892\n",
            "Epoch 582/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0865 - val_loss: 0.8616\n",
            "Epoch 583/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0299 - val_loss: 0.7735\n",
            "Epoch 584/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9483 - val_loss: 0.8442\n",
            "Epoch 585/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8625 - val_loss: 0.7753\n",
            "Epoch 586/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0221 - val_loss: 0.7401\n",
            "Epoch 587/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9978 - val_loss: 0.9046\n",
            "Epoch 588/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9452 - val_loss: 0.7374\n",
            "Epoch 589/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8712 - val_loss: 0.6640\n",
            "Epoch 590/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9071 - val_loss: 0.6897\n",
            "Epoch 591/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1226 - val_loss: 0.7071\n",
            "Epoch 592/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9621 - val_loss: 0.7290\n",
            "Epoch 593/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1307 - val_loss: 0.8387\n",
            "Epoch 594/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0740 - val_loss: 0.6128\n",
            "Epoch 595/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9140 - val_loss: 0.8144\n",
            "Epoch 596/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0372 - val_loss: 0.6369\n",
            "Epoch 597/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9668 - val_loss: 0.8941\n",
            "Epoch 598/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9522 - val_loss: 0.8126\n",
            "Epoch 599/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9417 - val_loss: 0.9354\n",
            "Epoch 600/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8759 - val_loss: 0.8495\n",
            "Epoch 601/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9400 - val_loss: 0.7985\n",
            "Epoch 602/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0296 - val_loss: 0.7205\n",
            "Epoch 603/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9921 - val_loss: 0.8587\n",
            "Epoch 604/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9186 - val_loss: 0.8007\n",
            "Epoch 605/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9262 - val_loss: 0.7331\n",
            "Epoch 606/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9706 - val_loss: 0.8334\n",
            "Epoch 607/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8975 - val_loss: 0.8325\n",
            "Epoch 608/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9179 - val_loss: 0.7465\n",
            "Epoch 609/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9719 - val_loss: 0.7703\n",
            "Epoch 610/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9982 - val_loss: 0.7166\n",
            "Epoch 611/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8905 - val_loss: 0.7370\n",
            "Epoch 612/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1121 - val_loss: 0.8178\n",
            "Epoch 613/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8923 - val_loss: 0.8966\n",
            "Epoch 614/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0308 - val_loss: 0.8565\n",
            "Epoch 615/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0986 - val_loss: 0.8070\n",
            "Epoch 616/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9147 - val_loss: 0.8171\n",
            "Epoch 617/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9607 - val_loss: 0.9205\n",
            "Epoch 618/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9156 - val_loss: 0.6990\n",
            "Epoch 619/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0961 - val_loss: 0.9614\n",
            "Epoch 620/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8880 - val_loss: 0.8810\n",
            "Epoch 621/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9103 - val_loss: 0.7381\n",
            "Epoch 622/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9864 - val_loss: 0.6885\n",
            "Epoch 623/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9034 - val_loss: 0.8332\n",
            "Epoch 624/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3335 - val_loss: 0.7885\n",
            "Epoch 625/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9784 - val_loss: 0.9414\n",
            "Epoch 626/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9650 - val_loss: 0.7348\n",
            "Epoch 627/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9686 - val_loss: 0.9170\n",
            "Epoch 628/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8999 - val_loss: 0.6897\n",
            "Epoch 629/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9794 - val_loss: 0.7297\n",
            "Epoch 630/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9132 - val_loss: 0.7706\n",
            "Epoch 631/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8679 - val_loss: 0.7221\n",
            "Epoch 632/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9157 - val_loss: 0.8895\n",
            "Epoch 633/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8839 - val_loss: 0.7270\n",
            "Epoch 634/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0277 - val_loss: 0.9313\n",
            "Epoch 635/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9542 - val_loss: 0.8455\n",
            "Epoch 636/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9578 - val_loss: 0.6219\n",
            "Epoch 637/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9492 - val_loss: 0.7001\n",
            "Epoch 638/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9460 - val_loss: 0.8787\n",
            "Epoch 639/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1566 - val_loss: 0.7712\n",
            "Epoch 640/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8781 - val_loss: 0.6765\n",
            "Epoch 641/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8752 - val_loss: 0.8021\n",
            "Epoch 642/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9148 - val_loss: 0.8190\n",
            "Epoch 643/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9755 - val_loss: 0.7413\n",
            "Epoch 644/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9299 - val_loss: 0.7496\n",
            "Epoch 645/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9936 - val_loss: 0.7058\n",
            "Epoch 646/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8972 - val_loss: 0.7655\n",
            "Epoch 647/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9882 - val_loss: 1.1431\n",
            "Epoch 648/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8848 - val_loss: 0.6477\n",
            "Epoch 649/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9181 - val_loss: 0.7031\n",
            "Epoch 650/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9967 - val_loss: 0.8349\n",
            "Epoch 651/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8604 - val_loss: 0.9113\n",
            "Epoch 652/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8693 - val_loss: 0.8223\n",
            "Epoch 653/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9310 - val_loss: 0.7175\n",
            "Epoch 654/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9554 - val_loss: 0.8327\n",
            "Epoch 655/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0203 - val_loss: 0.7462\n",
            "Epoch 656/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8889 - val_loss: 0.7709\n",
            "Epoch 657/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9829 - val_loss: 0.9805\n",
            "Epoch 658/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0179 - val_loss: 0.9236\n",
            "Epoch 659/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9843 - val_loss: 0.9614\n",
            "Epoch 660/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2077 - val_loss: 0.9639\n",
            "Epoch 661/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9677 - val_loss: 1.1220\n",
            "Epoch 662/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9451 - val_loss: 0.9181\n",
            "Epoch 663/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1099 - val_loss: 0.8495\n",
            "Epoch 664/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9524 - val_loss: 0.7016\n",
            "Epoch 665/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9804 - val_loss: 0.7791\n",
            "Epoch 666/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9123 - val_loss: 0.7630\n",
            "Epoch 667/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0739 - val_loss: 0.9431\n",
            "Epoch 668/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9675 - val_loss: 0.7839\n",
            "Epoch 669/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8850 - val_loss: 0.6669\n",
            "Epoch 670/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1674 - val_loss: 0.7588\n",
            "Epoch 671/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8892 - val_loss: 0.7171\n",
            "Epoch 672/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9369 - val_loss: 0.7188\n",
            "Epoch 673/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0655 - val_loss: 0.7036\n",
            "Epoch 674/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8723 - val_loss: 0.8307\n",
            "Epoch 675/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9071 - val_loss: 0.7034\n",
            "Epoch 676/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9958 - val_loss: 0.9389\n",
            "Epoch 677/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9802 - val_loss: 1.1973\n",
            "Epoch 678/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9147 - val_loss: 0.8257\n",
            "Epoch 679/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8796 - val_loss: 0.7643\n",
            "Epoch 680/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0836 - val_loss: 0.7854\n",
            "Epoch 681/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9100 - val_loss: 0.8408\n",
            "Epoch 682/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0598 - val_loss: 0.9252\n",
            "Epoch 683/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9569 - val_loss: 0.8089\n",
            "Epoch 684/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8811 - val_loss: 0.8404\n",
            "Epoch 685/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9601 - val_loss: 0.7645\n",
            "Epoch 686/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9285 - val_loss: 0.7034\n",
            "Epoch 687/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9330 - val_loss: 0.6847\n",
            "Epoch 688/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9081 - val_loss: 0.9260\n",
            "Epoch 689/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8629 - val_loss: 0.9607\n",
            "Epoch 690/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0148 - val_loss: 0.9878\n",
            "Epoch 691/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9709 - val_loss: 0.6998\n",
            "Epoch 692/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0464 - val_loss: 0.8124\n",
            "Epoch 693/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8718 - val_loss: 0.8004\n",
            "Epoch 694/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8263 - val_loss: 0.6069\n",
            "Epoch 695/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9867 - val_loss: 0.6396\n",
            "Epoch 696/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0531 - val_loss: 0.7729\n",
            "Epoch 697/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9341 - val_loss: 0.7434\n",
            "Epoch 698/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8611 - val_loss: 0.8184\n",
            "Epoch 699/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9801 - val_loss: 0.7681\n",
            "Epoch 700/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9328 - val_loss: 0.7283\n",
            "Epoch 701/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9667 - val_loss: 0.8957\n",
            "Epoch 702/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8755 - val_loss: 0.8343\n",
            "Epoch 703/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9633 - val_loss: 0.8004\n",
            "Epoch 704/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9048 - val_loss: 0.8437\n",
            "Epoch 705/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9283 - val_loss: 0.7693\n",
            "Epoch 706/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0613 - val_loss: 1.0534\n",
            "Epoch 707/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9167 - val_loss: 0.7207\n",
            "Epoch 708/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9180 - val_loss: 0.7234\n",
            "Epoch 709/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9074 - val_loss: 0.6644\n",
            "Epoch 710/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9378 - val_loss: 0.8841\n",
            "Epoch 711/5000\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.9784 - val_loss: 0.8141\n",
            "Epoch 712/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9344 - val_loss: 0.6714\n",
            "Epoch 713/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9565 - val_loss: 0.7445\n",
            "Epoch 714/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.8638 - val_loss: 0.6910\n",
            "Epoch 715/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9556 - val_loss: 0.8419\n",
            "Epoch 716/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9392 - val_loss: 0.9295\n",
            "Epoch 717/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0428 - val_loss: 0.6602\n",
            "Epoch 718/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8849 - val_loss: 0.6897\n",
            "Epoch 719/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9606 - val_loss: 0.6727\n",
            "Epoch 720/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8836 - val_loss: 0.7462\n",
            "Epoch 721/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0055 - val_loss: 0.7666\n",
            "Epoch 722/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9332 - val_loss: 0.6996\n",
            "Epoch 723/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9203 - val_loss: 0.8009\n",
            "Epoch 724/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9277 - val_loss: 0.7660\n",
            "Epoch 725/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0548 - val_loss: 0.7104\n",
            "Epoch 726/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9013 - val_loss: 0.6815\n",
            "Epoch 727/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8733 - val_loss: 0.8043\n",
            "Epoch 728/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9803 - val_loss: 0.7466\n",
            "Epoch 729/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9012 - val_loss: 0.7535\n",
            "Epoch 730/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0318 - val_loss: 0.7589\n",
            "Epoch 731/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9454 - val_loss: 0.9241\n",
            "Epoch 732/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9852 - val_loss: 0.7891\n",
            "Epoch 733/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8403 - val_loss: 0.6168\n",
            "Epoch 734/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0186 - val_loss: 0.9010\n",
            "Epoch 735/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8406 - val_loss: 0.8646\n",
            "Epoch 736/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9376 - val_loss: 0.8964\n",
            "Epoch 737/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0686 - val_loss: 0.7752\n",
            "Epoch 738/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0046 - val_loss: 0.6694\n",
            "Epoch 739/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9540 - val_loss: 0.6580\n",
            "Epoch 740/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9700 - val_loss: 0.8934\n",
            "Epoch 741/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8969 - val_loss: 0.8012\n",
            "Epoch 742/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8964 - val_loss: 0.7377\n",
            "Epoch 743/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9219 - val_loss: 0.7350\n",
            "Epoch 744/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9273 - val_loss: 0.6381\n",
            "Epoch 745/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9158 - val_loss: 0.8561\n",
            "Epoch 746/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9686 - val_loss: 0.7178\n",
            "Epoch 747/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1226 - val_loss: 1.0841\n",
            "Epoch 748/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8871 - val_loss: 0.7077\n",
            "Epoch 749/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9436 - val_loss: 0.8665\n",
            "Epoch 750/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9390 - val_loss: 0.7978\n",
            "Epoch 751/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9168 - val_loss: 0.7145\n",
            "Epoch 752/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0839 - val_loss: 0.8515\n",
            "Epoch 753/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9971 - val_loss: 0.7931\n",
            "Epoch 754/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9906 - val_loss: 0.6690\n",
            "Epoch 755/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8392 - val_loss: 0.8704\n",
            "Epoch 756/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1030 - val_loss: 0.9608\n",
            "Epoch 757/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0336 - val_loss: 0.8122\n",
            "Epoch 758/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9245 - val_loss: 0.6989\n",
            "Epoch 759/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8650 - val_loss: 0.6676\n",
            "Epoch 760/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9617 - val_loss: 0.7010\n",
            "Epoch 761/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9510 - val_loss: 0.7549\n",
            "Epoch 762/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9215 - val_loss: 0.8204\n",
            "Epoch 763/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1656 - val_loss: 0.7342\n",
            "Epoch 764/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0102 - val_loss: 0.6176\n",
            "Epoch 765/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9186 - val_loss: 0.6635\n",
            "Epoch 766/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8763 - val_loss: 0.6860\n",
            "Epoch 767/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9072 - val_loss: 0.7793\n",
            "Epoch 768/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8480 - val_loss: 0.7470\n",
            "Epoch 769/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9562 - val_loss: 0.6939\n",
            "Epoch 770/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0522 - val_loss: 0.8384\n",
            "Epoch 771/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8491 - val_loss: 0.7870\n",
            "Epoch 772/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9538 - val_loss: 0.8777\n",
            "Epoch 773/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9298 - val_loss: 0.6503\n",
            "Epoch 774/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8935 - val_loss: 0.8654\n",
            "Epoch 775/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9314 - val_loss: 0.8412\n",
            "Epoch 776/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9578 - val_loss: 0.7952\n",
            "Epoch 777/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9472 - val_loss: 0.7467\n",
            "Epoch 778/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9271 - val_loss: 0.8103\n",
            "Epoch 779/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9455 - val_loss: 0.7105\n",
            "Epoch 780/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8507 - val_loss: 0.8551\n",
            "Epoch 781/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9726 - val_loss: 0.6955\n",
            "Epoch 782/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9023 - val_loss: 0.8378\n",
            "Epoch 783/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8696 - val_loss: 0.7699\n",
            "Epoch 784/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9802 - val_loss: 0.7818\n",
            "Epoch 785/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9130 - val_loss: 0.6820\n",
            "Epoch 786/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9740 - val_loss: 0.7738\n",
            "Epoch 787/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1572 - val_loss: 0.7784\n",
            "Epoch 788/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9859 - val_loss: 0.7004\n",
            "Epoch 789/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9283 - val_loss: 0.8781\n",
            "Epoch 790/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9104 - val_loss: 0.8819\n",
            "Epoch 791/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8745 - val_loss: 0.7440\n",
            "Epoch 792/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9187 - val_loss: 0.7907\n",
            "Epoch 793/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9046 - val_loss: 0.8090\n",
            "Epoch 794/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8879 - val_loss: 1.0064\n",
            "Epoch 795/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9316 - val_loss: 0.7864\n",
            "Epoch 796/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0698 - val_loss: 0.8680\n",
            "Epoch 797/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9910 - val_loss: 0.6973\n",
            "Epoch 798/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9134 - val_loss: 0.7407\n",
            "Epoch 799/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8704 - val_loss: 0.7046\n",
            "Epoch 800/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9653 - val_loss: 0.5706\n",
            "Epoch 801/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9006 - val_loss: 0.6924\n",
            "Epoch 802/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9153 - val_loss: 0.7363\n",
            "Epoch 803/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9552 - val_loss: 0.7371\n",
            "Epoch 804/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8796 - val_loss: 0.9342\n",
            "Epoch 805/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9788 - val_loss: 0.8094\n",
            "Epoch 806/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9588 - val_loss: 0.7572\n",
            "Epoch 807/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9112 - val_loss: 0.6957\n",
            "Epoch 808/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8796 - val_loss: 1.0872\n",
            "Epoch 809/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0141 - val_loss: 0.7902\n",
            "Epoch 810/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9141 - val_loss: 0.6122\n",
            "Epoch 811/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9299 - val_loss: 0.8076\n",
            "Epoch 812/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9740 - val_loss: 0.6171\n",
            "Epoch 813/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9552 - val_loss: 0.7968\n",
            "Epoch 814/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9040 - val_loss: 0.7919\n",
            "Epoch 815/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9439 - val_loss: 0.7858\n",
            "Epoch 816/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9121 - val_loss: 0.7921\n",
            "Epoch 817/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9917 - val_loss: 0.7146\n",
            "Epoch 818/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9193 - val_loss: 0.6129\n",
            "Epoch 819/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9343 - val_loss: 0.6929\n",
            "Epoch 820/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9055 - val_loss: 0.8442\n",
            "Epoch 821/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8535 - val_loss: 0.9837\n",
            "Epoch 822/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9637 - val_loss: 0.6973\n",
            "Epoch 823/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9869 - val_loss: 0.8667\n",
            "Epoch 824/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9631 - val_loss: 0.6803\n",
            "Epoch 825/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8843 - val_loss: 0.8587\n",
            "Epoch 826/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0141 - val_loss: 0.7877\n",
            "Epoch 827/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9124 - val_loss: 0.6046\n",
            "Epoch 828/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0643 - val_loss: 0.7036\n",
            "Epoch 829/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9065 - val_loss: 0.6976\n",
            "Epoch 830/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9955 - val_loss: 0.7282\n",
            "Epoch 831/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9443 - val_loss: 0.7187\n",
            "Epoch 832/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8878 - val_loss: 0.8132\n",
            "Epoch 833/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9429 - val_loss: 0.8728\n",
            "Epoch 834/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9852 - val_loss: 0.8635\n",
            "Epoch 835/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8979 - val_loss: 0.7442\n",
            "Epoch 836/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9319 - val_loss: 0.8533\n",
            "Epoch 837/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9710 - val_loss: 0.7834\n",
            "Epoch 838/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8801 - val_loss: 0.6428\n",
            "Epoch 839/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9001 - val_loss: 1.0635\n",
            "Epoch 840/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9511 - val_loss: 0.6770\n",
            "Epoch 841/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9193 - val_loss: 0.7282\n",
            "Epoch 842/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8910 - val_loss: 0.8290\n",
            "Epoch 843/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8941 - val_loss: 0.7961\n",
            "Epoch 844/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9570 - val_loss: 0.7139\n",
            "Epoch 845/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9994 - val_loss: 0.8164\n",
            "Epoch 846/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9491 - val_loss: 0.8812\n",
            "Epoch 847/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0238 - val_loss: 0.8997\n",
            "Epoch 848/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9195 - val_loss: 0.9119\n",
            "Epoch 849/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8925 - val_loss: 0.7313\n",
            "Epoch 850/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9375 - val_loss: 0.7031\n",
            "Epoch 851/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0558 - val_loss: 0.8138\n",
            "Epoch 852/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8963 - val_loss: 0.6817\n",
            "Epoch 853/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8255 - val_loss: 0.7464\n",
            "Epoch 854/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8743 - val_loss: 0.7962\n",
            "Epoch 855/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9718 - val_loss: 0.7616\n",
            "Epoch 856/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0280 - val_loss: 0.8698\n",
            "Epoch 857/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0129 - val_loss: 0.6441\n",
            "Epoch 858/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8967 - val_loss: 0.6222\n",
            "Epoch 859/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9735 - val_loss: 0.7289\n",
            "Epoch 860/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9099 - val_loss: 0.8332\n",
            "Epoch 861/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9273 - val_loss: 0.9484\n",
            "Epoch 862/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9518 - val_loss: 0.7977\n",
            "Epoch 863/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9236 - val_loss: 0.7397\n",
            "Epoch 864/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9260 - val_loss: 0.6434\n",
            "Epoch 865/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9370 - val_loss: 0.7844\n",
            "Epoch 866/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9174 - val_loss: 0.6373\n",
            "Epoch 867/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9461 - val_loss: 0.8711\n",
            "Epoch 868/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8757 - val_loss: 0.8619\n",
            "Epoch 869/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0221 - val_loss: 0.8402\n",
            "Epoch 870/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9342 - val_loss: 0.7789\n",
            "Epoch 871/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9972 - val_loss: 0.7088\n",
            "Epoch 872/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9317 - val_loss: 0.8179\n",
            "Epoch 873/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0297 - val_loss: 0.7116\n",
            "Epoch 874/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9069 - val_loss: 0.7472\n",
            "Epoch 875/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8725 - val_loss: 0.8268\n",
            "Epoch 876/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8845 - val_loss: 0.9583\n",
            "Epoch 877/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8999 - val_loss: 0.7040\n",
            "Epoch 878/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8418 - val_loss: 0.7217\n",
            "Epoch 879/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8792 - val_loss: 0.7867\n",
            "Epoch 880/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8997 - val_loss: 0.6788\n",
            "Epoch 881/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9240 - val_loss: 0.9379\n",
            "Epoch 882/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9235 - val_loss: 0.7277\n",
            "Epoch 883/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8939 - val_loss: 0.8038\n",
            "Epoch 884/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9349 - val_loss: 0.7676\n",
            "Epoch 885/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8297 - val_loss: 0.7215\n",
            "Epoch 886/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8619 - val_loss: 0.7274\n",
            "Epoch 887/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0422 - val_loss: 0.6794\n",
            "Epoch 888/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9597 - val_loss: 0.7023\n",
            "Epoch 889/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9595 - val_loss: 0.7313\n",
            "Epoch 890/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8882 - val_loss: 0.7908\n",
            "Epoch 891/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9345 - val_loss: 0.7432\n",
            "Epoch 892/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0175 - val_loss: 0.6869\n",
            "Epoch 893/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9198 - val_loss: 0.8308\n",
            "Epoch 894/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9948 - val_loss: 0.6764\n",
            "Epoch 895/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8661 - val_loss: 0.5854\n",
            "Epoch 896/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0608 - val_loss: 0.7243\n",
            "Epoch 897/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9137 - val_loss: 0.9111\n",
            "Epoch 898/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9260 - val_loss: 0.8121\n",
            "Epoch 899/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9274 - val_loss: 0.8233\n",
            "Epoch 900/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1023 - val_loss: 0.7762\n",
            "Epoch 901/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9818 - val_loss: 0.8372\n",
            "Epoch 902/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9292 - val_loss: 0.7523\n",
            "Epoch 903/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9576 - val_loss: 1.0038\n",
            "Epoch 904/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9396 - val_loss: 0.6191\n",
            "Epoch 905/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9580 - val_loss: 0.7513\n",
            "Epoch 906/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9106 - val_loss: 0.7108\n",
            "Epoch 907/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9981 - val_loss: 0.7028\n",
            "Epoch 908/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9622 - val_loss: 0.8606\n",
            "Epoch 909/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9032 - val_loss: 0.6000\n",
            "Epoch 910/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8860 - val_loss: 0.6357\n",
            "Epoch 911/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9932 - val_loss: 0.8107\n",
            "Epoch 912/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9846 - val_loss: 0.7505\n",
            "Epoch 913/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8995 - val_loss: 0.7589\n",
            "Epoch 914/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9857 - val_loss: 0.6809\n",
            "Epoch 915/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8676 - val_loss: 0.7991\n",
            "Epoch 916/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8592 - val_loss: 0.7639\n",
            "Epoch 917/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9738 - val_loss: 0.9084\n",
            "Epoch 918/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9871 - val_loss: 0.6731\n",
            "Epoch 919/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9161 - val_loss: 0.9837\n",
            "Epoch 920/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9785 - val_loss: 0.8491\n",
            "Epoch 921/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0197 - val_loss: 1.1761\n",
            "Epoch 922/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9563 - val_loss: 0.9884\n",
            "Epoch 923/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9048 - val_loss: 0.7683\n",
            "Epoch 924/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8891 - val_loss: 0.6972\n",
            "Epoch 925/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9418 - val_loss: 0.7042\n",
            "Epoch 926/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9255 - val_loss: 0.9468\n",
            "Epoch 927/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9510 - val_loss: 0.7246\n",
            "Epoch 928/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9146 - val_loss: 0.8887\n",
            "Epoch 929/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9662 - val_loss: 0.7186\n",
            "Epoch 930/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0138 - val_loss: 0.7887\n",
            "Epoch 931/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9754 - val_loss: 0.9074\n",
            "Epoch 932/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9378 - val_loss: 0.7799\n",
            "Epoch 933/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9140 - val_loss: 0.7430\n",
            "Epoch 934/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9143 - val_loss: 0.9668\n",
            "Epoch 935/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8266 - val_loss: 0.8220\n",
            "Epoch 936/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8862 - val_loss: 0.7388\n",
            "Epoch 937/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9702 - val_loss: 0.7456\n",
            "Epoch 938/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9481 - val_loss: 0.6483\n",
            "Epoch 939/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8589 - val_loss: 0.7186\n",
            "Epoch 940/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9265 - val_loss: 0.7686\n",
            "Epoch 941/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9304 - val_loss: 0.7180\n",
            "Epoch 942/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9428 - val_loss: 0.6957\n",
            "Epoch 943/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8810 - val_loss: 0.7441\n",
            "Epoch 944/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0045 - val_loss: 0.6560\n",
            "Epoch 945/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9565 - val_loss: 0.7764\n",
            "Epoch 946/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8972 - val_loss: 0.8099\n",
            "Epoch 947/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9813 - val_loss: 0.7917\n",
            "Epoch 948/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9368 - val_loss: 0.7309\n",
            "Epoch 949/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9914 - val_loss: 0.6152\n",
            "Epoch 950/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8809 - val_loss: 0.8677\n",
            "Epoch 951/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9341 - val_loss: 0.7678\n",
            "Epoch 952/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8677 - val_loss: 0.7612\n",
            "Epoch 953/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8993 - val_loss: 0.7091\n",
            "Epoch 954/5000\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 0.9174 - val_loss: 0.7863\n",
            "Epoch 955/5000\n",
            "33/33 [==============================] - 1s 16ms/step - loss: 0.9263 - val_loss: 0.6867\n",
            "Epoch 956/5000\n",
            "33/33 [==============================] - 0s 11ms/step - loss: 0.8877 - val_loss: 0.7148\n",
            "Epoch 957/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9365 - val_loss: 0.7218\n",
            "Epoch 958/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9037 - val_loss: 0.7341\n",
            "Epoch 959/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9910 - val_loss: 0.6715\n",
            "Epoch 960/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9646 - val_loss: 0.6838\n",
            "Epoch 961/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9062 - val_loss: 0.7540\n",
            "Epoch 962/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8697 - val_loss: 0.7284\n",
            "Epoch 963/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9127 - val_loss: 0.6061\n",
            "Epoch 964/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9360 - val_loss: 0.7156\n",
            "Epoch 965/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9455 - val_loss: 0.6538\n",
            "Epoch 966/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9128 - val_loss: 0.6634\n",
            "Epoch 967/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9018 - val_loss: 0.7075\n",
            "Epoch 968/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9872 - val_loss: 0.7394\n",
            "Epoch 969/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8792 - val_loss: 0.6367\n",
            "Epoch 970/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9120 - val_loss: 0.8433\n",
            "Epoch 971/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0360 - val_loss: 0.7857\n",
            "Epoch 972/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8803 - val_loss: 0.7752\n",
            "Epoch 973/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0183 - val_loss: 0.7360\n",
            "Epoch 974/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9609 - val_loss: 0.6195\n",
            "Epoch 975/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0263 - val_loss: 0.8068\n",
            "Epoch 976/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9322 - val_loss: 0.6470\n",
            "Epoch 977/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8659 - val_loss: 0.5964\n",
            "Epoch 978/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9264 - val_loss: 0.7079\n",
            "Epoch 979/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9274 - val_loss: 0.7625\n",
            "Epoch 980/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8479 - val_loss: 0.7983\n",
            "Epoch 981/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9409 - val_loss: 0.6553\n",
            "Epoch 982/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8429 - val_loss: 0.8736\n",
            "Epoch 983/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0057 - val_loss: 0.7657\n",
            "Epoch 984/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0042 - val_loss: 0.7675\n",
            "Epoch 985/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8898 - val_loss: 0.8851\n",
            "Epoch 986/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8275 - val_loss: 0.6907\n",
            "Epoch 987/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9088 - val_loss: 0.8612\n",
            "Epoch 988/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8494 - val_loss: 0.7121\n",
            "Epoch 989/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9196 - val_loss: 0.7106\n",
            "Epoch 990/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9980 - val_loss: 0.7815\n",
            "Epoch 991/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8820 - val_loss: 0.6724\n",
            "Epoch 992/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9443 - val_loss: 0.8582\n",
            "Epoch 993/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0473 - val_loss: 0.7888\n",
            "Epoch 994/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8725 - val_loss: 0.7190\n",
            "Epoch 995/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9630 - val_loss: 0.6695\n",
            "Epoch 996/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8786 - val_loss: 0.6153\n",
            "Epoch 997/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0199 - val_loss: 0.6438\n",
            "Epoch 998/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9237 - val_loss: 0.7146\n",
            "Epoch 999/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9623 - val_loss: 0.7423\n",
            "Epoch 1000/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9640 - val_loss: 0.7032\n",
            "Epoch 1001/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0645 - val_loss: 1.0249\n",
            "Epoch 1002/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8704 - val_loss: 0.8336\n",
            "Epoch 1003/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9770 - val_loss: 0.7256\n",
            "Epoch 1004/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8810 - val_loss: 0.7011\n",
            "Epoch 1005/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8753 - val_loss: 0.6534\n",
            "Epoch 1006/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9660 - val_loss: 0.7416\n",
            "Epoch 1007/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9269 - val_loss: 0.7719\n",
            "Epoch 1008/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9598 - val_loss: 1.0090\n",
            "Epoch 1009/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9404 - val_loss: 0.7359\n",
            "Epoch 1010/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0403 - val_loss: 0.7975\n",
            "Epoch 1011/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8421 - val_loss: 0.8260\n",
            "Epoch 1012/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9120 - val_loss: 0.8331\n",
            "Epoch 1013/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0486 - val_loss: 0.7069\n",
            "Epoch 1014/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8705 - val_loss: 0.7600\n",
            "Epoch 1015/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8938 - val_loss: 0.7781\n",
            "Epoch 1016/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9080 - val_loss: 0.9227\n",
            "Epoch 1017/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8762 - val_loss: 0.7174\n",
            "Epoch 1018/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9889 - val_loss: 0.6380\n",
            "Epoch 1019/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9200 - val_loss: 0.6740\n",
            "Epoch 1020/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9069 - val_loss: 0.7602\n",
            "Epoch 1021/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9672 - val_loss: 0.6558\n",
            "Epoch 1022/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8525 - val_loss: 0.7077\n",
            "Epoch 1023/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8858 - val_loss: 0.6389\n",
            "Epoch 1024/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9154 - val_loss: 0.7149\n",
            "Epoch 1025/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9409 - val_loss: 0.8768\n",
            "Epoch 1026/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9050 - val_loss: 0.7690\n",
            "Epoch 1027/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8479 - val_loss: 0.7217\n",
            "Epoch 1028/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9883 - val_loss: 0.7829\n",
            "Epoch 1029/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9223 - val_loss: 0.8706\n",
            "Epoch 1030/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8976 - val_loss: 0.7466\n",
            "Epoch 1031/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9283 - val_loss: 0.6281\n",
            "Epoch 1032/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9619 - val_loss: 0.8894\n",
            "Epoch 1033/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9961 - val_loss: 0.6718\n",
            "Epoch 1034/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8989 - val_loss: 0.7707\n",
            "Epoch 1035/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8416 - val_loss: 0.7386\n",
            "Epoch 1036/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9409 - val_loss: 0.7574\n",
            "Epoch 1037/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9221 - val_loss: 0.6764\n",
            "Epoch 1038/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9027 - val_loss: 0.8456\n",
            "Epoch 1039/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9353 - val_loss: 0.7396\n",
            "Epoch 1040/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9502 - val_loss: 0.6581\n",
            "Epoch 1041/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8594 - val_loss: 0.8505\n",
            "Epoch 1042/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9250 - val_loss: 0.7239\n",
            "Epoch 1043/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9683 - val_loss: 0.8390\n",
            "Epoch 1044/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9425 - val_loss: 0.7914\n",
            "Epoch 1045/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8855 - val_loss: 0.8060\n",
            "Epoch 1046/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8705 - val_loss: 1.1331\n",
            "Epoch 1047/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1088 - val_loss: 0.6218\n",
            "Epoch 1048/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9060 - val_loss: 0.6904\n",
            "Epoch 1049/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8341 - val_loss: 1.0454\n",
            "Epoch 1050/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8978 - val_loss: 0.7398\n",
            "Epoch 1051/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9884 - val_loss: 0.8091\n",
            "Epoch 1052/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8928 - val_loss: 0.6951\n",
            "Epoch 1053/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8778 - val_loss: 0.6105\n",
            "Epoch 1054/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9314 - val_loss: 1.0951\n",
            "Epoch 1055/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9261 - val_loss: 0.7955\n",
            "Epoch 1056/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8602 - val_loss: 0.7228\n",
            "Epoch 1057/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8760 - val_loss: 0.7425\n",
            "Epoch 1058/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0720 - val_loss: 0.7612\n",
            "Epoch 1059/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0023 - val_loss: 0.7404\n",
            "Epoch 1060/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9299 - val_loss: 0.8639\n",
            "Epoch 1061/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8667 - val_loss: 0.6721\n",
            "Epoch 1062/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9361 - val_loss: 0.7129\n",
            "Epoch 1063/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8926 - val_loss: 0.8032\n",
            "Epoch 1064/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9901 - val_loss: 0.6679\n",
            "Epoch 1065/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0365 - val_loss: 0.8097\n",
            "Epoch 1066/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9212 - val_loss: 0.7476\n",
            "Epoch 1067/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8562 - val_loss: 1.0280\n",
            "Epoch 1068/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9363 - val_loss: 0.7157\n",
            "Epoch 1069/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8735 - val_loss: 0.5670\n",
            "Epoch 1070/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9225 - val_loss: 0.6263\n",
            "Epoch 1071/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9313 - val_loss: 0.8678\n",
            "Epoch 1072/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9057 - val_loss: 0.8756\n",
            "Epoch 1073/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9217 - val_loss: 0.7512\n",
            "Epoch 1074/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9350 - val_loss: 0.7123\n",
            "Epoch 1075/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9960 - val_loss: 0.9262\n",
            "Epoch 1076/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9215 - val_loss: 0.6708\n",
            "Epoch 1077/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9073 - val_loss: 0.7877\n",
            "Epoch 1078/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9489 - val_loss: 0.8563\n",
            "Epoch 1079/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9104 - val_loss: 0.9734\n",
            "Epoch 1080/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9432 - val_loss: 0.5547\n",
            "Epoch 1081/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8669 - val_loss: 0.9257\n",
            "Epoch 1082/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9614 - val_loss: 0.6462\n",
            "Epoch 1083/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8933 - val_loss: 0.9584\n",
            "Epoch 1084/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9494 - val_loss: 0.6611\n",
            "Epoch 1085/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9004 - val_loss: 0.7086\n",
            "Epoch 1086/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9830 - val_loss: 0.7666\n",
            "Epoch 1087/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9872 - val_loss: 0.7832\n",
            "Epoch 1088/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8500 - val_loss: 0.8301\n",
            "Epoch 1089/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9013 - val_loss: 0.9938\n",
            "Epoch 1090/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9120 - val_loss: 0.8133\n",
            "Epoch 1091/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9683 - val_loss: 0.7390\n",
            "Epoch 1092/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9631 - val_loss: 0.7484\n",
            "Epoch 1093/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8983 - val_loss: 0.6347\n",
            "Epoch 1094/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9920 - val_loss: 0.7487\n",
            "Epoch 1095/5000\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.9567 - val_loss: 0.9182\n",
            "Epoch 1096/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0058 - val_loss: 0.6474\n",
            "Epoch 1097/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0517 - val_loss: 0.6699\n",
            "Epoch 1098/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9454 - val_loss: 0.7146\n",
            "Epoch 1099/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8703 - val_loss: 0.7334\n",
            "Epoch 1100/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9388 - val_loss: 0.7474\n",
            "Epoch 1101/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9080 - val_loss: 0.7226\n",
            "Epoch 1102/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9331 - val_loss: 0.5501\n",
            "Epoch 1103/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9452 - val_loss: 0.6897\n",
            "Epoch 1104/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9035 - val_loss: 0.6861\n",
            "Epoch 1105/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9587 - val_loss: 0.7862\n",
            "Epoch 1106/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8997 - val_loss: 0.8037\n",
            "Epoch 1107/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.0236 - val_loss: 0.6747\n",
            "Epoch 1108/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9447 - val_loss: 0.8603\n",
            "Epoch 1109/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.8724 - val_loss: 0.7650\n",
            "Epoch 1110/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9610 - val_loss: 0.9399\n",
            "Epoch 1111/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9053 - val_loss: 0.6551\n",
            "Epoch 1112/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8409 - val_loss: 0.7409\n",
            "Epoch 1113/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9265 - val_loss: 0.7710\n",
            "Epoch 1114/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8244 - val_loss: 0.9222\n",
            "Epoch 1115/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9760 - val_loss: 0.6865\n",
            "Epoch 1116/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9969 - val_loss: 0.8634\n",
            "Epoch 1117/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9735 - val_loss: 0.6580\n",
            "Epoch 1118/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8849 - val_loss: 0.7215\n",
            "Epoch 1119/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9772 - val_loss: 1.0083\n",
            "Epoch 1120/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9222 - val_loss: 0.8221\n",
            "Epoch 1121/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9250 - val_loss: 0.8960\n",
            "Epoch 1122/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9005 - val_loss: 0.6513\n",
            "Epoch 1123/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9031 - val_loss: 0.9326\n",
            "Epoch 1124/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0444 - val_loss: 1.1284\n",
            "Epoch 1125/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8977 - val_loss: 0.7090\n",
            "Epoch 1126/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8673 - val_loss: 0.7989\n",
            "Epoch 1127/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9138 - val_loss: 0.7413\n",
            "Epoch 1128/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8849 - val_loss: 0.7522\n",
            "Epoch 1129/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9853 - val_loss: 0.6208\n",
            "Epoch 1130/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8785 - val_loss: 0.7884\n",
            "Epoch 1131/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9813 - val_loss: 0.8556\n",
            "Epoch 1132/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9292 - val_loss: 0.7013\n",
            "Epoch 1133/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8842 - val_loss: 0.8535\n",
            "Epoch 1134/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8521 - val_loss: 0.7775\n",
            "Epoch 1135/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0732 - val_loss: 0.8250\n",
            "Epoch 1136/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9245 - val_loss: 0.7149\n",
            "Epoch 1137/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9075 - val_loss: 0.7978\n",
            "Epoch 1138/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8390 - val_loss: 0.7025\n",
            "Epoch 1139/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8997 - val_loss: 0.8631\n",
            "Epoch 1140/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9182 - val_loss: 0.7921\n",
            "Epoch 1141/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9418 - val_loss: 0.7965\n",
            "Epoch 1142/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9543 - val_loss: 0.8041\n",
            "Epoch 1143/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9089 - val_loss: 0.6682\n",
            "Epoch 1144/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9913 - val_loss: 0.7935\n",
            "Epoch 1145/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9480 - val_loss: 0.7075\n",
            "Epoch 1146/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9525 - val_loss: 0.7138\n",
            "Epoch 1147/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8942 - val_loss: 0.8915\n",
            "Epoch 1148/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9242 - val_loss: 0.8593\n",
            "Epoch 1149/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8639 - val_loss: 0.9590\n",
            "Epoch 1150/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9580 - val_loss: 0.6110\n",
            "Epoch 1151/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9611 - val_loss: 0.7183\n",
            "Epoch 1152/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9664 - val_loss: 0.7208\n",
            "Epoch 1153/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8863 - val_loss: 0.7444\n",
            "Epoch 1154/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8786 - val_loss: 0.7103\n",
            "Epoch 1155/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8760 - val_loss: 0.8124\n",
            "Epoch 1156/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9045 - val_loss: 0.7360\n",
            "Epoch 1157/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9039 - val_loss: 0.8182\n",
            "Epoch 1158/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8553 - val_loss: 0.7014\n",
            "Epoch 1159/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9246 - val_loss: 0.6660\n",
            "Epoch 1160/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9791 - val_loss: 0.8502\n",
            "Epoch 1161/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8903 - val_loss: 0.9144\n",
            "Epoch 1162/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9703 - val_loss: 0.6543\n",
            "Epoch 1163/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9792 - val_loss: 0.7640\n",
            "Epoch 1164/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0777 - val_loss: 0.8667\n",
            "Epoch 1165/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9908 - val_loss: 0.7399\n",
            "Epoch 1166/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9446 - val_loss: 0.8176\n",
            "Epoch 1167/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9061 - val_loss: 0.8452\n",
            "Epoch 1168/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8333 - val_loss: 0.7448\n",
            "Epoch 1169/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8899 - val_loss: 0.7820\n",
            "Epoch 1170/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9717 - val_loss: 0.7296\n",
            "Epoch 1171/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8771 - val_loss: 0.7888\n",
            "Epoch 1172/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0193 - val_loss: 0.9253\n",
            "Epoch 1173/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0408 - val_loss: 0.7161\n",
            "Epoch 1174/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9638 - val_loss: 0.9786\n",
            "Epoch 1175/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8288 - val_loss: 0.6328\n",
            "Epoch 1176/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9662 - val_loss: 0.5713\n",
            "Epoch 1177/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8386 - val_loss: 0.6527\n",
            "Epoch 1178/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1579 - val_loss: 0.7693\n",
            "Epoch 1179/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8816 - val_loss: 0.7865\n",
            "Epoch 1180/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8533 - val_loss: 0.7933\n",
            "Epoch 1181/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8878 - val_loss: 0.8833\n",
            "Epoch 1182/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9384 - val_loss: 1.0375\n",
            "Epoch 1183/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0534 - val_loss: 0.7175\n",
            "Epoch 1184/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9445 - val_loss: 0.6904\n",
            "Epoch 1185/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9359 - val_loss: 0.6024\n",
            "Epoch 1186/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9218 - val_loss: 0.6920\n",
            "Epoch 1187/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8444 - val_loss: 0.8357\n",
            "Epoch 1188/5000\n",
            "29/33 [=========================>....] - ETA: 0s - loss: 0.8446Restoring model weights from the end of the best epoch: 188.\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9331 - val_loss: 0.6827\n",
            "Epoch 1188: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-75-b383c653ffcb>:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.4780021607875824\n",
            "Train loss: 1.1281331777572632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7dd9022f7130> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.0251615047454834\n",
            "dO18 RMSE: 0.9467000318989732\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0          26.130            0.19025\n",
            "1          26.984            0.40123\n",
            "2          24.656            0.17728\n",
            "3          26.356            0.03953\n",
            "4          23.962            0.30992\n",
            "5          24.848            0.15842\n",
            "6          25.080            0.05125\n",
            "7          26.546            2.14378\n",
            "8          25.682            0.94472\n",
            "9          24.028            0.45852\n",
            "10         23.944            0.15813\n",
            "11         23.752            0.52437\n",
            "12         26.018            0.96417\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       26.450130           1.090306\n",
            "1       26.450130           1.090306\n",
            "2       24.755249           1.095573\n",
            "3       24.832169           1.094656\n",
            "4       24.630112           1.096869\n",
            "5       24.363977           1.080198\n",
            "6       24.363977           1.080198\n",
            "7       24.630110           1.096869\n",
            "8       24.755245           1.095573\n",
            "9       24.363976           1.080198\n",
            "10      25.233698           1.076884\n",
            "11      24.656179           1.099141\n",
            "12      25.233698           1.076884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/MyDrive/amazon_rainforest_files/variational/model/random_all_0809_ensemble.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Grouped, random, ablating other models except kriging\n",
        "\n",
        "We can generate isoscapes for this model easily."
      ],
      "metadata": {
        "id": "opmE3xc0vcY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_random_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_train_random_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_test_random_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_validation_random_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_passthrough = [\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "columns_to_scale = []\n",
        "columns_to_standardize = ['lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature', 'Mean Annual Precipitation',]\n",
        "\n",
        "data = load_and_scale(grouped_random_fileset, columns_to_passthrough, columns_to_scale, columns_to_standardize)\n",
        "model = train_and_evaluate(data, \"random_ablated_0809_ensemble.tf\", training_batch_size=3)\n",
        "model.save(get_model_save_location(\"random_ablated_0809_ensemble.tf\"), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location('random_ablated_0809_ensemble.pkl'))"
      ],
      "metadata": {
        "id": "KCebsA7XvfRH",
        "outputId": "b862bfcf-c453-484c-a36b-213b9432b974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: GTiff/GeoTIFF\n",
            "Size is 541 x 467 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.922043, 5.233124)\n",
            "Pixel Size = (0.08333, -0.08333)\n",
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('lat_standardizer', StandardScaler(), ['lat']),\n",
            "                                ('long_standardizer', StandardScaler(),\n",
            "                                 ['long']),\n",
            "                                ('VPD_standardizer', StandardScaler(), ['VPD']),\n",
            "                                ('RH_standardizer', StandardScaler(), ['RH']),\n",
            "                                ('PET_standardizer', StandardScaler(), ['PET']),\n",
            "                                ('DEM_standardizer', StandardScaler(), ['DEM']),\n",
            "                                ('PA_standardizer', StandardScaler(), ['PA']),\n",
            "                                ('Mean Annual Temperature_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation'])])\n",
            "==================\n",
            "random_ablated_0809_ensemble.tf\n",
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_12 (InputLayer)          [(None, 12)]         0           []                               \n",
            "                                                                                                  \n",
            " dense_22 (Dense)               (None, 20)           260         ['input_12[0][0]']               \n",
            "                                                                                                  \n",
            " dense_23 (Dense)               (None, 20)           420         ['dense_22[0][0]']               \n",
            "                                                                                                  \n",
            " var_output (Dense)             (None, 1)            21          ['dense_23[0][0]']               \n",
            "                                                                                                  \n",
            " mean_output (Dense)            (None, 1)            21          ['dense_23[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.multiply_9 (TFOpLambda  (None, 1)           0           ['var_output[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.multiply_8 (TFOpLambda  (None, 1)           0           ['mean_output[0][0]']            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_15 (TFOpL  (None, 1)           0           ['tf.math.multiply_9[0][0]']     \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_14 (TFOpL  (None, 1)           0           ['tf.math.multiply_8[0][0]']     \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1)            0           ['tf.__operators__.add_15[0][0]']\n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenate)   (None, 2)            0           ['tf.__operators__.add_14[0][0]',\n",
            "                                                                  'lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 722\n",
            "Trainable params: 722\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5000\n",
            "33/33 [==============================] - 2s 14ms/step - loss: 4.0471 - val_loss: 2.9210\n",
            "Epoch 2/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.5039 - val_loss: 1.6343\n",
            "Epoch 3/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 2.2861 - val_loss: 1.1421\n",
            "Epoch 4/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 2.4323 - val_loss: 1.0783\n",
            "Epoch 5/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 2.4436 - val_loss: 0.7421\n",
            "Epoch 6/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.0721 - val_loss: 0.9578\n",
            "Epoch 7/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.1205 - val_loss: 0.8860\n",
            "Epoch 8/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.1397 - val_loss: 0.8133\n",
            "Epoch 9/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.9988 - val_loss: 0.8526\n",
            "Epoch 10/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.4083 - val_loss: 0.9020\n",
            "Epoch 11/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.2180 - val_loss: 0.7775\n",
            "Epoch 12/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 2.2156 - val_loss: 1.2474\n",
            "Epoch 13/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.8874 - val_loss: 1.1757\n",
            "Epoch 14/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.8419 - val_loss: 0.7834\n",
            "Epoch 15/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.5418 - val_loss: 0.9097\n",
            "Epoch 16/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.1458 - val_loss: 0.7623\n",
            "Epoch 17/5000\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 1.7943 - val_loss: 0.6591\n",
            "Epoch 18/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.2554 - val_loss: 0.7548\n",
            "Epoch 19/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.7998 - val_loss: 0.8562\n",
            "Epoch 20/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.1505 - val_loss: 1.2900\n",
            "Epoch 21/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.7261 - val_loss: 0.8734\n",
            "Epoch 22/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.0998 - val_loss: 0.9286\n",
            "Epoch 23/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.1167 - val_loss: 0.7808\n",
            "Epoch 24/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.9970 - val_loss: 1.0099\n",
            "Epoch 25/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.3782 - val_loss: 0.8839\n",
            "Epoch 26/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.8761 - val_loss: 1.0086\n",
            "Epoch 27/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.8698 - val_loss: 1.2248\n",
            "Epoch 28/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.0732 - val_loss: 0.9525\n",
            "Epoch 29/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.7795 - val_loss: 1.2342\n",
            "Epoch 30/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 2.0652 - val_loss: 0.8271\n",
            "Epoch 31/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.8069 - val_loss: 0.8110\n",
            "Epoch 32/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6348 - val_loss: 0.9835\n",
            "Epoch 33/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.4241 - val_loss: 1.1288\n",
            "Epoch 34/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.9888 - val_loss: 0.9989\n",
            "Epoch 35/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.2460 - val_loss: 0.8065\n",
            "Epoch 36/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.9095 - val_loss: 1.0802\n",
            "Epoch 37/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.9959 - val_loss: 0.7809\n",
            "Epoch 38/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.8375 - val_loss: 0.6899\n",
            "Epoch 39/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7196 - val_loss: 1.0193\n",
            "Epoch 40/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.9655 - val_loss: 1.2276\n",
            "Epoch 41/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6359 - val_loss: 1.1625\n",
            "Epoch 42/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.8036 - val_loss: 1.1607\n",
            "Epoch 43/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3674 - val_loss: 1.0494\n",
            "Epoch 44/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.7403 - val_loss: 0.7658\n",
            "Epoch 45/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 2.0112 - val_loss: 0.8944\n",
            "Epoch 46/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.8401 - val_loss: 0.6917\n",
            "Epoch 47/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.9709 - val_loss: 0.8472\n",
            "Epoch 48/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6951 - val_loss: 0.8242\n",
            "Epoch 49/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.7476 - val_loss: 0.6764\n",
            "Epoch 50/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.7965 - val_loss: 0.6906\n",
            "Epoch 51/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.8140 - val_loss: 0.6192\n",
            "Epoch 52/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7161 - val_loss: 0.8327\n",
            "Epoch 53/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7147 - val_loss: 0.7376\n",
            "Epoch 54/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6339 - val_loss: 0.7325\n",
            "Epoch 55/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4596 - val_loss: 0.7323\n",
            "Epoch 56/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4766 - val_loss: 0.9189\n",
            "Epoch 57/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4240 - val_loss: 0.8291\n",
            "Epoch 58/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6822 - val_loss: 0.5522\n",
            "Epoch 59/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.9438 - val_loss: 0.6133\n",
            "Epoch 60/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.7025 - val_loss: 0.5123\n",
            "Epoch 61/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.7394 - val_loss: 0.7026\n",
            "Epoch 62/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.8349 - val_loss: 0.9957\n",
            "Epoch 63/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5394 - val_loss: 0.6839\n",
            "Epoch 64/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5548 - val_loss: 0.9154\n",
            "Epoch 65/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5820 - val_loss: 0.6154\n",
            "Epoch 66/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6584 - val_loss: 0.7306\n",
            "Epoch 67/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5288 - val_loss: 0.8718\n",
            "Epoch 68/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3653 - val_loss: 0.7501\n",
            "Epoch 69/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.4609 - val_loss: 0.9931\n",
            "Epoch 70/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.8160 - val_loss: 0.8017\n",
            "Epoch 71/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7504 - val_loss: 0.6342\n",
            "Epoch 72/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6087 - val_loss: 0.7358\n",
            "Epoch 73/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5512 - val_loss: 1.0126\n",
            "Epoch 74/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.8891 - val_loss: 0.7436\n",
            "Epoch 75/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4928 - val_loss: 0.8122\n",
            "Epoch 76/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5071 - val_loss: 0.8918\n",
            "Epoch 77/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.0670 - val_loss: 0.8591\n",
            "Epoch 78/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5545 - val_loss: 0.5822\n",
            "Epoch 79/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.8042 - val_loss: 1.1168\n",
            "Epoch 80/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.7915 - val_loss: 0.5783\n",
            "Epoch 81/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.7164 - val_loss: 0.6314\n",
            "Epoch 82/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.9809 - val_loss: 0.6512\n",
            "Epoch 83/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6607 - val_loss: 0.8121\n",
            "Epoch 84/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6242 - val_loss: 0.7306\n",
            "Epoch 85/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6834 - val_loss: 0.7953\n",
            "Epoch 86/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5414 - val_loss: 0.7678\n",
            "Epoch 87/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5363 - val_loss: 0.7317\n",
            "Epoch 88/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.8149 - val_loss: 1.0421\n",
            "Epoch 89/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6174 - val_loss: 0.8136\n",
            "Epoch 90/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.9357 - val_loss: 0.7255\n",
            "Epoch 91/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7433 - val_loss: 0.6536\n",
            "Epoch 92/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3985 - val_loss: 0.7004\n",
            "Epoch 93/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5264 - val_loss: 0.7289\n",
            "Epoch 94/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6660 - val_loss: 0.8783\n",
            "Epoch 95/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.4998 - val_loss: 0.6387\n",
            "Epoch 96/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4827 - val_loss: 0.7513\n",
            "Epoch 97/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7813 - val_loss: 0.9209\n",
            "Epoch 98/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.0419 - val_loss: 0.6171\n",
            "Epoch 99/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4166 - val_loss: 0.6641\n",
            "Epoch 100/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.7531 - val_loss: 0.7676\n",
            "Epoch 101/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.7940 - val_loss: 1.0496\n",
            "Epoch 102/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4422 - val_loss: 0.8234\n",
            "Epoch 103/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6074 - val_loss: 0.7077\n",
            "Epoch 104/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6998 - val_loss: 0.6300\n",
            "Epoch 105/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.5273 - val_loss: 0.8844\n",
            "Epoch 106/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6931 - val_loss: 0.8754\n",
            "Epoch 107/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5312 - val_loss: 0.8315\n",
            "Epoch 108/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.5404 - val_loss: 0.6268\n",
            "Epoch 109/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6192 - val_loss: 0.9886\n",
            "Epoch 110/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 2.0685 - val_loss: 0.5909\n",
            "Epoch 111/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2447 - val_loss: 0.7915\n",
            "Epoch 112/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.7897 - val_loss: 0.7667\n",
            "Epoch 113/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6478 - val_loss: 0.7372\n",
            "Epoch 114/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.8092 - val_loss: 0.7734\n",
            "Epoch 115/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5294 - val_loss: 1.0062\n",
            "Epoch 116/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.7852 - val_loss: 0.8051\n",
            "Epoch 117/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4617 - val_loss: 0.7287\n",
            "Epoch 118/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3279 - val_loss: 0.7513\n",
            "Epoch 119/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3478 - val_loss: 0.8612\n",
            "Epoch 120/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.8128 - val_loss: 0.8224\n",
            "Epoch 121/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3550 - val_loss: 0.6501\n",
            "Epoch 122/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.4443 - val_loss: 0.9070\n",
            "Epoch 123/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.5232 - val_loss: 0.5348\n",
            "Epoch 124/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.5075 - val_loss: 0.7470\n",
            "Epoch 125/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.6439 - val_loss: 0.6586\n",
            "Epoch 126/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5986 - val_loss: 0.9194\n",
            "Epoch 127/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.9949 - val_loss: 0.5433\n",
            "Epoch 128/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5195 - val_loss: 0.9005\n",
            "Epoch 129/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.9183 - val_loss: 0.9507\n",
            "Epoch 130/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 2.1808 - val_loss: 0.7209\n",
            "Epoch 131/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5694 - val_loss: 1.2893\n",
            "Epoch 132/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5129 - val_loss: 0.8571\n",
            "Epoch 133/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5221 - val_loss: 0.8376\n",
            "Epoch 134/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4179 - val_loss: 0.7288\n",
            "Epoch 135/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6950 - val_loss: 0.6825\n",
            "Epoch 136/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4688 - val_loss: 0.7622\n",
            "Epoch 137/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3976 - val_loss: 0.7817\n",
            "Epoch 138/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5812 - val_loss: 0.8435\n",
            "Epoch 139/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6233 - val_loss: 0.6175\n",
            "Epoch 140/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3103 - val_loss: 1.0934\n",
            "Epoch 141/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6470 - val_loss: 1.0155\n",
            "Epoch 142/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.8990 - val_loss: 0.8692\n",
            "Epoch 143/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5202 - val_loss: 0.6639\n",
            "Epoch 144/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3844 - val_loss: 0.8769\n",
            "Epoch 145/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4396 - val_loss: 1.0272\n",
            "Epoch 146/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4886 - val_loss: 0.8125\n",
            "Epoch 147/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6685 - val_loss: 0.7447\n",
            "Epoch 148/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4672 - val_loss: 0.9026\n",
            "Epoch 149/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4423 - val_loss: 0.9503\n",
            "Epoch 150/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6105 - val_loss: 0.7122\n",
            "Epoch 151/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.5735 - val_loss: 0.8701\n",
            "Epoch 152/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6203 - val_loss: 0.7898\n",
            "Epoch 153/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5311 - val_loss: 0.8953\n",
            "Epoch 154/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5112 - val_loss: 0.8081\n",
            "Epoch 155/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7052 - val_loss: 1.1005\n",
            "Epoch 156/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5373 - val_loss: 0.8826\n",
            "Epoch 157/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.8839 - val_loss: 1.0312\n",
            "Epoch 158/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4252 - val_loss: 1.2210\n",
            "Epoch 159/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4030 - val_loss: 0.6655\n",
            "Epoch 160/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5960 - val_loss: 1.1064\n",
            "Epoch 161/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3181 - val_loss: 0.7762\n",
            "Epoch 162/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.4316 - val_loss: 0.9342\n",
            "Epoch 163/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3843 - val_loss: 0.9431\n",
            "Epoch 164/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5650 - val_loss: 0.7556\n",
            "Epoch 165/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5799 - val_loss: 1.0139\n",
            "Epoch 166/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3457 - val_loss: 1.0383\n",
            "Epoch 167/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5519 - val_loss: 0.8745\n",
            "Epoch 168/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4386 - val_loss: 0.9504\n",
            "Epoch 169/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5127 - val_loss: 0.6158\n",
            "Epoch 170/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3454 - val_loss: 0.8885\n",
            "Epoch 171/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3918 - val_loss: 0.6863\n",
            "Epoch 172/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2839 - val_loss: 0.6873\n",
            "Epoch 173/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5046 - val_loss: 0.6695\n",
            "Epoch 174/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.9609 - val_loss: 0.7772\n",
            "Epoch 175/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3772 - val_loss: 0.7324\n",
            "Epoch 176/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4949 - val_loss: 0.9603\n",
            "Epoch 177/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4442 - val_loss: 0.7491\n",
            "Epoch 178/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1710 - val_loss: 0.6961\n",
            "Epoch 179/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6659 - val_loss: 0.9082\n",
            "Epoch 180/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.5350 - val_loss: 0.9415\n",
            "Epoch 181/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3555 - val_loss: 0.6907\n",
            "Epoch 182/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4336 - val_loss: 0.6936\n",
            "Epoch 183/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.8358 - val_loss: 0.8057\n",
            "Epoch 184/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6544 - val_loss: 0.6770\n",
            "Epoch 185/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5315 - val_loss: 0.7911\n",
            "Epoch 186/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2092 - val_loss: 1.4284\n",
            "Epoch 187/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2942 - val_loss: 1.1764\n",
            "Epoch 188/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3054 - val_loss: 0.5183\n",
            "Epoch 189/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.4195 - val_loss: 0.8332\n",
            "Epoch 190/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4394 - val_loss: 0.9568\n",
            "Epoch 191/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4147 - val_loss: 2.4418\n",
            "Epoch 192/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5004 - val_loss: 0.7572\n",
            "Epoch 193/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3033 - val_loss: 1.2522\n",
            "Epoch 194/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.5346 - val_loss: 0.8061\n",
            "Epoch 195/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.7371 - val_loss: 0.8127\n",
            "Epoch 196/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3935 - val_loss: 0.8927\n",
            "Epoch 197/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1740 - val_loss: 0.8364\n",
            "Epoch 198/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5638 - val_loss: 1.2613\n",
            "Epoch 199/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4148 - val_loss: 0.8654\n",
            "Epoch 200/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4053 - val_loss: 1.1017\n",
            "Epoch 201/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.4795 - val_loss: 0.7871\n",
            "Epoch 202/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4127 - val_loss: 0.7636\n",
            "Epoch 203/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4974 - val_loss: 0.7577\n",
            "Epoch 204/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6401 - val_loss: 0.9490\n",
            "Epoch 205/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.8151 - val_loss: 0.7942\n",
            "Epoch 206/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.5291 - val_loss: 0.8571\n",
            "Epoch 207/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2675 - val_loss: 0.6702\n",
            "Epoch 208/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4519 - val_loss: 0.8379\n",
            "Epoch 209/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2947 - val_loss: 0.8708\n",
            "Epoch 210/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2751 - val_loss: 0.8110\n",
            "Epoch 211/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3639 - val_loss: 0.8322\n",
            "Epoch 212/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3099 - val_loss: 0.7965\n",
            "Epoch 213/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3217 - val_loss: 0.9380\n",
            "Epoch 214/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3650 - val_loss: 1.7102\n",
            "Epoch 215/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.6147 - val_loss: 0.7115\n",
            "Epoch 216/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4143 - val_loss: 1.0149\n",
            "Epoch 217/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5958 - val_loss: 1.2828\n",
            "Epoch 218/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3565 - val_loss: 0.7369\n",
            "Epoch 219/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.7411 - val_loss: 0.9123\n",
            "Epoch 220/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.3334 - val_loss: 1.0115\n",
            "Epoch 221/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2994 - val_loss: 0.8127\n",
            "Epoch 222/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3511 - val_loss: 0.8076\n",
            "Epoch 223/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4622 - val_loss: 1.4208\n",
            "Epoch 224/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.4324 - val_loss: 0.9837\n",
            "Epoch 225/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1168 - val_loss: 1.0353\n",
            "Epoch 226/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5438 - val_loss: 1.0388\n",
            "Epoch 227/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3382 - val_loss: 0.6536\n",
            "Epoch 228/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4488 - val_loss: 0.7153\n",
            "Epoch 229/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3269 - val_loss: 1.1315\n",
            "Epoch 230/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5003 - val_loss: 1.1236\n",
            "Epoch 231/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4616 - val_loss: 1.0413\n",
            "Epoch 232/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.0475 - val_loss: 0.8367\n",
            "Epoch 233/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4228 - val_loss: 1.5212\n",
            "Epoch 234/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4032 - val_loss: 0.9144\n",
            "Epoch 235/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.3965 - val_loss: 1.3036\n",
            "Epoch 236/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5534 - val_loss: 1.1288\n",
            "Epoch 237/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3403 - val_loss: 0.9936\n",
            "Epoch 238/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3344 - val_loss: 0.8171\n",
            "Epoch 239/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4937 - val_loss: 0.8457\n",
            "Epoch 240/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2204 - val_loss: 0.8458\n",
            "Epoch 241/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1670 - val_loss: 1.2131\n",
            "Epoch 242/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.5326 - val_loss: 1.0927\n",
            "Epoch 243/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3116 - val_loss: 1.5104\n",
            "Epoch 244/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1814 - val_loss: 0.9183\n",
            "Epoch 245/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6632 - val_loss: 0.8863\n",
            "Epoch 246/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6285 - val_loss: 0.6828\n",
            "Epoch 247/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0651 - val_loss: 0.8065\n",
            "Epoch 248/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3127 - val_loss: 0.7295\n",
            "Epoch 249/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.7331 - val_loss: 0.9083\n",
            "Epoch 250/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1567 - val_loss: 0.9731\n",
            "Epoch 251/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.6732 - val_loss: 1.0134\n",
            "Epoch 252/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3071 - val_loss: 0.8236\n",
            "Epoch 253/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2766 - val_loss: 0.7940\n",
            "Epoch 254/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.8472 - val_loss: 0.7961\n",
            "Epoch 255/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2264 - val_loss: 0.9178\n",
            "Epoch 256/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3685 - val_loss: 1.1690\n",
            "Epoch 257/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2138 - val_loss: 0.8031\n",
            "Epoch 258/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1604 - val_loss: 1.0500\n",
            "Epoch 259/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2845 - val_loss: 0.8586\n",
            "Epoch 260/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2755 - val_loss: 0.7914\n",
            "Epoch 261/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6446 - val_loss: 0.8240\n",
            "Epoch 262/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4490 - val_loss: 1.0250\n",
            "Epoch 263/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3039 - val_loss: 1.3018\n",
            "Epoch 264/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4998 - val_loss: 0.7909\n",
            "Epoch 265/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2353 - val_loss: 0.7986\n",
            "Epoch 266/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3129 - val_loss: 0.9336\n",
            "Epoch 267/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3210 - val_loss: 0.8957\n",
            "Epoch 268/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2815 - val_loss: 1.1724\n",
            "Epoch 269/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.6674 - val_loss: 1.0355\n",
            "Epoch 270/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4337 - val_loss: 1.1206\n",
            "Epoch 271/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2994 - val_loss: 1.0312\n",
            "Epoch 272/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3459 - val_loss: 0.7077\n",
            "Epoch 273/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3550 - val_loss: 0.9457\n",
            "Epoch 274/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2831 - val_loss: 0.7676\n",
            "Epoch 275/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3130 - val_loss: 1.0668\n",
            "Epoch 276/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3455 - val_loss: 0.9199\n",
            "Epoch 277/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2131 - val_loss: 0.7937\n",
            "Epoch 278/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.7840 - val_loss: 1.0855\n",
            "Epoch 279/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 2.0510 - val_loss: 0.9372\n",
            "Epoch 280/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3575 - val_loss: 1.0365\n",
            "Epoch 281/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4500 - val_loss: 0.9854\n",
            "Epoch 282/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3780 - val_loss: 0.9082\n",
            "Epoch 283/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2695 - val_loss: 1.5492\n",
            "Epoch 284/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2089 - val_loss: 0.8485\n",
            "Epoch 285/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2444 - val_loss: 0.8185\n",
            "Epoch 286/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2291 - val_loss: 1.2071\n",
            "Epoch 287/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2407 - val_loss: 0.8983\n",
            "Epoch 288/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1471 - val_loss: 1.1966\n",
            "Epoch 289/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2621 - val_loss: 0.9011\n",
            "Epoch 290/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.4054 - val_loss: 0.7918\n",
            "Epoch 291/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3927 - val_loss: 0.9904\n",
            "Epoch 292/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2059 - val_loss: 1.0114\n",
            "Epoch 293/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2785 - val_loss: 0.7697\n",
            "Epoch 294/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2247 - val_loss: 1.0341\n",
            "Epoch 295/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2383 - val_loss: 1.0130\n",
            "Epoch 296/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2972 - val_loss: 1.0504\n",
            "Epoch 297/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4194 - val_loss: 0.8423\n",
            "Epoch 298/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.3753 - val_loss: 0.7903\n",
            "Epoch 299/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.3054 - val_loss: 1.1105\n",
            "Epoch 300/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2865 - val_loss: 1.0900\n",
            "Epoch 301/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2679 - val_loss: 0.8919\n",
            "Epoch 302/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1769 - val_loss: 1.1268\n",
            "Epoch 303/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.3276 - val_loss: 1.0349\n",
            "Epoch 304/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0638 - val_loss: 0.9520\n",
            "Epoch 305/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2080 - val_loss: 0.9901\n",
            "Epoch 306/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3542 - val_loss: 0.7004\n",
            "Epoch 307/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4155 - val_loss: 0.9391\n",
            "Epoch 308/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.4768 - val_loss: 0.9463\n",
            "Epoch 309/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2174 - val_loss: 0.9359\n",
            "Epoch 310/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1059 - val_loss: 0.8334\n",
            "Epoch 311/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.4360 - val_loss: 0.8339\n",
            "Epoch 312/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1845 - val_loss: 0.7008\n",
            "Epoch 313/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3199 - val_loss: 1.1380\n",
            "Epoch 314/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2364 - val_loss: 0.9336\n",
            "Epoch 315/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1515 - val_loss: 0.9980\n",
            "Epoch 316/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1355 - val_loss: 0.9791\n",
            "Epoch 317/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2895 - val_loss: 0.9289\n",
            "Epoch 318/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3541 - val_loss: 0.7486\n",
            "Epoch 319/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.3449 - val_loss: 1.0219\n",
            "Epoch 320/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1522 - val_loss: 1.0423\n",
            "Epoch 321/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2792 - val_loss: 0.8135\n",
            "Epoch 322/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3387 - val_loss: 1.0128\n",
            "Epoch 323/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5370 - val_loss: 0.9657\n",
            "Epoch 324/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1951 - val_loss: 0.7200\n",
            "Epoch 325/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2966 - val_loss: 1.2329\n",
            "Epoch 326/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1408 - val_loss: 0.8287\n",
            "Epoch 327/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1518 - val_loss: 0.7865\n",
            "Epoch 328/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0691 - val_loss: 0.9016\n",
            "Epoch 329/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4347 - val_loss: 0.5589\n",
            "Epoch 330/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1264 - val_loss: 0.7097\n",
            "Epoch 331/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1296 - val_loss: 1.0760\n",
            "Epoch 332/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2170 - val_loss: 1.0318\n",
            "Epoch 333/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1467 - val_loss: 0.9239\n",
            "Epoch 334/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3821 - val_loss: 1.2255\n",
            "Epoch 335/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1166 - val_loss: 0.8190\n",
            "Epoch 336/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2463 - val_loss: 1.1267\n",
            "Epoch 337/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3012 - val_loss: 0.8002\n",
            "Epoch 338/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3359 - val_loss: 0.7414\n",
            "Epoch 339/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.8536 - val_loss: 0.9860\n",
            "Epoch 340/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2086 - val_loss: 1.3885\n",
            "Epoch 341/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2904 - val_loss: 0.8169\n",
            "Epoch 342/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3432 - val_loss: 0.8142\n",
            "Epoch 343/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3823 - val_loss: 0.9588\n",
            "Epoch 344/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0845 - val_loss: 0.6411\n",
            "Epoch 345/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2012 - val_loss: 0.9813\n",
            "Epoch 346/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.2530 - val_loss: 0.9654\n",
            "Epoch 347/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.5284 - val_loss: 0.7327\n",
            "Epoch 348/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1321 - val_loss: 0.8655\n",
            "Epoch 349/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1427 - val_loss: 0.8810\n",
            "Epoch 350/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4076 - val_loss: 0.8828\n",
            "Epoch 351/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1939 - val_loss: 1.0033\n",
            "Epoch 352/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1375 - val_loss: 0.7042\n",
            "Epoch 353/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2340 - val_loss: 0.7010\n",
            "Epoch 354/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0843 - val_loss: 0.7807\n",
            "Epoch 355/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1452 - val_loss: 0.9675\n",
            "Epoch 356/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1159 - val_loss: 1.0511\n",
            "Epoch 357/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1468 - val_loss: 0.8753\n",
            "Epoch 358/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0027 - val_loss: 0.9255\n",
            "Epoch 359/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0729 - val_loss: 1.0635\n",
            "Epoch 360/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1303 - val_loss: 0.8755\n",
            "Epoch 361/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0848 - val_loss: 0.9192\n",
            "Epoch 362/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1152 - val_loss: 0.9268\n",
            "Epoch 363/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1289 - val_loss: 1.0827\n",
            "Epoch 364/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2021 - val_loss: 0.9649\n",
            "Epoch 365/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9766 - val_loss: 1.2052\n",
            "Epoch 366/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1502 - val_loss: 1.2701\n",
            "Epoch 367/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2505 - val_loss: 0.9640\n",
            "Epoch 368/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2430 - val_loss: 0.7883\n",
            "Epoch 369/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3670 - val_loss: 1.2374\n",
            "Epoch 370/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3026 - val_loss: 1.0320\n",
            "Epoch 371/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1371 - val_loss: 1.3886\n",
            "Epoch 372/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0030 - val_loss: 0.9819\n",
            "Epoch 373/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1374 - val_loss: 0.7822\n",
            "Epoch 374/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.4223 - val_loss: 0.5903\n",
            "Epoch 375/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1835 - val_loss: 0.9798\n",
            "Epoch 376/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1726 - val_loss: 0.9405\n",
            "Epoch 377/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1990 - val_loss: 0.8719\n",
            "Epoch 378/5000\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 1.3765 - val_loss: 0.7915\n",
            "Epoch 379/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2261 - val_loss: 0.9197\n",
            "Epoch 380/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2499 - val_loss: 0.8794\n",
            "Epoch 381/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.7210 - val_loss: 1.1936\n",
            "Epoch 382/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1680 - val_loss: 0.7492\n",
            "Epoch 383/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1667 - val_loss: 0.8142\n",
            "Epoch 384/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3268 - val_loss: 0.9090\n",
            "Epoch 385/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1909 - val_loss: 0.7451\n",
            "Epoch 386/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0983 - val_loss: 0.8899\n",
            "Epoch 387/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2113 - val_loss: 1.2664\n",
            "Epoch 388/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.4163 - val_loss: 0.7170\n",
            "Epoch 389/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2419 - val_loss: 0.7618\n",
            "Epoch 390/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2275 - val_loss: 0.7837\n",
            "Epoch 391/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9980 - val_loss: 0.8539\n",
            "Epoch 392/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1789 - val_loss: 0.8066\n",
            "Epoch 393/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1039 - val_loss: 0.8401\n",
            "Epoch 394/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2621 - val_loss: 1.0609\n",
            "Epoch 395/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1490 - val_loss: 0.9069\n",
            "Epoch 396/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0365 - val_loss: 0.9155\n",
            "Epoch 397/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1153 - val_loss: 0.8739\n",
            "Epoch 398/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2025 - val_loss: 1.0255\n",
            "Epoch 399/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.1268 - val_loss: 1.5422\n",
            "Epoch 400/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2562 - val_loss: 1.1262\n",
            "Epoch 401/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0948 - val_loss: 0.8538\n",
            "Epoch 402/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1262 - val_loss: 0.8939\n",
            "Epoch 403/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0920 - val_loss: 1.3570\n",
            "Epoch 404/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1773 - val_loss: 0.7671\n",
            "Epoch 405/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2251 - val_loss: 0.6671\n",
            "Epoch 406/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1618 - val_loss: 0.9796\n",
            "Epoch 407/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1661 - val_loss: 1.0639\n",
            "Epoch 408/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2180 - val_loss: 1.1510\n",
            "Epoch 409/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0826 - val_loss: 1.1580\n",
            "Epoch 410/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4057 - val_loss: 1.1572\n",
            "Epoch 411/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1243 - val_loss: 0.8365\n",
            "Epoch 412/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1383 - val_loss: 0.9612\n",
            "Epoch 413/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1299 - val_loss: 0.9417\n",
            "Epoch 414/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.3253 - val_loss: 0.9231\n",
            "Epoch 415/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0524 - val_loss: 0.8022\n",
            "Epoch 416/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1862 - val_loss: 0.9862\n",
            "Epoch 417/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2677 - val_loss: 0.6821\n",
            "Epoch 418/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.8523 - val_loss: 0.7744\n",
            "Epoch 419/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9410 - val_loss: 0.8672\n",
            "Epoch 420/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2492 - val_loss: 0.6365\n",
            "Epoch 421/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0768 - val_loss: 0.6776\n",
            "Epoch 422/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0583 - val_loss: 0.6490\n",
            "Epoch 423/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0569 - val_loss: 0.7404\n",
            "Epoch 424/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1025 - val_loss: 0.6306\n",
            "Epoch 425/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0494 - val_loss: 0.8521\n",
            "Epoch 426/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9892 - val_loss: 0.7342\n",
            "Epoch 427/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2452 - val_loss: 0.8255\n",
            "Epoch 428/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0071 - val_loss: 0.9662\n",
            "Epoch 429/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2378 - val_loss: 0.7829\n",
            "Epoch 430/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2800 - val_loss: 0.9251\n",
            "Epoch 431/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1460 - val_loss: 1.1839\n",
            "Epoch 432/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0893 - val_loss: 0.6542\n",
            "Epoch 433/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1843 - val_loss: 0.8468\n",
            "Epoch 434/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2570 - val_loss: 1.1896\n",
            "Epoch 435/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1548 - val_loss: 0.8125\n",
            "Epoch 436/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0615 - val_loss: 1.1866\n",
            "Epoch 437/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1228 - val_loss: 0.9412\n",
            "Epoch 438/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1129 - val_loss: 0.8414\n",
            "Epoch 439/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0632 - val_loss: 0.8951\n",
            "Epoch 440/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0254 - val_loss: 0.7521\n",
            "Epoch 441/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0338 - val_loss: 0.7188\n",
            "Epoch 442/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0482 - val_loss: 1.2181\n",
            "Epoch 443/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1878 - val_loss: 0.7122\n",
            "Epoch 444/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1436 - val_loss: 1.0266\n",
            "Epoch 445/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1329 - val_loss: 0.9337\n",
            "Epoch 446/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1884 - val_loss: 0.8472\n",
            "Epoch 447/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0051 - val_loss: 0.9474\n",
            "Epoch 448/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1485 - val_loss: 0.8214\n",
            "Epoch 449/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0419 - val_loss: 0.8719\n",
            "Epoch 450/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1831 - val_loss: 0.8924\n",
            "Epoch 451/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0386 - val_loss: 0.8255\n",
            "Epoch 452/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9898 - val_loss: 0.6746\n",
            "Epoch 453/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0837 - val_loss: 0.8883\n",
            "Epoch 454/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0823 - val_loss: 0.7613\n",
            "Epoch 455/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1723 - val_loss: 0.7500\n",
            "Epoch 456/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0603 - val_loss: 0.9419\n",
            "Epoch 457/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1405 - val_loss: 0.8933\n",
            "Epoch 458/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0096 - val_loss: 0.6585\n",
            "Epoch 459/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0080 - val_loss: 0.7284\n",
            "Epoch 460/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1064 - val_loss: 0.8144\n",
            "Epoch 461/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0501 - val_loss: 0.7223\n",
            "Epoch 462/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3781 - val_loss: 0.9015\n",
            "Epoch 463/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.8412 - val_loss: 0.6847\n",
            "Epoch 464/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.2421 - val_loss: 0.7008\n",
            "Epoch 465/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0788 - val_loss: 0.8589\n",
            "Epoch 466/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0850 - val_loss: 0.7855\n",
            "Epoch 467/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0480 - val_loss: 0.6835\n",
            "Epoch 468/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1561 - val_loss: 0.7757\n",
            "Epoch 469/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9057 - val_loss: 0.7741\n",
            "Epoch 470/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0492 - val_loss: 0.6748\n",
            "Epoch 471/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0990 - val_loss: 0.6982\n",
            "Epoch 472/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9383 - val_loss: 1.0363\n",
            "Epoch 473/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1901 - val_loss: 0.9102\n",
            "Epoch 474/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0194 - val_loss: 0.7848\n",
            "Epoch 475/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0321 - val_loss: 1.0406\n",
            "Epoch 476/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1288 - val_loss: 1.1264\n",
            "Epoch 477/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0795 - val_loss: 1.0558\n",
            "Epoch 478/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0565 - val_loss: 0.7412\n",
            "Epoch 479/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2953 - val_loss: 0.6867\n",
            "Epoch 480/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9831 - val_loss: 0.8501\n",
            "Epoch 481/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0821 - val_loss: 0.8843\n",
            "Epoch 482/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9728 - val_loss: 0.8874\n",
            "Epoch 483/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1588 - val_loss: 0.8912\n",
            "Epoch 484/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0197 - val_loss: 0.7086\n",
            "Epoch 485/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9736 - val_loss: 0.8432\n",
            "Epoch 486/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1140 - val_loss: 0.6125\n",
            "Epoch 487/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1422 - val_loss: 0.8574\n",
            "Epoch 488/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.3426 - val_loss: 0.8316\n",
            "Epoch 489/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9923 - val_loss: 0.6090\n",
            "Epoch 490/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9335 - val_loss: 0.8502\n",
            "Epoch 491/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9961 - val_loss: 0.7574\n",
            "Epoch 492/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9666 - val_loss: 0.8325\n",
            "Epoch 493/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0545 - val_loss: 0.7394\n",
            "Epoch 494/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.6076 - val_loss: 0.7340\n",
            "Epoch 495/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9335 - val_loss: 0.8290\n",
            "Epoch 496/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0172 - val_loss: 0.7487\n",
            "Epoch 497/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9688 - val_loss: 0.7671\n",
            "Epoch 498/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9742 - val_loss: 0.7403\n",
            "Epoch 499/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1181 - val_loss: 0.5975\n",
            "Epoch 500/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.0312 - val_loss: 0.7528\n",
            "Epoch 501/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0916 - val_loss: 0.8281\n",
            "Epoch 502/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0155 - val_loss: 0.7767\n",
            "Epoch 503/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0123 - val_loss: 0.8220\n",
            "Epoch 504/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0703 - val_loss: 1.0094\n",
            "Epoch 505/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0763 - val_loss: 0.8413\n",
            "Epoch 506/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0468 - val_loss: 0.8241\n",
            "Epoch 507/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0191 - val_loss: 0.8852\n",
            "Epoch 508/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0834 - val_loss: 0.7853\n",
            "Epoch 509/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0109 - val_loss: 0.8274\n",
            "Epoch 510/5000\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 1.0500 - val_loss: 0.8530\n",
            "Epoch 511/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9600 - val_loss: 0.6706\n",
            "Epoch 512/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0400 - val_loss: 0.9146\n",
            "Epoch 513/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1541 - val_loss: 0.7081\n",
            "Epoch 514/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1338 - val_loss: 0.9337\n",
            "Epoch 515/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1017 - val_loss: 1.0176\n",
            "Epoch 516/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9762 - val_loss: 1.0059\n",
            "Epoch 517/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0067 - val_loss: 0.7004\n",
            "Epoch 518/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0726 - val_loss: 0.7552\n",
            "Epoch 519/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9398 - val_loss: 0.8744\n",
            "Epoch 520/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1204 - val_loss: 0.9319\n",
            "Epoch 521/5000\n",
            "33/33 [==============================] - 0s 13ms/step - loss: 1.9773 - val_loss: 0.7790\n",
            "Epoch 522/5000\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 1.0228 - val_loss: 0.7555\n",
            "Epoch 523/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.1360 - val_loss: 0.8698\n",
            "Epoch 524/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1189 - val_loss: 0.5796\n",
            "Epoch 525/5000\n",
            "33/33 [==============================] - 0s 12ms/step - loss: 1.1714 - val_loss: 0.8321\n",
            "Epoch 526/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.0999 - val_loss: 0.6886\n",
            "Epoch 527/5000\n",
            "33/33 [==============================] - 0s 14ms/step - loss: 1.3040 - val_loss: 0.7914\n",
            "Epoch 528/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0536 - val_loss: 0.8276\n",
            "Epoch 529/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0855 - val_loss: 0.7437\n",
            "Epoch 530/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0930 - val_loss: 0.7906\n",
            "Epoch 531/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0190 - val_loss: 0.8068\n",
            "Epoch 532/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9290 - val_loss: 0.8228\n",
            "Epoch 533/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0538 - val_loss: 0.7613\n",
            "Epoch 534/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1636 - val_loss: 0.6011\n",
            "Epoch 535/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0931 - val_loss: 0.7540\n",
            "Epoch 536/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9098 - val_loss: 0.7411\n",
            "Epoch 537/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0051 - val_loss: 0.7745\n",
            "Epoch 538/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1697 - val_loss: 0.7835\n",
            "Epoch 539/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1020 - val_loss: 0.7832\n",
            "Epoch 540/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0225 - val_loss: 0.9128\n",
            "Epoch 541/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0366 - val_loss: 0.9180\n",
            "Epoch 542/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2763 - val_loss: 0.8963\n",
            "Epoch 543/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1698 - val_loss: 0.7676\n",
            "Epoch 544/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1448 - val_loss: 0.8003\n",
            "Epoch 545/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0244 - val_loss: 0.7603\n",
            "Epoch 546/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0651 - val_loss: 0.9251\n",
            "Epoch 547/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9575 - val_loss: 0.7165\n",
            "Epoch 548/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0167 - val_loss: 1.0493\n",
            "Epoch 549/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9016 - val_loss: 0.6983\n",
            "Epoch 550/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9452 - val_loss: 0.8306\n",
            "Epoch 551/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0098 - val_loss: 0.9201\n",
            "Epoch 552/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0317 - val_loss: 0.6979\n",
            "Epoch 553/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1641 - val_loss: 0.7796\n",
            "Epoch 554/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0154 - val_loss: 0.7634\n",
            "Epoch 555/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8226 - val_loss: 0.7167\n",
            "Epoch 556/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3204 - val_loss: 0.9338\n",
            "Epoch 557/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0361 - val_loss: 0.8920\n",
            "Epoch 558/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9901 - val_loss: 0.7743\n",
            "Epoch 559/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9900 - val_loss: 0.8329\n",
            "Epoch 560/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9962 - val_loss: 0.6913\n",
            "Epoch 561/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0619 - val_loss: 0.7070\n",
            "Epoch 562/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0726 - val_loss: 0.7019\n",
            "Epoch 563/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1675 - val_loss: 0.7982\n",
            "Epoch 564/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9903 - val_loss: 0.7749\n",
            "Epoch 565/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1063 - val_loss: 0.8197\n",
            "Epoch 566/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9816 - val_loss: 0.6485\n",
            "Epoch 567/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0510 - val_loss: 1.0114\n",
            "Epoch 568/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9891 - val_loss: 0.7913\n",
            "Epoch 569/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0062 - val_loss: 0.6788\n",
            "Epoch 570/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0351 - val_loss: 0.7558\n",
            "Epoch 571/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0574 - val_loss: 0.8029\n",
            "Epoch 572/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1203 - val_loss: 0.6813\n",
            "Epoch 573/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9938 - val_loss: 0.8130\n",
            "Epoch 574/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9499 - val_loss: 0.7274\n",
            "Epoch 575/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9492 - val_loss: 0.9093\n",
            "Epoch 576/5000\n",
            "33/33 [==============================] - 0s 12ms/step - loss: 0.9686 - val_loss: 0.8364\n",
            "Epoch 577/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.0286 - val_loss: 0.6761\n",
            "Epoch 578/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9470 - val_loss: 0.8231\n",
            "Epoch 579/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9795 - val_loss: 0.6986\n",
            "Epoch 580/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0077 - val_loss: 0.8259\n",
            "Epoch 581/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0750 - val_loss: 0.6350\n",
            "Epoch 582/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2115 - val_loss: 0.8418\n",
            "Epoch 583/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1119 - val_loss: 0.7185\n",
            "Epoch 584/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0120 - val_loss: 0.8095\n",
            "Epoch 585/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9025 - val_loss: 0.7086\n",
            "Epoch 586/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1108 - val_loss: 0.6924\n",
            "Epoch 587/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0827 - val_loss: 0.8660\n",
            "Epoch 588/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0131 - val_loss: 0.7112\n",
            "Epoch 589/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9253 - val_loss: 0.6155\n",
            "Epoch 590/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9404 - val_loss: 0.6401\n",
            "Epoch 591/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2436 - val_loss: 0.6388\n",
            "Epoch 592/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0294 - val_loss: 0.6617\n",
            "Epoch 593/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2479 - val_loss: 0.8134\n",
            "Epoch 594/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1654 - val_loss: 0.5595\n",
            "Epoch 595/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9602 - val_loss: 0.7798\n",
            "Epoch 596/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1062 - val_loss: 0.5949\n",
            "Epoch 597/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0310 - val_loss: 0.8347\n",
            "Epoch 598/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0188 - val_loss: 0.8067\n",
            "Epoch 599/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9813 - val_loss: 0.9075\n",
            "Epoch 600/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8963 - val_loss: 0.8311\n",
            "Epoch 601/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9893 - val_loss: 0.7573\n",
            "Epoch 602/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1039 - val_loss: 0.6649\n",
            "Epoch 603/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0555 - val_loss: 0.7991\n",
            "Epoch 604/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9570 - val_loss: 0.7383\n",
            "Epoch 605/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9801 - val_loss: 0.6926\n",
            "Epoch 606/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0212 - val_loss: 0.8042\n",
            "Epoch 607/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9368 - val_loss: 0.7902\n",
            "Epoch 608/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9781 - val_loss: 0.7083\n",
            "Epoch 609/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0369 - val_loss: 0.7410\n",
            "Epoch 610/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0636 - val_loss: 0.6658\n",
            "Epoch 611/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9269 - val_loss: 0.6997\n",
            "Epoch 612/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1972 - val_loss: 0.7602\n",
            "Epoch 613/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9282 - val_loss: 0.8337\n",
            "Epoch 614/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0994 - val_loss: 0.8117\n",
            "Epoch 615/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2035 - val_loss: 0.7651\n",
            "Epoch 616/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9573 - val_loss: 0.7844\n",
            "Epoch 617/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9981 - val_loss: 0.9320\n",
            "Epoch 618/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9491 - val_loss: 0.6610\n",
            "Epoch 619/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2040 - val_loss: 0.9259\n",
            "Epoch 620/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9376 - val_loss: 0.8637\n",
            "Epoch 621/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9508 - val_loss: 0.6875\n",
            "Epoch 622/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0585 - val_loss: 0.6366\n",
            "Epoch 623/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9267 - val_loss: 0.7799\n",
            "Epoch 624/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.4862 - val_loss: 0.7249\n",
            "Epoch 625/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0317 - val_loss: 0.8868\n",
            "Epoch 626/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0379 - val_loss: 0.6808\n",
            "Epoch 627/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0231 - val_loss: 0.8906\n",
            "Epoch 628/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9413 - val_loss: 0.6181\n",
            "Epoch 629/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0288 - val_loss: 0.6901\n",
            "Epoch 630/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9590 - val_loss: 0.7224\n",
            "Epoch 631/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8970 - val_loss: 0.6511\n",
            "Epoch 632/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9364 - val_loss: 0.8419\n",
            "Epoch 633/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9117 - val_loss: 0.6904\n",
            "Epoch 634/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0934 - val_loss: 0.8876\n",
            "Epoch 635/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0062 - val_loss: 0.8062\n",
            "Epoch 636/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0106 - val_loss: 0.5650\n",
            "Epoch 637/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9999 - val_loss: 0.6390\n",
            "Epoch 638/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9966 - val_loss: 0.8356\n",
            "Epoch 639/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.2575 - val_loss: 0.7347\n",
            "Epoch 640/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9305 - val_loss: 0.6272\n",
            "Epoch 641/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9065 - val_loss: 0.7554\n",
            "Epoch 642/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9606 - val_loss: 0.7542\n",
            "Epoch 643/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0154 - val_loss: 0.6779\n",
            "Epoch 644/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9657 - val_loss: 0.6949\n",
            "Epoch 645/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0423 - val_loss: 0.6572\n",
            "Epoch 646/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9381 - val_loss: 0.7095\n",
            "Epoch 647/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0343 - val_loss: 1.1192\n",
            "Epoch 648/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9110 - val_loss: 0.6355\n",
            "Epoch 649/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9531 - val_loss: 0.6518\n",
            "Epoch 650/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0599 - val_loss: 0.7813\n",
            "Epoch 651/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9007 - val_loss: 0.8571\n",
            "Epoch 652/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8799 - val_loss: 0.7796\n",
            "Epoch 653/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9797 - val_loss: 0.6621\n",
            "Epoch 654/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0011 - val_loss: 0.7713\n",
            "Epoch 655/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0948 - val_loss: 0.6885\n",
            "Epoch 656/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9167 - val_loss: 0.7207\n",
            "Epoch 657/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0431 - val_loss: 0.9285\n",
            "Epoch 658/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0947 - val_loss: 0.8620\n",
            "Epoch 659/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0239 - val_loss: 0.9087\n",
            "Epoch 660/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.3362 - val_loss: 0.8825\n",
            "Epoch 661/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0274 - val_loss: 1.0491\n",
            "Epoch 662/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9847 - val_loss: 0.8649\n",
            "Epoch 663/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1884 - val_loss: 0.7902\n",
            "Epoch 664/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9841 - val_loss: 0.6410\n",
            "Epoch 665/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0471 - val_loss: 0.7161\n",
            "Epoch 666/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9598 - val_loss: 0.6957\n",
            "Epoch 667/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1516 - val_loss: 0.8821\n",
            "Epoch 668/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0099 - val_loss: 0.7063\n",
            "Epoch 669/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9041 - val_loss: 0.6438\n",
            "Epoch 670/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.2657 - val_loss: 0.7092\n",
            "Epoch 671/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9128 - val_loss: 0.6696\n",
            "Epoch 672/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9681 - val_loss: 0.6373\n",
            "Epoch 673/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1344 - val_loss: 0.6437\n",
            "Epoch 674/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8937 - val_loss: 0.7739\n",
            "Epoch 675/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9276 - val_loss: 0.6443\n",
            "Epoch 676/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0574 - val_loss: 0.8824\n",
            "Epoch 677/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0169 - val_loss: 1.1565\n",
            "Epoch 678/5000\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.9406 - val_loss: 0.7669\n",
            "Epoch 679/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.8999 - val_loss: 0.7169\n",
            "Epoch 680/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1589 - val_loss: 0.7085\n",
            "Epoch 681/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9514 - val_loss: 0.7747\n",
            "Epoch 682/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1199 - val_loss: 0.8795\n",
            "Epoch 683/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9871 - val_loss: 0.7422\n",
            "Epoch 684/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9032 - val_loss: 0.8038\n",
            "Epoch 685/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0043 - val_loss: 0.6877\n",
            "Epoch 686/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9755 - val_loss: 0.6584\n",
            "Epoch 687/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9686 - val_loss: 0.6382\n",
            "Epoch 688/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9338 - val_loss: 0.8622\n",
            "Epoch 689/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8947 - val_loss: 0.9114\n",
            "Epoch 690/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0737 - val_loss: 0.9411\n",
            "Epoch 691/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0168 - val_loss: 0.6461\n",
            "Epoch 692/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1131 - val_loss: 0.7493\n",
            "Epoch 693/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8770 - val_loss: 0.7482\n",
            "Epoch 694/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.8432 - val_loss: 0.5558\n",
            "Epoch 695/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0320 - val_loss: 0.6061\n",
            "Epoch 696/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1212 - val_loss: 0.7269\n",
            "Epoch 697/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9659 - val_loss: 0.6987\n",
            "Epoch 698/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8790 - val_loss: 0.7811\n",
            "Epoch 699/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0323 - val_loss: 0.7478\n",
            "Epoch 700/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9557 - val_loss: 0.6632\n",
            "Epoch 701/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0007 - val_loss: 0.8205\n",
            "Epoch 702/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8965 - val_loss: 0.8125\n",
            "Epoch 703/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0044 - val_loss: 0.7093\n",
            "Epoch 704/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9295 - val_loss: 0.7976\n",
            "Epoch 705/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9691 - val_loss: 0.7245\n",
            "Epoch 706/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.1285 - val_loss: 0.9803\n",
            "Epoch 707/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9531 - val_loss: 0.6533\n",
            "Epoch 708/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9515 - val_loss: 0.6723\n",
            "Epoch 709/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9279 - val_loss: 0.6275\n",
            "Epoch 710/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9714 - val_loss: 0.8343\n",
            "Epoch 711/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0275 - val_loss: 0.7748\n",
            "Epoch 712/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9615 - val_loss: 0.6057\n",
            "Epoch 713/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9852 - val_loss: 0.7056\n",
            "Epoch 714/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8786 - val_loss: 0.6238\n",
            "Epoch 715/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9923 - val_loss: 0.7880\n",
            "Epoch 716/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9726 - val_loss: 0.9084\n",
            "Epoch 717/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0981 - val_loss: 0.5972\n",
            "Epoch 718/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9143 - val_loss: 0.6229\n",
            "Epoch 719/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9987 - val_loss: 0.6235\n",
            "Epoch 720/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9107 - val_loss: 0.6906\n",
            "Epoch 721/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0526 - val_loss: 0.7081\n",
            "Epoch 722/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9717 - val_loss: 0.6610\n",
            "Epoch 723/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9541 - val_loss: 0.7525\n",
            "Epoch 724/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9556 - val_loss: 0.6977\n",
            "Epoch 725/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.1149 - val_loss: 0.6581\n",
            "Epoch 726/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9285 - val_loss: 0.6228\n",
            "Epoch 727/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8891 - val_loss: 0.7620\n",
            "Epoch 728/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0238 - val_loss: 0.6863\n",
            "Epoch 729/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9319 - val_loss: 0.7110\n",
            "Epoch 730/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0806 - val_loss: 0.7069\n",
            "Epoch 731/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9807 - val_loss: 0.8662\n",
            "Epoch 732/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0117 - val_loss: 0.7406\n",
            "Epoch 733/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8540 - val_loss: 0.5594\n",
            "Epoch 734/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0669 - val_loss: 0.8462\n",
            "Epoch 735/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8574 - val_loss: 0.8107\n",
            "Epoch 736/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9676 - val_loss: 0.8389\n",
            "Epoch 737/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1244 - val_loss: 0.7055\n",
            "Epoch 738/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0596 - val_loss: 0.6211\n",
            "Epoch 739/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9869 - val_loss: 0.5975\n",
            "Epoch 740/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0055 - val_loss: 0.8477\n",
            "Epoch 741/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9175 - val_loss: 0.7678\n",
            "Epoch 742/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9113 - val_loss: 0.7126\n",
            "Epoch 743/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9525 - val_loss: 0.6900\n",
            "Epoch 744/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9583 - val_loss: 0.5994\n",
            "Epoch 745/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9422 - val_loss: 0.7877\n",
            "Epoch 746/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0032 - val_loss: 0.6659\n",
            "Epoch 747/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2047 - val_loss: 1.0434\n",
            "Epoch 748/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9065 - val_loss: 0.6498\n",
            "Epoch 749/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9625 - val_loss: 0.8076\n",
            "Epoch 750/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9709 - val_loss: 0.7392\n",
            "Epoch 751/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9498 - val_loss: 0.6739\n",
            "Epoch 752/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1270 - val_loss: 0.8077\n",
            "Epoch 753/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0399 - val_loss: 0.7279\n",
            "Epoch 754/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0427 - val_loss: 0.6214\n",
            "Epoch 755/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8642 - val_loss: 0.8089\n",
            "Epoch 756/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1534 - val_loss: 0.9266\n",
            "Epoch 757/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0786 - val_loss: 0.7697\n",
            "Epoch 758/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9628 - val_loss: 0.6292\n",
            "Epoch 759/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8837 - val_loss: 0.6148\n",
            "Epoch 760/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9978 - val_loss: 0.6374\n",
            "Epoch 761/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9848 - val_loss: 0.7089\n",
            "Epoch 762/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9512 - val_loss: 0.7501\n",
            "Epoch 763/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.2380 - val_loss: 0.7003\n",
            "Epoch 764/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0520 - val_loss: 0.5590\n",
            "Epoch 765/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9328 - val_loss: 0.6101\n",
            "Epoch 766/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8967 - val_loss: 0.6360\n",
            "Epoch 767/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9316 - val_loss: 0.7070\n",
            "Epoch 768/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8609 - val_loss: 0.6731\n",
            "Epoch 769/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9977 - val_loss: 0.6369\n",
            "Epoch 770/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0984 - val_loss: 0.7988\n",
            "Epoch 771/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8694 - val_loss: 0.7355\n",
            "Epoch 772/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9711 - val_loss: 0.7953\n",
            "Epoch 773/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9576 - val_loss: 0.5965\n",
            "Epoch 774/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9130 - val_loss: 0.8060\n",
            "Epoch 775/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9542 - val_loss: 0.7763\n",
            "Epoch 776/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9930 - val_loss: 0.7206\n",
            "Epoch 777/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9737 - val_loss: 0.6980\n",
            "Epoch 778/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9456 - val_loss: 0.7612\n",
            "Epoch 779/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9833 - val_loss: 0.6453\n",
            "Epoch 780/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8635 - val_loss: 0.7960\n",
            "Epoch 781/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0081 - val_loss: 0.6482\n",
            "Epoch 782/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9297 - val_loss: 0.7894\n",
            "Epoch 783/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8961 - val_loss: 0.7076\n",
            "Epoch 784/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0326 - val_loss: 0.7127\n",
            "Epoch 785/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9387 - val_loss: 0.6383\n",
            "Epoch 786/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0018 - val_loss: 0.7048\n",
            "Epoch 787/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2261 - val_loss: 0.7268\n",
            "Epoch 788/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0159 - val_loss: 0.6341\n",
            "Epoch 789/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9596 - val_loss: 0.8354\n",
            "Epoch 790/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9314 - val_loss: 0.8170\n",
            "Epoch 791/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8997 - val_loss: 0.7031\n",
            "Epoch 792/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9378 - val_loss: 0.7423\n",
            "Epoch 793/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9289 - val_loss: 0.7709\n",
            "Epoch 794/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9033 - val_loss: 0.9450\n",
            "Epoch 795/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9543 - val_loss: 0.7388\n",
            "Epoch 796/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1184 - val_loss: 0.8107\n",
            "Epoch 797/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0275 - val_loss: 0.6621\n",
            "Epoch 798/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9327 - val_loss: 0.6848\n",
            "Epoch 799/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8844 - val_loss: 0.6449\n",
            "Epoch 800/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9876 - val_loss: 0.5181\n",
            "Epoch 801/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9281 - val_loss: 0.6585\n",
            "Epoch 802/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9337 - val_loss: 0.7032\n",
            "Epoch 803/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9889 - val_loss: 0.6870\n",
            "Epoch 804/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8998 - val_loss: 0.8753\n",
            "Epoch 805/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0125 - val_loss: 0.7314\n",
            "Epoch 806/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9943 - val_loss: 0.6801\n",
            "Epoch 807/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9352 - val_loss: 0.6504\n",
            "Epoch 808/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8910 - val_loss: 1.0235\n",
            "Epoch 809/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0533 - val_loss: 0.7059\n",
            "Epoch 810/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9423 - val_loss: 0.5563\n",
            "Epoch 811/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9627 - val_loss: 0.7722\n",
            "Epoch 812/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0025 - val_loss: 0.5555\n",
            "Epoch 813/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9877 - val_loss: 0.7392\n",
            "Epoch 814/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9249 - val_loss: 0.7546\n",
            "Epoch 815/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9798 - val_loss: 0.7090\n",
            "Epoch 816/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9272 - val_loss: 0.7331\n",
            "Epoch 817/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0423 - val_loss: 0.6470\n",
            "Epoch 818/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9458 - val_loss: 0.5711\n",
            "Epoch 819/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9664 - val_loss: 0.6525\n",
            "Epoch 820/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9307 - val_loss: 0.7810\n",
            "Epoch 821/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8764 - val_loss: 0.9321\n",
            "Epoch 822/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9836 - val_loss: 0.6551\n",
            "Epoch 823/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0142 - val_loss: 0.7947\n",
            "Epoch 824/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9904 - val_loss: 0.6374\n",
            "Epoch 825/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9068 - val_loss: 0.7815\n",
            "Epoch 826/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0621 - val_loss: 0.7134\n",
            "Epoch 827/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9344 - val_loss: 0.5812\n",
            "Epoch 828/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1117 - val_loss: 0.6776\n",
            "Epoch 829/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9324 - val_loss: 0.6605\n",
            "Epoch 830/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0285 - val_loss: 0.6865\n",
            "Epoch 831/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9698 - val_loss: 0.6878\n",
            "Epoch 832/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9017 - val_loss: 0.7529\n",
            "Epoch 833/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9646 - val_loss: 0.8357\n",
            "Epoch 834/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0250 - val_loss: 0.8280\n",
            "Epoch 835/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9183 - val_loss: 0.6848\n",
            "Epoch 836/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9636 - val_loss: 0.8126\n",
            "Epoch 837/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.9977 - val_loss: 0.7429\n",
            "Epoch 838/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9024 - val_loss: 0.6111\n",
            "Epoch 839/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9217 - val_loss: 1.0048\n",
            "Epoch 840/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9810 - val_loss: 0.6374\n",
            "Epoch 841/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9429 - val_loss: 0.6661\n",
            "Epoch 842/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9109 - val_loss: 0.7787\n",
            "Epoch 843/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9251 - val_loss: 0.7374\n",
            "Epoch 844/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9960 - val_loss: 0.6712\n",
            "Epoch 845/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0273 - val_loss: 0.7620\n",
            "Epoch 846/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9759 - val_loss: 0.8165\n",
            "Epoch 847/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0614 - val_loss: 0.8128\n",
            "Epoch 848/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9432 - val_loss: 0.8384\n",
            "Epoch 849/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9060 - val_loss: 0.6988\n",
            "Epoch 850/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9682 - val_loss: 0.6548\n",
            "Epoch 851/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1011 - val_loss: 0.7771\n",
            "Epoch 852/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9109 - val_loss: 0.6272\n",
            "Epoch 853/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8372 - val_loss: 0.7053\n",
            "Epoch 854/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8817 - val_loss: 0.7270\n",
            "Epoch 855/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9994 - val_loss: 0.7094\n",
            "Epoch 856/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0670 - val_loss: 0.8278\n",
            "Epoch 857/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0448 - val_loss: 0.6138\n",
            "Epoch 858/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9224 - val_loss: 0.5993\n",
            "Epoch 859/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0040 - val_loss: 0.6818\n",
            "Epoch 860/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9347 - val_loss: 0.7710\n",
            "Epoch 861/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9534 - val_loss: 0.8814\n",
            "Epoch 862/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9629 - val_loss: 0.7352\n",
            "Epoch 863/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9439 - val_loss: 0.6748\n",
            "Epoch 864/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9355 - val_loss: 0.6023\n",
            "Epoch 865/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9555 - val_loss: 0.7207\n",
            "Epoch 866/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9399 - val_loss: 0.6007\n",
            "Epoch 867/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9851 - val_loss: 0.7995\n",
            "Epoch 868/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8961 - val_loss: 0.8006\n",
            "Epoch 869/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0476 - val_loss: 0.7503\n",
            "Epoch 870/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9600 - val_loss: 0.7421\n",
            "Epoch 871/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0363 - val_loss: 0.6619\n",
            "Epoch 872/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9580 - val_loss: 0.7350\n",
            "Epoch 873/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0772 - val_loss: 0.6804\n",
            "Epoch 874/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9232 - val_loss: 0.6902\n",
            "Epoch 875/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8873 - val_loss: 0.7370\n",
            "Epoch 876/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9062 - val_loss: 0.8857\n",
            "Epoch 877/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9205 - val_loss: 0.6535\n",
            "Epoch 878/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.8663 - val_loss: 0.6683\n",
            "Epoch 879/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8974 - val_loss: 0.7189\n",
            "Epoch 880/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9240 - val_loss: 0.6296\n",
            "Epoch 881/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9495 - val_loss: 0.8691\n",
            "Epoch 882/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9386 - val_loss: 0.6540\n",
            "Epoch 883/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9123 - val_loss: 0.7555\n",
            "Epoch 884/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9605 - val_loss: 0.6982\n",
            "Epoch 885/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.8489 - val_loss: 0.6801\n",
            "Epoch 886/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8699 - val_loss: 0.6797\n",
            "Epoch 887/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0854 - val_loss: 0.6351\n",
            "Epoch 888/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9876 - val_loss: 0.6476\n",
            "Epoch 889/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9849 - val_loss: 0.6697\n",
            "Epoch 890/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9029 - val_loss: 0.7299\n",
            "Epoch 891/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9650 - val_loss: 0.6690\n",
            "Epoch 892/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0592 - val_loss: 0.6497\n",
            "Epoch 893/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9397 - val_loss: 0.7571\n",
            "Epoch 894/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0129 - val_loss: 0.6085\n",
            "Epoch 895/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8819 - val_loss: 0.5540\n",
            "Epoch 896/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0995 - val_loss: 0.6834\n",
            "Epoch 897/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9250 - val_loss: 0.8175\n",
            "Epoch 898/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9475 - val_loss: 0.7483\n",
            "Epoch 899/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9513 - val_loss: 0.7563\n",
            "Epoch 900/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.1453 - val_loss: 0.7289\n",
            "Epoch 901/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0067 - val_loss: 0.7810\n",
            "Epoch 902/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9449 - val_loss: 0.7112\n",
            "Epoch 903/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9743 - val_loss: 0.9142\n",
            "Epoch 904/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9746 - val_loss: 0.5752\n",
            "Epoch 905/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9891 - val_loss: 0.6968\n",
            "Epoch 906/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9316 - val_loss: 0.6679\n",
            "Epoch 907/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0322 - val_loss: 0.6546\n",
            "Epoch 908/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9760 - val_loss: 0.7938\n",
            "Epoch 909/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9220 - val_loss: 0.5725\n",
            "Epoch 910/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9061 - val_loss: 0.5954\n",
            "Epoch 911/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0210 - val_loss: 0.7590\n",
            "Epoch 912/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0170 - val_loss: 0.7043\n",
            "Epoch 913/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9302 - val_loss: 0.7138\n",
            "Epoch 914/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0051 - val_loss: 0.6238\n",
            "Epoch 915/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8818 - val_loss: 0.7524\n",
            "Epoch 916/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8672 - val_loss: 0.7256\n",
            "Epoch 917/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0014 - val_loss: 0.8345\n",
            "Epoch 918/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0161 - val_loss: 0.6354\n",
            "Epoch 919/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9413 - val_loss: 0.9110\n",
            "Epoch 920/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.0094 - val_loss: 0.7683\n",
            "Epoch 921/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0524 - val_loss: 1.0775\n",
            "Epoch 922/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9872 - val_loss: 0.9189\n",
            "Epoch 923/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9265 - val_loss: 0.6918\n",
            "Epoch 924/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9017 - val_loss: 0.6555\n",
            "Epoch 925/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9672 - val_loss: 0.6672\n",
            "Epoch 926/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9456 - val_loss: 0.8762\n",
            "Epoch 927/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9783 - val_loss: 0.6787\n",
            "Epoch 928/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9358 - val_loss: 0.8345\n",
            "Epoch 929/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9870 - val_loss: 0.6524\n",
            "Epoch 930/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0410 - val_loss: 0.7177\n",
            "Epoch 931/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0048 - val_loss: 0.8330\n",
            "Epoch 932/5000\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.9587 - val_loss: 0.7241\n",
            "Epoch 933/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9307 - val_loss: 0.6987\n",
            "Epoch 934/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9345 - val_loss: 0.8690\n",
            "Epoch 935/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8405 - val_loss: 0.7623\n",
            "Epoch 936/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8999 - val_loss: 0.6838\n",
            "Epoch 937/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9994 - val_loss: 0.6925\n",
            "Epoch 938/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9616 - val_loss: 0.6134\n",
            "Epoch 939/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8741 - val_loss: 0.6757\n",
            "Epoch 940/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9462 - val_loss: 0.7216\n",
            "Epoch 941/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9522 - val_loss: 0.6666\n",
            "Epoch 942/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9707 - val_loss: 0.6278\n",
            "Epoch 943/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9039 - val_loss: 0.6984\n",
            "Epoch 944/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0354 - val_loss: 0.6170\n",
            "Epoch 945/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9861 - val_loss: 0.7029\n",
            "Epoch 946/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9071 - val_loss: 0.7691\n",
            "Epoch 947/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9899 - val_loss: 0.7603\n",
            "Epoch 948/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9563 - val_loss: 0.6713\n",
            "Epoch 949/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0143 - val_loss: 0.5714\n",
            "Epoch 950/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8959 - val_loss: 0.7842\n",
            "Epoch 951/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9542 - val_loss: 0.7269\n",
            "Epoch 952/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8844 - val_loss: 0.7217\n",
            "Epoch 953/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9188 - val_loss: 0.6592\n",
            "Epoch 954/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9393 - val_loss: 0.7248\n",
            "Epoch 955/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9574 - val_loss: 0.6354\n",
            "Epoch 956/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9003 - val_loss: 0.6660\n",
            "Epoch 957/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9639 - val_loss: 0.6681\n",
            "Epoch 958/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9277 - val_loss: 0.6594\n",
            "Epoch 959/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0167 - val_loss: 0.6368\n",
            "Epoch 960/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9869 - val_loss: 0.6380\n",
            "Epoch 961/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9284 - val_loss: 0.6978\n",
            "Epoch 962/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8871 - val_loss: 0.6746\n",
            "Epoch 963/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9337 - val_loss: 0.5747\n",
            "Epoch 964/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9538 - val_loss: 0.6833\n",
            "Epoch 965/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9771 - val_loss: 0.6001\n",
            "Epoch 966/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9387 - val_loss: 0.6321\n",
            "Epoch 967/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9236 - val_loss: 0.6570\n",
            "Epoch 968/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0110 - val_loss: 0.7003\n",
            "Epoch 969/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8917 - val_loss: 0.6168\n",
            "Epoch 970/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9236 - val_loss: 0.7860\n",
            "Epoch 971/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0725 - val_loss: 0.7475\n",
            "Epoch 972/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8920 - val_loss: 0.7134\n",
            "Epoch 973/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0507 - val_loss: 0.7023\n",
            "Epoch 974/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9849 - val_loss: 0.5776\n",
            "Epoch 975/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0558 - val_loss: 0.7523\n",
            "Epoch 976/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9531 - val_loss: 0.6172\n",
            "Epoch 977/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8847 - val_loss: 0.5678\n",
            "Epoch 978/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9406 - val_loss: 0.6606\n",
            "Epoch 979/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9454 - val_loss: 0.7184\n",
            "Epoch 980/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8669 - val_loss: 0.7340\n",
            "Epoch 981/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9605 - val_loss: 0.6118\n",
            "Epoch 982/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.8560 - val_loss: 0.7915\n",
            "Epoch 983/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0339 - val_loss: 0.7317\n",
            "Epoch 984/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0343 - val_loss: 0.7202\n",
            "Epoch 985/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9076 - val_loss: 0.8134\n",
            "Epoch 986/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8470 - val_loss: 0.6473\n",
            "Epoch 987/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9260 - val_loss: 0.8345\n",
            "Epoch 988/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8676 - val_loss: 0.6411\n",
            "Epoch 989/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9373 - val_loss: 0.6744\n",
            "Epoch 990/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.0284 - val_loss: 0.7308\n",
            "Epoch 991/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9001 - val_loss: 0.6369\n",
            "Epoch 992/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9847 - val_loss: 0.8214\n",
            "Epoch 993/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0887 - val_loss: 0.7438\n",
            "Epoch 994/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8866 - val_loss: 0.6824\n",
            "Epoch 995/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9885 - val_loss: 0.6195\n",
            "Epoch 996/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8970 - val_loss: 0.5611\n",
            "Epoch 997/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0524 - val_loss: 0.6095\n",
            "Epoch 998/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9448 - val_loss: 0.6815\n",
            "Epoch 999/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9854 - val_loss: 0.6844\n",
            "Epoch 1000/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9891 - val_loss: 0.6543\n",
            "Epoch 1001/5000\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 1.1023 - val_loss: 0.9116\n",
            "Epoch 1002/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8841 - val_loss: 0.7694\n",
            "Epoch 1003/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0076 - val_loss: 0.6767\n",
            "Epoch 1004/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8959 - val_loss: 0.6476\n",
            "Epoch 1005/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8909 - val_loss: 0.6104\n",
            "Epoch 1006/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9748 - val_loss: 0.7025\n",
            "Epoch 1007/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9421 - val_loss: 0.7201\n",
            "Epoch 1008/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9865 - val_loss: 0.9397\n",
            "Epoch 1009/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9604 - val_loss: 0.6766\n",
            "Epoch 1010/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0691 - val_loss: 0.7428\n",
            "Epoch 1011/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8553 - val_loss: 0.7524\n",
            "Epoch 1012/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9277 - val_loss: 0.7534\n",
            "Epoch 1013/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 1.0760 - val_loss: 0.6777\n",
            "Epoch 1014/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8854 - val_loss: 0.7302\n",
            "Epoch 1015/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9104 - val_loss: 0.7142\n",
            "Epoch 1016/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9259 - val_loss: 0.8945\n",
            "Epoch 1017/5000\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.9014 - val_loss: 0.6515\n",
            "Epoch 1018/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.0192 - val_loss: 0.6090\n",
            "Epoch 1019/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9453 - val_loss: 0.6532\n",
            "Epoch 1020/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9224 - val_loss: 0.7101\n",
            "Epoch 1021/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9914 - val_loss: 0.5983\n",
            "Epoch 1022/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8629 - val_loss: 0.6756\n",
            "Epoch 1023/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9064 - val_loss: 0.6088\n",
            "Epoch 1024/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9346 - val_loss: 0.6714\n",
            "Epoch 1025/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9594 - val_loss: 0.7992\n",
            "Epoch 1026/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9278 - val_loss: 0.7133\n",
            "Epoch 1027/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.8676 - val_loss: 0.6666\n",
            "Epoch 1028/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0095 - val_loss: 0.7336\n",
            "Epoch 1029/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9443 - val_loss: 0.8256\n",
            "Epoch 1030/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9153 - val_loss: 0.6830\n",
            "Epoch 1031/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9442 - val_loss: 0.6161\n",
            "Epoch 1032/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9900 - val_loss: 0.7977\n",
            "Epoch 1033/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0226 - val_loss: 0.6068\n",
            "Epoch 1034/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9147 - val_loss: 0.7190\n",
            "Epoch 1035/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8585 - val_loss: 0.6980\n",
            "Epoch 1036/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9690 - val_loss: 0.6647\n",
            "Epoch 1037/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9392 - val_loss: 0.6286\n",
            "Epoch 1038/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9311 - val_loss: 0.7782\n",
            "Epoch 1039/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9589 - val_loss: 0.7066\n",
            "Epoch 1040/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9707 - val_loss: 0.6074\n",
            "Epoch 1041/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8880 - val_loss: 0.7973\n",
            "Epoch 1042/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9387 - val_loss: 0.6758\n",
            "Epoch 1043/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9879 - val_loss: 0.7896\n",
            "Epoch 1044/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9580 - val_loss: 0.7407\n",
            "Epoch 1045/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9035 - val_loss: 0.7622\n",
            "Epoch 1046/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8923 - val_loss: 0.9939\n",
            "Epoch 1047/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1430 - val_loss: 0.5943\n",
            "Epoch 1048/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9162 - val_loss: 0.6512\n",
            "Epoch 1049/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8462 - val_loss: 0.9750\n",
            "Epoch 1050/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9134 - val_loss: 0.7080\n",
            "Epoch 1051/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0289 - val_loss: 0.7279\n",
            "Epoch 1052/5000\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.9163 - val_loss: 0.6364\n",
            "Epoch 1053/5000\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8989 - val_loss: 0.5806\n",
            "Epoch 1054/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9561 - val_loss: 0.9836\n",
            "Epoch 1055/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.9422 - val_loss: 0.7551\n",
            "Epoch 1056/5000\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.8789 - val_loss: 0.6540\n",
            "Epoch 1057/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8996 - val_loss: 0.6858\n",
            "Epoch 1058/5000\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1052 - val_loss: 0.7107\n",
            "Epoch 1059/5000\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 1.0312 - val_loss: 0.6655\n",
            "Epoch 1060/5000\n",
            "32/33 [============================>.] - ETA: 0s - loss: 0.9457Restoring model weights from the end of the best epoch: 60.\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.9451 - val_loss: 0.7524\n",
            "Epoch 1060: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-75-b383c653ffcb>:9: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim((0, 10))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.5123336315155029\n",
            "Train loss: 1.7024805545806885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7dd8fa039900> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.313475489616394\n",
            "dO18 RMSE: 0.9823125455460783\n",
            "EXPECTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0          26.130            0.19025\n",
            "1          26.984            0.40123\n",
            "2          24.656            0.17728\n",
            "3          26.356            0.03953\n",
            "4          23.962            0.30992\n",
            "5          24.848            0.15842\n",
            "6          25.080            0.05125\n",
            "7          26.546            2.14378\n",
            "8          25.682            0.94472\n",
            "9          24.028            0.45852\n",
            "10         23.944            0.15813\n",
            "11         23.752            0.52437\n",
            "12         26.018            0.96417\n",
            "\n",
            "PREDICTED:\n",
            "    d18O_cel_mean  d18O_cel_variance\n",
            "0       25.793682           0.753814\n",
            "1       25.793682           0.753814\n",
            "2       25.071218           0.752012\n",
            "3       25.115622           0.750865\n",
            "4       25.128965           0.754161\n",
            "5       25.054588           0.752553\n",
            "6       25.054588           0.752553\n",
            "7       25.128975           0.754161\n",
            "8       25.071230           0.752012\n",
            "9       25.054575           0.752553\n",
            "10      25.455435           0.749170\n",
            "11      25.116690           0.754351\n",
            "12      25.455435           0.749170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/MyDrive/amazon_rainforest_files/variational/model/random_ablated_0809_ensemble.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Grouped, fixed, all columns\n",
        "\n",
        "We can't (easily) generate isoscapes with these because the isoscapes for 'predkrig_br_lat_ISORG', 'Iso_Oxi_Stack_mean_TERZER',\n",
        "                   'isoscape_fullmodel_d18O_prec_REGRESSION' are not easily retrievable... but I'm curious how much better the model is if these columns are included."
      ],
      "metadata": {
        "id": "wPDSSm__DR53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_fixed_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_train_fixed_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_test_fixed_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_validation_fixed_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_passthrough = [\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "columns_to_scale = []\n",
        "columns_to_standardize = [\n",
        "    'lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature','Mean Annual Precipitation',\n",
        "    'Iso_Oxi_Stack_mean_TERZER',\n",
        "    'isoscape_fullmodel_d18O_prec_REGRESSION']\n",
        "\n",
        "data = load_and_scale(grouped_fixed_fileset, columns_to_passthrough, columns_to_scale, columns_to_standardize)\n",
        "model = train_and_evaluate(data, \"fixed_all_0809_ensemble\", training_batch_size=3)\n",
        "model.save(get_model_save_location(\"fixed_all_0809_ensemble.tf\"), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location('fixed_all_0809_ensemble.pkl'))"
      ],
      "metadata": {
        "id": "V9_5iUkDVoCV",
        "outputId": "ab71ea2e-c289-48db-db54-b1726a50b23b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: GTiff/GeoTIFF\n",
            "Size is 541 x 467 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.922043, 5.233124)\n",
            "Pixel Size = (0.08333, -0.08333)\n",
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('lat_standardizer', StandardScaler(), ['lat']),\n",
            "                                ('long_standardizer', StandardScaler(),\n",
            "                                 ['long']),\n",
            "                                ('VPD_standardizer', StandardScaler(), ['VPD']),\n",
            "                                ('RH_standardizer', StandardScaler(), ['RH']),\n",
            "                                ('PET_standardizer', StandardScaler(), ['PET']),\n",
            "                                ('DEM_standardizer', StandardScaler(), ['DEM']),\n",
            "                                ('PA_standardizer'...\n",
            "                                ('Mean Annual Temperature_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation']),\n",
            "                                ('Iso_Oxi_Stack_mean_TERZER_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Iso_Oxi_Stack_mean_TERZER']),\n",
            "                                ('isoscape_fullmodel_d18O_prec_REGRESSION_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['isoscape_fullmodel_d18O_prec_REGRESSION'])])\n",
            "==================\n",
            "fixed_all_0809_ensemble\n",
            "Model: \"model_16\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_18 (InputLayer)          [(None, 14)]         0           []                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_33 (S  (None, 12)          0           ['input_18[0][0]']               \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_34 (Dense)               (None, 20)           260         ['tf.__operators__.getitem_33[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_34 (S  (None,)             0           ['input_18[0][0]']               \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_35 (Dense)               (None, 20)           420         ['dense_34[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_35 (S  (None,)             0           ['input_18[0][0]']               \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " tf.expand_dims_22 (TFOpLambda)  (None, 1)           0           ['tf.__operators__.getitem_34[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " pred_mean_residual (Dense)     (None, 1)            21          ['dense_35[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_23 (TFOpLambda)  (None, 1)           0           ['tf.__operators__.getitem_35[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " pred_var_output (Dense)        (None, 1)            21          ['dense_35[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_21 (TFOpL  (None, 1)           0           ['tf.expand_dims_22[0][0]',      \n",
            " ambda)                                                           'pred_mean_residual[0][0]']     \n",
            "                                                                                                  \n",
            " lambda_16 (Lambda)             (None, 1)            0           ['tf.expand_dims_23[0][0]',      \n",
            "                                                                  'pred_var_output[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate_16 (Concatenate)   (None, 2)            0           ['tf.__operators__.add_21[0][0]',\n",
            "                                                                  'lambda_16[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 722\n",
            "Trainable params: 722\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-5e0a630df2b7>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrouped_fixed_fileset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_passthrough\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_standardize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fixed_all_0809_ensemble\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_model_save_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fixed_all_0809_ensemble.tf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_scaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_model_save_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fixed_all_0809_ensemble.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-108-b383c653ffcb>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(sp, run_id, training_batch_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   history, model = train_or_update_variational_model(\n\u001b[0m\u001b[1;32m     25\u001b[0m       \u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       lr=0.0001, model_file=run_id+\".h5\", use_checkpoint=False)\n",
            "\u001b[0;32m<ipython-input-103-e326eb4ccdaf>\u001b[0m in \u001b[0;36mtrain_or_update_variational_model\u001b[0;34m(sp, hidden_layers, epochs, batch_size, lr, model_file, use_checkpoint)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mget_model_save_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         custom_objects={\"kl_divergence\": kl_divergence})\n\u001b[0;32m---> 63\u001b[0;31m   history = model.fit(sp.train.X, sp.train.Y, verbose=0, epochs=epochs,\n\u001b[0m\u001b[1;32m     64\u001b[0m                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Grouped fixed, ablating other columns besides kriging"
      ],
      "metadata": {
        "id": "h1uuaygyDvXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_fixed_fileset = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_train_fixed_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_test_fixed_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/canonical/uc_davis_validation_fixed_grouped.csv'),\n",
        "}\n",
        "\n",
        "columns_to_passthrough = [\n",
        "    'ordinary_kriging_linear_d18O_predicted_mean',\n",
        "    'ordinary_kriging_linear_d18O_predicted_variance']\n",
        "columns_to_scale = []\n",
        "columns_to_standardize =  ['lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "    'Mean Annual Temperature','Mean Annual Precipitation']\n",
        "\n",
        "data = load_and_scale(grouped_fixed_fileset, columns_to_passthrough, columns_to_scale, columns_to_standardize)\n",
        "model = train_and_evaluate(data, 'fixed_ablated_0809_ensemble', training_batch_size=3)\n",
        "model.save(get_model_save_location('fixed_ablated_0809_ensemble.tf'), save_format='tf')\n",
        "dump(data.feature_scaler, get_model_save_location('fixed_ablated_0809_ensemble.pkl'))"
      ],
      "metadata": {
        "id": "WQ60MVzlD5DJ",
        "outputId": "052813a7-24d2-4fda-a7fc-0cc9ea7812a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver: GTiff/GeoTIFF\n",
            "Size is 541 x 467 x 1\n",
            "Projection is GEOGCS[\"SIRGAS 2000\",DATUM[\"Sistema_de_Referencia_Geocentrico_para_las_AmericaS_2000\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6674\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4674\"]]\n",
            "Origin = (-73.922043, 5.233124)\n",
            "Pixel Size = (0.08333, -0.08333)\n",
            "ColumnTransformer(remainder='passthrough',\n",
            "                  transformers=[('lat_standardizer', StandardScaler(), ['lat']),\n",
            "                                ('long_standardizer', StandardScaler(),\n",
            "                                 ['long']),\n",
            "                                ('VPD_standardizer', StandardScaler(), ['VPD']),\n",
            "                                ('RH_standardizer', StandardScaler(), ['RH']),\n",
            "                                ('PET_standardizer', StandardScaler(), ['PET']),\n",
            "                                ('DEM_standardizer', StandardScaler(), ['DEM']),\n",
            "                                ('PA_standardizer', StandardScaler(), ['PA']),\n",
            "                                ('Mean Annual Temperature_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Temperature']),\n",
            "                                ('Mean Annual Precipitation_standardizer',\n",
            "                                 StandardScaler(),\n",
            "                                 ['Mean Annual Precipitation'])])\n",
            "==================\n",
            "fixed_ablated_0809_ensemble\n",
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_19 (InputLayer)          [(None, 12)]         0           []                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_36 (S  (None, 10)          0           ['input_19[0][0]']               \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_36 (Dense)               (None, 20)           220         ['tf.__operators__.getitem_36[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_37 (S  (None,)             0           ['input_19[0][0]']               \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " dense_37 (Dense)               (None, 20)           420         ['dense_36[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_38 (S  (None,)             0           ['input_19[0][0]']               \n",
            " licingOpLambda)                                                                                  \n",
            "                                                                                                  \n",
            " tf.expand_dims_24 (TFOpLambda)  (None, 1)           0           ['tf.__operators__.getitem_37[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " pred_mean_residual (Dense)     (None, 1)            21          ['dense_37[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_25 (TFOpLambda)  (None, 1)           0           ['tf.__operators__.getitem_38[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " pred_var_output (Dense)        (None, 1)            21          ['dense_37[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_22 (TFOpL  (None, 1)           0           ['tf.expand_dims_24[0][0]',      \n",
            " ambda)                                                           'pred_mean_residual[0][0]']     \n",
            "                                                                                                  \n",
            " lambda_17 (Lambda)             (None, 1)            0           ['tf.expand_dims_25[0][0]',      \n",
            "                                                                  'pred_var_output[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate_17 (Concatenate)   (None, 2)            0           ['tf.__operators__.add_22[0][0]',\n",
            "                                                                  'lambda_17[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 682\n",
            "Trainable params: 682\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}