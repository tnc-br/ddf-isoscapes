{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnc-br/ddf-isoscapes/blob/split_experiments/dnn/variational_split_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DNN regressors\n",
        "\n",
        "A variational model to find the mean/variance of O18 ratios at a particular lat/lon in the Brazilian Amazon. At the bottom of the colab, there are utilities to generate isoscapes from this model."
      ],
      "metadata": {
        "id": "-0IfT3kGwgK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "henIPlAPCb4i"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "#@title Debugging\n",
        "# See https://zohaib.me/debugging-in-google-collab-notebook/ for tips,\n",
        "# as well as docs for pdb and ipdb.\n",
        "DEBUG = False #@param {type:\"boolean\"}\n",
        "GDRIVE_BASE = \"/content/drive\" #@param\n",
        "TRAIN = \"/MyDrive/amazon_rainforest_files/amazon_sample_data/uc_davis_2023_08_12_train_random_ungrouped.csv\" #@param\n",
        "VALIDATION = \"/MyDrive/amazon_rainforest_files/amazon_sample_data/uc_davis_2023_08_12_validation_random_ungrouped.csv\" #@param\n",
        "TEST = \"/MyDrive/amazon_rainforest_files/amazon_sample_data/uc_davis_2023_08_12_test_random_ungrouped.csv\" #@param\n",
        "RASTER_BASE = \"/MyDrive/amazon_rainforest_files/amazon_rasters/\" #@param\n",
        "MODEL_SAVE_LOCATION = \"/MyDrive/amazon_rainforest_files/variational/model/\" #@param\n",
        "OUTPUT_RASTER_BASE = \"/MyDrive/amazon_rainforest_files/variational/rasters/\" #@param\n",
        "EXP_ID = \"ungrouped_random\" #@param\n",
        "\n",
        "def format_dataframe_path(param) -> str:\n",
        "  root = GDRIVE_BASE if GDRIVE_BASE else \"\"\n",
        "  return f\"{root}{param}\"\n",
        "\n",
        "def get_model_save_location(filename) -> str:\n",
        "  root = GDRIVE_BASE if GDRIVE_BASE else \"\"\n",
        "  return f\"{root}{MODEL_SAVE_LOCATION}{filename}\"\n",
        "\n",
        "RUN_ID = \"variational_ucd_yearly.\" + \\\n",
        " (TAXONOMIC_FAMILY if TAXONOMIC_FAMILY else \"all\") + \\\n",
        " \".\" + EXP_ID + \".\" + str(datetime.date.today())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xeNtt6exCb4n",
        "outputId": "6af5f26a-1c43-4ced-d6e9-49584f820ec4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Access data stored on Google Drive\n",
        "if GDRIVE_BASE:\n",
        "    from google.colab import drive\n",
        "    drive.mount(GDRIVE_BASE)\n",
        "\n",
        "if DEBUG:\n",
        "    %pip install -Uqq ipdb\n",
        "    import ipdb\n",
        "    %pdb on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQrEv57zCb4q"
      },
      "source": [
        "#Data preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path: str):\n",
        "  df = pd.read_csv(path, encoding=\"ISO-8859-1\", sep=',')\n",
        "  df = df[df['d18O_cel_variance'].notna()]\n",
        "\n",
        "  # Family is too sparse. Too many families exist in validation/test that won't\n",
        "  # exist in train, so drop it.\n",
        "  X = df.drop([\"d18O_cel_mean\", \"d18O_cel_variance\", \"Code\", \"Family\", \"Unnamed: 0\"], axis=1)\n",
        "  Y = df[[\"d18O_cel_mean\", \"d18O_cel_variance\"]]\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "6XMee1aHfcik"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardization"
      ],
      "metadata": {
        "id": "DtkKhMOtb6cS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, Normalizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "def create_feature_scaler(X: pd.DataFrame):\n",
        "  columns_to_normalize = ['lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "       'Mean Annual Temperature', 'Mean Annual Precipitation',\n",
        "       'Iso_Oxi_Stack_mean_TERZER', 'predkrig_br_lat_ISORG',\n",
        "       'isoscape_fullmodel_d18O_prec_REGRESSION']\n",
        "  feature_scaler = ColumnTransformer([\n",
        "      ('feature_normalizer', Normalizer(), columns_to_normalize)],\n",
        "      remainder='passthrough')\n",
        "  feature_scaler.fit(X_train)\n",
        "  return feature_scaler\n",
        "\n",
        "def create_label_scaler(Y: pd.DataFrame):\n",
        "  # CODE REVIEW QUESTION: Standardization of variances will produce negative\n",
        "  # variances. Any workarounds or should I just not try it?\n",
        "  label_scaler = ColumnTransformer([\n",
        "      ('label_std_scaler', StandardScaler(), ['d18O_cel_mean'])],\n",
        "      remainder='passthrough')\n",
        "  label_scaler.fit(Y)\n",
        "  return label_scaler\n",
        "\n",
        "def scale(X: pd.DataFrame, Y: pd.DataFrame, feature_scaler, label_scaler):\n",
        "  # transform() outputs numpy arrays :(  need to convert back to DataFrame.\n",
        "  X_standardized = pd.DataFrame(feature_scaler.transform(X),\n",
        "                        index=X.index, columns=X.columns)\n",
        "  Y_standardized = pd.DataFrame(label_scaler.transform(Y),\n",
        "                                      index=Y.index, columns=Y.columns)\n",
        "  return X_standardized, Y_standardized"
      ],
      "metadata": {
        "id": "XSDwdvMkb7w8"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just a class organization, holds each scaled dataset and the scaler used.\n",
        "# Useful for unscaling predictions.\n",
        "class ScaledDataset():\n",
        "  def __init__(self, feature_scaler, label_scaler,\n",
        "               X_train = None, X_val = None, X_test = None,\n",
        "               Y_train = None, Y_val = None, Y_test = None):\n",
        "    self.feature_scaler = feature_scaler\n",
        "    self.label_scaler = label_scaler\n",
        "    self.X_train = X_train\n",
        "    self.X_val = X_val\n",
        "    self.X_test = X_test\n",
        "    self.Y_train = Y_train\n",
        "    self.Y_val = Y_val\n",
        "    self.Y_test = Y_test\n",
        "\n",
        "\n",
        "def load_and_scale(config: Dict) -> ScaledDataset:\n",
        "  X_train, Y_train = load_dataset(format_dataframe_path(config['TRAIN']))\n",
        "  X_val, Y_val = load_dataset(format_dataframe_path(config['VALIDATION']))\n",
        "  X_test, Y_test = load_dataset(format_dataframe_path(config['TEST']))\n",
        "\n",
        "  feature_scaler = create_feature_scaler(X_train)\n",
        "  label_scaler = create_label_scaler(Y_train)\n",
        "  X_train_scaled, Y_train_scaled = scale(X_train, Y_train, feature_scaler, label_scaler)\n",
        "  X_val_scaled, Y_val_scaled = scale(X_val, Y_val, feature_scaler, label_scaler)\n",
        "  X_test_scaled, Y_test_scaled = scale(X_test, Y_test, feature_scaler, label_scaler)\n",
        "  return ScaledDataset(\n",
        "      feature_scaler, label_scaler,\n",
        "      X_train=X_train_scaled, X_val=X_val_scaled, X_test=X_test_scaled,\n",
        "      Y_train=Y_train_scaled, Y_val=Y_val_scaled, Y_test=Y_test_scaled)\n"
      ],
      "metadata": {
        "id": "_kf2e_fKon2P"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition\n",
        "\n"
      ],
      "metadata": {
        "id": "usGznR593LZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KL Loss function:"
      ],
      "metadata": {
        "id": "khK7C8WvU8ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# log(σ2/σ1) + ( σ1^2+(μ1−μ2)^2 ) / 2* σ^2   − 1/2\n",
        "def kl_divergence(real, predicted):\n",
        "    real_value = tf.gather(real, [0], axis=1)\n",
        "    real_std = tf.math.sqrt(tf.gather(real, [1], axis=1))\n",
        "\n",
        "    predicted_value = tf.gather(predicted, [0], axis=1)\n",
        "    predicted_std = tf.math.sqrt(tf.gather(predicted, [1], axis=1))\n",
        "\n",
        "    kl_loss = -0.5 + tf.math.log(predicted_std/real_std) + \\\n",
        "     (tf.square(real_std) + tf.square(real_value - predicted_value))/ \\\n",
        "     (2*tf.square(predicted_std))\n",
        "\n",
        "    return tf.math.reduce_mean(kl_loss)\n",
        "\n",
        "def symmetric_kl(real, predicted):\n",
        "  return kl_divergence(real, predicted)"
      ],
      "metadata": {
        "id": "urGjYNNnemX6"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the loss function:"
      ],
      "metadata": {
        "id": "fJzBFWQVeqNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "\n",
        "test_real = tf.convert_to_tensor(np.array([[1, 0.02]]))\n",
        "test_pred = tf.convert_to_tensor(np.array([[0.98, 0.021]]))\n",
        "\n",
        "# https://screenshot.googleplex.com/5WM9dinAbhR26ZS\n",
        "assert float(kl_divergence(test_real, test_pred)) == pytest.approx(0.0101094, 1e-5)\n",
        "\n",
        "test_neg_real = tf.convert_to_tensor(np.array([[32.32, 0.0344]]))\n",
        "test_neg_pred = tf.convert_to_tensor(np.array([[32.01, -0.322]]))\n",
        "\n",
        "# Negative variance causes NaN\n",
        "assert tf.math.is_nan(kl_divergence(test_neg_real, test_neg_pred))\n",
        "\n",
        "test_real_2d = tf.convert_to_tensor(np.array(\n",
        "    [[1.00, 0.020],\n",
        "     [1.01, 0.042]]))\n",
        "test_pred_2d = tf.convert_to_tensor(np.array(\n",
        "    [[0.98, 0.021],\n",
        "     [0.99, 0.012]]))\n",
        "\n",
        "# Should reduce to the average loss of all rows.\n",
        "assert float(kl_divergence(test_real_2d, test_pred_2d)) == pytest.approx(\n",
        "    sum([0.0101094, 0.6402851])/2, 1e-5)"
      ],
      "metadata": {
        "id": "48TaPd70erSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model definition"
      ],
      "metadata": {
        "id": "8rI6qPRh7oO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=500, min_delta=0.001, verbose=1,\n",
        "    restore_best_weights=True, start_from_epoch=0)\n",
        "\n",
        "# I was experimenting with models that took longer to train, and used this\n",
        "# checkpointing callback to periodically save the model. It's optional.\n",
        "def get_checkpoint_callback(model_file):\n",
        "  return ModelCheckpoint(\n",
        "      get_model_save_location(model_file),\n",
        "      monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
        "\n",
        "def train_vars(X: pd.DataFrame,\n",
        "        Y: pd.DataFrame,\n",
        "        hidden_layers: List[int],\n",
        "        epochs: int,\n",
        "        batch_size: int,\n",
        "        lr: float,\n",
        "        validation_data: Tuple[pd.DataFrame, pd.DataFrame],\n",
        "        model_file=None,\n",
        "        use_checkpoint=False):\n",
        "  callbacks_list = [early_stop, get_checkpoint_callback(model_file)]\n",
        "  if not use_checkpoint:\n",
        "    inputs = keras.Input(shape=(X.shape[1],))\n",
        "    x = inputs\n",
        "    for layer_size in hidden_layers:\n",
        "      x = keras.layers.Dense(\n",
        "          layer_size, activation='relu')(x)\n",
        "    mean_output = keras.layers.Dense(1, name='mean_output')(x)\n",
        "\n",
        "    # We can not have negative variance. Apply very little variance.\n",
        "    var_output = keras.layers.Dense(1, name='var_output')(x)\n",
        "    abs_var = keras.layers.Lambda(lambda t: tf.abs(t))(var_output)\n",
        "\n",
        "    # Output mean, |variance| tuples.\n",
        "    outputs = keras.layers.concatenate([mean_output, abs_var])\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Later epochs seem to benefit from lower learning rate... but it takes\n",
        "    # a while to get there.\n",
        "    decay = keras.optimizers.schedules.ExponentialDecay(\n",
        "       lr, decay_steps=100, decay_rate=0.5, staircase=True)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss=symmetric_kl)\n",
        "    model.summary()\n",
        "  else:\n",
        "    model = keras.models.load_model(get_model_save_location(model_file),\n",
        "                                    custom_objects={\"symmetric_kl\": symmetric_kl})\n",
        "  history = model.fit(X, Y, verbose=0, epochs=epochs, batch_size=batch_size,\n",
        "                      validation_data=validation_data, shuffle=True, callbacks=callbacks_list)\n",
        "  return history, model"
      ],
      "metadata": {
        "id": "HCkGSPUo3KqY"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def render_plot_loss(history, name):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title(name + ' model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.yscale(\"log\")\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['loss', 'val_loss'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def destandardize_predictions(df: pd.DataFrame, sd: ScaledDataset):\n",
        "  means = pd.DataFrame(\n",
        "      sd.label_scaler.named_transformers_['label_std_scaler'].inverse_transform(df[['d18O_cel_mean']]),\n",
        "      index=df.index, columns=['d18O_cel_mean'])\n",
        "  vars = df['d18O_cel_variance']\n",
        "  return means.join(vars)\n",
        "\n",
        "def train_and_evaluate(sd: ScaledDataset, run_id: str):\n",
        "  print(\"==================\")\n",
        "  print(run_id)\n",
        "  history, vars_model = train_vars(sd.X_train, sd.Y_train, hidden_layers=[20, 20],\n",
        "                                 epochs=5000, batch_size=5, lr=0.0001,\n",
        "                                 validation_data=(sd.X_val, sd.Y_val),\n",
        "                                 model_file=run_id+\".h5\", use_checkpoint=False)\n",
        "  render_plot_loss(history, run_id+\" kl_loss\")\n",
        "  vars_model.save(get_model_save_location(run_id+\".h5\"), save_format=\"h5\")\n",
        "\n",
        "  vars_model.evaluate(x=sd.X_test, y=sd.Y_test)\n",
        "  predictions = vars_model.predict_on_batch(sd.X_test)\n",
        "  print(\"EXPECTED:\")\n",
        "  print(sd.Y_test.to_string())\n",
        "  print()\n",
        "  print(\"PREDICTED:\")\n",
        "  predictions =  destandardize_prediction(pd.DataFrame(predictions, columns=['d18O_cel_mean', 'd18O_cel_variance']))\n",
        "  print(predictions.to_string())\n",
        "\n",
        "  rmse = np.sqrt(mean_squared_error(Y_test['d18O_cel_mean'], predictions['d18O_cel_mean']))\n",
        "  print(\"RMSE: \"+ str(rmse))"
      ],
      "metadata": {
        "id": "DALuUm8UOgNu"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and evaluate the model with each set of data."
      ],
      "metadata": {
        "id": "WF_1T_zZtK0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1"
      ],
      "metadata": {
        "id": "q6vAjessuMSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ungrouped_random = {\n",
        "    'TRAIN' : \"/MyDrive/amazon_rainforest_files/amazon_sample_data/uc_davis_2023_08_12_train_random_ungrouped.csv\",\n",
        "    'TEST' : \"/MyDrive/amazon_rainforest_files/amazon_sample_data/uc_davis_2023_08_12_test_random_ungrouped.csv\",\n",
        "    'VALIDATION' : \"/MyDrive/amazon_rainforest_files/amazon_sample_data/uc_davis_2023_08_12_validation_random_ungrouped.csv\",\n",
        "}\n",
        "\n",
        "ungrouped_random_scaled = load_and_scale(ungrouped_random)\n",
        "train_and_evaluate(ungrouped_random_scaled, \"ungrouped_random\")"
      ],
      "metadata": {
        "id": "qM5zP9M9tQqE",
        "outputId": "7899d2c2-7016-4b52-aa9b-882517f923cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================\n",
            "ungrouped_random\n",
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_8 (InputLayer)           [(None, 12)]         0           []                               \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 20)           260         ['input_8[0][0]']                \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 20)           420         ['dense_14[0][0]']               \n",
            "                                                                                                  \n",
            " var_output (Dense)             (None, 1)            21          ['dense_15[0][0]']               \n",
            "                                                                                                  \n",
            " mean_output (Dense)            (None, 1)            21          ['dense_15[0][0]']               \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1)            0           ['var_output[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 2)            0           ['mean_output[0][0]',            \n",
            "                                                                  'lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 722\n",
            "Trainable params: 722\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}