{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnc-br/ddf-isoscapes/blob/split_experiments/dnn/variational_split_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational model\n",
        "\n",
        "Find the mean/variance of O18 ratios (as well as N15 and C13 in the future) at a particular lat/lon across Brazil. At the bottom of the colab, train and evaluate 4 different versions of the model with different data partitioning strategies."
      ],
      "metadata": {
        "id": "-0IfT3kGwgK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "henIPlAPCb4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dfa3d9d-7b03-4fb1-8c34-553ab568a28d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import os\n",
        "from typing import List, Tuple, Dict\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.python.ops import math_ops\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "#@title Debugging\n",
        "# See https://zohaib.me/debugging-in-google-collab-notebook/ for tips,\n",
        "# as well as docs for pdb and ipdb.\n",
        "DEBUG = False #@param {type:\"boolean\"}\n",
        "USE_LOCAL_DRIVE = False #@param {type:\"boolean\"}\n",
        "LOCAL_DIR = \"/usr/local/google/home/ruru/Downloads/variational/variational/model/\" #@param\n",
        "GDRIVE_DIR = \"MyDrive/amazon_rainforest_files/\" #@param\n",
        "FP_ROOT = LOCAL_DIR\n",
        "\n",
        "def get_model_save_location(filename) -> str:\n",
        "  root = '' if USE_LOCAL_DRIVE else '/content/drive'\n",
        "  return os.path.join(root, GDRIVE_DIR,'variational/model', filename)\n",
        "\n",
        "# Access data stored on Google Drive if not reading data locally.\n",
        "if not USE_LOCAL_DRIVE:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  global FP_ROOT\n",
        "  FP_ROOT = os.path.join('/content/drive', GDRIVE_DIR)\n",
        "\n",
        "if DEBUG:\n",
        "    %pip install -Uqq ipdb\n",
        "    import ipdb\n",
        "    %pdb on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQrEv57zCb4q"
      },
      "source": [
        "# Data preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path: str):\n",
        "  df = pd.read_csv(path, encoding=\"ISO-8859-1\", sep=',')\n",
        "  df = df[df['d18O_cel_variance'].notna()]\n",
        "\n",
        "  # Family is too sparse. Too many families exist in validation/test that won't\n",
        "  # exist in train, so drop it.\n",
        "  X = df.drop([\"d18O_cel_mean\", \"d18O_cel_variance\", \"Code\", \"Family\", \"Unnamed: 0\"], axis=1)\n",
        "  Y = df[[\"d18O_cel_mean\", \"d18O_cel_variance\"]]\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "6XMee1aHfcik"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardization"
      ],
      "metadata": {
        "id": "DtkKhMOtb6cS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class FeaturesToLabels:\n",
        "  def __init__(self, X: pd.DataFrame, Y: pd.DataFrame):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "\n",
        "  def as_tuple(self):\n",
        "    return (self.X, self.Y)\n",
        "\n",
        "\n",
        "def create_feature_scaler(X: pd.DataFrame) -> ColumnTransformer:\n",
        "  columns_to_normalize = ['lat', 'long', 'VPD', 'RH', 'PET', 'DEM', 'PA',\n",
        "       'Mean Annual Temperature', 'Mean Annual Precipitation',\n",
        "       'Iso_Oxi_Stack_mean_TERZER', 'predkrig_br_lat_ISORG',\n",
        "       'isoscape_fullmodel_d18O_prec_REGRESSION']\n",
        "  feature_scaler = ColumnTransformer([\n",
        "      ('feature_normalizer', Normalizer(), columns_to_normalize)],\n",
        "      remainder='passthrough')\n",
        "  feature_scaler.fit(X)\n",
        "  return feature_scaler\n",
        "\n",
        "def create_label_scaler(Y: pd.DataFrame) -> ColumnTransformer:\n",
        "  label_scaler = ColumnTransformer([\n",
        "      ('mean_std_scaler', StandardScaler(), ['d18O_cel_mean']),\n",
        "      ('var_std_scaler', StandardScaler(), ['d18O_cel_variance'])],\n",
        "      remainder='passthrough')\n",
        "  label_scaler.fit(Y)\n",
        "  return label_scaler\n",
        "\n",
        "def scale(X: pd.DataFrame, Y: pd.DataFrame, feature_scaler, label_scaler):\n",
        "  # transform() outputs numpy arrays :(  need to convert back to DataFrame.\n",
        "  X_standardized = pd.DataFrame(feature_scaler.transform(X),\n",
        "                        index=X.index, columns=X.columns)\n",
        "  # Y_standardized = pd.DataFrame(label_scaler.transform(Y),\n",
        "  #                                     index=Y.index, columns=Y.columns)\n",
        "  return FeaturesToLabels(X_standardized, Y)"
      ],
      "metadata": {
        "id": "XSDwdvMkb7w8"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just a class organization, holds each scaled dataset and the scaler used.\n",
        "# Useful for unscaling predictions.\n",
        "@dataclass\n",
        "class ScaledPartitions():\n",
        "  def __init__(self,\n",
        "               feature_scaler: ColumnTransformer,\n",
        "               label_scaler: ColumnTransformer,\n",
        "               train: FeaturesToLabels, val: FeaturesToLabels,\n",
        "               test: FeaturesToLabels):\n",
        "    self.feature_scaler = feature_scaler\n",
        "    self.label_scaler = label_scaler\n",
        "    self.train = train\n",
        "    self.val = val\n",
        "    self.test = test\n",
        "\n",
        "\n",
        "def load_and_scale(config: Dict) -> ScaledPartitions:\n",
        "  X_train, Y_train = load_dataset(config['TRAIN'])\n",
        "  X_val, Y_val = load_dataset(config['VALIDATION'])\n",
        "  X_test, Y_test = load_dataset(config['TEST'])\n",
        "\n",
        "  feature_scaler = create_feature_scaler(X_train)\n",
        "  label_scaler = create_label_scaler(Y_train)\n",
        "  train = scale(X_train, Y_train, feature_scaler, label_scaler)\n",
        "  val = scale(X_val, Y_val, feature_scaler, label_scaler)\n",
        "  test = scale(X_test, Y_test, feature_scaler, label_scaler)\n",
        "  return ScaledPartitions(feature_scaler, label_scaler, train, val, test)\n"
      ],
      "metadata": {
        "id": "_kf2e_fKon2P"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition\n",
        "\n"
      ],
      "metadata": {
        "id": "usGznR593LZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KL Loss function:"
      ],
      "metadata": {
        "id": "khK7C8WvU8ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# log(σ2/σ1) + ( σ1^2+(μ1−μ2)^2 ) / 2* σ^2   − 1/2\n",
        "def kl_divergence(real, predicted):\n",
        "    real_value = tf.gather(real, [0], axis=1)\n",
        "    real_std = tf.math.sqrt(tf.gather(real, [1], axis=1))\n",
        "\n",
        "    predicted_value = tf.gather(predicted, [0], axis=1)\n",
        "    predicted_std = tf.math.sqrt(tf.gather(predicted, [1], axis=1))\n",
        "\n",
        "    kl_loss = -0.5 + tf.math.log(predicted_std/real_std) + \\\n",
        "     (tf.square(real_std) + tf.square(real_value - predicted_value))/ \\\n",
        "     (2*tf.square(predicted_std))\n",
        "\n",
        "    return tf.math.reduce_mean(kl_loss)"
      ],
      "metadata": {
        "id": "urGjYNNnemX6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the loss function:"
      ],
      "metadata": {
        "id": "fJzBFWQVeqNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "\n",
        "test_real = tf.convert_to_tensor(np.array([[1, 0.02]]))\n",
        "test_pred = tf.convert_to_tensor(np.array([[0.98, 0.021]]))\n",
        "\n",
        "# https://screenshot.googleplex.com/5WM9dinAbhR26ZS\n",
        "assert float(kl_divergence(test_real, test_pred)) == pytest.approx(0.0101094, 1e-5)\n",
        "\n",
        "test_neg_real = tf.convert_to_tensor(np.array([[32.32, 0.0344]]))\n",
        "test_neg_pred = tf.convert_to_tensor(np.array([[32.01, -0.322]]))\n",
        "\n",
        "# Negative variance causes NaN\n",
        "assert tf.math.is_nan(kl_divergence(test_neg_real, test_neg_pred))\n",
        "\n",
        "# Calculated manually by computing the result of this equation in wolfram alpha:\n",
        "# log(σ2/σ1) + ( σ1^2+(μ1−μ2)^2 ) / 2* σ^2   − 1/2\n",
        "test_real_2d = tf.convert_to_tensor(np.array(\n",
        "    [[1.00, 0.020],\n",
        "     [1.01, 0.042]]))\n",
        "test_pred_2d = tf.convert_to_tensor(np.array(\n",
        "    [[0.98, 0.021],\n",
        "     [0.99, 0.012]]))\n",
        "\n",
        "# Should reduce to the average loss of all rows.\n",
        "assert float(kl_divergence(test_real_2d, test_pred_2d)) == pytest.approx(\n",
        "    sum([0.0101094, 0.6402851])/2, 1e-5)"
      ],
      "metadata": {
        "id": "48TaPd70erSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model definition"
      ],
      "metadata": {
        "id": "8rI6qPRh7oO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "def get_early_stopping_callback():\n",
        "  return EarlyStopping(monitor='val_loss', patience=1000, min_delta=0.001,\n",
        "                       verbose=0, restore_best_weights=True, start_from_epoch=0)\n",
        "\n",
        "tf.keras.utils.set_random_seed(18731)\n",
        "\n",
        "# I was experimenting with models that took longer to train, and used this\n",
        "# checkpointing callback to periodically save the model. It's optional.\n",
        "def get_checkpoint_callback(model_file):\n",
        "  return ModelCheckpoint(\n",
        "      get_model_save_location(model_file),\n",
        "      monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
        "\n",
        "def train_or_update_variational_model(\n",
        "        sp: ScaledPartitions,\n",
        "        hidden_layers: List[int],\n",
        "        epochs: int,\n",
        "        batch_size: int,\n",
        "        lr: float,\n",
        "        validation_data: FeaturesToLabels,\n",
        "        model_file=None,\n",
        "        use_checkpoint=False):\n",
        "  callbacks_list = [get_early_stopping_callback(),\n",
        "                    get_checkpoint_callback(model_file)]\n",
        "  if not use_checkpoint:\n",
        "    inputs = keras.Input(shape=(sp.train.X.shape[1],))\n",
        "    x = inputs\n",
        "    for layer_size in hidden_layers:\n",
        "      x = keras.layers.Dense(\n",
        "          layer_size, activation='relu')(x)\n",
        "    mean_output = keras.layers.Dense(1, name='mean_output')(x)\n",
        "\n",
        "    # We can not have negative variance. Apply very little variance.\n",
        "    var_output = keras.layers.Dense(1, name='var_output')(x)\n",
        "\n",
        "    # Invert the normalization on our outputs\n",
        "    mean_scaler = sp.label_scaler.named_transformers_['mean_std_scaler']\n",
        "    untransformed_mean = mean_output * mean_scaler.var_ + mean_scaler.mean_\n",
        "\n",
        "    var_scaler = sp.label_scaler.named_transformers_['var_std_scaler']\n",
        "    untransformed_var = var_output * var_scaler.var_ + var_scaler.mean_\n",
        "\n",
        "    untransformed_abs_var = keras.layers.Lambda(lambda t: tf.abs(t))(untransformed_var)\n",
        "\n",
        "    # Output mean, |variance| tuples.\n",
        "    outputs = keras.layers.concatenate([untransformed_mean, untransformed_abs_var])\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Later epochs seem to benefit from lower learning rate... but it takes\n",
        "    # a while to get there.\n",
        "    decay = keras.optimizers.schedules.ExponentialDecay(\n",
        "       lr, decay_steps=100, decay_rate=0.5, staircase=True)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss=kl_divergence)\n",
        "    model.summary()\n",
        "  else:\n",
        "    model = keras.models.load_model(\n",
        "        get_model_save_location(model_file),\n",
        "        custom_objects={\"kl_divergence\": kl_divergence})\n",
        "  history = model.fit(sp.train.X, sp.train.Y, verbose=1, epochs=epochs, batch_size=batch_size,\n",
        "                      validation_data=validation_data.as_tuple(),\n",
        "                      shuffle=True, callbacks=callbacks_list)\n",
        "  return history, model"
      ],
      "metadata": {
        "id": "HCkGSPUo3KqY"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def render_plot_loss(history, name):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title(name + ' model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.yscale(\"log\")\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['loss', 'val_loss'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def destandardize(sd: ScaledPartitions, df: pd.DataFrame):\n",
        "  means = pd.DataFrame(\n",
        "      sd.label_scaler.named_transformers_['label_std_scaler'].inverse_transform(df[['d18O_cel_mean']]),\n",
        "      index=df.index, columns=['d18O_cel_mean'])\n",
        "  vars = df['d18O_cel_variance']\n",
        "  return means.join(vars)\n",
        "\n",
        "def train_and_evaluate(sp: ScaledPartitions, run_id: str, training_batch_size=5):\n",
        "  print(\"==================\")\n",
        "  print(run_id)\n",
        "  history, model = train_or_update_variational_model(\n",
        "      sp, hidden_layers=[20, 20], epochs=5000, batch_size=training_batch_size,\n",
        "      lr=0.0001, validation_data=sp.val,\n",
        "      model_file=run_id+\".h5\", use_checkpoint=False)\n",
        "  render_plot_loss(history, run_id+\" kl_loss\")\n",
        "  model.save(get_model_save_location(run_id+\".h5\"), save_format=\"h5\")\n",
        "\n",
        "  model.evaluate(x=sp.test.X, y=sp.test.Y)\n",
        "  predictions = model.predict_on_batch(sp.test.X)\n",
        "  print(\"EXPECTED:\")\n",
        "  Y_destandardized = destandardize(sp, sp.test.Y)\n",
        "  print(Y_destandardized.to_string())\n",
        "  print()\n",
        "  print(\"PREDICTED:\")\n",
        "  predictions =  destandardize(sp, pd.DataFrame(predictions, columns=['d18O_cel_mean', 'd18O_cel_variance']))\n",
        "  print(predictions.to_string())\n",
        "\n",
        "  rmse = np.sqrt(mean_squared_error(Y_destandardized['d18O_cel_mean'], predictions['d18O_cel_mean']))\n",
        "  print(\"RMSE: \"+ str(rmse))"
      ],
      "metadata": {
        "id": "DALuUm8UOgNu"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and evaluate the model with each set of data.\n",
        "\n",
        "Use the same model configured the same way for every run, with the exception of the training batch size setting, which is 1 for grouped and 5 for ungrouped."
      ],
      "metadata": {
        "id": "WF_1T_zZtK0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Ungrouped, random"
      ],
      "metadata": {
        "id": "q6vAjessuMSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ungrouped_random = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, \"amazon_sample_data/uc_davis_2023_08_12_train_random_ungrouped.csv\"),\n",
        "    'TEST' : os.path.join(FP_ROOT, \"amazon_sample_data/uc_davis_2023_08_12_test_random_ungrouped.csv\"),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, \"amazon_sample_data/uc_davis_2023_08_12_validation_random_ungrouped.csv\"),\n",
        "}\n",
        "\n",
        "ungrouped_random_scaled = load_and_scale(ungrouped_random)\n",
        "train_and_evaluate(ungrouped_random_scaled, \"ungrouped_random\", training_batch_size=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qM5zP9M9tQqE",
        "outputId": "56852d60-c60f-4853-b15e-4f9e266f2649"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================\n",
            "ungrouped_random\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_8 (InputLayer)           [(None, 12)]         0           []                               \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 20)           260         ['input_8[0][0]']                \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 20)           420         ['dense_14[0][0]']               \n",
            "                                                                                                  \n",
            " var_output (Dense)             (None, 1)            21          ['dense_15[0][0]']               \n",
            "                                                                                                  \n",
            " mean_output (Dense)            (None, 1)            21          ['dense_15[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.multiply_11 (TFOpLambd  (None, 1)           0           ['var_output[0][0]']             \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.multiply_10 (TFOpLambd  (None, 1)           0           ['mean_output[0][0]']            \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_11 (TFOpL  (None, 1)           0           ['tf.math.multiply_11[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_10 (TFOpL  (None, 1)           0           ['tf.math.multiply_10[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1)            0           ['tf.__operators__.add_11[0][0]']\n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 2)            0           ['tf.__operators__.add_10[0][0]',\n",
            "                                                                  'lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 722\n",
            "Trainable params: 722\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5000\n",
            "493/493 [==============================] - 3s 3ms/step - loss: 0.4667 - val_loss: 0.5889\n",
            "Epoch 2/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4583 - val_loss: 0.5890\n",
            "Epoch 3/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4576 - val_loss: 0.5894\n",
            "Epoch 4/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4574 - val_loss: 0.5901\n",
            "Epoch 5/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4574 - val_loss: 0.5906\n",
            "Epoch 6/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4573 - val_loss: 0.5907\n",
            "Epoch 7/5000\n",
            "493/493 [==============================] - 1s 3ms/step - loss: 0.4570 - val_loss: 0.5899\n",
            "Epoch 8/5000\n",
            "493/493 [==============================] - 2s 3ms/step - loss: 0.4568 - val_loss: 0.5866\n",
            "Epoch 9/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4572 - val_loss: 0.5881\n",
            "Epoch 10/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4568 - val_loss: 0.5874\n",
            "Epoch 11/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4565 - val_loss: 0.5864\n",
            "Epoch 12/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4564 - val_loss: 0.5870\n",
            "Epoch 13/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4563 - val_loss: 0.5852\n",
            "Epoch 14/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4564 - val_loss: 0.5852\n",
            "Epoch 15/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4560 - val_loss: 0.5851\n",
            "Epoch 16/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4560 - val_loss: 0.5861\n",
            "Epoch 17/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4562 - val_loss: 0.5862\n",
            "Epoch 18/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4558 - val_loss: 0.5833\n",
            "Epoch 19/5000\n",
            "493/493 [==============================] - 2s 3ms/step - loss: 0.4555 - val_loss: 0.5839\n",
            "Epoch 20/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4556 - val_loss: 0.5844\n",
            "Epoch 21/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4556 - val_loss: 0.5852\n",
            "Epoch 22/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4550 - val_loss: 0.5812\n",
            "Epoch 23/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4551 - val_loss: 0.5800\n",
            "Epoch 24/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4549 - val_loss: 0.5787\n",
            "Epoch 25/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4546 - val_loss: 0.5821\n",
            "Epoch 26/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4546 - val_loss: 0.5833\n",
            "Epoch 27/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4543 - val_loss: 0.5805\n",
            "Epoch 28/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4543 - val_loss: 0.5808\n",
            "Epoch 29/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4541 - val_loss: 0.5789\n",
            "Epoch 30/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4538 - val_loss: 0.5783\n",
            "Epoch 31/5000\n",
            "493/493 [==============================] - 1s 3ms/step - loss: 0.4535 - val_loss: 0.5736\n",
            "Epoch 32/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4535 - val_loss: 0.5742\n",
            "Epoch 33/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4535 - val_loss: 0.5743\n",
            "Epoch 34/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4531 - val_loss: 0.5741\n",
            "Epoch 35/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4531 - val_loss: 0.5720\n",
            "Epoch 36/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4526 - val_loss: 0.5766\n",
            "Epoch 37/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4526 - val_loss: 0.5735\n",
            "Epoch 38/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4524 - val_loss: 0.5745\n",
            "Epoch 39/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4522 - val_loss: 0.5727\n",
            "Epoch 40/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4520 - val_loss: 0.5709\n",
            "Epoch 41/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4517 - val_loss: 0.5717\n",
            "Epoch 42/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4515 - val_loss: 0.5689\n",
            "Epoch 43/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4512 - val_loss: 0.5684\n",
            "Epoch 44/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4508 - val_loss: 0.5711\n",
            "Epoch 45/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4508 - val_loss: 0.5660\n",
            "Epoch 46/5000\n",
            "493/493 [==============================] - 1s 3ms/step - loss: 0.4504 - val_loss: 0.5627\n",
            "Epoch 47/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4502 - val_loss: 0.5636\n",
            "Epoch 48/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4497 - val_loss: 0.5690\n",
            "Epoch 49/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4498 - val_loss: 0.5631\n",
            "Epoch 50/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4495 - val_loss: 0.5600\n",
            "Epoch 51/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4494 - val_loss: 0.5590\n",
            "Epoch 52/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4489 - val_loss: 0.5616\n",
            "Epoch 53/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4489 - val_loss: 0.5602\n",
            "Epoch 54/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4486 - val_loss: 0.5563\n",
            "Epoch 55/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4481 - val_loss: 0.5551\n",
            "Epoch 56/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4480 - val_loss: 0.5521\n",
            "Epoch 57/5000\n",
            "493/493 [==============================] - 1s 3ms/step - loss: 0.4475 - val_loss: 0.5495\n",
            "Epoch 58/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4476 - val_loss: 0.5476\n",
            "Epoch 59/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4470 - val_loss: 0.5468\n",
            "Epoch 60/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4467 - val_loss: 0.5506\n",
            "Epoch 61/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4464 - val_loss: 0.5529\n",
            "Epoch 62/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4462 - val_loss: 0.5503\n",
            "Epoch 63/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4458 - val_loss: 0.5464\n",
            "Epoch 64/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4457 - val_loss: 0.5427\n",
            "Epoch 65/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4454 - val_loss: 0.5479\n",
            "Epoch 66/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4449 - val_loss: 0.5409\n",
            "Epoch 67/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4448 - val_loss: 0.5420\n",
            "Epoch 68/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4440 - val_loss: 0.5429\n",
            "Epoch 69/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4441 - val_loss: 0.5390\n",
            "Epoch 70/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4436 - val_loss: 0.5352\n",
            "Epoch 71/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4432 - val_loss: 0.5406\n",
            "Epoch 72/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4428 - val_loss: 0.5415\n",
            "Epoch 73/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4425 - val_loss: 0.5390\n",
            "Epoch 74/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4423 - val_loss: 0.5305\n",
            "Epoch 75/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4419 - val_loss: 0.5247\n",
            "Epoch 76/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4416 - val_loss: 0.5345\n",
            "Epoch 77/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4414 - val_loss: 0.5273\n",
            "Epoch 78/5000\n",
            "493/493 [==============================] - 2s 4ms/step - loss: 0.4412 - val_loss: 0.5235\n",
            "Epoch 79/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4408 - val_loss: 0.5277\n",
            "Epoch 80/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4401 - val_loss: 0.5215\n",
            "Epoch 81/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4401 - val_loss: 0.5190\n",
            "Epoch 82/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4401 - val_loss: 0.5192\n",
            "Epoch 83/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4393 - val_loss: 0.5218\n",
            "Epoch 84/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4390 - val_loss: 0.5211\n",
            "Epoch 85/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4388 - val_loss: 0.5176\n",
            "Epoch 86/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4386 - val_loss: 0.5160\n",
            "Epoch 87/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4384 - val_loss: 0.5141\n",
            "Epoch 88/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4377 - val_loss: 0.5171\n",
            "Epoch 89/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4377 - val_loss: 0.5142\n",
            "Epoch 90/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4373 - val_loss: 0.5095\n",
            "Epoch 91/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4367 - val_loss: 0.5099\n",
            "Epoch 92/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4364 - val_loss: 0.5041\n",
            "Epoch 93/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4359 - val_loss: 0.5179\n",
            "Epoch 94/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4355 - val_loss: 0.5115\n",
            "Epoch 95/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4349 - val_loss: 0.5055\n",
            "Epoch 96/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4352 - val_loss: 0.5085\n",
            "Epoch 97/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4343 - val_loss: 0.5100\n",
            "Epoch 98/5000\n",
            "493/493 [==============================] - 1s 3ms/step - loss: 0.4341 - val_loss: 0.5071\n",
            "Epoch 99/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4335 - val_loss: 0.4997\n",
            "Epoch 100/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4342 - val_loss: 0.5027\n",
            "Epoch 101/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4335 - val_loss: 0.4999\n",
            "Epoch 102/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4331 - val_loss: 0.5020\n",
            "Epoch 103/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4329 - val_loss: 0.4952\n",
            "Epoch 104/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4326 - val_loss: 0.4958\n",
            "Epoch 105/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4321 - val_loss: 0.4877\n",
            "Epoch 106/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4325 - val_loss: 0.4909\n",
            "Epoch 107/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4318 - val_loss: 0.4899\n",
            "Epoch 108/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4317 - val_loss: 0.4891\n",
            "Epoch 109/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4316 - val_loss: 0.4900\n",
            "Epoch 110/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4313 - val_loss: 0.4852\n",
            "Epoch 111/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4310 - val_loss: 0.4863\n",
            "Epoch 112/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4308 - val_loss: 0.4873\n",
            "Epoch 113/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4306 - val_loss: 0.4863\n",
            "Epoch 114/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4306 - val_loss: 0.4840\n",
            "Epoch 115/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4302 - val_loss: 0.4806\n",
            "Epoch 116/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4301 - val_loss: 0.4848\n",
            "Epoch 117/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4305 - val_loss: 0.4807\n",
            "Epoch 118/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4293 - val_loss: 0.4731\n",
            "Epoch 119/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4298 - val_loss: 0.4777\n",
            "Epoch 120/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4296 - val_loss: 0.4748\n",
            "Epoch 121/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4302 - val_loss: 0.4758\n",
            "Epoch 122/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4293 - val_loss: 0.4731\n",
            "Epoch 123/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4291 - val_loss: 0.4717\n",
            "Epoch 124/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4292 - val_loss: 0.4718\n",
            "Epoch 125/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4288 - val_loss: 0.4705\n",
            "Epoch 126/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4287 - val_loss: 0.4728\n",
            "Epoch 127/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4285 - val_loss: 0.4726\n",
            "Epoch 128/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4285 - val_loss: 0.4698\n",
            "Epoch 129/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4284 - val_loss: 0.4655\n",
            "Epoch 130/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4283 - val_loss: 0.4632\n",
            "Epoch 131/5000\n",
            "493/493 [==============================] - 2s 4ms/step - loss: 0.4279 - val_loss: 0.4660\n",
            "Epoch 132/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4279 - val_loss: 0.4683\n",
            "Epoch 133/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4279 - val_loss: 0.4644\n",
            "Epoch 134/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4277 - val_loss: 0.4634\n",
            "Epoch 135/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4276 - val_loss: 0.4617\n",
            "Epoch 136/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4273 - val_loss: 0.4632\n",
            "Epoch 137/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4272 - val_loss: 0.4580\n",
            "Epoch 138/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4273 - val_loss: 0.4560\n",
            "Epoch 139/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4268 - val_loss: 0.4564\n",
            "Epoch 140/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4266 - val_loss: 0.4564\n",
            "Epoch 141/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4265 - val_loss: 0.4588\n",
            "Epoch 142/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4268 - val_loss: 0.4553\n",
            "Epoch 143/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4268 - val_loss: 0.4535\n",
            "Epoch 144/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4265 - val_loss: 0.4560\n",
            "Epoch 145/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4260 - val_loss: 0.4582\n",
            "Epoch 146/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4263 - val_loss: 0.4535\n",
            "Epoch 147/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4258 - val_loss: 0.4533\n",
            "Epoch 148/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4258 - val_loss: 0.4508\n",
            "Epoch 149/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4258 - val_loss: 0.4499\n",
            "Epoch 150/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4258 - val_loss: 0.4568\n",
            "Epoch 151/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4257 - val_loss: 0.4521\n",
            "Epoch 152/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4253 - val_loss: 0.4526\n",
            "Epoch 153/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4254 - val_loss: 0.4558\n",
            "Epoch 154/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4252 - val_loss: 0.4544\n",
            "Epoch 155/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4250 - val_loss: 0.4519\n",
            "Epoch 156/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4252 - val_loss: 0.4522\n",
            "Epoch 157/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4247 - val_loss: 0.4508\n",
            "Epoch 158/5000\n",
            "493/493 [==============================] - 2s 4ms/step - loss: 0.4243 - val_loss: 0.4469\n",
            "Epoch 159/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4249 - val_loss: 0.4491\n",
            "Epoch 160/5000\n",
            "493/493 [==============================] - 1s 1ms/step - loss: 0.4250 - val_loss: 0.4499\n",
            "Epoch 161/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4245 - val_loss: 0.4483\n",
            "Epoch 162/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4242 - val_loss: 0.4474\n",
            "Epoch 163/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4239 - val_loss: 0.4471\n",
            "Epoch 164/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4239 - val_loss: 0.4500\n",
            "Epoch 165/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4238 - val_loss: 0.4490\n",
            "Epoch 166/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4241 - val_loss: 0.4496\n",
            "Epoch 167/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4236 - val_loss: 0.4478\n",
            "Epoch 168/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4233 - val_loss: 0.4509\n",
            "Epoch 169/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4234 - val_loss: 0.4473\n",
            "Epoch 170/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4233 - val_loss: 0.4427\n",
            "Epoch 171/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4233 - val_loss: 0.4491\n",
            "Epoch 172/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4233 - val_loss: 0.4468\n",
            "Epoch 173/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4230 - val_loss: 0.4467\n",
            "Epoch 174/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4230 - val_loss: 0.4425\n",
            "Epoch 175/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4228 - val_loss: 0.4442\n",
            "Epoch 176/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4228 - val_loss: 0.4493\n",
            "Epoch 177/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4224 - val_loss: 0.4511\n",
            "Epoch 178/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4222 - val_loss: 0.4484\n",
            "Epoch 179/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4223 - val_loss: 0.4481\n",
            "Epoch 180/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4221 - val_loss: 0.4525\n",
            "Epoch 181/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4221 - val_loss: 0.4416\n",
            "Epoch 182/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4219 - val_loss: 0.4461\n",
            "Epoch 183/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4216 - val_loss: 0.4451\n",
            "Epoch 184/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4214 - val_loss: 0.4443\n",
            "Epoch 185/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4214 - val_loss: 0.4488\n",
            "Epoch 186/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4212 - val_loss: 0.4468\n",
            "Epoch 187/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4212 - val_loss: 0.4485\n",
            "Epoch 188/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4209 - val_loss: 0.4436\n",
            "Epoch 189/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4204 - val_loss: 0.4336\n",
            "Epoch 190/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4208 - val_loss: 0.4450\n",
            "Epoch 191/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4206 - val_loss: 0.4457\n",
            "Epoch 192/5000\n",
            "493/493 [==============================] - 1s 3ms/step - loss: 0.4205 - val_loss: 0.4449\n",
            "Epoch 193/5000\n",
            "493/493 [==============================] - 1s 3ms/step - loss: 0.4203 - val_loss: 0.4393\n",
            "Epoch 194/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4201 - val_loss: 0.4431\n",
            "Epoch 195/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4200 - val_loss: 0.4460\n",
            "Epoch 196/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4196 - val_loss: 0.4446\n",
            "Epoch 197/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4202 - val_loss: 0.4445\n",
            "Epoch 198/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4194 - val_loss: 0.4364\n",
            "Epoch 199/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4191 - val_loss: 0.4392\n",
            "Epoch 200/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4192 - val_loss: 0.4374\n",
            "Epoch 201/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4189 - val_loss: 0.4399\n",
            "Epoch 202/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4187 - val_loss: 0.4375\n",
            "Epoch 203/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4194 - val_loss: 0.4405\n",
            "Epoch 204/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4182 - val_loss: 0.4435\n",
            "Epoch 205/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4182 - val_loss: 0.4349\n",
            "Epoch 206/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4179 - val_loss: 0.4442\n",
            "Epoch 207/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4180 - val_loss: 0.4380\n",
            "Epoch 208/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4179 - val_loss: 0.4411\n",
            "Epoch 209/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4174 - val_loss: 0.4389\n",
            "Epoch 210/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4174 - val_loss: 0.4446\n",
            "Epoch 211/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4175 - val_loss: 0.4354\n",
            "Epoch 212/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4174 - val_loss: 0.4403\n",
            "Epoch 213/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4171 - val_loss: 0.4396\n",
            "Epoch 214/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4169 - val_loss: 0.4414\n",
            "Epoch 215/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4170 - val_loss: 0.4380\n",
            "Epoch 216/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4170 - val_loss: 0.4348\n",
            "Epoch 217/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4171 - val_loss: 0.4349\n",
            "Epoch 218/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4165 - val_loss: 0.4351\n",
            "Epoch 219/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4165 - val_loss: 0.4393\n",
            "Epoch 220/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4160 - val_loss: 0.4377\n",
            "Epoch 221/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4161 - val_loss: 0.4382\n",
            "Epoch 222/5000\n",
            "493/493 [==============================] - 1s 3ms/step - loss: 0.4153 - val_loss: 0.4328\n",
            "Epoch 223/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4153 - val_loss: 0.4389\n",
            "Epoch 224/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4149 - val_loss: 0.4382\n",
            "Epoch 225/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4143 - val_loss: 0.4404\n",
            "Epoch 226/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4143 - val_loss: 0.4457\n",
            "Epoch 227/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4146 - val_loss: 0.4448\n",
            "Epoch 228/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4141 - val_loss: 0.4468\n",
            "Epoch 229/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4137 - val_loss: 0.4428\n",
            "Epoch 230/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4136 - val_loss: 0.4508\n",
            "Epoch 231/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4132 - val_loss: 0.4548\n",
            "Epoch 232/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4132 - val_loss: 0.4584\n",
            "Epoch 233/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4129 - val_loss: 0.4566\n",
            "Epoch 234/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4130 - val_loss: 0.4478\n",
            "Epoch 235/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4126 - val_loss: 0.4527\n",
            "Epoch 236/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4129 - val_loss: 0.4494\n",
            "Epoch 237/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4124 - val_loss: 0.4460\n",
            "Epoch 238/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4122 - val_loss: 0.4482\n",
            "Epoch 239/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4121 - val_loss: 0.4516\n",
            "Epoch 240/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4121 - val_loss: 0.4365\n",
            "Epoch 241/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4120 - val_loss: 0.4463\n",
            "Epoch 242/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4115 - val_loss: 0.4391\n",
            "Epoch 243/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4113 - val_loss: 0.4500\n",
            "Epoch 244/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4112 - val_loss: 0.4453\n",
            "Epoch 245/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4110 - val_loss: 0.4348\n",
            "Epoch 246/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4110 - val_loss: 0.4498\n",
            "Epoch 247/5000\n",
            "493/493 [==============================] - 1s 3ms/step - loss: 0.4108 - val_loss: 0.4388\n",
            "Epoch 248/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4108 - val_loss: 0.4380\n",
            "Epoch 249/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4104 - val_loss: 0.4388\n",
            "Epoch 250/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4106 - val_loss: 0.4423\n",
            "Epoch 251/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4095 - val_loss: 0.4452\n",
            "Epoch 252/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4105 - val_loss: 0.4485\n",
            "Epoch 253/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4101 - val_loss: 0.4414\n",
            "Epoch 254/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4097 - val_loss: 0.4355\n",
            "Epoch 255/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4097 - val_loss: 0.4385\n",
            "Epoch 256/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4096 - val_loss: 0.4461\n",
            "Epoch 257/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4094 - val_loss: 0.4373\n",
            "Epoch 258/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4093 - val_loss: 0.4397\n",
            "Epoch 259/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4091 - val_loss: 0.4430\n",
            "Epoch 260/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4090 - val_loss: 0.4427\n",
            "Epoch 261/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4087 - val_loss: 0.4466\n",
            "Epoch 262/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4085 - val_loss: 0.4438\n",
            "Epoch 263/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4083 - val_loss: 0.4386\n",
            "Epoch 264/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4081 - val_loss: 0.4446\n",
            "Epoch 265/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4084 - val_loss: 0.4496\n",
            "Epoch 266/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4079 - val_loss: 0.4395\n",
            "Epoch 267/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4077 - val_loss: 0.4397\n",
            "Epoch 268/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4076 - val_loss: 0.4482\n",
            "Epoch 269/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4074 - val_loss: 0.4377\n",
            "Epoch 270/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4072 - val_loss: 0.4405\n",
            "Epoch 271/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4077 - val_loss: 0.4423\n",
            "Epoch 272/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4071 - val_loss: 0.4485\n",
            "Epoch 273/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4070 - val_loss: 0.4540\n",
            "Epoch 274/5000\n",
            "493/493 [==============================] - 1s 3ms/step - loss: 0.4066 - val_loss: 0.4399\n",
            "Epoch 275/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4067 - val_loss: 0.4461\n",
            "Epoch 276/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4062 - val_loss: 0.4406\n",
            "Epoch 277/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4064 - val_loss: 0.4363\n",
            "Epoch 278/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4065 - val_loss: 0.4389\n",
            "Epoch 279/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4061 - val_loss: 0.4397\n",
            "Epoch 280/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4063 - val_loss: 0.4429\n",
            "Epoch 281/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4056 - val_loss: 0.4427\n",
            "Epoch 282/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4056 - val_loss: 0.4434\n",
            "Epoch 283/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4054 - val_loss: 0.4464\n",
            "Epoch 284/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4054 - val_loss: 0.4461\n",
            "Epoch 285/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4055 - val_loss: 0.4425\n",
            "Epoch 286/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4050 - val_loss: 0.4412\n",
            "Epoch 287/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4050 - val_loss: 0.4370\n",
            "Epoch 288/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4048 - val_loss: 0.4396\n",
            "Epoch 289/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4046 - val_loss: 0.4451\n",
            "Epoch 290/5000\n",
            "493/493 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4531\n",
            "Epoch 291/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4044 - val_loss: 0.4400\n",
            "Epoch 292/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4038 - val_loss: 0.4565\n",
            "Epoch 293/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4039 - val_loss: 0.4436\n",
            "Epoch 294/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4046 - val_loss: 0.4472\n",
            "Epoch 295/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4039 - val_loss: 0.4478\n",
            "Epoch 296/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4035 - val_loss: 0.4651\n",
            "Epoch 297/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4036 - val_loss: 0.4454\n",
            "Epoch 298/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4030 - val_loss: 0.4484\n",
            "Epoch 299/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4034 - val_loss: 0.4357\n",
            "Epoch 300/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4033 - val_loss: 0.4345\n",
            "Epoch 301/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4024 - val_loss: 0.4686\n",
            "Epoch 302/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4031 - val_loss: 0.4543\n",
            "Epoch 303/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4030 - val_loss: 0.4540\n",
            "Epoch 304/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4031 - val_loss: 0.4483\n",
            "Epoch 305/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4021 - val_loss: 0.4467\n",
            "Epoch 306/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4024 - val_loss: 0.4664\n",
            "Epoch 307/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4025 - val_loss: 0.4450\n",
            "Epoch 308/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4022 - val_loss: 0.4525\n",
            "Epoch 309/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4019 - val_loss: 0.4521\n",
            "Epoch 310/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4021 - val_loss: 0.4521\n",
            "Epoch 311/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4012 - val_loss: 0.4441\n",
            "Epoch 312/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4017 - val_loss: 0.4547\n",
            "Epoch 313/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4019 - val_loss: 0.4550\n",
            "Epoch 314/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4013 - val_loss: 0.4477\n",
            "Epoch 315/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4005 - val_loss: 0.4560\n",
            "Epoch 316/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4009 - val_loss: 0.4428\n",
            "Epoch 317/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4006 - val_loss: 0.4490\n",
            "Epoch 318/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4000 - val_loss: 0.4593\n",
            "Epoch 319/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4006 - val_loss: 0.4483\n",
            "Epoch 320/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4000 - val_loss: 0.4540\n",
            "Epoch 321/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3999 - val_loss: 0.4657\n",
            "Epoch 322/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4001 - val_loss: 0.4501\n",
            "Epoch 323/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4001 - val_loss: 0.4596\n",
            "Epoch 324/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4000 - val_loss: 0.4560\n",
            "Epoch 325/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3997 - val_loss: 0.4513\n",
            "Epoch 326/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.4001 - val_loss: 0.4601\n",
            "Epoch 327/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3993 - val_loss: 0.4629\n",
            "Epoch 328/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3990 - val_loss: 0.4522\n",
            "Epoch 329/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3992 - val_loss: 0.4479\n",
            "Epoch 330/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3993 - val_loss: 0.4544\n",
            "Epoch 331/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3987 - val_loss: 0.4659\n",
            "Epoch 332/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3987 - val_loss: 0.4637\n",
            "Epoch 333/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3985 - val_loss: 0.4668\n",
            "Epoch 334/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3984 - val_loss: 0.4715\n",
            "Epoch 335/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3982 - val_loss: 0.4596\n",
            "Epoch 336/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3980 - val_loss: 0.4516\n",
            "Epoch 337/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3984 - val_loss: 0.4678\n",
            "Epoch 338/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3977 - val_loss: 0.4781\n",
            "Epoch 339/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3980 - val_loss: 0.4599\n",
            "Epoch 340/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3976 - val_loss: 0.4733\n",
            "Epoch 341/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3975 - val_loss: 0.4699\n",
            "Epoch 342/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3970 - val_loss: 0.4727\n",
            "Epoch 343/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3973 - val_loss: 0.4673\n",
            "Epoch 344/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3967 - val_loss: 0.4618\n",
            "Epoch 345/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3966 - val_loss: 0.4826\n",
            "Epoch 346/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3970 - val_loss: 0.4766\n",
            "Epoch 347/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3966 - val_loss: 0.4553\n",
            "Epoch 348/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3964 - val_loss: 0.4700\n",
            "Epoch 349/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3961 - val_loss: 0.4818\n",
            "Epoch 350/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3961 - val_loss: 0.4695\n",
            "Epoch 351/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3961 - val_loss: 0.4837\n",
            "Epoch 352/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3957 - val_loss: 0.4788\n",
            "Epoch 353/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3953 - val_loss: 0.4908\n",
            "Epoch 354/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3957 - val_loss: 0.4864\n",
            "Epoch 355/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3955 - val_loss: 0.4879\n",
            "Epoch 356/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3948 - val_loss: 0.4679\n",
            "Epoch 357/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3954 - val_loss: 0.4973\n",
            "Epoch 358/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3949 - val_loss: 0.4817\n",
            "Epoch 359/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3947 - val_loss: 0.4880\n",
            "Epoch 360/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3941 - val_loss: 0.4840\n",
            "Epoch 361/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3948 - val_loss: 0.4826\n",
            "Epoch 362/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3942 - val_loss: 0.4793\n",
            "Epoch 363/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3941 - val_loss: 0.4646\n",
            "Epoch 364/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3942 - val_loss: 0.5004\n",
            "Epoch 365/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3939 - val_loss: 0.4870\n",
            "Epoch 366/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3939 - val_loss: 0.5110\n",
            "Epoch 367/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3935 - val_loss: 0.4954\n",
            "Epoch 368/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3933 - val_loss: 0.4863\n",
            "Epoch 369/5000\n",
            "493/493 [==============================] - 1s 3ms/step - loss: 0.3934 - val_loss: 0.4937\n",
            "Epoch 370/5000\n",
            "493/493 [==============================] - 4s 8ms/step - loss: 0.3932 - val_loss: 0.4975\n",
            "Epoch 371/5000\n",
            "493/493 [==============================] - 2s 5ms/step - loss: 0.3932 - val_loss: 0.4911\n",
            "Epoch 372/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3919 - val_loss: 0.4884\n",
            "Epoch 373/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3935 - val_loss: 0.4944\n",
            "Epoch 374/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3921 - val_loss: 0.4973\n",
            "Epoch 375/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3920 - val_loss: 0.5051\n",
            "Epoch 376/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3928 - val_loss: 0.4949\n",
            "Epoch 377/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3916 - val_loss: 0.5020\n",
            "Epoch 378/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3918 - val_loss: 0.4975\n",
            "Epoch 379/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3921 - val_loss: 0.4966\n",
            "Epoch 380/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3914 - val_loss: 0.5157\n",
            "Epoch 381/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3909 - val_loss: 0.5134\n",
            "Epoch 382/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3903 - val_loss: 0.5086\n",
            "Epoch 383/5000\n",
            "493/493 [==============================] - 1s 3ms/step - loss: 0.3903 - val_loss: 0.5219\n",
            "Epoch 384/5000\n",
            "493/493 [==============================] - 2s 3ms/step - loss: 0.3902 - val_loss: 0.5283\n",
            "Epoch 385/5000\n",
            "493/493 [==============================] - 2s 4ms/step - loss: 0.3905 - val_loss: 0.5173\n",
            "Epoch 386/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3898 - val_loss: 0.5307\n",
            "Epoch 387/5000\n",
            "493/493 [==============================] - 1s 3ms/step - loss: 0.3899 - val_loss: 0.5187\n",
            "Epoch 388/5000\n",
            "493/493 [==============================] - 2s 4ms/step - loss: 0.3893 - val_loss: 0.5378\n",
            "Epoch 389/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3892 - val_loss: 0.5245\n",
            "Epoch 390/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3889 - val_loss: 0.5307\n",
            "Epoch 391/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3889 - val_loss: 0.5364\n",
            "Epoch 392/5000\n",
            "493/493 [==============================] - 1s 3ms/step - loss: 0.3895 - val_loss: 0.5449\n",
            "Epoch 393/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3896 - val_loss: 0.5271\n",
            "Epoch 394/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3884 - val_loss: 0.5360\n",
            "Epoch 395/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3884 - val_loss: 0.5333\n",
            "Epoch 396/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3885 - val_loss: 0.5179\n",
            "Epoch 397/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3880 - val_loss: 0.5550\n",
            "Epoch 398/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3879 - val_loss: 0.5260\n",
            "Epoch 399/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3879 - val_loss: 0.5289\n",
            "Epoch 400/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3875 - val_loss: 0.5547\n",
            "Epoch 401/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3875 - val_loss: 0.5112\n",
            "Epoch 402/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3872 - val_loss: 0.5377\n",
            "Epoch 403/5000\n",
            "493/493 [==============================] - 3s 6ms/step - loss: 0.3870 - val_loss: 0.5210\n",
            "Epoch 404/5000\n",
            "493/493 [==============================] - 2s 4ms/step - loss: 0.3870 - val_loss: 0.5526\n",
            "Epoch 405/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3869 - val_loss: 0.5800\n",
            "Epoch 406/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3866 - val_loss: 0.5380\n",
            "Epoch 407/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3865 - val_loss: 0.5390\n",
            "Epoch 408/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3868 - val_loss: 0.5554\n",
            "Epoch 409/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3862 - val_loss: 0.5472\n",
            "Epoch 410/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3860 - val_loss: 0.5264\n",
            "Epoch 411/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3858 - val_loss: 0.5648\n",
            "Epoch 412/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3855 - val_loss: 0.5523\n",
            "Epoch 413/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3853 - val_loss: 0.5477\n",
            "Epoch 414/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3851 - val_loss: 0.5413\n",
            "Epoch 415/5000\n",
            "493/493 [==============================] - 1s 3ms/step - loss: 0.3852 - val_loss: 0.5735\n",
            "Epoch 416/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3851 - val_loss: 0.5581\n",
            "Epoch 417/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3843 - val_loss: 0.6311\n",
            "Epoch 418/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3846 - val_loss: 0.5645\n",
            "Epoch 419/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3842 - val_loss: 0.5693\n",
            "Epoch 420/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3846 - val_loss: 0.5531\n",
            "Epoch 421/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3841 - val_loss: 0.5522\n",
            "Epoch 422/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3840 - val_loss: 0.5554\n",
            "Epoch 423/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3834 - val_loss: 0.5427\n",
            "Epoch 424/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3834 - val_loss: 0.5406\n",
            "Epoch 425/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3832 - val_loss: 0.5554\n",
            "Epoch 426/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3832 - val_loss: 0.5790\n",
            "Epoch 427/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3831 - val_loss: 0.5623\n",
            "Epoch 428/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3822 - val_loss: 0.5740\n",
            "Epoch 429/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3829 - val_loss: 0.5701\n",
            "Epoch 430/5000\n",
            "493/493 [==============================] - 1s 2ms/step - loss: 0.3821 - val_loss: 0.5952\n",
            "Epoch 431/5000\n",
            "293/493 [================>.............] - ETA: 0s - loss: 0.3829"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-e80f85b20f8e>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mungrouped_random_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mungrouped_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mungrouped_random_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ungrouped_random\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-e9de558a6c0f>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(sp, run_id, training_batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   history, model = train_or_update_variational_model(\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-e5165275ff44>\u001b[0m in \u001b[0;36mtrain_or_update_variational_model\u001b[0;34m(sp, hidden_layers, epochs, batch_size, lr, validation_data, model_file, use_checkpoint)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mget_model_save_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         custom_objects={\"kl_divergence\": kl_divergence})\n\u001b[0;32m---> 63\u001b[0;31m   history = model.fit(sp.train.X, sp.train.Y, verbose=1, epochs=epochs, batch_size=batch_size,\n\u001b[0m\u001b[1;32m     64\u001b[0m                       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                       shuffle=True, callbacks=callbacks_list)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[0;32m--> 142\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m    143\u001b[0m     return concrete_function._call_flat(\n\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \"\"\"\n\u001b[1;32m    341\u001b[0m     args, kwargs, filtered_flat_args = (\n\u001b[0;32m--> 342\u001b[0;31m         self._function_spec.canonicalize_function_inputs(args, kwargs))\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/function_spec.py\u001b[0m in \u001b[0;36mcanonicalize_function_inputs\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34mf\"{self._name}({', '.join(args)})\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0mcanonicalize_function_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m     \"\"\"Canonicalizes `args` and `kwargs`.\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Ungrouped, fixed"
      ],
      "metadata": {
        "id": "yIQGGzsIup_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ungrouped_fixed = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, \"amazon_sample_data/uc_davis_2023_08_12_train_fixed_ungrouped.csv\"),\n",
        "    'TEST' : os.path.join(FP_ROOT, \"amazon_sample_data/uc_davis_2023_08_12_test_fixed_ungrouped.csv\"),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, \"amazon_sample_data/uc_davis_2023_08_12_validation_fixed_ungrouped.csv\"),\n",
        "}\n",
        "\n",
        "ungrouped_fixed_scaled = load_and_scale(ungrouped_fixed)\n",
        "train_and_evaluate(ungrouped_fixed_scaled, \"ungrouped_fixed\", training_batch_size=3)"
      ],
      "metadata": {
        "id": "-TkJGJ9Xux24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5d09697c-b9ef-4bb4-de80-dfcb171b9b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================\n",
            "ungrouped_fixed\n",
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)          [(None, 12)]         0           []                               \n",
            "                                                                                                  \n",
            " dense_20 (Dense)               (None, 20)           260         ['input_11[0][0]']               \n",
            "                                                                                                  \n",
            " dense_21 (Dense)               (None, 20)           420         ['dense_20[0][0]']               \n",
            "                                                                                                  \n",
            " var_output (Dense)             (None, 1)            21          ['dense_21[0][0]']               \n",
            "                                                                                                  \n",
            " mean_output (Dense)            (None, 1)            21          ['dense_21[0][0]']               \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1)            0           ['var_output[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenate)   (None, 2)            0           ['mean_output[0][0]',            \n",
            "                                                                  'lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 722\n",
            "Trainable params: 722\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Restoring model weights from the end of the best epoch: 1031.\n",
            "Epoch 1531: early stopping\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 2ms/step - loss: 2.8060\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f83fb36d3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "EXPECTED:\n",
            "     d18O_cel_mean  d18O_cel_variance\n",
            "0        25.120750           0.844007\n",
            "1        25.120750           0.844007\n",
            "2        25.120750           0.844007\n",
            "3        25.120750           0.844007\n",
            "4        25.120750           0.844007\n",
            "5        25.120750           0.844007\n",
            "6        25.120750           0.844007\n",
            "7        25.120750           0.844007\n",
            "8        25.120750           0.844007\n",
            "9        25.120750           0.844007\n",
            "10       25.120750           0.844007\n",
            "11       25.120750           0.844007\n",
            "12       25.120750           0.844007\n",
            "13       25.120750           0.844007\n",
            "14       25.120750           0.844007\n",
            "15       25.120750           0.844007\n",
            "16       25.120750           0.844007\n",
            "17       25.120750           0.844007\n",
            "18       25.120750           0.844007\n",
            "19       25.120750           0.844007\n",
            "20       25.120750           0.844007\n",
            "21       25.120750           0.844007\n",
            "22       25.120750           0.844007\n",
            "23       25.120750           0.844007\n",
            "24       25.120750           0.844007\n",
            "25       25.120750           0.844007\n",
            "26       25.120750           0.844007\n",
            "27       25.120750           0.844007\n",
            "28       25.120750           0.844007\n",
            "29       25.120750           0.844007\n",
            "30       25.120750           0.844007\n",
            "31       25.120750           0.844007\n",
            "32       25.120750           0.844007\n",
            "33       25.120750           0.844007\n",
            "34       25.120750           0.844007\n",
            "35       25.120750           0.844007\n",
            "36       25.120750           0.844007\n",
            "37       25.120750           0.844007\n",
            "38       25.120750           0.844007\n",
            "39       25.120750           0.844007\n",
            "40       24.777670           0.736571\n",
            "41       24.777670           0.736571\n",
            "42       24.777670           0.736571\n",
            "43       24.777670           0.736571\n",
            "44       24.777670           0.736571\n",
            "45       25.551850           0.312355\n",
            "46       25.551850           0.312355\n",
            "47       25.551850           0.312355\n",
            "48       25.551850           0.312355\n",
            "49       25.551850           0.312355\n",
            "50       25.115885           0.033519\n",
            "51       25.115885           0.033519\n",
            "52       25.115885           0.033519\n",
            "53       25.115885           0.033519\n",
            "54       25.115885           0.033519\n",
            "55       25.987815           5.280825\n",
            "56       25.987815           5.280825\n",
            "57       25.987815           5.280825\n",
            "58       25.987815           5.280825\n",
            "59       25.987815           5.280825\n",
            "60       24.132031           0.389042\n",
            "61       24.132031           0.389042\n",
            "62       24.132031           0.389042\n",
            "63       24.132031           0.389042\n",
            "64       27.559140           0.780698\n",
            "65       27.559140           0.780698\n",
            "66       27.559140           0.780698\n",
            "67       27.559140           0.780698\n",
            "68       27.559140           0.780698\n",
            "69       27.559140           0.780698\n",
            "70       27.559140           0.780698\n",
            "71       27.559140           0.780698\n",
            "72       27.559140           0.780698\n",
            "73       27.559140           0.780698\n",
            "74       27.260748           0.830341\n",
            "75       27.260748           0.830341\n",
            "76       27.260748           0.830341\n",
            "77       27.260748           0.830341\n",
            "78       27.260748           0.830341\n",
            "79       27.260748           0.830341\n",
            "80       27.260748           0.830341\n",
            "81       27.260748           0.830341\n",
            "82       27.260748           0.830341\n",
            "83       27.260748           0.830341\n",
            "84       27.260748           0.830341\n",
            "85       27.260748           0.830341\n",
            "86       27.260748           0.830341\n",
            "87       27.260748           0.830341\n",
            "88       27.260748           0.830341\n",
            "89       27.260748           0.830341\n",
            "90       27.260748           0.830341\n",
            "91       27.260748           0.830341\n",
            "92       27.260748           0.830341\n",
            "93       27.260748           0.830341\n",
            "94       27.260748           0.830341\n",
            "95       27.260748           0.830341\n",
            "96       27.260748           0.830341\n",
            "97       27.260748           0.830341\n",
            "98       27.260748           0.830341\n",
            "99       27.260748           0.830341\n",
            "100      27.260748           0.830341\n",
            "101      27.260748           0.830341\n",
            "102      27.260748           0.830341\n",
            "103      27.260748           0.830341\n",
            "104      27.260748           0.830341\n",
            "105      27.260748           0.830341\n",
            "106      27.260748           0.830341\n",
            "107      27.260748           0.830341\n",
            "108      27.260748           0.830341\n",
            "\n",
            "PREDICTED:\n",
            "     d18O_cel_mean  d18O_cel_variance\n",
            "0        26.905420           0.536932\n",
            "1        26.905420           0.536932\n",
            "2        26.905420           0.536932\n",
            "3        26.905420           0.536932\n",
            "4        26.905420           0.536932\n",
            "5        26.905420           0.536932\n",
            "6        26.905420           0.536932\n",
            "7        26.905420           0.536932\n",
            "8        26.905420           0.536932\n",
            "9        26.905420           0.536932\n",
            "10       26.905420           0.536932\n",
            "11       26.905420           0.536932\n",
            "12       26.905420           0.536932\n",
            "13       26.905420           0.536932\n",
            "14       26.905420           0.536932\n",
            "15       26.905420           0.536932\n",
            "16       26.905420           0.536932\n",
            "17       26.905420           0.536932\n",
            "18       26.905420           0.536932\n",
            "19       26.905420           0.536932\n",
            "20       26.905420           0.536932\n",
            "21       26.905420           0.536932\n",
            "22       26.905420           0.536932\n",
            "23       26.905420           0.536932\n",
            "24       26.905420           0.536932\n",
            "25       26.905420           0.536932\n",
            "26       26.905420           0.536932\n",
            "27       26.905420           0.536932\n",
            "28       26.905420           0.536932\n",
            "29       26.905420           0.536932\n",
            "30       26.905420           0.536932\n",
            "31       26.905420           0.536932\n",
            "32       26.905420           0.536932\n",
            "33       26.905420           0.536932\n",
            "34       26.905420           0.536932\n",
            "35       26.905420           0.536932\n",
            "36       26.905420           0.536932\n",
            "37       26.905420           0.536932\n",
            "38       26.905420           0.536932\n",
            "39       26.905420           0.536932\n",
            "40       25.050524           2.297126\n",
            "41       25.050524           2.297126\n",
            "42       25.050524           2.297126\n",
            "43       25.050524           2.297126\n",
            "44       25.050524           2.297126\n",
            "45       25.106853           2.422469\n",
            "46       25.106853           2.422469\n",
            "47       25.106853           2.422469\n",
            "48       25.106853           2.422469\n",
            "49       25.106853           2.422469\n",
            "50       25.156002           2.385101\n",
            "51       25.156002           2.385101\n",
            "52       25.156002           2.385101\n",
            "53       25.156002           2.385101\n",
            "54       25.156002           2.385101\n",
            "55       25.018280           2.489763\n",
            "56       25.018280           2.489763\n",
            "57       25.018280           2.489763\n",
            "58       25.018280           2.489763\n",
            "59       25.018280           2.489763\n",
            "60       25.126596           2.407570\n",
            "61       25.126596           2.407570\n",
            "62       25.126596           2.407570\n",
            "63       25.126596           2.407570\n",
            "64       23.200083           3.390790\n",
            "65       23.200083           3.390790\n",
            "66       23.200083           3.390790\n",
            "67       23.200083           3.390790\n",
            "68       23.200083           3.390790\n",
            "69       23.200083           3.390790\n",
            "70       23.200083           3.390790\n",
            "71       23.200083           3.390790\n",
            "72       23.200083           3.390790\n",
            "73       23.200083           3.390790\n",
            "74       23.285538           3.304089\n",
            "75       23.285538           3.304089\n",
            "76       23.285538           3.304089\n",
            "77       23.285538           3.304089\n",
            "78       23.285538           3.304089\n",
            "79       23.285538           3.304089\n",
            "80       23.285538           3.304089\n",
            "81       23.285538           3.304089\n",
            "82       23.285538           3.304089\n",
            "83       23.285538           3.304089\n",
            "84       23.285538           3.304089\n",
            "85       23.285538           3.304089\n",
            "86       23.285538           3.304089\n",
            "87       23.285538           3.304089\n",
            "88       23.285538           3.304089\n",
            "89       23.285538           3.304089\n",
            "90       23.285538           3.304089\n",
            "91       23.285538           3.304089\n",
            "92       23.285538           3.304089\n",
            "93       23.285538           3.304089\n",
            "94       23.285538           3.304089\n",
            "95       23.285538           3.304089\n",
            "96       23.285538           3.304089\n",
            "97       23.285538           3.304089\n",
            "98       23.285538           3.304089\n",
            "99       23.285538           3.304089\n",
            "100      23.285538           3.304089\n",
            "101      23.285538           3.304089\n",
            "102      23.285538           3.304089\n",
            "103      23.285538           3.304089\n",
            "104      23.285538           3.304089\n",
            "105      23.285538           3.304089\n",
            "106      23.285538           3.304089\n",
            "107      23.285538           3.304089\n",
            "108      23.285536           3.304089\n",
            "RMSE: 2.8422163418172057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-07-12 21:21:56.328651: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [109,12]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Grouped, random"
      ],
      "metadata": {
        "id": "n8pNR1CrvF2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_random = {\n",
        "    'TRAIN' : os.path.join(FP_ROOT, 'amazon_sample_data/uc_davis_2023_08_12_train_random_grouped.csv'),\n",
        "    'TEST' : os.path.join(FP_ROOT, 'amazon_sample_data/uc_davis_2023_08_12_test_random_grouped.csv'),\n",
        "    'VALIDATION' : os.path.join(FP_ROOT, 'amazon_sample_data/uc_davis_2023_08_12_validation_random_grouped.csv'),\n",
        "}\n",
        "\n",
        "grouped_random_scaled = load_and_scale(grouped_random)\n",
        "train_and_evaluate(grouped_random_scaled, \"grouped_random\", training_batch_size=3)"
      ],
      "metadata": {
        "id": "rPONfgkjvJWz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de827a16-7308-40c6-f868-c7235d5dabdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 225/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6166 - val_loss: 1.0536\n",
            "Epoch 226/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6164 - val_loss: 1.0535\n",
            "Epoch 227/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6167 - val_loss: 1.0531\n",
            "Epoch 228/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6164 - val_loss: 1.0543\n",
            "Epoch 229/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6165 - val_loss: 1.0552\n",
            "Epoch 230/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6163 - val_loss: 1.0543\n",
            "Epoch 231/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6164 - val_loss: 1.0528\n",
            "Epoch 232/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6165 - val_loss: 1.0540\n",
            "Epoch 233/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6160 - val_loss: 1.0539\n",
            "Epoch 234/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6164 - val_loss: 1.0526\n",
            "Epoch 235/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6163 - val_loss: 1.0555\n",
            "Epoch 236/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6160 - val_loss: 1.0534\n",
            "Epoch 237/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6160 - val_loss: 1.0545\n",
            "Epoch 238/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6158 - val_loss: 1.0537\n",
            "Epoch 239/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6159 - val_loss: 1.0554\n",
            "Epoch 240/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6157 - val_loss: 1.0534\n",
            "Epoch 241/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6160 - val_loss: 1.0546\n",
            "Epoch 242/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6157 - val_loss: 1.0552\n",
            "Epoch 243/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6161 - val_loss: 1.0555\n",
            "Epoch 244/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6158 - val_loss: 1.0530\n",
            "Epoch 245/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6157 - val_loss: 1.0553\n",
            "Epoch 246/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6156 - val_loss: 1.0536\n",
            "Epoch 247/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6155 - val_loss: 1.0535\n",
            "Epoch 248/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6161 - val_loss: 1.0516\n",
            "Epoch 249/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6151 - val_loss: 1.0538\n",
            "Epoch 250/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6154 - val_loss: 1.0540\n",
            "Epoch 251/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6155 - val_loss: 1.0528\n",
            "Epoch 252/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6154 - val_loss: 1.0533\n",
            "Epoch 253/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6153 - val_loss: 1.0532\n",
            "Epoch 254/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6150 - val_loss: 1.0530\n",
            "Epoch 255/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6152 - val_loss: 1.0538\n",
            "Epoch 256/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6152 - val_loss: 1.0526\n",
            "Epoch 257/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6148 - val_loss: 1.0541\n",
            "Epoch 258/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6147 - val_loss: 1.0542\n",
            "Epoch 259/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6147 - val_loss: 1.0532\n",
            "Epoch 260/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6147 - val_loss: 1.0529\n",
            "Epoch 261/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6147 - val_loss: 1.0544\n",
            "Epoch 262/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6150 - val_loss: 1.0531\n",
            "Epoch 263/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6147 - val_loss: 1.0543\n",
            "Epoch 264/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6145 - val_loss: 1.0543\n",
            "Epoch 265/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6147 - val_loss: 1.0518\n",
            "Epoch 266/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6143 - val_loss: 1.0533\n",
            "Epoch 267/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6143 - val_loss: 1.0540\n",
            "Epoch 268/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6143 - val_loss: 1.0543\n",
            "Epoch 269/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6145 - val_loss: 1.0532\n",
            "Epoch 270/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6145 - val_loss: 1.0558\n",
            "Epoch 271/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6144 - val_loss: 1.0538\n",
            "Epoch 272/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6140 - val_loss: 1.0544\n",
            "Epoch 273/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6140 - val_loss: 1.0546\n",
            "Epoch 274/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6140 - val_loss: 1.0542\n",
            "Epoch 275/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6138 - val_loss: 1.0537\n",
            "Epoch 276/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6139 - val_loss: 1.0543\n",
            "Epoch 277/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6135 - val_loss: 1.0539\n",
            "Epoch 278/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6135 - val_loss: 1.0533\n",
            "Epoch 279/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6135 - val_loss: 1.0533\n",
            "Epoch 280/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6136 - val_loss: 1.0524\n",
            "Epoch 281/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6136 - val_loss: 1.0540\n",
            "Epoch 282/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6137 - val_loss: 1.0517\n",
            "Epoch 283/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6134 - val_loss: 1.0541\n",
            "Epoch 284/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 1.0528\n",
            "Epoch 285/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 1.0534\n",
            "Epoch 286/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 1.0530\n",
            "Epoch 287/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 1.0537\n",
            "Epoch 288/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6133 - val_loss: 1.0544\n",
            "Epoch 289/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6137 - val_loss: 1.0544\n",
            "Epoch 290/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 1.0548\n",
            "Epoch 291/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 1.0524\n",
            "Epoch 292/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6132 - val_loss: 1.0545\n",
            "Epoch 293/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 1.0535\n",
            "Epoch 294/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6126 - val_loss: 1.0530\n",
            "Epoch 295/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6126 - val_loss: 1.0524\n",
            "Epoch 296/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6127 - val_loss: 1.0522\n",
            "Epoch 297/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6126 - val_loss: 1.0518\n",
            "Epoch 298/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6125 - val_loss: 1.0514\n",
            "Epoch 299/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6127 - val_loss: 1.0538\n",
            "Epoch 300/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6124 - val_loss: 1.0533\n",
            "Epoch 301/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6123 - val_loss: 1.0533\n",
            "Epoch 302/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6122 - val_loss: 1.0539\n",
            "Epoch 303/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6125 - val_loss: 1.0528\n",
            "Epoch 304/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6121 - val_loss: 1.0534\n",
            "Epoch 305/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6123 - val_loss: 1.0530\n",
            "Epoch 306/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6121 - val_loss: 1.0524\n",
            "Epoch 307/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6120 - val_loss: 1.0542\n",
            "Epoch 308/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6120 - val_loss: 1.0516\n",
            "Epoch 309/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6120 - val_loss: 1.0543\n",
            "Epoch 310/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6115 - val_loss: 1.0535\n",
            "Epoch 311/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6118 - val_loss: 1.0515\n",
            "Epoch 312/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6116 - val_loss: 1.0540\n",
            "Epoch 313/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6116 - val_loss: 1.0519\n",
            "Epoch 314/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6116 - val_loss: 1.0534\n",
            "Epoch 315/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6113 - val_loss: 1.0531\n",
            "Epoch 316/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6115 - val_loss: 1.0540\n",
            "Epoch 317/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6117 - val_loss: 1.0518\n",
            "Epoch 318/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6119 - val_loss: 1.0537\n",
            "Epoch 319/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6113 - val_loss: 1.0528\n",
            "Epoch 320/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6114 - val_loss: 1.0517\n",
            "Epoch 321/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6110 - val_loss: 1.0534\n",
            "Epoch 322/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6112 - val_loss: 1.0541\n",
            "Epoch 323/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6110 - val_loss: 1.0520\n",
            "Epoch 324/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6112 - val_loss: 1.0513\n",
            "Epoch 325/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6107 - val_loss: 1.0528\n",
            "Epoch 326/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6109 - val_loss: 1.0511\n",
            "Epoch 327/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6106 - val_loss: 1.0534\n",
            "Epoch 328/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6105 - val_loss: 1.0527\n",
            "Epoch 329/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6105 - val_loss: 1.0543\n",
            "Epoch 330/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6106 - val_loss: 1.0539\n",
            "Epoch 331/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6106 - val_loss: 1.0546\n",
            "Epoch 332/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6105 - val_loss: 1.0519\n",
            "Epoch 333/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6105 - val_loss: 1.0530\n",
            "Epoch 334/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6103 - val_loss: 1.0523\n",
            "Epoch 335/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6100 - val_loss: 1.0533\n",
            "Epoch 336/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6100 - val_loss: 1.0533\n",
            "Epoch 337/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6099 - val_loss: 1.0536\n",
            "Epoch 338/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6100 - val_loss: 1.0524\n",
            "Epoch 339/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6099 - val_loss: 1.0531\n",
            "Epoch 340/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6099 - val_loss: 1.0530\n",
            "Epoch 341/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6101 - val_loss: 1.0518\n",
            "Epoch 342/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6101 - val_loss: 1.0544\n",
            "Epoch 343/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6097 - val_loss: 1.0524\n",
            "Epoch 344/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6096 - val_loss: 1.0513\n",
            "Epoch 345/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6095 - val_loss: 1.0532\n",
            "Epoch 346/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6095 - val_loss: 1.0535\n",
            "Epoch 347/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6094 - val_loss: 1.0527\n",
            "Epoch 348/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6093 - val_loss: 1.0534\n",
            "Epoch 349/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6098 - val_loss: 1.0552\n",
            "Epoch 350/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6093 - val_loss: 1.0518\n",
            "Epoch 351/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6092 - val_loss: 1.0531\n",
            "Epoch 352/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6089 - val_loss: 1.0526\n",
            "Epoch 353/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6092 - val_loss: 1.0543\n",
            "Epoch 354/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6093 - val_loss: 1.0511\n",
            "Epoch 355/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6089 - val_loss: 1.0527\n",
            "Epoch 356/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6088 - val_loss: 1.0528\n",
            "Epoch 357/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6087 - val_loss: 1.0544\n",
            "Epoch 358/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6084 - val_loss: 1.0527\n",
            "Epoch 359/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6088 - val_loss: 1.0533\n",
            "Epoch 360/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6087 - val_loss: 1.0528\n",
            "Epoch 361/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6085 - val_loss: 1.0531\n",
            "Epoch 362/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6086 - val_loss: 1.0523\n",
            "Epoch 363/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6084 - val_loss: 1.0533\n",
            "Epoch 364/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6084 - val_loss: 1.0542\n",
            "Epoch 365/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6082 - val_loss: 1.0537\n",
            "Epoch 366/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6084 - val_loss: 1.0548\n",
            "Epoch 367/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6080 - val_loss: 1.0541\n",
            "Epoch 368/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6080 - val_loss: 1.0523\n",
            "Epoch 369/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6078 - val_loss: 1.0524\n",
            "Epoch 370/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6081 - val_loss: 1.0537\n",
            "Epoch 371/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.6079 - val_loss: 1.0540\n",
            "Epoch 372/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6078 - val_loss: 1.0534\n",
            "Epoch 373/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.6075 - val_loss: 1.0528\n",
            "Epoch 374/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6077 - val_loss: 1.0546\n",
            "Epoch 375/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6074 - val_loss: 1.0519\n",
            "Epoch 376/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6076 - val_loss: 1.0523\n",
            "Epoch 377/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6077 - val_loss: 1.0526\n",
            "Epoch 378/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6074 - val_loss: 1.0507\n",
            "Epoch 379/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6074 - val_loss: 1.0512\n",
            "Epoch 380/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6078 - val_loss: 1.0526\n",
            "Epoch 381/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6069 - val_loss: 1.0529\n",
            "Epoch 382/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6072 - val_loss: 1.0538\n",
            "Epoch 383/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6072 - val_loss: 1.0507\n",
            "Epoch 384/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6069 - val_loss: 1.0518\n",
            "Epoch 385/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6067 - val_loss: 1.0534\n",
            "Epoch 386/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6067 - val_loss: 1.0536\n",
            "Epoch 387/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6065 - val_loss: 1.0528\n",
            "Epoch 388/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6066 - val_loss: 1.0528\n",
            "Epoch 389/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6064 - val_loss: 1.0536\n",
            "Epoch 390/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6065 - val_loss: 1.0546\n",
            "Epoch 391/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.6064 - val_loss: 1.0536\n",
            "Epoch 392/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6064 - val_loss: 1.0539\n",
            "Epoch 393/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6064 - val_loss: 1.0521\n",
            "Epoch 394/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6063 - val_loss: 1.0524\n",
            "Epoch 395/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6062 - val_loss: 1.0530\n",
            "Epoch 396/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6063 - val_loss: 1.0514\n",
            "Epoch 397/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6061 - val_loss: 1.0534\n",
            "Epoch 398/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6060 - val_loss: 1.0533\n",
            "Epoch 399/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6059 - val_loss: 1.0522\n",
            "Epoch 400/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6060 - val_loss: 1.0514\n",
            "Epoch 401/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6058 - val_loss: 1.0511\n",
            "Epoch 402/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6054 - val_loss: 1.0525\n",
            "Epoch 403/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6058 - val_loss: 1.0544\n",
            "Epoch 404/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6055 - val_loss: 1.0521\n",
            "Epoch 405/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6056 - val_loss: 1.0526\n",
            "Epoch 406/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6053 - val_loss: 1.0539\n",
            "Epoch 407/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6054 - val_loss: 1.0541\n",
            "Epoch 408/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6060 - val_loss: 1.0508\n",
            "Epoch 409/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6050 - val_loss: 1.0532\n",
            "Epoch 410/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6050 - val_loss: 1.0538\n",
            "Epoch 411/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6049 - val_loss: 1.0542\n",
            "Epoch 412/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6053 - val_loss: 1.0513\n",
            "Epoch 413/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6046 - val_loss: 1.0535\n",
            "Epoch 414/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6049 - val_loss: 1.0548\n",
            "Epoch 415/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.6048 - val_loss: 1.0524\n",
            "Epoch 416/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.6045 - val_loss: 1.0518\n",
            "Epoch 417/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.6047 - val_loss: 1.0533\n",
            "Epoch 418/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6047 - val_loss: 1.0548\n",
            "Epoch 419/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6045 - val_loss: 1.0534\n",
            "Epoch 420/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6043 - val_loss: 1.0517\n",
            "Epoch 421/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6043 - val_loss: 1.0530\n",
            "Epoch 422/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6043 - val_loss: 1.0517\n",
            "Epoch 423/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6042 - val_loss: 1.0541\n",
            "Epoch 424/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6040 - val_loss: 1.0524\n",
            "Epoch 425/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6041 - val_loss: 1.0531\n",
            "Epoch 426/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6038 - val_loss: 1.0531\n",
            "Epoch 427/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6045 - val_loss: 1.0536\n",
            "Epoch 428/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6039 - val_loss: 1.0541\n",
            "Epoch 429/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6036 - val_loss: 1.0522\n",
            "Epoch 430/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6039 - val_loss: 1.0525\n",
            "Epoch 431/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6036 - val_loss: 1.0534\n",
            "Epoch 432/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6036 - val_loss: 1.0533\n",
            "Epoch 433/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6031 - val_loss: 1.0522\n",
            "Epoch 434/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6034 - val_loss: 1.0518\n",
            "Epoch 435/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6034 - val_loss: 1.0527\n",
            "Epoch 436/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6034 - val_loss: 1.0544\n",
            "Epoch 437/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6034 - val_loss: 1.0525\n",
            "Epoch 438/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6032 - val_loss: 1.0515\n",
            "Epoch 439/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6030 - val_loss: 1.0542\n",
            "Epoch 440/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6031 - val_loss: 1.0520\n",
            "Epoch 441/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6027 - val_loss: 1.0520\n",
            "Epoch 442/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6027 - val_loss: 1.0521\n",
            "Epoch 443/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6027 - val_loss: 1.0529\n",
            "Epoch 444/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6029 - val_loss: 1.0541\n",
            "Epoch 445/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.6025 - val_loss: 1.0517\n",
            "Epoch 446/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6024 - val_loss: 1.0527\n",
            "Epoch 447/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6026 - val_loss: 1.0514\n",
            "Epoch 448/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6023 - val_loss: 1.0547\n",
            "Epoch 449/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6023 - val_loss: 1.0521\n",
            "Epoch 450/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6021 - val_loss: 1.0515\n",
            "Epoch 451/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6021 - val_loss: 1.0526\n",
            "Epoch 452/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6019 - val_loss: 1.0535\n",
            "Epoch 453/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6023 - val_loss: 1.0515\n",
            "Epoch 454/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6016 - val_loss: 1.0525\n",
            "Epoch 455/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6017 - val_loss: 1.0533\n",
            "Epoch 456/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6017 - val_loss: 1.0526\n",
            "Epoch 457/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6015 - val_loss: 1.0523\n",
            "Epoch 458/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6018 - val_loss: 1.0509\n",
            "Epoch 459/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6015 - val_loss: 1.0528\n",
            "Epoch 460/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6014 - val_loss: 1.0530\n",
            "Epoch 461/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6014 - val_loss: 1.0535\n",
            "Epoch 462/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6012 - val_loss: 1.0526\n",
            "Epoch 463/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6011 - val_loss: 1.0524\n",
            "Epoch 464/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6012 - val_loss: 1.0526\n",
            "Epoch 465/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6010 - val_loss: 1.0541\n",
            "Epoch 466/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6010 - val_loss: 1.0532\n",
            "Epoch 467/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6008 - val_loss: 1.0539\n",
            "Epoch 468/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6012 - val_loss: 1.0539\n",
            "Epoch 469/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6007 - val_loss: 1.0513\n",
            "Epoch 470/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6007 - val_loss: 1.0522\n",
            "Epoch 471/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6005 - val_loss: 1.0532\n",
            "Epoch 472/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6005 - val_loss: 1.0521\n",
            "Epoch 473/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6004 - val_loss: 1.0528\n",
            "Epoch 474/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6004 - val_loss: 1.0525\n",
            "Epoch 475/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6003 - val_loss: 1.0524\n",
            "Epoch 476/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5999 - val_loss: 1.0530\n",
            "Epoch 477/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6004 - val_loss: 1.0543\n",
            "Epoch 478/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6008 - val_loss: 1.0531\n",
            "Epoch 479/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6002 - val_loss: 1.0515\n",
            "Epoch 480/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5997 - val_loss: 1.0521\n",
            "Epoch 481/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5997 - val_loss: 1.0520\n",
            "Epoch 482/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6000 - val_loss: 1.0511\n",
            "Epoch 483/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.6005 - val_loss: 1.0554\n",
            "Epoch 484/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5995 - val_loss: 1.0524\n",
            "Epoch 485/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5995 - val_loss: 1.0515\n",
            "Epoch 486/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5992 - val_loss: 1.0533\n",
            "Epoch 487/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5993 - val_loss: 1.0523\n",
            "Epoch 488/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5992 - val_loss: 1.0520\n",
            "Epoch 489/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5992 - val_loss: 1.0537\n",
            "Epoch 490/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5992 - val_loss: 1.0520\n",
            "Epoch 491/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5990 - val_loss: 1.0517\n",
            "Epoch 492/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5993 - val_loss: 1.0512\n",
            "Epoch 493/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5989 - val_loss: 1.0511\n",
            "Epoch 494/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5989 - val_loss: 1.0531\n",
            "Epoch 495/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5990 - val_loss: 1.0523\n",
            "Epoch 496/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5987 - val_loss: 1.0522\n",
            "Epoch 497/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5986 - val_loss: 1.0530\n",
            "Epoch 498/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5986 - val_loss: 1.0526\n",
            "Epoch 499/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5989 - val_loss: 1.0503\n",
            "Epoch 500/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5984 - val_loss: 1.0540\n",
            "Epoch 501/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5980 - val_loss: 1.0522\n",
            "Epoch 502/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5980 - val_loss: 1.0535\n",
            "Epoch 503/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5986 - val_loss: 1.0545\n",
            "Epoch 504/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5990 - val_loss: 1.0499\n",
            "Epoch 505/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5980 - val_loss: 1.0530\n",
            "Epoch 506/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5981 - val_loss: 1.0515\n",
            "Epoch 507/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5979 - val_loss: 1.0531\n",
            "Epoch 508/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5979 - val_loss: 1.0515\n",
            "Epoch 509/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5977 - val_loss: 1.0533\n",
            "Epoch 510/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5975 - val_loss: 1.0518\n",
            "Epoch 511/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5983 - val_loss: 1.0554\n",
            "Epoch 512/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5972 - val_loss: 1.0531\n",
            "Epoch 513/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5974 - val_loss: 1.0514\n",
            "Epoch 514/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5970 - val_loss: 1.0523\n",
            "Epoch 515/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5970 - val_loss: 1.0516\n",
            "Epoch 516/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5970 - val_loss: 1.0511\n",
            "Epoch 517/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5970 - val_loss: 1.0540\n",
            "Epoch 518/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5976 - val_loss: 1.0514\n",
            "Epoch 519/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5971 - val_loss: 1.0529\n",
            "Epoch 520/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5967 - val_loss: 1.0531\n",
            "Epoch 521/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5966 - val_loss: 1.0527\n",
            "Epoch 522/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5970 - val_loss: 1.0516\n",
            "Epoch 523/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5968 - val_loss: 1.0542\n",
            "Epoch 524/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5966 - val_loss: 1.0523\n",
            "Epoch 525/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5969 - val_loss: 1.0497\n",
            "Epoch 526/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5962 - val_loss: 1.0536\n",
            "Epoch 527/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5963 - val_loss: 1.0544\n",
            "Epoch 528/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5961 - val_loss: 1.0521\n",
            "Epoch 529/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5958 - val_loss: 1.0518\n",
            "Epoch 530/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5962 - val_loss: 1.0530\n",
            "Epoch 531/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5961 - val_loss: 1.0541\n",
            "Epoch 532/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5955 - val_loss: 1.0529\n",
            "Epoch 533/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5955 - val_loss: 1.0532\n",
            "Epoch 534/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5956 - val_loss: 1.0533\n",
            "Epoch 535/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5957 - val_loss: 1.0526\n",
            "Epoch 536/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5953 - val_loss: 1.0518\n",
            "Epoch 537/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5954 - val_loss: 1.0513\n",
            "Epoch 538/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5951 - val_loss: 1.0532\n",
            "Epoch 539/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5959 - val_loss: 1.0510\n",
            "Epoch 540/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5953 - val_loss: 1.0531\n",
            "Epoch 541/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5948 - val_loss: 1.0532\n",
            "Epoch 542/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5952 - val_loss: 1.0545\n",
            "Epoch 543/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5955 - val_loss: 1.0522\n",
            "Epoch 544/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5952 - val_loss: 1.0537\n",
            "Epoch 545/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5945 - val_loss: 1.0521\n",
            "Epoch 546/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5946 - val_loss: 1.0513\n",
            "Epoch 547/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5944 - val_loss: 1.0517\n",
            "Epoch 548/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5943 - val_loss: 1.0521\n",
            "Epoch 549/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5944 - val_loss: 1.0520\n",
            "Epoch 550/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5941 - val_loss: 1.0527\n",
            "Epoch 551/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5941 - val_loss: 1.0534\n",
            "Epoch 552/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5941 - val_loss: 1.0512\n",
            "Epoch 553/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5941 - val_loss: 1.0531\n",
            "Epoch 554/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5942 - val_loss: 1.0507\n",
            "Epoch 555/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5938 - val_loss: 1.0525\n",
            "Epoch 556/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5938 - val_loss: 1.0530\n",
            "Epoch 557/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5934 - val_loss: 1.0532\n",
            "Epoch 558/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5939 - val_loss: 1.0512\n",
            "Epoch 559/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5934 - val_loss: 1.0514\n",
            "Epoch 560/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5936 - val_loss: 1.0536\n",
            "Epoch 561/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5933 - val_loss: 1.0524\n",
            "Epoch 562/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5934 - val_loss: 1.0527\n",
            "Epoch 563/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5930 - val_loss: 1.0541\n",
            "Epoch 564/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5935 - val_loss: 1.0544\n",
            "Epoch 565/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5933 - val_loss: 1.0507\n",
            "Epoch 566/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5931 - val_loss: 1.0523\n",
            "Epoch 567/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5932 - val_loss: 1.0522\n",
            "Epoch 568/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5928 - val_loss: 1.0508\n",
            "Epoch 569/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5927 - val_loss: 1.0537\n",
            "Epoch 570/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5924 - val_loss: 1.0515\n",
            "Epoch 571/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5924 - val_loss: 1.0508\n",
            "Epoch 572/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5925 - val_loss: 1.0534\n",
            "Epoch 573/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5921 - val_loss: 1.0524\n",
            "Epoch 574/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5927 - val_loss: 1.0503\n",
            "Epoch 575/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5920 - val_loss: 1.0536\n",
            "Epoch 576/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5920 - val_loss: 1.0546\n",
            "Epoch 577/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5920 - val_loss: 1.0543\n",
            "Epoch 578/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5917 - val_loss: 1.0515\n",
            "Epoch 579/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5917 - val_loss: 1.0520\n",
            "Epoch 580/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5917 - val_loss: 1.0528\n",
            "Epoch 581/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5915 - val_loss: 1.0532\n",
            "Epoch 582/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5914 - val_loss: 1.0515\n",
            "Epoch 583/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5913 - val_loss: 1.0541\n",
            "Epoch 584/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5912 - val_loss: 1.0525\n",
            "Epoch 585/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5914 - val_loss: 1.0512\n",
            "Epoch 586/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5910 - val_loss: 1.0518\n",
            "Epoch 587/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5912 - val_loss: 1.0534\n",
            "Epoch 588/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5908 - val_loss: 1.0530\n",
            "Epoch 589/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5907 - val_loss: 1.0534\n",
            "Epoch 590/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5910 - val_loss: 1.0503\n",
            "Epoch 591/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5906 - val_loss: 1.0516\n",
            "Epoch 592/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5906 - val_loss: 1.0519\n",
            "Epoch 593/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5906 - val_loss: 1.0530\n",
            "Epoch 594/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5904 - val_loss: 1.0534\n",
            "Epoch 595/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5909 - val_loss: 1.0526\n",
            "Epoch 596/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5909 - val_loss: 1.0510\n",
            "Epoch 597/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5899 - val_loss: 1.0524\n",
            "Epoch 598/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5900 - val_loss: 1.0532\n",
            "Epoch 599/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5899 - val_loss: 1.0518\n",
            "Epoch 600/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5902 - val_loss: 1.0541\n",
            "Epoch 601/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5908 - val_loss: 1.0564\n",
            "Epoch 602/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5903 - val_loss: 1.0502\n",
            "Epoch 603/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5897 - val_loss: 1.0503\n",
            "Epoch 604/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5904 - val_loss: 1.0510\n",
            "Epoch 605/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5895 - val_loss: 1.0532\n",
            "Epoch 606/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5892 - val_loss: 1.0530\n",
            "Epoch 607/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5893 - val_loss: 1.0526\n",
            "Epoch 608/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5895 - val_loss: 1.0542\n",
            "Epoch 609/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5896 - val_loss: 1.0517\n",
            "Epoch 610/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5893 - val_loss: 1.0544\n",
            "Epoch 611/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5888 - val_loss: 1.0535\n",
            "Epoch 612/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5890 - val_loss: 1.0534\n",
            "Epoch 613/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5886 - val_loss: 1.0521\n",
            "Epoch 614/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5884 - val_loss: 1.0525\n",
            "Epoch 615/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5886 - val_loss: 1.0537\n",
            "Epoch 616/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5883 - val_loss: 1.0523\n",
            "Epoch 617/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5883 - val_loss: 1.0516\n",
            "Epoch 618/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5881 - val_loss: 1.0508\n",
            "Epoch 619/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5888 - val_loss: 1.0553\n",
            "Epoch 620/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5883 - val_loss: 1.0511\n",
            "Epoch 621/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5884 - val_loss: 1.0511\n",
            "Epoch 622/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5879 - val_loss: 1.0523\n",
            "Epoch 623/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5880 - val_loss: 1.0532\n",
            "Epoch 624/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5878 - val_loss: 1.0542\n",
            "Epoch 625/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5875 - val_loss: 1.0525\n",
            "Epoch 626/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5874 - val_loss: 1.0522\n",
            "Epoch 627/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5875 - val_loss: 1.0537\n",
            "Epoch 628/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5872 - val_loss: 1.0511\n",
            "Epoch 629/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5872 - val_loss: 1.0525\n",
            "Epoch 630/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5873 - val_loss: 1.0528\n",
            "Epoch 631/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5872 - val_loss: 1.0514\n",
            "Epoch 632/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5871 - val_loss: 1.0496\n",
            "Epoch 633/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 1.0517\n",
            "Epoch 634/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 1.0542\n",
            "Epoch 635/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5868 - val_loss: 1.0540\n",
            "Epoch 636/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5871 - val_loss: 1.0540\n",
            "Epoch 637/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5865 - val_loss: 1.0516\n",
            "Epoch 638/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5865 - val_loss: 1.0514\n",
            "Epoch 639/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 1.0510\n",
            "Epoch 640/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 1.0501\n",
            "Epoch 641/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5861 - val_loss: 1.0507\n",
            "Epoch 642/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5861 - val_loss: 1.0509\n",
            "Epoch 643/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 1.0543\n",
            "Epoch 644/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5861 - val_loss: 1.0514\n",
            "Epoch 645/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5859 - val_loss: 1.0540\n",
            "Epoch 646/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5858 - val_loss: 1.0497\n",
            "Epoch 647/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5854 - val_loss: 1.0518\n",
            "Epoch 648/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5856 - val_loss: 1.0528\n",
            "Epoch 649/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5853 - val_loss: 1.0518\n",
            "Epoch 650/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5856 - val_loss: 1.0520\n",
            "Epoch 651/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5854 - val_loss: 1.0515\n",
            "Epoch 652/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5854 - val_loss: 1.0499\n",
            "Epoch 653/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5856 - val_loss: 1.0534\n",
            "Epoch 654/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5850 - val_loss: 1.0538\n",
            "Epoch 655/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5849 - val_loss: 1.0518\n",
            "Epoch 656/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5849 - val_loss: 1.0524\n",
            "Epoch 657/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5851 - val_loss: 1.0530\n",
            "Epoch 658/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5848 - val_loss: 1.0535\n",
            "Epoch 659/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5851 - val_loss: 1.0494\n",
            "Epoch 660/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5847 - val_loss: 1.0506\n",
            "Epoch 661/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5849 - val_loss: 1.0548\n",
            "Epoch 662/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5842 - val_loss: 1.0520\n",
            "Epoch 663/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5841 - val_loss: 1.0524\n",
            "Epoch 664/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5841 - val_loss: 1.0506\n",
            "Epoch 665/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5841 - val_loss: 1.0527\n",
            "Epoch 666/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5842 - val_loss: 1.0528\n",
            "Epoch 667/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5842 - val_loss: 1.0504\n",
            "Epoch 668/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5839 - val_loss: 1.0545\n",
            "Epoch 669/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5835 - val_loss: 1.0519\n",
            "Epoch 670/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5838 - val_loss: 1.0508\n",
            "Epoch 671/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5837 - val_loss: 1.0514\n",
            "Epoch 672/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5838 - val_loss: 1.0555\n",
            "Epoch 673/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5835 - val_loss: 1.0525\n",
            "Epoch 674/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5833 - val_loss: 1.0516\n",
            "Epoch 675/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5830 - val_loss: 1.0512\n",
            "Epoch 676/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5828 - val_loss: 1.0534\n",
            "Epoch 677/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5831 - val_loss: 1.0517\n",
            "Epoch 678/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5828 - val_loss: 1.0535\n",
            "Epoch 679/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5829 - val_loss: 1.0509\n",
            "Epoch 680/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5828 - val_loss: 1.0540\n",
            "Epoch 681/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5825 - val_loss: 1.0505\n",
            "Epoch 682/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5822 - val_loss: 1.0515\n",
            "Epoch 683/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5823 - val_loss: 1.0513\n",
            "Epoch 684/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5826 - val_loss: 1.0525\n",
            "Epoch 685/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5823 - val_loss: 1.0513\n",
            "Epoch 686/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5821 - val_loss: 1.0509\n",
            "Epoch 687/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5823 - val_loss: 1.0509\n",
            "Epoch 688/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5822 - val_loss: 1.0523\n",
            "Epoch 689/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5818 - val_loss: 1.0527\n",
            "Epoch 690/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5817 - val_loss: 1.0518\n",
            "Epoch 691/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5814 - val_loss: 1.0510\n",
            "Epoch 692/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5813 - val_loss: 1.0520\n",
            "Epoch 693/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5811 - val_loss: 1.0526\n",
            "Epoch 694/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5811 - val_loss: 1.0515\n",
            "Epoch 695/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5813 - val_loss: 1.0535\n",
            "Epoch 696/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5823 - val_loss: 1.0546\n",
            "Epoch 697/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5810 - val_loss: 1.0526\n",
            "Epoch 698/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5807 - val_loss: 1.0526\n",
            "Epoch 699/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5807 - val_loss: 1.0515\n",
            "Epoch 700/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5808 - val_loss: 1.0530\n",
            "Epoch 701/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5808 - val_loss: 1.0499\n",
            "Epoch 702/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5809 - val_loss: 1.0511\n",
            "Epoch 703/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5803 - val_loss: 1.0525\n",
            "Epoch 704/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5811 - val_loss: 1.0521\n",
            "Epoch 705/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5808 - val_loss: 1.0526\n",
            "Epoch 706/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5799 - val_loss: 1.0510\n",
            "Epoch 707/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5800 - val_loss: 1.0543\n",
            "Epoch 708/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5802 - val_loss: 1.0502\n",
            "Epoch 709/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5797 - val_loss: 1.0514\n",
            "Epoch 710/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5804 - val_loss: 1.0547\n",
            "Epoch 711/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5797 - val_loss: 1.0502\n",
            "Epoch 712/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5793 - val_loss: 1.0510\n",
            "Epoch 713/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5791 - val_loss: 1.0525\n",
            "Epoch 714/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5794 - val_loss: 1.0517\n",
            "Epoch 715/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5795 - val_loss: 1.0547\n",
            "Epoch 716/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5793 - val_loss: 1.0514\n",
            "Epoch 717/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5788 - val_loss: 1.0516\n",
            "Epoch 718/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5792 - val_loss: 1.0527\n",
            "Epoch 719/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5790 - val_loss: 1.0498\n",
            "Epoch 720/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5791 - val_loss: 1.0534\n",
            "Epoch 721/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5784 - val_loss: 1.0521\n",
            "Epoch 722/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5792 - val_loss: 1.0500\n",
            "Epoch 723/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5790 - val_loss: 1.0521\n",
            "Epoch 724/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5782 - val_loss: 1.0529\n",
            "Epoch 725/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5784 - val_loss: 1.0545\n",
            "Epoch 726/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5785 - val_loss: 1.0497\n",
            "Epoch 727/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5780 - val_loss: 1.0527\n",
            "Epoch 728/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5785 - val_loss: 1.0525\n",
            "Epoch 729/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5778 - val_loss: 1.0537\n",
            "Epoch 730/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5782 - val_loss: 1.0537\n",
            "Epoch 731/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5775 - val_loss: 1.0528\n",
            "Epoch 732/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5778 - val_loss: 1.0518\n",
            "Epoch 733/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5773 - val_loss: 1.0513\n",
            "Epoch 734/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5773 - val_loss: 1.0533\n",
            "Epoch 735/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5774 - val_loss: 1.0522\n",
            "Epoch 736/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5770 - val_loss: 1.0517\n",
            "Epoch 737/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5776 - val_loss: 1.0538\n",
            "Epoch 738/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5768 - val_loss: 1.0516\n",
            "Epoch 739/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5771 - val_loss: 1.0504\n",
            "Epoch 740/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5765 - val_loss: 1.0520\n",
            "Epoch 741/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5771 - val_loss: 1.0528\n",
            "Epoch 742/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5771 - val_loss: 1.0502\n",
            "Epoch 743/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5764 - val_loss: 1.0537\n",
            "Epoch 744/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5765 - val_loss: 1.0512\n",
            "Epoch 745/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5770 - val_loss: 1.0532\n",
            "Epoch 746/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5761 - val_loss: 1.0521\n",
            "Epoch 747/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5761 - val_loss: 1.0531\n",
            "Epoch 748/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5759 - val_loss: 1.0517\n",
            "Epoch 749/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5759 - val_loss: 1.0509\n",
            "Epoch 750/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5757 - val_loss: 1.0520\n",
            "Epoch 751/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5758 - val_loss: 1.0540\n",
            "Epoch 752/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5756 - val_loss: 1.0527\n",
            "Epoch 753/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5754 - val_loss: 1.0505\n",
            "Epoch 754/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5755 - val_loss: 1.0523\n",
            "Epoch 755/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5754 - val_loss: 1.0516\n",
            "Epoch 756/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5751 - val_loss: 1.0525\n",
            "Epoch 757/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5754 - val_loss: 1.0530\n",
            "Epoch 758/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5749 - val_loss: 1.0508\n",
            "Epoch 759/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5748 - val_loss: 1.0530\n",
            "Epoch 760/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5750 - val_loss: 1.0514\n",
            "Epoch 761/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5748 - val_loss: 1.0518\n",
            "Epoch 762/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5746 - val_loss: 1.0529\n",
            "Epoch 763/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5744 - val_loss: 1.0515\n",
            "Epoch 764/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5746 - val_loss: 1.0531\n",
            "Epoch 765/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5745 - val_loss: 1.0510\n",
            "Epoch 766/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5740 - val_loss: 1.0526\n",
            "Epoch 767/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5742 - val_loss: 1.0515\n",
            "Epoch 768/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5739 - val_loss: 1.0518\n",
            "Epoch 769/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5744 - val_loss: 1.0535\n",
            "Epoch 770/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5746 - val_loss: 1.0543\n",
            "Epoch 771/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5741 - val_loss: 1.0514\n",
            "Epoch 772/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5741 - val_loss: 1.0486\n",
            "Epoch 773/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5737 - val_loss: 1.0520\n",
            "Epoch 774/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5735 - val_loss: 1.0529\n",
            "Epoch 775/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5733 - val_loss: 1.0517\n",
            "Epoch 776/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5734 - val_loss: 1.0530\n",
            "Epoch 777/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5736 - val_loss: 1.0533\n",
            "Epoch 778/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5731 - val_loss: 1.0540\n",
            "Epoch 779/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5741 - val_loss: 1.0477\n",
            "Epoch 780/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5725 - val_loss: 1.0502\n",
            "Epoch 781/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5724 - val_loss: 1.0521\n",
            "Epoch 782/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5725 - val_loss: 1.0523\n",
            "Epoch 783/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5730 - val_loss: 1.0493\n",
            "Epoch 784/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5723 - val_loss: 1.0536\n",
            "Epoch 785/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5728 - val_loss: 1.0497\n",
            "Epoch 786/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5721 - val_loss: 1.0526\n",
            "Epoch 787/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5725 - val_loss: 1.0507\n",
            "Epoch 788/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5722 - val_loss: 1.0549\n",
            "Epoch 789/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5726 - val_loss: 1.0527\n",
            "Epoch 790/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5724 - val_loss: 1.0559\n",
            "Epoch 791/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5715 - val_loss: 1.0539\n",
            "Epoch 792/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5714 - val_loss: 1.0541\n",
            "Epoch 793/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5719 - val_loss: 1.0501\n",
            "Epoch 794/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5714 - val_loss: 1.0540\n",
            "Epoch 795/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5717 - val_loss: 1.0506\n",
            "Epoch 796/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5717 - val_loss: 1.0521\n",
            "Epoch 797/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5712 - val_loss: 1.0522\n",
            "Epoch 798/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5708 - val_loss: 1.0532\n",
            "Epoch 799/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5712 - val_loss: 1.0502\n",
            "Epoch 800/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5701 - val_loss: 1.0529\n",
            "Epoch 801/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5707 - val_loss: 1.0548\n",
            "Epoch 802/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5704 - val_loss: 1.0511\n",
            "Epoch 803/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5705 - val_loss: 1.0541\n",
            "Epoch 804/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5702 - val_loss: 1.0529\n",
            "Epoch 805/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5699 - val_loss: 1.0522\n",
            "Epoch 806/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5701 - val_loss: 1.0525\n",
            "Epoch 807/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5700 - val_loss: 1.0504\n",
            "Epoch 808/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5698 - val_loss: 1.0512\n",
            "Epoch 809/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5695 - val_loss: 1.0515\n",
            "Epoch 810/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5699 - val_loss: 1.0504\n",
            "Epoch 811/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5693 - val_loss: 1.0532\n",
            "Epoch 812/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5696 - val_loss: 1.0553\n",
            "Epoch 813/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5700 - val_loss: 1.0504\n",
            "Epoch 814/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5693 - val_loss: 1.0546\n",
            "Epoch 815/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5687 - val_loss: 1.0516\n",
            "Epoch 816/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5689 - val_loss: 1.0535\n",
            "Epoch 817/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5686 - val_loss: 1.0518\n",
            "Epoch 818/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5689 - val_loss: 1.0496\n",
            "Epoch 819/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5685 - val_loss: 1.0516\n",
            "Epoch 820/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5684 - val_loss: 1.0519\n",
            "Epoch 821/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5691 - val_loss: 1.0527\n",
            "Epoch 822/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5679 - val_loss: 1.0519\n",
            "Epoch 823/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5683 - val_loss: 1.0510\n",
            "Epoch 824/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5680 - val_loss: 1.0535\n",
            "Epoch 825/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5679 - val_loss: 1.0505\n",
            "Epoch 826/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5677 - val_loss: 1.0518\n",
            "Epoch 827/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5676 - val_loss: 1.0518\n",
            "Epoch 828/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5676 - val_loss: 1.0544\n",
            "Epoch 829/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5675 - val_loss: 1.0539\n",
            "Epoch 830/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5676 - val_loss: 1.0509\n",
            "Epoch 831/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5672 - val_loss: 1.0507\n",
            "Epoch 832/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5672 - val_loss: 1.0544\n",
            "Epoch 833/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5670 - val_loss: 1.0508\n",
            "Epoch 834/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5673 - val_loss: 1.0503\n",
            "Epoch 835/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5665 - val_loss: 1.0513\n",
            "Epoch 836/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5663 - val_loss: 1.0529\n",
            "Epoch 837/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5666 - val_loss: 1.0517\n",
            "Epoch 838/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5666 - val_loss: 1.0516\n",
            "Epoch 839/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5664 - val_loss: 1.0537\n",
            "Epoch 840/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5663 - val_loss: 1.0507\n",
            "Epoch 841/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5662 - val_loss: 1.0535\n",
            "Epoch 842/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5661 - val_loss: 1.0545\n",
            "Epoch 843/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5661 - val_loss: 1.0515\n",
            "Epoch 844/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5661 - val_loss: 1.0511\n",
            "Epoch 845/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5663 - val_loss: 1.0528\n",
            "Epoch 846/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5654 - val_loss: 1.0528\n",
            "Epoch 847/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5653 - val_loss: 1.0508\n",
            "Epoch 848/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5652 - val_loss: 1.0519\n",
            "Epoch 849/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5654 - val_loss: 1.0520\n",
            "Epoch 850/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5650 - val_loss: 1.0507\n",
            "Epoch 851/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5648 - val_loss: 1.0516\n",
            "Epoch 852/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5650 - val_loss: 1.0536\n",
            "Epoch 853/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5647 - val_loss: 1.0510\n",
            "Epoch 854/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5647 - val_loss: 1.0524\n",
            "Epoch 855/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5646 - val_loss: 1.0505\n",
            "Epoch 856/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5643 - val_loss: 1.0521\n",
            "Epoch 857/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5646 - val_loss: 1.0525\n",
            "Epoch 858/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5647 - val_loss: 1.0547\n",
            "Epoch 859/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5640 - val_loss: 1.0515\n",
            "Epoch 860/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5638 - val_loss: 1.0524\n",
            "Epoch 861/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5646 - val_loss: 1.0533\n",
            "Epoch 862/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5642 - val_loss: 1.0538\n",
            "Epoch 863/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5635 - val_loss: 1.0517\n",
            "Epoch 864/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5637 - val_loss: 1.0507\n",
            "Epoch 865/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5637 - val_loss: 1.0517\n",
            "Epoch 866/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5639 - val_loss: 1.0487\n",
            "Epoch 867/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5637 - val_loss: 1.0512\n",
            "Epoch 868/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5633 - val_loss: 1.0548\n",
            "Epoch 869/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5629 - val_loss: 1.0522\n",
            "Epoch 870/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5627 - val_loss: 1.0515\n",
            "Epoch 871/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5632 - val_loss: 1.0542\n",
            "Epoch 872/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5630 - val_loss: 1.0488\n",
            "Epoch 873/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5624 - val_loss: 1.0510\n",
            "Epoch 874/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5626 - val_loss: 1.0506\n",
            "Epoch 875/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5624 - val_loss: 1.0544\n",
            "Epoch 876/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5623 - val_loss: 1.0536\n",
            "Epoch 877/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5620 - val_loss: 1.0533\n",
            "Epoch 878/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5622 - val_loss: 1.0509\n",
            "Epoch 879/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5618 - val_loss: 1.0534\n",
            "Epoch 880/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5618 - val_loss: 1.0517\n",
            "Epoch 881/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5620 - val_loss: 1.0494\n",
            "Epoch 882/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5620 - val_loss: 1.0509\n",
            "Epoch 883/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5621 - val_loss: 1.0499\n",
            "Epoch 884/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5622 - val_loss: 1.0558\n",
            "Epoch 885/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5614 - val_loss: 1.0533\n",
            "Epoch 886/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5615 - val_loss: 1.0497\n",
            "Epoch 887/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5615 - val_loss: 1.0542\n",
            "Epoch 888/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5620 - val_loss: 1.0495\n",
            "Epoch 889/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5621 - val_loss: 1.0551\n",
            "Epoch 890/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5605 - val_loss: 1.0510\n",
            "Epoch 891/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5611 - val_loss: 1.0504\n",
            "Epoch 892/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5605 - val_loss: 1.0509\n",
            "Epoch 893/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5607 - val_loss: 1.0501\n",
            "Epoch 894/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5607 - val_loss: 1.0513\n",
            "Epoch 895/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5605 - val_loss: 1.0524\n",
            "Epoch 896/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5602 - val_loss: 1.0508\n",
            "Epoch 897/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5603 - val_loss: 1.0536\n",
            "Epoch 898/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5598 - val_loss: 1.0528\n",
            "Epoch 899/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5599 - val_loss: 1.0532\n",
            "Epoch 900/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5609 - val_loss: 1.0513\n",
            "Epoch 901/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5594 - val_loss: 1.0512\n",
            "Epoch 902/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5601 - val_loss: 1.0552\n",
            "Epoch 903/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5596 - val_loss: 1.0518\n",
            "Epoch 904/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5595 - val_loss: 1.0509\n",
            "Epoch 905/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5590 - val_loss: 1.0510\n",
            "Epoch 906/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5593 - val_loss: 1.0522\n",
            "Epoch 907/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5593 - val_loss: 1.0505\n",
            "Epoch 908/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5592 - val_loss: 1.0560\n",
            "Epoch 909/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5589 - val_loss: 1.0548\n",
            "Epoch 910/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5592 - val_loss: 1.0514\n",
            "Epoch 911/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5588 - val_loss: 1.0544\n",
            "Epoch 912/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5596 - val_loss: 1.0513\n",
            "Epoch 913/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5589 - val_loss: 1.0529\n",
            "Epoch 914/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5584 - val_loss: 1.0546\n",
            "Epoch 915/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5583 - val_loss: 1.0526\n",
            "Epoch 916/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5581 - val_loss: 1.0525\n",
            "Epoch 917/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5579 - val_loss: 1.0521\n",
            "Epoch 918/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5578 - val_loss: 1.0518\n",
            "Epoch 919/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5578 - val_loss: 1.0519\n",
            "Epoch 920/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5577 - val_loss: 1.0507\n",
            "Epoch 921/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5575 - val_loss: 1.0531\n",
            "Epoch 922/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5573 - val_loss: 1.0530\n",
            "Epoch 923/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5573 - val_loss: 1.0533\n",
            "Epoch 924/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5575 - val_loss: 1.0514\n",
            "Epoch 925/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5574 - val_loss: 1.0542\n",
            "Epoch 926/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5570 - val_loss: 1.0536\n",
            "Epoch 927/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5573 - val_loss: 1.0515\n",
            "Epoch 928/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5572 - val_loss: 1.0501\n",
            "Epoch 929/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5574 - val_loss: 1.0503\n",
            "Epoch 930/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5570 - val_loss: 1.0560\n",
            "Epoch 931/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5566 - val_loss: 1.0530\n",
            "Epoch 932/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5565 - val_loss: 1.0534\n",
            "Epoch 933/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5565 - val_loss: 1.0539\n",
            "Epoch 934/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5569 - val_loss: 1.0502\n",
            "Epoch 935/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5558 - val_loss: 1.0541\n",
            "Epoch 936/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5565 - val_loss: 1.0549\n",
            "Epoch 937/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5557 - val_loss: 1.0527\n",
            "Epoch 938/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5554 - val_loss: 1.0530\n",
            "Epoch 939/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5555 - val_loss: 1.0501\n",
            "Epoch 940/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5556 - val_loss: 1.0532\n",
            "Epoch 941/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5553 - val_loss: 1.0531\n",
            "Epoch 942/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5552 - val_loss: 1.0539\n",
            "Epoch 943/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5553 - val_loss: 1.0559\n",
            "Epoch 944/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5552 - val_loss: 1.0514\n",
            "Epoch 945/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5548 - val_loss: 1.0498\n",
            "Epoch 946/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5552 - val_loss: 1.0549\n",
            "Epoch 947/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5554 - val_loss: 1.0498\n",
            "Epoch 948/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5546 - val_loss: 1.0542\n",
            "Epoch 949/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5543 - val_loss: 1.0538\n",
            "Epoch 950/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5543 - val_loss: 1.0542\n",
            "Epoch 951/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5541 - val_loss: 1.0543\n",
            "Epoch 952/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5548 - val_loss: 1.0499\n",
            "Epoch 953/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5547 - val_loss: 1.0506\n",
            "Epoch 954/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5542 - val_loss: 1.0511\n",
            "Epoch 955/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5540 - val_loss: 1.0531\n",
            "Epoch 956/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5548 - val_loss: 1.0549\n",
            "Epoch 957/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5539 - val_loss: 1.0507\n",
            "Epoch 958/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5535 - val_loss: 1.0534\n",
            "Epoch 959/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5535 - val_loss: 1.0536\n",
            "Epoch 960/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5537 - val_loss: 1.0535\n",
            "Epoch 961/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5535 - val_loss: 1.0545\n",
            "Epoch 962/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5537 - val_loss: 1.0528\n",
            "Epoch 963/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5535 - val_loss: 1.0554\n",
            "Epoch 964/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5529 - val_loss: 1.0503\n",
            "Epoch 965/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5531 - val_loss: 1.0505\n",
            "Epoch 966/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5528 - val_loss: 1.0534\n",
            "Epoch 967/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5524 - val_loss: 1.0530\n",
            "Epoch 968/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5525 - val_loss: 1.0525\n",
            "Epoch 969/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5527 - val_loss: 1.0516\n",
            "Epoch 970/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5534 - val_loss: 1.0547\n",
            "Epoch 971/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5535 - val_loss: 1.0511\n",
            "Epoch 972/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5520 - val_loss: 1.0514\n",
            "Epoch 973/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5522 - val_loss: 1.0543\n",
            "Epoch 974/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5519 - val_loss: 1.0532\n",
            "Epoch 975/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5521 - val_loss: 1.0524\n",
            "Epoch 976/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5519 - val_loss: 1.0501\n",
            "Epoch 977/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5530 - val_loss: 1.0546\n",
            "Epoch 978/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5513 - val_loss: 1.0545\n",
            "Epoch 979/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5515 - val_loss: 1.0512\n",
            "Epoch 980/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5514 - val_loss: 1.0524\n",
            "Epoch 981/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5519 - val_loss: 1.0512\n",
            "Epoch 982/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5513 - val_loss: 1.0520\n",
            "Epoch 983/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5509 - val_loss: 1.0516\n",
            "Epoch 984/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5510 - val_loss: 1.0524\n",
            "Epoch 985/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5515 - val_loss: 1.0511\n",
            "Epoch 986/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5512 - val_loss: 1.0576\n",
            "Epoch 987/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5508 - val_loss: 1.0572\n",
            "Epoch 988/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5503 - val_loss: 1.0553\n",
            "Epoch 989/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5514 - val_loss: 1.0591\n",
            "Epoch 990/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5509 - val_loss: 1.0504\n",
            "Epoch 991/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5500 - val_loss: 1.0536\n",
            "Epoch 992/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5503 - val_loss: 1.0501\n",
            "Epoch 993/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5499 - val_loss: 1.0540\n",
            "Epoch 994/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5501 - val_loss: 1.0554\n",
            "Epoch 995/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5502 - val_loss: 1.0566\n",
            "Epoch 996/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5496 - val_loss: 1.0524\n",
            "Epoch 997/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5495 - val_loss: 1.0514\n",
            "Epoch 998/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5500 - val_loss: 1.0532\n",
            "Epoch 999/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5493 - val_loss: 1.0529\n",
            "Epoch 1000/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5493 - val_loss: 1.0508\n",
            "Epoch 1001/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5495 - val_loss: 1.0527\n",
            "Epoch 1002/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5489 - val_loss: 1.0534\n",
            "Epoch 1003/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5498 - val_loss: 1.0489\n",
            "Epoch 1004/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5493 - val_loss: 1.0507\n",
            "Epoch 1005/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5488 - val_loss: 1.0537\n",
            "Epoch 1006/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5486 - val_loss: 1.0555\n",
            "Epoch 1007/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5486 - val_loss: 1.0521\n",
            "Epoch 1008/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5482 - val_loss: 1.0531\n",
            "Epoch 1009/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5497 - val_loss: 1.0485\n",
            "Epoch 1010/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5495 - val_loss: 1.0509\n",
            "Epoch 1011/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5485 - val_loss: 1.0532\n",
            "Epoch 1012/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5484 - val_loss: 1.0558\n",
            "Epoch 1013/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5483 - val_loss: 1.0566\n",
            "Epoch 1014/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5480 - val_loss: 1.0541\n",
            "Epoch 1015/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5478 - val_loss: 1.0515\n",
            "Epoch 1016/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5476 - val_loss: 1.0543\n",
            "Epoch 1017/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5475 - val_loss: 1.0528\n",
            "Epoch 1018/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5478 - val_loss: 1.0534\n",
            "Epoch 1019/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5480 - val_loss: 1.0499\n",
            "Epoch 1020/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5472 - val_loss: 1.0538\n",
            "Epoch 1021/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5475 - val_loss: 1.0527\n",
            "Epoch 1022/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5471 - val_loss: 1.0531\n",
            "Epoch 1023/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5480 - val_loss: 1.0567\n",
            "Epoch 1024/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5474 - val_loss: 1.0524\n",
            "Epoch 1025/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5469 - val_loss: 1.0519\n",
            "Epoch 1026/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5467 - val_loss: 1.0544\n",
            "Epoch 1027/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5468 - val_loss: 1.0512\n",
            "Epoch 1028/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5465 - val_loss: 1.0518\n",
            "Epoch 1029/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5469 - val_loss: 1.0536\n",
            "Epoch 1030/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5468 - val_loss: 1.0502\n",
            "Epoch 1031/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5460 - val_loss: 1.0540\n",
            "Epoch 1032/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5463 - val_loss: 1.0539\n",
            "Epoch 1033/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5463 - val_loss: 1.0526\n",
            "Epoch 1034/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5468 - val_loss: 1.0577\n",
            "Epoch 1035/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5457 - val_loss: 1.0535\n",
            "Epoch 1036/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5463 - val_loss: 1.0535\n",
            "Epoch 1037/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5456 - val_loss: 1.0537\n",
            "Epoch 1038/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5464 - val_loss: 1.0519\n",
            "Epoch 1039/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5456 - val_loss: 1.0516\n",
            "Epoch 1040/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5454 - val_loss: 1.0531\n",
            "Epoch 1041/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5452 - val_loss: 1.0531\n",
            "Epoch 1042/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5459 - val_loss: 1.0539\n",
            "Epoch 1043/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5459 - val_loss: 1.0520\n",
            "Epoch 1044/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5452 - val_loss: 1.0552\n",
            "Epoch 1045/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5453 - val_loss: 1.0526\n",
            "Epoch 1046/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5451 - val_loss: 1.0557\n",
            "Epoch 1047/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5448 - val_loss: 1.0542\n",
            "Epoch 1048/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5449 - val_loss: 1.0497\n",
            "Epoch 1049/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5451 - val_loss: 1.0527\n",
            "Epoch 1050/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5448 - val_loss: 1.0541\n",
            "Epoch 1051/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5447 - val_loss: 1.0536\n",
            "Epoch 1052/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5445 - val_loss: 1.0516\n",
            "Epoch 1053/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5444 - val_loss: 1.0540\n",
            "Epoch 1054/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5455 - val_loss: 1.0564\n",
            "Epoch 1055/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5446 - val_loss: 1.0557\n",
            "Epoch 1056/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5443 - val_loss: 1.0535\n",
            "Epoch 1057/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5448 - val_loss: 1.0516\n",
            "Epoch 1058/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5436 - val_loss: 1.0528\n",
            "Epoch 1059/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5438 - val_loss: 1.0551\n",
            "Epoch 1060/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5441 - val_loss: 1.0496\n",
            "Epoch 1061/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5436 - val_loss: 1.0525\n",
            "Epoch 1062/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5435 - val_loss: 1.0521\n",
            "Epoch 1063/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5439 - val_loss: 1.0499\n",
            "Epoch 1064/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5438 - val_loss: 1.0567\n",
            "Epoch 1065/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5432 - val_loss: 1.0547\n",
            "Epoch 1066/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5432 - val_loss: 1.0538\n",
            "Epoch 1067/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5433 - val_loss: 1.0544\n",
            "Epoch 1068/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5431 - val_loss: 1.0525\n",
            "Epoch 1069/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5428 - val_loss: 1.0530\n",
            "Epoch 1070/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5425 - val_loss: 1.0546\n",
            "Epoch 1071/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5427 - val_loss: 1.0520\n",
            "Epoch 1072/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5430 - val_loss: 1.0549\n",
            "Epoch 1073/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5426 - val_loss: 1.0535\n",
            "Epoch 1074/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5426 - val_loss: 1.0533\n",
            "Epoch 1075/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5426 - val_loss: 1.0513\n",
            "Epoch 1076/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5421 - val_loss: 1.0530\n",
            "Epoch 1077/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5420 - val_loss: 1.0552\n",
            "Epoch 1078/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5426 - val_loss: 1.0507\n",
            "Epoch 1079/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5423 - val_loss: 1.0510\n",
            "Epoch 1080/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5428 - val_loss: 1.0539\n",
            "Epoch 1081/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5419 - val_loss: 1.0515\n",
            "Epoch 1082/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5425 - val_loss: 1.0510\n",
            "Epoch 1083/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5419 - val_loss: 1.0552\n",
            "Epoch 1084/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5417 - val_loss: 1.0537\n",
            "Epoch 1085/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5418 - val_loss: 1.0544\n",
            "Epoch 1086/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5413 - val_loss: 1.0523\n",
            "Epoch 1087/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5417 - val_loss: 1.0565\n",
            "Epoch 1088/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5422 - val_loss: 1.0524\n",
            "Epoch 1089/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5415 - val_loss: 1.0523\n",
            "Epoch 1090/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5417 - val_loss: 1.0523\n",
            "Epoch 1091/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5419 - val_loss: 1.0491\n",
            "Epoch 1092/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5413 - val_loss: 1.0558\n",
            "Epoch 1093/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5406 - val_loss: 1.0550\n",
            "Epoch 1094/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5406 - val_loss: 1.0533\n",
            "Epoch 1095/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5408 - val_loss: 1.0544\n",
            "Epoch 1096/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5412 - val_loss: 1.0502\n",
            "Epoch 1097/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5408 - val_loss: 1.0529\n",
            "Epoch 1098/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5403 - val_loss: 1.0540\n",
            "Epoch 1099/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5405 - val_loss: 1.0502\n",
            "Epoch 1100/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5402 - val_loss: 1.0554\n",
            "Epoch 1101/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5403 - val_loss: 1.0515\n",
            "Epoch 1102/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5414 - val_loss: 1.0525\n",
            "Epoch 1103/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5400 - val_loss: 1.0499\n",
            "Epoch 1104/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5399 - val_loss: 1.0529\n",
            "Epoch 1105/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5401 - val_loss: 1.0504\n",
            "Epoch 1106/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5397 - val_loss: 1.0533\n",
            "Epoch 1107/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5406 - val_loss: 1.0506\n",
            "Epoch 1108/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5407 - val_loss: 1.0592\n",
            "Epoch 1109/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5393 - val_loss: 1.0514\n",
            "Epoch 1110/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5398 - val_loss: 1.0500\n",
            "Epoch 1111/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5396 - val_loss: 1.0535\n",
            "Epoch 1112/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5391 - val_loss: 1.0541\n",
            "Epoch 1113/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5391 - val_loss: 1.0512\n",
            "Epoch 1114/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5396 - val_loss: 1.0519\n",
            "Epoch 1115/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5389 - val_loss: 1.0517\n",
            "Epoch 1116/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5400 - val_loss: 1.0509\n",
            "Epoch 1117/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5391 - val_loss: 1.0500\n",
            "Epoch 1118/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5389 - val_loss: 1.0526\n",
            "Epoch 1119/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5394 - val_loss: 1.0563\n",
            "Epoch 1120/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5396 - val_loss: 1.0579\n",
            "Epoch 1121/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5391 - val_loss: 1.0538\n",
            "Epoch 1122/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5383 - val_loss: 1.0516\n",
            "Epoch 1123/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5385 - val_loss: 1.0528\n",
            "Epoch 1124/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5386 - val_loss: 1.0490\n",
            "Epoch 1125/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5385 - val_loss: 1.0549\n",
            "Epoch 1126/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5379 - val_loss: 1.0520\n",
            "Epoch 1127/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5386 - val_loss: 1.0501\n",
            "Epoch 1128/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5381 - val_loss: 1.0525\n",
            "Epoch 1129/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5378 - val_loss: 1.0551\n",
            "Epoch 1130/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5381 - val_loss: 1.0511\n",
            "Epoch 1131/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5377 - val_loss: 1.0529\n",
            "Epoch 1132/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5379 - val_loss: 1.0555\n",
            "Epoch 1133/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5374 - val_loss: 1.0532\n",
            "Epoch 1134/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5380 - val_loss: 1.0512\n",
            "Epoch 1135/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5380 - val_loss: 1.0554\n",
            "Epoch 1136/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5372 - val_loss: 1.0536\n",
            "Epoch 1137/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5373 - val_loss: 1.0521\n",
            "Epoch 1138/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5372 - val_loss: 1.0514\n",
            "Epoch 1139/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5371 - val_loss: 1.0511\n",
            "Epoch 1140/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5371 - val_loss: 1.0518\n",
            "Epoch 1141/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5374 - val_loss: 1.0530\n",
            "Epoch 1142/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5373 - val_loss: 1.0507\n",
            "Epoch 1143/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5372 - val_loss: 1.0568\n",
            "Epoch 1144/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5375 - val_loss: 1.0497\n",
            "Epoch 1145/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5368 - val_loss: 1.0509\n",
            "Epoch 1146/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5369 - val_loss: 1.0537\n",
            "Epoch 1147/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5367 - val_loss: 1.0515\n",
            "Epoch 1148/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5368 - val_loss: 1.0518\n",
            "Epoch 1149/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5369 - val_loss: 1.0515\n",
            "Epoch 1150/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5365 - val_loss: 1.0511\n",
            "Epoch 1151/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5369 - val_loss: 1.0576\n",
            "Epoch 1152/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5361 - val_loss: 1.0563\n",
            "Epoch 1153/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5362 - val_loss: 1.0523\n",
            "Epoch 1154/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5363 - val_loss: 1.0499\n",
            "Epoch 1155/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5366 - val_loss: 1.0538\n",
            "Epoch 1156/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5365 - val_loss: 1.0517\n",
            "Epoch 1157/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5361 - val_loss: 1.0532\n",
            "Epoch 1158/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5360 - val_loss: 1.0503\n",
            "Epoch 1159/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5367 - val_loss: 1.0488\n",
            "Epoch 1160/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5359 - val_loss: 1.0519\n",
            "Epoch 1161/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5360 - val_loss: 1.0549\n",
            "Epoch 1162/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5359 - val_loss: 1.0551\n",
            "Epoch 1163/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5363 - val_loss: 1.0510\n",
            "Epoch 1164/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5359 - val_loss: 1.0525\n",
            "Epoch 1165/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5355 - val_loss: 1.0554\n",
            "Epoch 1166/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5360 - val_loss: 1.0509\n",
            "Epoch 1167/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5355 - val_loss: 1.0499\n",
            "Epoch 1168/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5355 - val_loss: 1.0542\n",
            "Epoch 1169/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5359 - val_loss: 1.0525\n",
            "Epoch 1170/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5369 - val_loss: 1.0568\n",
            "Epoch 1171/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5360 - val_loss: 1.0510\n",
            "Epoch 1172/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5356 - val_loss: 1.0517\n",
            "Epoch 1173/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5359 - val_loss: 1.0518\n",
            "Epoch 1174/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5349 - val_loss: 1.0524\n",
            "Epoch 1175/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5355 - val_loss: 1.0506\n",
            "Epoch 1176/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5347 - val_loss: 1.0550\n",
            "Epoch 1177/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5360 - val_loss: 1.0483\n",
            "Epoch 1178/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5346 - val_loss: 1.0516\n",
            "Epoch 1179/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5345 - val_loss: 1.0522\n",
            "Epoch 1180/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5346 - val_loss: 1.0536\n",
            "Epoch 1181/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5344 - val_loss: 1.0504\n",
            "Epoch 1182/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5344 - val_loss: 1.0517\n",
            "Epoch 1183/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5347 - val_loss: 1.0515\n",
            "Epoch 1184/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5346 - val_loss: 1.0547\n",
            "Epoch 1185/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5344 - val_loss: 1.0548\n",
            "Epoch 1186/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5342 - val_loss: 1.0554\n",
            "Epoch 1187/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5342 - val_loss: 1.0527\n",
            "Epoch 1188/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5342 - val_loss: 1.0521\n",
            "Epoch 1189/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5352 - val_loss: 1.0482\n",
            "Epoch 1190/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5360 - val_loss: 1.0540\n",
            "Epoch 1191/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5346 - val_loss: 1.0497\n",
            "Epoch 1192/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5342 - val_loss: 1.0537\n",
            "Epoch 1193/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5352 - val_loss: 1.0468\n",
            "Epoch 1194/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5330 - val_loss: 1.0509\n",
            "Epoch 1195/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5336 - val_loss: 1.0523\n",
            "Epoch 1196/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5336 - val_loss: 1.0547\n",
            "Epoch 1197/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5334 - val_loss: 1.0523\n",
            "Epoch 1198/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5335 - val_loss: 1.0554\n",
            "Epoch 1199/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5334 - val_loss: 1.0513\n",
            "Epoch 1200/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5334 - val_loss: 1.0540\n",
            "Epoch 1201/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5336 - val_loss: 1.0554\n",
            "Epoch 1202/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5329 - val_loss: 1.0498\n",
            "Epoch 1203/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5332 - val_loss: 1.0495\n",
            "Epoch 1204/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5333 - val_loss: 1.0528\n",
            "Epoch 1205/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5334 - val_loss: 1.0513\n",
            "Epoch 1206/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5337 - val_loss: 1.0485\n",
            "Epoch 1207/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5331 - val_loss: 1.0493\n",
            "Epoch 1208/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5328 - val_loss: 1.0515\n",
            "Epoch 1209/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5334 - val_loss: 1.0524\n",
            "Epoch 1210/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5329 - val_loss: 1.0546\n",
            "Epoch 1211/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5329 - val_loss: 1.0513\n",
            "Epoch 1212/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5326 - val_loss: 1.0513\n",
            "Epoch 1213/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5331 - val_loss: 1.0540\n",
            "Epoch 1214/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5328 - val_loss: 1.0512\n",
            "Epoch 1215/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5326 - val_loss: 1.0503\n",
            "Epoch 1216/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5327 - val_loss: 1.0533\n",
            "Epoch 1217/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5326 - val_loss: 1.0509\n",
            "Epoch 1218/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5326 - val_loss: 1.0533\n",
            "Epoch 1219/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5322 - val_loss: 1.0502\n",
            "Epoch 1220/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5323 - val_loss: 1.0509\n",
            "Epoch 1221/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5325 - val_loss: 1.0495\n",
            "Epoch 1222/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5326 - val_loss: 1.0524\n",
            "Epoch 1223/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5324 - val_loss: 1.0520\n",
            "Epoch 1224/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5321 - val_loss: 1.0548\n",
            "Epoch 1225/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5325 - val_loss: 1.0478\n",
            "Epoch 1226/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5324 - val_loss: 1.0526\n",
            "Epoch 1227/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5321 - val_loss: 1.0541\n",
            "Epoch 1228/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5319 - val_loss: 1.0531\n",
            "Epoch 1229/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5321 - val_loss: 1.0485\n",
            "Epoch 1230/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5316 - val_loss: 1.0496\n",
            "Epoch 1231/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5315 - val_loss: 1.0519\n",
            "Epoch 1232/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5323 - val_loss: 1.0488\n",
            "Epoch 1233/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5318 - val_loss: 1.0534\n",
            "Epoch 1234/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5319 - val_loss: 1.0502\n",
            "Epoch 1235/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5324 - val_loss: 1.0473\n",
            "Epoch 1236/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5333 - val_loss: 1.0524\n",
            "Epoch 1237/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5322 - val_loss: 1.0489\n",
            "Epoch 1238/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5312 - val_loss: 1.0508\n",
            "Epoch 1239/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5317 - val_loss: 1.0504\n",
            "Epoch 1240/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5318 - val_loss: 1.0557\n",
            "Epoch 1241/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5317 - val_loss: 1.0495\n",
            "Epoch 1242/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5321 - val_loss: 1.0545\n",
            "Epoch 1243/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5319 - val_loss: 1.0483\n",
            "Epoch 1244/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5322 - val_loss: 1.0578\n",
            "Epoch 1245/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5310 - val_loss: 1.0501\n",
            "Epoch 1246/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5311 - val_loss: 1.0521\n",
            "Epoch 1247/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5314 - val_loss: 1.0535\n",
            "Epoch 1248/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5312 - val_loss: 1.0490\n",
            "Epoch 1249/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5315 - val_loss: 1.0521\n",
            "Epoch 1250/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5309 - val_loss: 1.0495\n",
            "Epoch 1251/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5312 - val_loss: 1.0525\n",
            "Epoch 1252/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5308 - val_loss: 1.0498\n",
            "Epoch 1253/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5311 - val_loss: 1.0533\n",
            "Epoch 1254/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5304 - val_loss: 1.0514\n",
            "Epoch 1255/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5307 - val_loss: 1.0488\n",
            "Epoch 1256/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5309 - val_loss: 1.0534\n",
            "Epoch 1257/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5314 - val_loss: 1.0545\n",
            "Epoch 1258/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5310 - val_loss: 1.0473\n",
            "Epoch 1259/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5313 - val_loss: 1.0458\n",
            "Epoch 1260/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5306 - val_loss: 1.0543\n",
            "Epoch 1261/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5314 - val_loss: 1.0500\n",
            "Epoch 1262/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5313 - val_loss: 1.0563\n",
            "Epoch 1263/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5307 - val_loss: 1.0494\n",
            "Epoch 1264/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5305 - val_loss: 1.0508\n",
            "Epoch 1265/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5303 - val_loss: 1.0503\n",
            "Epoch 1266/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5306 - val_loss: 1.0525\n",
            "Epoch 1267/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5313 - val_loss: 1.0475\n",
            "Epoch 1268/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5309 - val_loss: 1.0551\n",
            "Epoch 1269/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5305 - val_loss: 1.0523\n",
            "Epoch 1270/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5312 - val_loss: 1.0562\n",
            "Epoch 1271/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5308 - val_loss: 1.0545\n",
            "Epoch 1272/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5298 - val_loss: 1.0484\n",
            "Epoch 1273/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5303 - val_loss: 1.0476\n",
            "Epoch 1274/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5300 - val_loss: 1.0494\n",
            "Epoch 1275/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5317 - val_loss: 1.0461\n",
            "Epoch 1276/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5296 - val_loss: 1.0532\n",
            "Epoch 1277/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5298 - val_loss: 1.0541\n",
            "Epoch 1278/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5304 - val_loss: 1.0488\n",
            "Epoch 1279/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5300 - val_loss: 1.0497\n",
            "Epoch 1280/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5298 - val_loss: 1.0492\n",
            "Epoch 1281/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5302 - val_loss: 1.0576\n",
            "Epoch 1282/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5297 - val_loss: 1.0540\n",
            "Epoch 1283/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5295 - val_loss: 1.0527\n",
            "Epoch 1284/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5293 - val_loss: 1.0515\n",
            "Epoch 1285/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5293 - val_loss: 1.0495\n",
            "Epoch 1286/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5306 - val_loss: 1.0466\n",
            "Epoch 1287/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5295 - val_loss: 1.0488\n",
            "Epoch 1288/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5298 - val_loss: 1.0560\n",
            "Epoch 1289/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5301 - val_loss: 1.0505\n",
            "Epoch 1290/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5293 - val_loss: 1.0508\n",
            "Epoch 1291/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5292 - val_loss: 1.0536\n",
            "Epoch 1292/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5292 - val_loss: 1.0501\n",
            "Epoch 1293/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5294 - val_loss: 1.0494\n",
            "Epoch 1294/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5292 - val_loss: 1.0525\n",
            "Epoch 1295/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5294 - val_loss: 1.0539\n",
            "Epoch 1296/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5293 - val_loss: 1.0494\n",
            "Epoch 1297/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5295 - val_loss: 1.0505\n",
            "Epoch 1298/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5290 - val_loss: 1.0483\n",
            "Epoch 1299/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5289 - val_loss: 1.0482\n",
            "Epoch 1300/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5293 - val_loss: 1.0524\n",
            "Epoch 1301/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5287 - val_loss: 1.0510\n",
            "Epoch 1302/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5292 - val_loss: 1.0466\n",
            "Epoch 1303/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5289 - val_loss: 1.0470\n",
            "Epoch 1304/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5290 - val_loss: 1.0514\n",
            "Epoch 1305/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5292 - val_loss: 1.0544\n",
            "Epoch 1306/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5289 - val_loss: 1.0505\n",
            "Epoch 1307/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5287 - val_loss: 1.0498\n",
            "Epoch 1308/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5289 - val_loss: 1.0485\n",
            "Epoch 1309/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5292 - val_loss: 1.0462\n",
            "Epoch 1310/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5292 - val_loss: 1.0472\n",
            "Epoch 1311/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5291 - val_loss: 1.0508\n",
            "Epoch 1312/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5297 - val_loss: 1.0551\n",
            "Epoch 1313/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5290 - val_loss: 1.0532\n",
            "Epoch 1314/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5288 - val_loss: 1.0501\n",
            "Epoch 1315/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5292 - val_loss: 1.0517\n",
            "Epoch 1316/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5289 - val_loss: 1.0468\n",
            "Epoch 1317/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5291 - val_loss: 1.0482\n",
            "Epoch 1318/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5285 - val_loss: 1.0517\n",
            "Epoch 1319/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5287 - val_loss: 1.0486\n",
            "Epoch 1320/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5288 - val_loss: 1.0466\n",
            "Epoch 1321/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5282 - val_loss: 1.0517\n",
            "Epoch 1322/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5284 - val_loss: 1.0498\n",
            "Epoch 1323/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5285 - val_loss: 1.0491\n",
            "Epoch 1324/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5284 - val_loss: 1.0470\n",
            "Epoch 1325/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5284 - val_loss: 1.0485\n",
            "Epoch 1326/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5283 - val_loss: 1.0497\n",
            "Epoch 1327/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5288 - val_loss: 1.0527\n",
            "Epoch 1328/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5308 - val_loss: 1.0497\n",
            "Epoch 1329/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5293 - val_loss: 1.0557\n",
            "Epoch 1330/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5286 - val_loss: 1.0496\n",
            "Epoch 1331/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5279 - val_loss: 1.0479\n",
            "Epoch 1332/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5281 - val_loss: 1.0481\n",
            "Epoch 1333/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5283 - val_loss: 1.0501\n",
            "Epoch 1334/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5288 - val_loss: 1.0470\n",
            "Epoch 1335/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5288 - val_loss: 1.0512\n",
            "Epoch 1336/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5280 - val_loss: 1.0480\n",
            "Epoch 1337/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 1.0530\n",
            "Epoch 1338/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5284 - val_loss: 1.0476\n",
            "Epoch 1339/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 1.0495\n",
            "Epoch 1340/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 1.0485\n",
            "Epoch 1341/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5280 - val_loss: 1.0489\n",
            "Epoch 1342/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5284 - val_loss: 1.0455\n",
            "Epoch 1343/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 1.0517\n",
            "Epoch 1344/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5277 - val_loss: 1.0501\n",
            "Epoch 1345/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5279 - val_loss: 1.0492\n",
            "Epoch 1346/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5275 - val_loss: 1.0491\n",
            "Epoch 1347/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5279 - val_loss: 1.0476\n",
            "Epoch 1348/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5286 - val_loss: 1.0462\n",
            "Epoch 1349/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5285 - val_loss: 1.0510\n",
            "Epoch 1350/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5274 - val_loss: 1.0500\n",
            "Epoch 1351/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5278 - val_loss: 1.0479\n",
            "Epoch 1352/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5278 - val_loss: 1.0480\n",
            "Epoch 1353/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5278 - val_loss: 1.0494\n",
            "Epoch 1354/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5276 - val_loss: 1.0505\n",
            "Epoch 1355/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5280 - val_loss: 1.0489\n",
            "Epoch 1356/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5282 - val_loss: 1.0494\n",
            "Epoch 1357/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5280 - val_loss: 1.0492\n",
            "Epoch 1358/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5280 - val_loss: 1.0459\n",
            "Epoch 1359/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5280 - val_loss: 1.0514\n",
            "Epoch 1360/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5275 - val_loss: 1.0505\n",
            "Epoch 1361/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5272 - val_loss: 1.0482\n",
            "Epoch 1362/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5276 - val_loss: 1.0496\n",
            "Epoch 1363/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5275 - val_loss: 1.0496\n",
            "Epoch 1364/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5273 - val_loss: 1.0477\n",
            "Epoch 1365/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5279 - val_loss: 1.0467\n",
            "Epoch 1366/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5272 - val_loss: 1.0461\n",
            "Epoch 1367/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5273 - val_loss: 1.0474\n",
            "Epoch 1368/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5275 - val_loss: 1.0471\n",
            "Epoch 1369/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5273 - val_loss: 1.0478\n",
            "Epoch 1370/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 1.0467\n",
            "Epoch 1371/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5277 - val_loss: 1.0460\n",
            "Epoch 1372/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5269 - val_loss: 1.0499\n",
            "Epoch 1373/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5269 - val_loss: 1.0509\n",
            "Epoch 1374/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5278 - val_loss: 1.0514\n",
            "Epoch 1375/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5272 - val_loss: 1.0459\n",
            "Epoch 1376/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5272 - val_loss: 1.0492\n",
            "Epoch 1377/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5279 - val_loss: 1.0461\n",
            "Epoch 1378/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5277 - val_loss: 1.0458\n",
            "Epoch 1379/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5272 - val_loss: 1.0488\n",
            "Epoch 1380/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5272 - val_loss: 1.0451\n",
            "Epoch 1381/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5276 - val_loss: 1.0451\n",
            "Epoch 1382/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5268 - val_loss: 1.0497\n",
            "Epoch 1383/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 1.0474\n",
            "Epoch 1384/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5267 - val_loss: 1.0501\n",
            "Epoch 1385/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5268 - val_loss: 1.0481\n",
            "Epoch 1386/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 1.0497\n",
            "Epoch 1387/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5280 - val_loss: 1.0468\n",
            "Epoch 1388/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5275 - val_loss: 1.0471\n",
            "Epoch 1389/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5273 - val_loss: 1.0512\n",
            "Epoch 1390/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5273 - val_loss: 1.0450\n",
            "Epoch 1391/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5274 - val_loss: 1.0456\n",
            "Epoch 1392/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5267 - val_loss: 1.0472\n",
            "Epoch 1393/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5273 - val_loss: 1.0521\n",
            "Epoch 1394/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5272 - val_loss: 1.0467\n",
            "Epoch 1395/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 1.0513\n",
            "Epoch 1396/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5266 - val_loss: 1.0474\n",
            "Epoch 1397/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5267 - val_loss: 1.0497\n",
            "Epoch 1398/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5277 - val_loss: 1.0486\n",
            "Epoch 1399/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 1.0453\n",
            "Epoch 1400/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5274 - val_loss: 1.0530\n",
            "Epoch 1401/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5270 - val_loss: 1.0489\n",
            "Epoch 1402/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5276 - val_loss: 1.0456\n",
            "Epoch 1403/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5267 - val_loss: 1.0490\n",
            "Epoch 1404/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5279 - val_loss: 1.0529\n",
            "Epoch 1405/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5278 - val_loss: 1.0454\n",
            "Epoch 1406/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 1.0473\n",
            "Epoch 1407/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5267 - val_loss: 1.0447\n",
            "Epoch 1408/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5266 - val_loss: 1.0456\n",
            "Epoch 1409/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5278 - val_loss: 1.0479\n",
            "Epoch 1410/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5268 - val_loss: 1.0463\n",
            "Epoch 1411/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5268 - val_loss: 1.0491\n",
            "Epoch 1412/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5266 - val_loss: 1.0489\n",
            "Epoch 1413/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 1.0493\n",
            "Epoch 1414/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5266 - val_loss: 1.0476\n",
            "Epoch 1415/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5273 - val_loss: 1.0452\n",
            "Epoch 1416/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5273 - val_loss: 1.0482\n",
            "Epoch 1417/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 1.0471\n",
            "Epoch 1418/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5268 - val_loss: 1.0512\n",
            "Epoch 1419/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5262 - val_loss: 1.0483\n",
            "Epoch 1420/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5264 - val_loss: 1.0468\n",
            "Epoch 1421/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 1.0444\n",
            "Epoch 1422/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5275 - val_loss: 1.0409\n",
            "Epoch 1423/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5265 - val_loss: 1.0495\n",
            "Epoch 1424/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5268 - val_loss: 1.0464\n",
            "Epoch 1425/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5265 - val_loss: 1.0469\n",
            "Epoch 1426/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5270 - val_loss: 1.0458\n",
            "Epoch 1427/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5276 - val_loss: 1.0508\n",
            "Epoch 1428/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5277 - val_loss: 1.0440\n",
            "Epoch 1429/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 1.0451\n",
            "Epoch 1430/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 1.0474\n",
            "Epoch 1431/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5272 - val_loss: 1.0444\n",
            "Epoch 1432/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 1.0477\n",
            "Epoch 1433/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5276 - val_loss: 1.0549\n",
            "Epoch 1434/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 1.0513\n",
            "Epoch 1435/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5272 - val_loss: 1.0526\n",
            "Epoch 1436/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5292 - val_loss: 1.0400\n",
            "Epoch 1437/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5261 - val_loss: 1.0489\n",
            "Epoch 1438/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5272 - val_loss: 1.0472\n",
            "Epoch 1439/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 1.0471\n",
            "Epoch 1440/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5269 - val_loss: 1.0514\n",
            "Epoch 1441/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 1.0469\n",
            "Epoch 1442/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5267 - val_loss: 1.0457\n",
            "Epoch 1443/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5282 - val_loss: 1.0417\n",
            "Epoch 1444/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5282 - val_loss: 1.0512\n",
            "Epoch 1445/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5265 - val_loss: 1.0431\n",
            "Epoch 1446/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 1.0468\n",
            "Epoch 1447/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 1.0436\n",
            "Epoch 1448/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5266 - val_loss: 1.0498\n",
            "Epoch 1449/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 1.0455\n",
            "Epoch 1450/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5266 - val_loss: 1.0445\n",
            "Epoch 1451/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5279 - val_loss: 1.0412\n",
            "Epoch 1452/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 1.0497\n",
            "Epoch 1453/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5261 - val_loss: 1.0477\n",
            "Epoch 1454/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 1.0486\n",
            "Epoch 1455/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 1.0468\n",
            "Epoch 1456/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5265 - val_loss: 1.0503\n",
            "Epoch 1457/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5273 - val_loss: 1.0446\n",
            "Epoch 1458/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 1.0440\n",
            "Epoch 1459/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 1.0437\n",
            "Epoch 1460/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5273 - val_loss: 1.0532\n",
            "Epoch 1461/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 1.0448\n",
            "Epoch 1462/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5267 - val_loss: 1.0488\n",
            "Epoch 1463/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 1.0461\n",
            "Epoch 1464/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 1.0460\n",
            "Epoch 1465/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5266 - val_loss: 1.0448\n",
            "Epoch 1466/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5260 - val_loss: 1.0458\n",
            "Epoch 1467/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5268 - val_loss: 1.0515\n",
            "Epoch 1468/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5259 - val_loss: 1.0462\n",
            "Epoch 1469/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0441\n",
            "Epoch 1470/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0432\n",
            "Epoch 1471/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5265 - val_loss: 1.0444\n",
            "Epoch 1472/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5261 - val_loss: 1.0464\n",
            "Epoch 1473/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 1.0449\n",
            "Epoch 1474/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5265 - val_loss: 1.0508\n",
            "Epoch 1475/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5270 - val_loss: 1.0424\n",
            "Epoch 1476/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 1.0444\n",
            "Epoch 1477/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5265 - val_loss: 1.0462\n",
            "Epoch 1478/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5261 - val_loss: 1.0425\n",
            "Epoch 1479/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5268 - val_loss: 1.0438\n",
            "Epoch 1480/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0471\n",
            "Epoch 1481/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0455\n",
            "Epoch 1482/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0467\n",
            "Epoch 1483/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5261 - val_loss: 1.0505\n",
            "Epoch 1484/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 1.0499\n",
            "Epoch 1485/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0476\n",
            "Epoch 1486/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5265 - val_loss: 1.0418\n",
            "Epoch 1487/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 1.0461\n",
            "Epoch 1488/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 1.0451\n",
            "Epoch 1489/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5261 - val_loss: 1.0474\n",
            "Epoch 1490/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 1.0457\n",
            "Epoch 1491/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5269 - val_loss: 1.0483\n",
            "Epoch 1492/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0465\n",
            "Epoch 1493/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 1.0440\n",
            "Epoch 1494/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0463\n",
            "Epoch 1495/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 1.0447\n",
            "Epoch 1496/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5269 - val_loss: 1.0478\n",
            "Epoch 1497/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0431\n",
            "Epoch 1498/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0429\n",
            "Epoch 1499/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 1.0428\n",
            "Epoch 1500/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5274 - val_loss: 1.0388\n",
            "Epoch 1501/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0439\n",
            "Epoch 1502/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 1.0473\n",
            "Epoch 1503/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 1.0463\n",
            "Epoch 1504/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0452\n",
            "Epoch 1505/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0462\n",
            "Epoch 1506/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 1.0443\n",
            "Epoch 1507/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 1.0439\n",
            "Epoch 1508/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 1.0476\n",
            "Epoch 1509/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5256 - val_loss: 1.0468\n",
            "Epoch 1510/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 1.0442\n",
            "Epoch 1511/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 1.0437\n",
            "Epoch 1512/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5256 - val_loss: 1.0433\n",
            "Epoch 1513/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 1.0465\n",
            "Epoch 1514/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5257 - val_loss: 1.0435\n",
            "Epoch 1515/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 1.0460\n",
            "Epoch 1516/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0456\n",
            "Epoch 1517/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0463\n",
            "Epoch 1518/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5273 - val_loss: 1.0422\n",
            "Epoch 1519/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5267 - val_loss: 1.0461\n",
            "Epoch 1520/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0444\n",
            "Epoch 1521/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 1.0456\n",
            "Epoch 1522/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5261 - val_loss: 1.0477\n",
            "Epoch 1523/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5259 - val_loss: 1.0424\n",
            "Epoch 1524/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0430\n",
            "Epoch 1525/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0436\n",
            "Epoch 1526/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0432\n",
            "Epoch 1527/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 1.0457\n",
            "Epoch 1528/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0423\n",
            "Epoch 1529/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0433\n",
            "Epoch 1530/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 1.0407\n",
            "Epoch 1531/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0429\n",
            "Epoch 1532/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5267 - val_loss: 1.0453\n",
            "Epoch 1533/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0451\n",
            "Epoch 1534/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0476\n",
            "Epoch 1535/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 1.0445\n",
            "Epoch 1536/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0427\n",
            "Epoch 1537/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0439\n",
            "Epoch 1538/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 1.0442\n",
            "Epoch 1539/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 1.0472\n",
            "Epoch 1540/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0442\n",
            "Epoch 1541/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5261 - val_loss: 1.0480\n",
            "Epoch 1542/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0437\n",
            "Epoch 1543/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0453\n",
            "Epoch 1544/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 1.0394\n",
            "Epoch 1545/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 1.0418\n",
            "Epoch 1546/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0454\n",
            "Epoch 1547/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 1.0430\n",
            "Epoch 1548/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0464\n",
            "Epoch 1549/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0455\n",
            "Epoch 1550/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0458\n",
            "Epoch 1551/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0459\n",
            "Epoch 1552/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5261 - val_loss: 1.0471\n",
            "Epoch 1553/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0445\n",
            "Epoch 1554/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0446\n",
            "Epoch 1555/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0415\n",
            "Epoch 1556/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0402\n",
            "Epoch 1557/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 1.0447\n",
            "Epoch 1558/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0451\n",
            "Epoch 1559/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 1.0491\n",
            "Epoch 1560/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0444\n",
            "Epoch 1561/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0441\n",
            "Epoch 1562/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0465\n",
            "Epoch 1563/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5276 - val_loss: 1.0417\n",
            "Epoch 1564/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0472\n",
            "Epoch 1565/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0473\n",
            "Epoch 1566/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0452\n",
            "Epoch 1567/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 1.0437\n",
            "Epoch 1568/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 1.0422\n",
            "Epoch 1569/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5255 - val_loss: 1.0461\n",
            "Epoch 1570/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0432\n",
            "Epoch 1571/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0394\n",
            "Epoch 1572/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0430\n",
            "Epoch 1573/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0419\n",
            "Epoch 1574/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 1.0468\n",
            "Epoch 1575/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0416\n",
            "Epoch 1576/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0425\n",
            "Epoch 1577/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0469\n",
            "Epoch 1578/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0442\n",
            "Epoch 1579/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0460\n",
            "Epoch 1580/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 1.0417\n",
            "Epoch 1581/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0446\n",
            "Epoch 1582/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0409\n",
            "Epoch 1583/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0462\n",
            "Epoch 1584/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0424\n",
            "Epoch 1585/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0437\n",
            "Epoch 1586/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 1.0455\n",
            "Epoch 1587/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0463\n",
            "Epoch 1588/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0455\n",
            "Epoch 1589/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0430\n",
            "Epoch 1590/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0487\n",
            "Epoch 1591/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0467\n",
            "Epoch 1592/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5265 - val_loss: 1.0407\n",
            "Epoch 1593/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0425\n",
            "Epoch 1594/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0493\n",
            "Epoch 1595/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0469\n",
            "Epoch 1596/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0470\n",
            "Epoch 1597/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0433\n",
            "Epoch 1598/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0436\n",
            "Epoch 1599/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0433\n",
            "Epoch 1600/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0442\n",
            "Epoch 1601/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0422\n",
            "Epoch 1602/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 1.0448\n",
            "Epoch 1603/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0448\n",
            "Epoch 1604/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0425\n",
            "Epoch 1605/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0418\n",
            "Epoch 1606/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0454\n",
            "Epoch 1607/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 1.0473\n",
            "Epoch 1608/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0404\n",
            "Epoch 1609/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0439\n",
            "Epoch 1610/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0432\n",
            "Epoch 1611/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0421\n",
            "Epoch 1612/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0439\n",
            "Epoch 1613/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5260 - val_loss: 1.0486\n",
            "Epoch 1614/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0432\n",
            "Epoch 1615/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0403\n",
            "Epoch 1616/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0399\n",
            "Epoch 1617/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0436\n",
            "Epoch 1618/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0447\n",
            "Epoch 1619/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5256 - val_loss: 1.0421\n",
            "Epoch 1620/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0444\n",
            "Epoch 1621/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 1.0468\n",
            "Epoch 1622/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0455\n",
            "Epoch 1623/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0417\n",
            "Epoch 1624/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0400\n",
            "Epoch 1625/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0435\n",
            "Epoch 1626/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0428\n",
            "Epoch 1627/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0457\n",
            "Epoch 1628/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0438\n",
            "Epoch 1629/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0455\n",
            "Epoch 1630/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0436\n",
            "Epoch 1631/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0459\n",
            "Epoch 1632/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0429\n",
            "Epoch 1633/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5253 - val_loss: 1.0428\n",
            "Epoch 1634/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0465\n",
            "Epoch 1635/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0464\n",
            "Epoch 1636/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0424\n",
            "Epoch 1637/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0420\n",
            "Epoch 1638/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0436\n",
            "Epoch 1639/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0443\n",
            "Epoch 1640/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0444\n",
            "Epoch 1641/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0431\n",
            "Epoch 1642/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0424\n",
            "Epoch 1643/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0445\n",
            "Epoch 1644/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0458\n",
            "Epoch 1645/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0430\n",
            "Epoch 1646/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0400\n",
            "Epoch 1647/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5249 - val_loss: 1.0416\n",
            "Epoch 1648/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0433\n",
            "Epoch 1649/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0448\n",
            "Epoch 1650/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0430\n",
            "Epoch 1651/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5268 - val_loss: 1.0468\n",
            "Epoch 1652/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0429\n",
            "Epoch 1653/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0409\n",
            "Epoch 1654/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0420\n",
            "Epoch 1655/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5255 - val_loss: 1.0462\n",
            "Epoch 1656/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 1.0444\n",
            "Epoch 1657/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0435\n",
            "Epoch 1658/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0419\n",
            "Epoch 1659/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0446\n",
            "Epoch 1660/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0425\n",
            "Epoch 1661/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0408\n",
            "Epoch 1662/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0467\n",
            "Epoch 1663/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 1.0427\n",
            "Epoch 1664/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0437\n",
            "Epoch 1665/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0462\n",
            "Epoch 1666/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0456\n",
            "Epoch 1667/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0474\n",
            "Epoch 1668/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0454\n",
            "Epoch 1669/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 1.0405\n",
            "Epoch 1670/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 1.0444\n",
            "Epoch 1671/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0397\n",
            "Epoch 1672/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0444\n",
            "Epoch 1673/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0445\n",
            "Epoch 1674/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 1.0439\n",
            "Epoch 1675/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 1.0451\n",
            "Epoch 1676/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0467\n",
            "Epoch 1677/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0425\n",
            "Epoch 1678/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0457\n",
            "Epoch 1679/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0421\n",
            "Epoch 1680/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 1.0425\n",
            "Epoch 1681/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0417\n",
            "Epoch 1682/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0397\n",
            "Epoch 1683/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0448\n",
            "Epoch 1684/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 1.0414\n",
            "Epoch 1685/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0442\n",
            "Epoch 1686/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 1.0416\n",
            "Epoch 1687/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0436\n",
            "Epoch 1688/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0423\n",
            "Epoch 1689/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0440\n",
            "Epoch 1690/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0456\n",
            "Epoch 1691/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0451\n",
            "Epoch 1692/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0471\n",
            "Epoch 1693/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0444\n",
            "Epoch 1694/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 1.0409\n",
            "Epoch 1695/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0405\n",
            "Epoch 1696/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0394\n",
            "Epoch 1697/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0400\n",
            "Epoch 1698/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0463\n",
            "Epoch 1699/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0441\n",
            "Epoch 1700/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0412\n",
            "Epoch 1701/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5248 - val_loss: 1.0433\n",
            "Epoch 1702/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0417\n",
            "Epoch 1703/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0460\n",
            "Epoch 1704/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0469\n",
            "Epoch 1705/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0426\n",
            "Epoch 1706/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0433\n",
            "Epoch 1707/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0428\n",
            "Epoch 1708/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0432\n",
            "Epoch 1709/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5253 - val_loss: 1.0441\n",
            "Epoch 1710/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0433\n",
            "Epoch 1711/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0447\n",
            "Epoch 1712/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0406\n",
            "Epoch 1713/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0399\n",
            "Epoch 1714/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0472\n",
            "Epoch 1715/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0447\n",
            "Epoch 1716/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0455\n",
            "Epoch 1717/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0431\n",
            "Epoch 1718/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0386\n",
            "Epoch 1719/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0446\n",
            "Epoch 1720/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0448\n",
            "Epoch 1721/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 1.0447\n",
            "Epoch 1722/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0415\n",
            "Epoch 1723/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 1.0377\n",
            "Epoch 1724/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0405\n",
            "Epoch 1725/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5254 - val_loss: 1.0406\n",
            "Epoch 1726/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0455\n",
            "Epoch 1727/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5254 - val_loss: 1.0478\n",
            "Epoch 1728/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0439\n",
            "Epoch 1729/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5250 - val_loss: 1.0408\n",
            "Epoch 1730/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 1.0490\n",
            "Epoch 1731/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0412\n",
            "Epoch 1732/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0417\n",
            "Epoch 1733/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0449\n",
            "Epoch 1734/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0409\n",
            "Epoch 1735/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0444\n",
            "Epoch 1736/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0455\n",
            "Epoch 1737/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0428\n",
            "Epoch 1738/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0409\n",
            "Epoch 1739/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0409\n",
            "Epoch 1740/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5261 - val_loss: 1.0392\n",
            "Epoch 1741/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0472\n",
            "Epoch 1742/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0457\n",
            "Epoch 1743/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0456\n",
            "Epoch 1744/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0453\n",
            "Epoch 1745/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0408\n",
            "Epoch 1746/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0416\n",
            "Epoch 1747/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 1.0422\n",
            "Epoch 1748/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0416\n",
            "Epoch 1749/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0442\n",
            "Epoch 1750/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 1.0375\n",
            "Epoch 1751/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0431\n",
            "Epoch 1752/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0465\n",
            "Epoch 1753/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0444\n",
            "Epoch 1754/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0411\n",
            "Epoch 1755/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0424\n",
            "Epoch 1756/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5249 - val_loss: 1.0450\n",
            "Epoch 1757/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5255 - val_loss: 1.0440\n",
            "Epoch 1758/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5259 - val_loss: 1.0440\n",
            "Epoch 1759/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5248 - val_loss: 1.0468\n",
            "Epoch 1760/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5254 - val_loss: 1.0421\n",
            "Epoch 1761/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0407\n",
            "Epoch 1762/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0439\n",
            "Epoch 1763/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 1.0407\n",
            "Epoch 1764/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0454\n",
            "Epoch 1765/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 1.0407\n",
            "Epoch 1766/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0478\n",
            "Epoch 1767/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0429\n",
            "Epoch 1768/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0436\n",
            "Epoch 1769/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0444\n",
            "Epoch 1770/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0420\n",
            "Epoch 1771/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0403\n",
            "Epoch 1772/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0452\n",
            "Epoch 1773/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0458\n",
            "Epoch 1774/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 1.0392\n",
            "Epoch 1775/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0452\n",
            "Epoch 1776/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0486\n",
            "Epoch 1777/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0445\n",
            "Epoch 1778/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0420\n",
            "Epoch 1779/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0438\n",
            "Epoch 1780/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0450\n",
            "Epoch 1781/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0403\n",
            "Epoch 1782/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0404\n",
            "Epoch 1783/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0429\n",
            "Epoch 1784/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0404\n",
            "Epoch 1785/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0457\n",
            "Epoch 1786/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0415\n",
            "Epoch 1787/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0431\n",
            "Epoch 1788/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0427\n",
            "Epoch 1789/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0430\n",
            "Epoch 1790/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0448\n",
            "Epoch 1791/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0478\n",
            "Epoch 1792/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0439\n",
            "Epoch 1793/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0444\n",
            "Epoch 1794/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0440\n",
            "Epoch 1795/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0459\n",
            "Epoch 1796/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0436\n",
            "Epoch 1797/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0423\n",
            "Epoch 1798/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0388\n",
            "Epoch 1799/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0431\n",
            "Epoch 1800/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0424\n",
            "Epoch 1801/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 1.0407\n",
            "Epoch 1802/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0439\n",
            "Epoch 1803/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0442\n",
            "Epoch 1804/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0452\n",
            "Epoch 1805/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0455\n",
            "Epoch 1806/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0395\n",
            "Epoch 1807/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0458\n",
            "Epoch 1808/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0466\n",
            "Epoch 1809/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0404\n",
            "Epoch 1810/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0464\n",
            "Epoch 1811/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0455\n",
            "Epoch 1812/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0422\n",
            "Epoch 1813/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0434\n",
            "Epoch 1814/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0425\n",
            "Epoch 1815/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5248 - val_loss: 1.0447\n",
            "Epoch 1816/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0435\n",
            "Epoch 1817/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0440\n",
            "Epoch 1818/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5254 - val_loss: 1.0487\n",
            "Epoch 1819/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0422\n",
            "Epoch 1820/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0426\n",
            "Epoch 1821/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0441\n",
            "Epoch 1822/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0417\n",
            "Epoch 1823/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0447\n",
            "Epoch 1824/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0423\n",
            "Epoch 1825/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0481\n",
            "Epoch 1826/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0461\n",
            "Epoch 1827/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0453\n",
            "Epoch 1828/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0386\n",
            "Epoch 1829/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0408\n",
            "Epoch 1830/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0440\n",
            "Epoch 1831/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0453\n",
            "Epoch 1832/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0434\n",
            "Epoch 1833/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0434\n",
            "Epoch 1834/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0408\n",
            "Epoch 1835/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0439\n",
            "Epoch 1836/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 1.0379\n",
            "Epoch 1837/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 1.0479\n",
            "Epoch 1838/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0442\n",
            "Epoch 1839/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5245 - val_loss: 1.0440\n",
            "Epoch 1840/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0431\n",
            "Epoch 1841/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0428\n",
            "Epoch 1842/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0442\n",
            "Epoch 1843/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0383\n",
            "Epoch 1844/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5246 - val_loss: 1.0434\n",
            "Epoch 1845/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5247 - val_loss: 1.0430\n",
            "Epoch 1846/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5245 - val_loss: 1.0443\n",
            "Epoch 1847/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5245 - val_loss: 1.0451\n",
            "Epoch 1848/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5250 - val_loss: 1.0421\n",
            "Epoch 1849/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0477\n",
            "Epoch 1850/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0395\n",
            "Epoch 1851/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5249 - val_loss: 1.0437\n",
            "Epoch 1852/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5251 - val_loss: 1.0434\n",
            "Epoch 1853/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5257 - val_loss: 1.0430\n",
            "Epoch 1854/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0421\n",
            "Epoch 1855/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0454\n",
            "Epoch 1856/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 1.0401\n",
            "Epoch 1857/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0414\n",
            "Epoch 1858/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0404\n",
            "Epoch 1859/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0454\n",
            "Epoch 1860/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0428\n",
            "Epoch 1861/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0440\n",
            "Epoch 1862/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0441\n",
            "Epoch 1863/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0450\n",
            "Epoch 1864/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 1.0396\n",
            "Epoch 1865/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0457\n",
            "Epoch 1866/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0444\n",
            "Epoch 1867/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0457\n",
            "Epoch 1868/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0461\n",
            "Epoch 1869/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0411\n",
            "Epoch 1870/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0430\n",
            "Epoch 1871/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5243 - val_loss: 1.0429\n",
            "Epoch 1872/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5250 - val_loss: 1.0404\n",
            "Epoch 1873/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5249 - val_loss: 1.0454\n",
            "Epoch 1874/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5245 - val_loss: 1.0445\n",
            "Epoch 1875/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5245 - val_loss: 1.0440\n",
            "Epoch 1876/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0452\n",
            "Epoch 1877/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5247 - val_loss: 1.0446\n",
            "Epoch 1878/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0432\n",
            "Epoch 1879/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0410\n",
            "Epoch 1880/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0437\n",
            "Epoch 1881/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0433\n",
            "Epoch 1882/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0448\n",
            "Epoch 1883/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0425\n",
            "Epoch 1884/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0456\n",
            "Epoch 1885/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 1.0419\n",
            "Epoch 1886/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0421\n",
            "Epoch 1887/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5249 - val_loss: 1.0402\n",
            "Epoch 1888/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0429\n",
            "Epoch 1889/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0473\n",
            "Epoch 1890/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0451\n",
            "Epoch 1891/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0462\n",
            "Epoch 1892/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5243 - val_loss: 1.0451\n",
            "Epoch 1893/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0429\n",
            "Epoch 1894/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5250 - val_loss: 1.0473\n",
            "Epoch 1895/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0425\n",
            "Epoch 1896/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0418\n",
            "Epoch 1897/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5244 - val_loss: 1.0480\n",
            "Epoch 1898/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0477\n",
            "Epoch 1899/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0468\n",
            "Epoch 1900/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0394\n",
            "Epoch 1901/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5267 - val_loss: 1.0472\n",
            "Epoch 1902/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5247 - val_loss: 1.0447\n",
            "Epoch 1903/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5246 - val_loss: 1.0399\n",
            "Epoch 1904/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0467\n",
            "Epoch 1905/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0418\n",
            "Epoch 1906/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0433\n",
            "Epoch 1907/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0428\n",
            "Epoch 1908/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0439\n",
            "Epoch 1909/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5244 - val_loss: 1.0425\n",
            "Epoch 1910/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5243 - val_loss: 1.0424\n",
            "Epoch 1911/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5245 - val_loss: 1.0414\n",
            "Epoch 1912/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0449\n",
            "Epoch 1913/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5242 - val_loss: 1.0439\n",
            "Epoch 1914/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0401\n",
            "Epoch 1915/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0448\n",
            "Epoch 1916/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0412\n",
            "Epoch 1917/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0460\n",
            "Epoch 1918/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5250 - val_loss: 1.0429\n",
            "Epoch 1919/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5252 - val_loss: 1.0439\n",
            "Epoch 1920/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0490\n",
            "Epoch 1921/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0446\n",
            "Epoch 1922/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0481\n",
            "Epoch 1923/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 1.0373\n",
            "Epoch 1924/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0387\n",
            "Epoch 1925/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5248 - val_loss: 1.0482\n",
            "Epoch 1926/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0447\n",
            "Epoch 1927/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0460\n",
            "Epoch 1928/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0461\n",
            "Epoch 1929/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0446\n",
            "Epoch 1930/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0402\n",
            "Epoch 1931/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0412\n",
            "Epoch 1932/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0451\n",
            "Epoch 1933/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0427\n",
            "Epoch 1934/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0420\n",
            "Epoch 1935/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0485\n",
            "Epoch 1936/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5268 - val_loss: 1.0400\n",
            "Epoch 1937/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5248 - val_loss: 1.0454\n",
            "Epoch 1938/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0443\n",
            "Epoch 1939/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0424\n",
            "Epoch 1940/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5243 - val_loss: 1.0413\n",
            "Epoch 1941/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0443\n",
            "Epoch 1942/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0410\n",
            "Epoch 1943/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5242 - val_loss: 1.0464\n",
            "Epoch 1944/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0400\n",
            "Epoch 1945/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0448\n",
            "Epoch 1946/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0432\n",
            "Epoch 1947/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0486\n",
            "Epoch 1948/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0450\n",
            "Epoch 1949/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0455\n",
            "Epoch 1950/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0426\n",
            "Epoch 1951/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0399\n",
            "Epoch 1952/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0422\n",
            "Epoch 1953/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0447\n",
            "Epoch 1954/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0458\n",
            "Epoch 1955/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0464\n",
            "Epoch 1956/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0411\n",
            "Epoch 1957/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0415\n",
            "Epoch 1958/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0429\n",
            "Epoch 1959/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 1.0404\n",
            "Epoch 1960/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0464\n",
            "Epoch 1961/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0429\n",
            "Epoch 1962/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0443\n",
            "Epoch 1963/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0462\n",
            "Epoch 1964/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0454\n",
            "Epoch 1965/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0400\n",
            "Epoch 1966/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0379\n",
            "Epoch 1967/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0470\n",
            "Epoch 1968/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0422\n",
            "Epoch 1969/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0446\n",
            "Epoch 1970/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0438\n",
            "Epoch 1971/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5241 - val_loss: 1.0442\n",
            "Epoch 1972/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0448\n",
            "Epoch 1973/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0469\n",
            "Epoch 1974/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0445\n",
            "Epoch 1975/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5245 - val_loss: 1.0439\n",
            "Epoch 1976/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0416\n",
            "Epoch 1977/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0463\n",
            "Epoch 1978/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0389\n",
            "Epoch 1979/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0466\n",
            "Epoch 1980/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0439\n",
            "Epoch 1981/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0438\n",
            "Epoch 1982/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0413\n",
            "Epoch 1983/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0432\n",
            "Epoch 1984/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0425\n",
            "Epoch 1985/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0402\n",
            "Epoch 1986/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0503\n",
            "Epoch 1987/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0455\n",
            "Epoch 1988/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0465\n",
            "Epoch 1989/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0417\n",
            "Epoch 1990/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0419\n",
            "Epoch 1991/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0399\n",
            "Epoch 1992/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0477\n",
            "Epoch 1993/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0395\n",
            "Epoch 1994/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0440\n",
            "Epoch 1995/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0517\n",
            "Epoch 1996/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0447\n",
            "Epoch 1997/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0484\n",
            "Epoch 1998/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0392\n",
            "Epoch 1999/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5246 - val_loss: 1.0462\n",
            "Epoch 2000/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0447\n",
            "Epoch 2001/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0436\n",
            "Epoch 2002/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0447\n",
            "Epoch 2003/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0423\n",
            "Epoch 2004/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0430\n",
            "Epoch 2005/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0393\n",
            "Epoch 2006/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0447\n",
            "Epoch 2007/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0446\n",
            "Epoch 2008/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0445\n",
            "Epoch 2009/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0462\n",
            "Epoch 2010/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0452\n",
            "Epoch 2011/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0415\n",
            "Epoch 2012/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0473\n",
            "Epoch 2013/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0404\n",
            "Epoch 2014/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0459\n",
            "Epoch 2015/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0418\n",
            "Epoch 2016/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0423\n",
            "Epoch 2017/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0459\n",
            "Epoch 2018/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0419\n",
            "Epoch 2019/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0419\n",
            "Epoch 2020/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0425\n",
            "Epoch 2021/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0405\n",
            "Epoch 2022/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0457\n",
            "Epoch 2023/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0450\n",
            "Epoch 2024/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0430\n",
            "Epoch 2025/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0449\n",
            "Epoch 2026/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0450\n",
            "Epoch 2027/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0438\n",
            "Epoch 2028/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0478\n",
            "Epoch 2029/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0454\n",
            "Epoch 2030/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0420\n",
            "Epoch 2031/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0443\n",
            "Epoch 2032/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0426\n",
            "Epoch 2033/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0414\n",
            "Epoch 2034/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0456\n",
            "Epoch 2035/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5248 - val_loss: 1.0381\n",
            "Epoch 2036/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0445\n",
            "Epoch 2037/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0406\n",
            "Epoch 2038/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0427\n",
            "Epoch 2039/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0437\n",
            "Epoch 2040/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0407\n",
            "Epoch 2041/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5249 - val_loss: 1.0442\n",
            "Epoch 2042/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5247 - val_loss: 1.0443\n",
            "Epoch 2043/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0426\n",
            "Epoch 2044/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0435\n",
            "Epoch 2045/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0434\n",
            "Epoch 2046/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0473\n",
            "Epoch 2047/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0446\n",
            "Epoch 2048/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0456\n",
            "Epoch 2049/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5250 - val_loss: 1.0392\n",
            "Epoch 2050/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0453\n",
            "Epoch 2051/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0442\n",
            "Epoch 2052/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0456\n",
            "Epoch 2053/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0483\n",
            "Epoch 2054/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0424\n",
            "Epoch 2055/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0430\n",
            "Epoch 2056/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0412\n",
            "Epoch 2057/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5243 - val_loss: 1.0401\n",
            "Epoch 2058/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0447\n",
            "Epoch 2059/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0401\n",
            "Epoch 2060/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0462\n",
            "Epoch 2061/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 1.0466\n",
            "Epoch 2062/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5244 - val_loss: 1.0416\n",
            "Epoch 2063/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0429\n",
            "Epoch 2064/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0446\n",
            "Epoch 2065/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5248 - val_loss: 1.0396\n",
            "Epoch 2066/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0431\n",
            "Epoch 2067/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0471\n",
            "Epoch 2068/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5253 - val_loss: 1.0434\n",
            "Epoch 2069/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5242 - val_loss: 1.0461\n",
            "Epoch 2070/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5247 - val_loss: 1.0403\n",
            "Epoch 2071/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0481\n",
            "Epoch 2072/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0466\n",
            "Epoch 2073/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5244 - val_loss: 1.0417\n",
            "Epoch 2074/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0457\n",
            "Epoch 2075/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0423\n",
            "Epoch 2076/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0406\n",
            "Epoch 2077/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0448\n",
            "Epoch 2078/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0446\n",
            "Epoch 2079/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0458\n",
            "Epoch 2080/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0435\n",
            "Epoch 2081/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0419\n",
            "Epoch 2082/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5241 - val_loss: 1.0419\n",
            "Epoch 2083/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0420\n",
            "Epoch 2084/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0499\n",
            "Epoch 2085/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0425\n",
            "Epoch 2086/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0480\n",
            "Epoch 2087/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0427\n",
            "Epoch 2088/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0466\n",
            "Epoch 2089/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0445\n",
            "Epoch 2090/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0446\n",
            "Epoch 2091/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5239 - val_loss: 1.0435\n",
            "Epoch 2092/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0434\n",
            "Epoch 2093/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5250 - val_loss: 1.0408\n",
            "Epoch 2094/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0448\n",
            "Epoch 2095/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0468\n",
            "Epoch 2096/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0478\n",
            "Epoch 2097/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0393\n",
            "Epoch 2098/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0422\n",
            "Epoch 2099/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0478\n",
            "Epoch 2100/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0448\n",
            "Epoch 2101/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0404\n",
            "Epoch 2102/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0432\n",
            "Epoch 2103/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0447\n",
            "Epoch 2104/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0462\n",
            "Epoch 2105/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0452\n",
            "Epoch 2106/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0483\n",
            "Epoch 2107/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0438\n",
            "Epoch 2108/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0411\n",
            "Epoch 2109/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0443\n",
            "Epoch 2110/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0425\n",
            "Epoch 2111/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0479\n",
            "Epoch 2112/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0475\n",
            "Epoch 2113/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0459\n",
            "Epoch 2114/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0455\n",
            "Epoch 2115/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 1.0419\n",
            "Epoch 2116/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0442\n",
            "Epoch 2117/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0411\n",
            "Epoch 2118/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0457\n",
            "Epoch 2119/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0431\n",
            "Epoch 2120/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0472\n",
            "Epoch 2121/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0434\n",
            "Epoch 2122/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0417\n",
            "Epoch 2123/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0413\n",
            "Epoch 2124/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0433\n",
            "Epoch 2125/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0470\n",
            "Epoch 2126/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0461\n",
            "Epoch 2127/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0452\n",
            "Epoch 2128/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0420\n",
            "Epoch 2129/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0454\n",
            "Epoch 2130/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0471\n",
            "Epoch 2131/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0425\n",
            "Epoch 2132/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0451\n",
            "Epoch 2133/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 1.0442\n",
            "Epoch 2134/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0418\n",
            "Epoch 2135/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0426\n",
            "Epoch 2136/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0446\n",
            "Epoch 2137/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0448\n",
            "Epoch 2138/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0440\n",
            "Epoch 2139/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5244 - val_loss: 1.0421\n",
            "Epoch 2140/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 1.0481\n",
            "Epoch 2141/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0431\n",
            "Epoch 2142/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0385\n",
            "Epoch 2143/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0471\n",
            "Epoch 2144/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0424\n",
            "Epoch 2145/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0423\n",
            "Epoch 2146/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0441\n",
            "Epoch 2147/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0446\n",
            "Epoch 2148/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0436\n",
            "Epoch 2149/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0447\n",
            "Epoch 2150/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0449\n",
            "Epoch 2151/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0478\n",
            "Epoch 2152/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5240 - val_loss: 1.0421\n",
            "Epoch 2153/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0458\n",
            "Epoch 2154/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0417\n",
            "Epoch 2155/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0428\n",
            "Epoch 2156/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0410\n",
            "Epoch 2157/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5245 - val_loss: 1.0430\n",
            "Epoch 2158/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 1.0465\n",
            "Epoch 2159/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0454\n",
            "Epoch 2160/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0443\n",
            "Epoch 2161/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0404\n",
            "Epoch 2162/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0454\n",
            "Epoch 2163/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5239 - val_loss: 1.0449\n",
            "Epoch 2164/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0446\n",
            "Epoch 2165/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5235 - val_loss: 1.0467\n",
            "Epoch 2166/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0467\n",
            "Epoch 2167/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0442\n",
            "Epoch 2168/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0454\n",
            "Epoch 2169/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5246 - val_loss: 1.0413\n",
            "Epoch 2170/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0451\n",
            "Epoch 2171/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0444\n",
            "Epoch 2172/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0447\n",
            "Epoch 2173/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5239 - val_loss: 1.0459\n",
            "Epoch 2174/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5244 - val_loss: 1.0425\n",
            "Epoch 2175/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5246 - val_loss: 1.0439\n",
            "Epoch 2176/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0422\n",
            "Epoch 2177/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0462\n",
            "Epoch 2178/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5235 - val_loss: 1.0445\n",
            "Epoch 2179/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0436\n",
            "Epoch 2180/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0427\n",
            "Epoch 2181/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0412\n",
            "Epoch 2182/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0448\n",
            "Epoch 2183/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0468\n",
            "Epoch 2184/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0463\n",
            "Epoch 2185/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0434\n",
            "Epoch 2186/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 1.0383\n",
            "Epoch 2187/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0413\n",
            "Epoch 2188/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0465\n",
            "Epoch 2189/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0423\n",
            "Epoch 2190/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0474\n",
            "Epoch 2191/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5235 - val_loss: 1.0428\n",
            "Epoch 2192/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0418\n",
            "Epoch 2193/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0479\n",
            "Epoch 2194/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0448\n",
            "Epoch 2195/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0418\n",
            "Epoch 2196/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0407\n",
            "Epoch 2197/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0419\n",
            "Epoch 2198/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0426\n",
            "Epoch 2199/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5242 - val_loss: 1.0438\n",
            "Epoch 2200/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5243 - val_loss: 1.0463\n",
            "Epoch 2201/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0445\n",
            "Epoch 2202/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0454\n",
            "Epoch 2203/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0457\n",
            "Epoch 2204/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5237 - val_loss: 1.0436\n",
            "Epoch 2205/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0431\n",
            "Epoch 2206/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0434\n",
            "Epoch 2207/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0448\n",
            "Epoch 2208/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0420\n",
            "Epoch 2209/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0471\n",
            "Epoch 2210/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0428\n",
            "Epoch 2211/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 1.0447\n",
            "Epoch 2212/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0468\n",
            "Epoch 2213/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5239 - val_loss: 1.0452\n",
            "Epoch 2214/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5244 - val_loss: 1.0407\n",
            "Epoch 2215/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5251 - val_loss: 1.0453\n",
            "Epoch 2216/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5240 - val_loss: 1.0406\n",
            "Epoch 2217/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 1.0458\n",
            "Epoch 2218/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5245 - val_loss: 1.0432\n",
            "Epoch 2219/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 1.0458\n",
            "Epoch 2220/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0466\n",
            "Epoch 2221/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0497\n",
            "Epoch 2222/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5237 - val_loss: 1.0404\n",
            "Epoch 2223/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0418\n",
            "Epoch 2224/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0448\n",
            "Epoch 2225/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0423\n",
            "Epoch 2226/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 1.0494\n",
            "Epoch 2227/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0415\n",
            "Epoch 2228/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5235 - val_loss: 1.0446\n",
            "Epoch 2229/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 1.0456\n",
            "Epoch 2230/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 1.0472\n",
            "Epoch 2231/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 1.0419\n",
            "Epoch 2232/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0463\n",
            "Epoch 2233/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0431\n",
            "Epoch 2234/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0455\n",
            "Epoch 2235/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5235 - val_loss: 1.0431\n",
            "Epoch 2236/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0407\n",
            "Epoch 2237/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0421\n",
            "Epoch 2238/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0442\n",
            "Epoch 2239/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0466\n",
            "Epoch 2240/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0428\n",
            "Epoch 2241/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0481\n",
            "Epoch 2242/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0446\n",
            "Epoch 2243/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 1.0455\n",
            "Epoch 2244/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5235 - val_loss: 1.0449\n",
            "Epoch 2245/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 1.0427\n",
            "Epoch 2246/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5239 - val_loss: 1.0458\n",
            "Epoch 2247/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 1.0451\n",
            "Epoch 2248/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0409\n",
            "Epoch 2249/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0454\n",
            "Epoch 2250/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0442\n",
            "Epoch 2251/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5239 - val_loss: 1.0434\n",
            "Epoch 2252/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0424\n",
            "Epoch 2253/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0441\n",
            "Epoch 2254/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0417\n",
            "Epoch 2255/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 1.0480\n",
            "Epoch 2256/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0438\n",
            "Epoch 2257/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0428\n",
            "Epoch 2258/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0477\n",
            "Epoch 2259/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0454\n",
            "Epoch 2260/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0414\n",
            "Epoch 2261/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0478\n",
            "Epoch 2262/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5243 - val_loss: 1.0451\n",
            "Epoch 2263/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 1.0422\n",
            "Epoch 2264/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0429\n",
            "Epoch 2265/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0427\n",
            "Epoch 2266/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0431\n",
            "Epoch 2267/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0466\n",
            "Epoch 2268/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0417\n",
            "Epoch 2269/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0424\n",
            "Epoch 2270/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0466\n",
            "Epoch 2271/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0440\n",
            "Epoch 2272/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0404\n",
            "Epoch 2273/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0448\n",
            "Epoch 2274/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0438\n",
            "Epoch 2275/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0466\n",
            "Epoch 2276/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0438\n",
            "Epoch 2277/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5237 - val_loss: 1.0445\n",
            "Epoch 2278/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5241 - val_loss: 1.0433\n",
            "Epoch 2279/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0483\n",
            "Epoch 2280/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5237 - val_loss: 1.0473\n",
            "Epoch 2281/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 1.0452\n",
            "Epoch 2282/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0435\n",
            "Epoch 2283/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0457\n",
            "Epoch 2284/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5244 - val_loss: 1.0409\n",
            "Epoch 2285/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 1.0434\n",
            "Epoch 2286/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 1.0472\n",
            "Epoch 2287/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0438\n",
            "Epoch 2288/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0482\n",
            "Epoch 2289/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0488\n",
            "Epoch 2290/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5240 - val_loss: 1.0433\n",
            "Epoch 2291/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0392\n",
            "Epoch 2292/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0418\n",
            "Epoch 2293/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0460\n",
            "Epoch 2294/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5233 - val_loss: 1.0461\n",
            "Epoch 2295/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5243 - val_loss: 1.0432\n",
            "Epoch 2296/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5237 - val_loss: 1.0457\n",
            "Epoch 2297/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 1.0427\n",
            "Epoch 2298/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0461\n",
            "Epoch 2299/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 1.0417\n",
            "Epoch 2300/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0439\n",
            "Epoch 2301/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0437\n",
            "Epoch 2302/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0469\n",
            "Epoch 2303/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0474\n",
            "Epoch 2304/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0475\n",
            "Epoch 2305/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0440\n",
            "Epoch 2306/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0423\n",
            "Epoch 2307/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0403\n",
            "Epoch 2308/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0426\n",
            "Epoch 2309/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0441\n",
            "Epoch 2310/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0449\n",
            "Epoch 2311/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0466\n",
            "Epoch 2312/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0479\n",
            "Epoch 2313/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0441\n",
            "Epoch 2314/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0490\n",
            "Epoch 2315/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0430\n",
            "Epoch 2316/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0442\n",
            "Epoch 2317/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5240 - val_loss: 1.0467\n",
            "Epoch 2318/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0435\n",
            "Epoch 2319/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0400\n",
            "Epoch 2320/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5245 - val_loss: 1.0443\n",
            "Epoch 2321/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5239 - val_loss: 1.0407\n",
            "Epoch 2322/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0453\n",
            "Epoch 2323/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5242 - val_loss: 1.0417\n",
            "Epoch 2324/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0486\n",
            "Epoch 2325/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0442\n",
            "Epoch 2326/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0434\n",
            "Epoch 2327/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5237 - val_loss: 1.0450\n",
            "Epoch 2328/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0433\n",
            "Epoch 2329/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0448\n",
            "Epoch 2330/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0471\n",
            "Epoch 2331/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0441\n",
            "Epoch 2332/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0405\n",
            "Epoch 2333/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0458\n",
            "Epoch 2334/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0418\n",
            "Epoch 2335/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0403\n",
            "Epoch 2336/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0450\n",
            "Epoch 2337/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0431\n",
            "Epoch 2338/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0490\n",
            "Epoch 2339/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0411\n",
            "Epoch 2340/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5275 - val_loss: 1.0543\n",
            "Epoch 2341/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0439\n",
            "Epoch 2342/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0413\n",
            "Epoch 2343/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0478\n",
            "Epoch 2344/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 1.0388\n",
            "Epoch 2345/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5239 - val_loss: 1.0412\n",
            "Epoch 2346/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0481\n",
            "Epoch 2347/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0468\n",
            "Epoch 2348/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0441\n",
            "Epoch 2349/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0472\n",
            "Epoch 2350/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0432\n",
            "Epoch 2351/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5233 - val_loss: 1.0437\n",
            "Epoch 2352/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0484\n",
            "Epoch 2353/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0421\n",
            "Epoch 2354/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0423\n",
            "Epoch 2355/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0433\n",
            "Epoch 2356/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0471\n",
            "Epoch 2357/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 1.0456\n",
            "Epoch 2358/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0424\n",
            "Epoch 2359/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0404\n",
            "Epoch 2360/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5237 - val_loss: 1.0430\n",
            "Epoch 2361/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5231 - val_loss: 1.0451\n",
            "Epoch 2362/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 1.0474\n",
            "Epoch 2363/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0449\n",
            "Epoch 2364/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0448\n",
            "Epoch 2365/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 1.0478\n",
            "Epoch 2366/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5237 - val_loss: 1.0445\n",
            "Epoch 2367/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5235 - val_loss: 1.0470\n",
            "Epoch 2368/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0416\n",
            "Epoch 2369/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5240 - val_loss: 1.0434\n",
            "Epoch 2370/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0435\n",
            "Epoch 2371/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0452\n",
            "Epoch 2372/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0422\n",
            "Epoch 2373/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0433\n",
            "Epoch 2374/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0480\n",
            "Epoch 2375/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0423\n",
            "Epoch 2376/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 1.0493\n",
            "Epoch 2377/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0447\n",
            "Epoch 2378/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0429\n",
            "Epoch 2379/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0446\n",
            "Epoch 2380/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0416\n",
            "Epoch 2381/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0431\n",
            "Epoch 2382/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0410\n",
            "Epoch 2383/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0439\n",
            "Epoch 2384/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0459\n",
            "Epoch 2385/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0437\n",
            "Epoch 2386/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0443\n",
            "Epoch 2387/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 1.0450\n",
            "Epoch 2388/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 1.0466\n",
            "Epoch 2389/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0454\n",
            "Epoch 2390/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0451\n",
            "Epoch 2391/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0459\n",
            "Epoch 2392/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0472\n",
            "Epoch 2393/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0446\n",
            "Epoch 2394/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0424\n",
            "Epoch 2395/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0458\n",
            "Epoch 2396/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0469\n",
            "Epoch 2397/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 1.0424\n",
            "Epoch 2398/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0448\n",
            "Epoch 2399/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5240 - val_loss: 1.0442\n",
            "Epoch 2400/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0434\n",
            "Epoch 2401/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5233 - val_loss: 1.0421\n",
            "Epoch 2402/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5240 - val_loss: 1.0463\n",
            "Epoch 2403/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5235 - val_loss: 1.0408\n",
            "Epoch 2404/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5239 - val_loss: 1.0409\n",
            "Epoch 2405/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0473\n",
            "Epoch 2406/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5239 - val_loss: 1.0421\n",
            "Epoch 2407/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5232 - val_loss: 1.0462\n",
            "Epoch 2408/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5251 - val_loss: 1.0512\n",
            "Epoch 2409/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0426\n",
            "Epoch 2410/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0411\n",
            "Epoch 2411/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0425\n",
            "Epoch 2412/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0465\n",
            "Epoch 2413/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0400\n",
            "Epoch 2414/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5245 - val_loss: 1.0492\n",
            "Epoch 2415/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0454\n",
            "Epoch 2416/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0415\n",
            "Epoch 2417/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0450\n",
            "Epoch 2418/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0477\n",
            "Epoch 2419/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0450\n",
            "Epoch 2420/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0434\n",
            "Epoch 2421/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5241 - val_loss: 1.0465\n",
            "Epoch 2422/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 1.0482\n",
            "Epoch 2423/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5243 - val_loss: 1.0398\n",
            "Epoch 2424/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5229 - val_loss: 1.0435\n",
            "Epoch 2425/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0456\n",
            "Epoch 2426/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 1.0435\n",
            "Epoch 2427/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5231 - val_loss: 1.0471\n",
            "Epoch 2428/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5235 - val_loss: 1.0471\n",
            "Epoch 2429/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0459\n",
            "Epoch 2430/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0443\n",
            "Epoch 2431/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 1.0417\n",
            "Epoch 2432/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 1.0452\n",
            "Epoch 2433/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0449\n",
            "Epoch 2434/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0477\n",
            "Epoch 2435/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0420\n",
            "Epoch 2436/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0447\n",
            "Epoch 2437/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0495\n",
            "Epoch 2438/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0485\n",
            "Epoch 2439/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0428\n",
            "Epoch 2440/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0426\n",
            "Epoch 2441/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0492\n",
            "Epoch 2442/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5228 - val_loss: 1.0429\n",
            "Epoch 2443/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0459\n",
            "Epoch 2444/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0415\n",
            "Epoch 2445/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0426\n",
            "Epoch 2446/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0408\n",
            "Epoch 2447/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0430\n",
            "Epoch 2448/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0453\n",
            "Epoch 2449/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0458\n",
            "Epoch 2450/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0429\n",
            "Epoch 2451/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5241 - val_loss: 1.0409\n",
            "Epoch 2452/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0488\n",
            "Epoch 2453/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0473\n",
            "Epoch 2454/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0433\n",
            "Epoch 2455/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5232 - val_loss: 1.0459\n",
            "Epoch 2456/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5233 - val_loss: 1.0429\n",
            "Epoch 2457/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0477\n",
            "Epoch 2458/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0451\n",
            "Epoch 2459/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0435\n",
            "Epoch 2460/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 1.0436\n",
            "Epoch 2461/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0431\n",
            "Epoch 2462/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0457\n",
            "Epoch 2463/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5235 - val_loss: 1.0422\n",
            "Epoch 2464/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0444\n",
            "Epoch 2465/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0461\n",
            "Epoch 2466/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0456\n",
            "Epoch 2467/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0441\n",
            "Epoch 2468/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0480\n",
            "Epoch 2469/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0448\n",
            "Epoch 2470/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0406\n",
            "Epoch 2471/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0450\n",
            "Epoch 2472/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0470\n",
            "Epoch 2473/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0406\n",
            "Epoch 2474/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0456\n",
            "Epoch 2475/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0482\n",
            "Epoch 2476/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0438\n",
            "Epoch 2477/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0416\n",
            "Epoch 2478/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0502\n",
            "Epoch 2479/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0443\n",
            "Epoch 2480/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0484\n",
            "Epoch 2481/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0437\n",
            "Epoch 2482/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5232 - val_loss: 1.0411\n",
            "Epoch 2483/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0460\n",
            "Epoch 2484/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0421\n",
            "Epoch 2485/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0458\n",
            "Epoch 2486/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0423\n",
            "Epoch 2487/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0420\n",
            "Epoch 2488/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0434\n",
            "Epoch 2489/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0431\n",
            "Epoch 2490/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0425\n",
            "Epoch 2491/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0429\n",
            "Epoch 2492/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5237 - val_loss: 1.0449\n",
            "Epoch 2493/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0481\n",
            "Epoch 2494/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 1.0429\n",
            "Epoch 2495/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5230 - val_loss: 1.0442\n",
            "Epoch 2496/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0485\n",
            "Epoch 2497/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0504\n",
            "Epoch 2498/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0468\n",
            "Epoch 2499/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 1.0427\n",
            "Epoch 2500/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0464\n",
            "Epoch 2501/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0448\n",
            "Epoch 2502/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0436\n",
            "Epoch 2503/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0449\n",
            "Epoch 2504/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0425\n",
            "Epoch 2505/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5233 - val_loss: 1.0416\n",
            "Epoch 2506/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0445\n",
            "Epoch 2507/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0446\n",
            "Epoch 2508/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5239 - val_loss: 1.0504\n",
            "Epoch 2509/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0437\n",
            "Epoch 2510/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5231 - val_loss: 1.0436\n",
            "Epoch 2511/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0434\n",
            "Epoch 2512/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5250 - val_loss: 1.0477\n",
            "Epoch 2513/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0469\n",
            "Epoch 2514/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5229 - val_loss: 1.0451\n",
            "Epoch 2515/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0417\n",
            "Epoch 2516/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5232 - val_loss: 1.0414\n",
            "Epoch 2517/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 1.0467\n",
            "Epoch 2518/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0439\n",
            "Epoch 2519/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0426\n",
            "Epoch 2520/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5233 - val_loss: 1.0481\n",
            "Epoch 2521/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5235 - val_loss: 1.0501\n",
            "Epoch 2522/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0411\n",
            "Epoch 2523/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0434\n",
            "Epoch 2524/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5228 - val_loss: 1.0429\n",
            "Epoch 2525/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5239 - val_loss: 1.0462\n",
            "Epoch 2526/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 1.0423\n",
            "Epoch 2527/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5230 - val_loss: 1.0426\n",
            "Epoch 2528/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 1.0456\n",
            "Epoch 2529/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 1.0479\n",
            "Epoch 2530/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0396\n",
            "Epoch 2531/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5241 - val_loss: 1.0472\n",
            "Epoch 2532/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 1.0475\n",
            "Epoch 2533/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0426\n",
            "Epoch 2534/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0488\n",
            "Epoch 2535/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0424\n",
            "Epoch 2536/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0420\n",
            "Epoch 2537/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0394\n",
            "Epoch 2538/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0467\n",
            "Epoch 2539/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0467\n",
            "Epoch 2540/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0475\n",
            "Epoch 2541/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0442\n",
            "Epoch 2542/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0456\n",
            "Epoch 2543/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5231 - val_loss: 1.0425\n",
            "Epoch 2544/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0441\n",
            "Epoch 2545/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0428\n",
            "Epoch 2546/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0494\n",
            "Epoch 2547/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0435\n",
            "Epoch 2548/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0448\n",
            "Epoch 2549/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0470\n",
            "Epoch 2550/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0462\n",
            "Epoch 2551/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5228 - val_loss: 1.0455\n",
            "Epoch 2552/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0453\n",
            "Epoch 2553/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0464\n",
            "Epoch 2554/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0440\n",
            "Epoch 2555/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5239 - val_loss: 1.0405\n",
            "Epoch 2556/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5237 - val_loss: 1.0420\n",
            "Epoch 2557/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0452\n",
            "Epoch 2558/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0473\n",
            "Epoch 2559/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0465\n",
            "Epoch 2560/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0445\n",
            "Epoch 2561/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0449\n",
            "Epoch 2562/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0484\n",
            "Epoch 2563/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0392\n",
            "Epoch 2564/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0444\n",
            "Epoch 2565/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0408\n",
            "Epoch 2566/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0412\n",
            "Epoch 2567/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0445\n",
            "Epoch 2568/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0423\n",
            "Epoch 2569/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0445\n",
            "Epoch 2570/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5227 - val_loss: 1.0447\n",
            "Epoch 2571/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5229 - val_loss: 1.0443\n",
            "Epoch 2572/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0503\n",
            "Epoch 2573/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5228 - val_loss: 1.0466\n",
            "Epoch 2574/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0459\n",
            "Epoch 2575/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0428\n",
            "Epoch 2576/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5227 - val_loss: 1.0438\n",
            "Epoch 2577/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0478\n",
            "Epoch 2578/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0476\n",
            "Epoch 2579/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 1.0500\n",
            "Epoch 2580/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0448\n",
            "Epoch 2581/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0410\n",
            "Epoch 2582/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0413\n",
            "Epoch 2583/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0513\n",
            "Epoch 2584/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5228 - val_loss: 1.0446\n",
            "Epoch 2585/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0402\n",
            "Epoch 2586/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0403\n",
            "Epoch 2587/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0444\n",
            "Epoch 2588/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0475\n",
            "Epoch 2589/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0450\n",
            "Epoch 2590/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5235 - val_loss: 1.0447\n",
            "Epoch 2591/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0447\n",
            "Epoch 2592/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0435\n",
            "Epoch 2593/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5229 - val_loss: 1.0451\n",
            "Epoch 2594/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0427\n",
            "Epoch 2595/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5237 - val_loss: 1.0466\n",
            "Epoch 2596/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0462\n",
            "Epoch 2597/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5229 - val_loss: 1.0446\n",
            "Epoch 2598/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0456\n",
            "Epoch 2599/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0493\n",
            "Epoch 2600/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5231 - val_loss: 1.0476\n",
            "Epoch 2601/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5229 - val_loss: 1.0440\n",
            "Epoch 2602/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5237 - val_loss: 1.0410\n",
            "Epoch 2603/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0449\n",
            "Epoch 2604/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0453\n",
            "Epoch 2605/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5233 - val_loss: 1.0449\n",
            "Epoch 2606/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5231 - val_loss: 1.0435\n",
            "Epoch 2607/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0457\n",
            "Epoch 2608/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0454\n",
            "Epoch 2609/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0496\n",
            "Epoch 2610/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 1.0406\n",
            "Epoch 2611/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0407\n",
            "Epoch 2612/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5229 - val_loss: 1.0449\n",
            "Epoch 2613/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0442\n",
            "Epoch 2614/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0463\n",
            "Epoch 2615/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5233 - val_loss: 1.0423\n",
            "Epoch 2616/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0473\n",
            "Epoch 2617/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0438\n",
            "Epoch 2618/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0490\n",
            "Epoch 2619/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 1.0401\n",
            "Epoch 2620/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5227 - val_loss: 1.0438\n",
            "Epoch 2621/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 1.0443\n",
            "Epoch 2622/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 1.0441\n",
            "Epoch 2623/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5232 - val_loss: 1.0475\n",
            "Epoch 2624/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0486\n",
            "Epoch 2625/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0446\n",
            "Epoch 2626/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0423\n",
            "Epoch 2627/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0460\n",
            "Epoch 2628/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 1.0416\n",
            "Epoch 2629/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0489\n",
            "Epoch 2630/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0491\n",
            "Epoch 2631/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5227 - val_loss: 1.0463\n",
            "Epoch 2632/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5233 - val_loss: 1.0421\n",
            "Epoch 2633/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5229 - val_loss: 1.0460\n",
            "Epoch 2634/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5229 - val_loss: 1.0461\n",
            "Epoch 2635/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5228 - val_loss: 1.0455\n",
            "Epoch 2636/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5231 - val_loss: 1.0470\n",
            "Epoch 2637/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0407\n",
            "Epoch 2638/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0465\n",
            "Epoch 2639/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5228 - val_loss: 1.0467\n",
            "Epoch 2640/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5227 - val_loss: 1.0465\n",
            "Epoch 2641/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0460\n",
            "Epoch 2642/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5229 - val_loss: 1.0447\n",
            "Epoch 2643/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0409\n",
            "Epoch 2644/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5228 - val_loss: 1.0456\n",
            "Epoch 2645/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0407\n",
            "Epoch 2646/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5229 - val_loss: 1.0427\n",
            "Epoch 2647/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5229 - val_loss: 1.0461\n",
            "Epoch 2648/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0430\n",
            "Epoch 2649/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 1.0500\n",
            "Epoch 2650/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 1.0412\n",
            "Epoch 2651/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0466\n",
            "Epoch 2652/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 1.0434\n",
            "Epoch 2653/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0460\n",
            "Epoch 2654/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5239 - val_loss: 1.0506\n",
            "Epoch 2655/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0457\n",
            "Epoch 2656/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5228 - val_loss: 1.0456\n",
            "Epoch 2657/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0454\n",
            "Epoch 2658/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0478\n",
            "Epoch 2659/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5229 - val_loss: 1.0466\n",
            "Epoch 2660/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5227 - val_loss: 1.0471\n",
            "Epoch 2661/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5227 - val_loss: 1.0422\n",
            "Epoch 2662/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0431\n",
            "Epoch 2663/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0446\n",
            "Epoch 2664/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 1.0437\n",
            "Epoch 2665/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0417\n",
            "Epoch 2666/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0498\n",
            "Epoch 2667/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0482\n",
            "Epoch 2668/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0447\n",
            "Epoch 2669/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0434\n",
            "Epoch 2670/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5228 - val_loss: 1.0430\n",
            "Epoch 2671/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5229 - val_loss: 1.0463\n",
            "Epoch 2672/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5229 - val_loss: 1.0450\n",
            "Epoch 2673/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0455\n",
            "Epoch 2674/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0421\n",
            "Epoch 2675/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0470\n",
            "Epoch 2676/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5231 - val_loss: 1.0424\n",
            "Epoch 2677/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 1.0465\n",
            "Epoch 2678/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0449\n",
            "Epoch 2679/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5229 - val_loss: 1.0430\n",
            "Epoch 2680/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0432\n",
            "Epoch 2681/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0423\n",
            "Epoch 2682/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 1.0513\n",
            "Epoch 2683/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0452\n",
            "Epoch 2684/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0428\n",
            "Epoch 2685/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5231 - val_loss: 1.0466\n",
            "Epoch 2686/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5230 - val_loss: 1.0444\n",
            "Epoch 2687/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 1.0470\n",
            "Epoch 2688/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5227 - val_loss: 1.0455\n",
            "Epoch 2689/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 1.0429\n",
            "Epoch 2690/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0484\n",
            "Epoch 2691/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5229 - val_loss: 1.0412\n",
            "Epoch 2692/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0462\n",
            "Epoch 2693/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0437\n",
            "Epoch 2694/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5229 - val_loss: 1.0469\n",
            "Epoch 2695/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 1.0407\n",
            "Epoch 2696/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 1.0455\n",
            "Epoch 2697/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0497\n",
            "Epoch 2698/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0426\n",
            "Epoch 2699/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5228 - val_loss: 1.0430\n",
            "Epoch 2700/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5228 - val_loss: 1.0471\n",
            "Epoch 2701/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5237 - val_loss: 1.0423\n",
            "Epoch 2702/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5229 - val_loss: 1.0482\n",
            "Epoch 2703/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5235 - val_loss: 1.0422\n",
            "Epoch 2704/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5227 - val_loss: 1.0449\n",
            "Epoch 2705/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5230 - val_loss: 1.0470\n",
            "Epoch 2706/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 1.0419\n",
            "Epoch 2707/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5226 - val_loss: 1.0451\n",
            "Epoch 2708/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0423\n",
            "Epoch 2709/5000\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.5230 - val_loss: 1.0443\n",
            "Epoch 2710/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0487\n",
            "Epoch 2711/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0472\n",
            "Epoch 2712/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5229 - val_loss: 1.0484\n",
            "Epoch 2713/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5226 - val_loss: 1.0455\n",
            "Epoch 2714/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 1.0454\n",
            "Epoch 2715/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 1.0485\n",
            "Epoch 2716/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5226 - val_loss: 1.0448\n",
            "Epoch 2717/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0459\n",
            "Epoch 2718/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5228 - val_loss: 1.0437\n",
            "Epoch 2719/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5225 - val_loss: 1.0444\n",
            "Epoch 2720/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0456\n",
            "Epoch 2721/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 1.0470\n",
            "Epoch 2722/5000\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 1.0460\n",
            "Epoch 2723/5000\n",
            " 1/36 [..............................] - ETA: 0s - loss: 1.5012Restoring model weights from the end of the best epoch: 1723.\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.5226 - val_loss: 1.0464\n",
            "Epoch 2723: early stopping\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 87ms/step - loss: 0.6941\n",
            "EXPECTED:\n",
            "   d18O_cel_mean  d18O_cel_variance\n",
            "0      25.882000           0.233670\n",
            "1      25.654000           0.334480\n",
            "2      27.207456           1.041823\n",
            "3      25.163333           0.807832\n",
            "4      24.030630           0.579233\n",
            "5      26.040923           0.222859\n",
            "\n",
            "PREDICTED:\n",
            "   d18O_cel_mean  d18O_cel_variance\n",
            "0      25.129105           1.809810\n",
            "1      25.098963           1.822455\n",
            "2      25.111458           1.816504\n",
            "3      24.711420           1.989376\n",
            "4      24.711418           1.989376\n",
            "5      24.726463           1.982928\n",
            "RMSE: 1.130164146106385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-07-12 21:44:34.743611: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [6,12]\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Grouped, fixed"
      ],
      "metadata": {
        "id": "opmE3xc0vcY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_fixed = {\n",
        "    'TRAIN': os.path.join(FP_ROOT, \"amazon_sample_data/uc_davis_2023_08_12_train_fixed_grouped.csv\"),\n",
        "    'TEST': os.path.join(FP_ROOT, \"amazon_sample_data/uc_davis_2023_08_12_test_fixed_grouped.csv\"),\n",
        "    'VALIDATION': os.path.join(FP_ROOT, \"amazon_sample_data/uc_davis_2023_08_12_validation_fixed_grouped.csv\"),\n",
        "}\n",
        "\n",
        "grouped_fixed_scaled = load_and_scale(grouped_fixed)\n",
        "train_and_evaluate(grouped_fixed_scaled, \"grouped_fixed\", training_batch_size=3)"
      ],
      "metadata": {
        "id": "KCebsA7XvfRH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}