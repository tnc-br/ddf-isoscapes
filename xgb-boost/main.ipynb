{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uyB5N9yngMo-"
      },
      "source": [
        "Copyright 2023 Google LLC.\n",
        "SPDX-License-Identifier: Apache-2.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4qCnFGsvx39s"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8kW8zuCdZ6W"
      },
      "outputs": [],
      "source": [
        "from osgeo import gdal, gdal_array\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib import rc\n",
        "from typing import List\n",
        "from numpy.random import MT19937, RandomState, SeedSequence\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from io import StringIO\n",
        "import xgboost as xgb\n",
        "import os\n",
        "import math\n",
        "import glob\n",
        "\n",
        "rc('animation', html='jshtml')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0tG92Yw1CYk"
      },
      "outputs": [],
      "source": [
        "# Raster directory. Contains:\n",
        "# iso_O_cellulose.tif: Isoscape of 18O from Precipitation; <-- MODELING TARGET\n",
        "# Iso_Oxi_Stack.tif: Isoscape of 18O from Precipitation; <-- Model input\n",
        "# R.rh_Stack.tif: Atmospheric Relative humidity <-- Model input\n",
        "# R.vpd_Stack.tif: Vapor Pressure Deficit - VPD <-- Model input\n",
        "# Temperature_Stack.tif: Atmospheric Temperature <-- Model input\n",
        "RASTER_BASE = \"/MyDrive/amazon_rainforest_files/amazon_rasters/\" #@param\n",
        "SAMPLE_DATA_BASE = \"/MyDrive/amazon_rainforest_files/amazon_sample_data/\" #@param\n",
        "TEST_DATA_BASE = \"/MyDrive/amazon_rainforest_files/amazon_test_data/\" #@param\n",
        "GDRIVE_BASE = \"/content/drive\" #@param\n",
        "\n",
        "REBUILD_MODEL = False #@param {type:\"boolean\"}\n",
        "MODEL_BASE = \"/MyDrive/amazon_rainforest_files/amazon_isoscape_models/\" #@param\n",
        "\n",
        "# How often should XGB log training metadata? 0 is the default, which indicates never.\n",
        "XGB_VERBOSITY_LEVEL = 0 #@param \n",
        "\n",
        "# Used to compute invalid terrain when making predictions. Leave disabled if on a low memory. \n",
        "LOAD_WATER_MASK_GEOTIFF = False #@param {type:\"boolean\"}\n",
        "LOAD_TREE_MASK_GEOTIFF = False #@param {type:\"boolean\"}\n",
        "\n",
        "# If true, requires soil and plant soil nitrogen geotiffs. Also requires the following files:\n",
        "# RASTER_BASE/raster_krig_d15N_soil_plant.tiff\n",
        "# RASTER_BASE/raster_krig_d15N_soil.tiff\n",
        "REGENERATE_PLANT_NITROGEN_GEOTIFF = False #@param {type:\"boolean\"}\n",
        "\n",
        "# If false, requires XGB oxygen isoscape in MODEL_BASE/predicted_isoscape_xgboost.tiff\n",
        "REGENERATE_XGB_OXYGEN_ISOSCAPE = False #@param {type:\"boolean\"}\n",
        "\n",
        "# If false, requires MODEL_BASE/xgb_means_oxygen_isoscape.tiff and MODEL_BASE/xgb_variances_oxygen_isoscape.tiff \n",
        "REGENERATE_OXYGEN_XGB_MEANS_VARIANCES = False #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Debugging\n",
        "# See https://zohaib.me/debugging-in-google-collab-notebook/ for tips,\n",
        "# as well as docs for pdb and ipdb.\n",
        "DEBUG = False #@param {type:\"boolean\"}\n",
        "if DEBUG:\n",
        "    %pip install -Uqq ipdb\n",
        "    import ipdb\n",
        "    %pdb on"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WlM3FIGXx6F2"
      },
      "source": [
        "# Data Types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtFtoKnsx83s"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class AmazonGeoTiff:\n",
        "  \"\"\"Represents a geotiff from our dataset.\"\"\"\n",
        "  gdal_dataset: gdal.Dataset\n",
        "  image_value_array: np.ndarray # ndarray of floats\n",
        "  image_mask_array: np.ndarray # ndarray of uint8\n",
        "  masked_image: np.ma.masked_array\n",
        "  yearly_masked_image: np.ma.masked_array\n",
        "\n",
        "@dataclass\n",
        "class Bounds:\n",
        "  \"\"\"Represents geographic bounds and size information.\"\"\"\n",
        "  minx: float\n",
        "  maxx: float\n",
        "  miny: float\n",
        "  maxy: float\n",
        "  pixel_size_x: float\n",
        "  pixel_size_y: float\n",
        "  raster_size_x: float\n",
        "  raster_size_y: float\n",
        "\n",
        "  def to_matplotlib(self) -> List[float]:\n",
        "    return [self.minx, self.maxx, self.miny, self.maxy]\n",
        "\n",
        "@dataclass\n",
        "class PartitionedDataset:\n",
        "  train: pd.DataFrame\n",
        "  test: pd.DataFrame\n",
        "  validation: pd.DataFrame"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use Global Params to access files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_raster_path(filename: str) -> str:\n",
        "  return f\"{GDRIVE_BASE}{RASTER_BASE}{filename}\"\n",
        "\n",
        "def get_model_path(filename: str) -> str:\n",
        "  return f\"{GDRIVE_BASE}{MODEL_BASE}{filename}\"\n",
        "\n",
        "def get_sample_db_path(filename: str) -> str:\n",
        "  return f\"{GDRIVE_BASE}{SAMPLE_DATA_BASE}{filename}\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C4eZ_EpvyAcu"
      },
      "source": [
        "## Utils for loading Rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b89tGCn8ztnO"
      },
      "outputs": [],
      "source": [
        "def print_raster_info(raster):\n",
        "  dataset = raster\n",
        "  print(\"Driver: {}/{}\".format(dataset.GetDriver().ShortName,\n",
        "                              dataset.GetDriver().LongName))\n",
        "  print(\"Size is {} x {} x {}\".format(dataset.RasterXSize,\n",
        "                                      dataset.RasterYSize,\n",
        "                                      dataset.RasterCount))\n",
        "  print(\"Projection is {}\".format(dataset.GetProjection()))\n",
        "  geotransform = dataset.GetGeoTransform()\n",
        "  if geotransform:\n",
        "      print(\"Origin = ({}, {})\".format(geotransform[0], geotransform[3]))\n",
        "      print(\"Pixel Size = ({}, {})\".format(geotransform[1], geotransform[5]))\n",
        "\n",
        "  for band in range(dataset.RasterCount):\n",
        "    band = dataset.GetRasterBand(band+1)\n",
        "    #print(\"Band Type={}\".format(gdal.GetDataTypeName(band.DataType)))\n",
        "\n",
        "    min = band.GetMinimum()\n",
        "    max = band.GetMaximum()\n",
        "    if not min or not max:\n",
        "        (min,max) = band.ComputeRasterMinMax(False)\n",
        "    #print(\"Min={:.3f}, Max={:.3f}\".format(min,max))\n",
        "\n",
        "    if band.GetOverviewCount() > 0:\n",
        "        print(\"Band has {} overviews\".format(band.GetOverviewCount()))\n",
        "\n",
        "    if band.GetRasterColorTable():\n",
        "        print(\"Band has a color table with {} entries\".format(band.GetRasterColorTable().GetCount()))\n",
        "\n",
        "def load_raster(path: str, use_only_band_index: int = -1) -> AmazonGeoTiff:\n",
        "  \"\"\"\n",
        "  TODO: Refactor (is_single_band, etc., should be a better design)\n",
        "  --> Find a way to simplify this logic. Maybe it needs to be more abstract.\n",
        "  \"\"\"\n",
        "  dataset = gdal.Open(path, gdal.GA_ReadOnly)\n",
        "  print_raster_info(dataset)\n",
        "  image_datatype = dataset.GetRasterBand(1).DataType\n",
        "  mask_datatype = dataset.GetRasterBand(1).GetMaskBand().DataType\n",
        "  image = np.zeros((dataset.RasterYSize, dataset.RasterXSize, 12),\n",
        "                  dtype=gdal_array.GDALTypeCodeToNumericTypeCode(image_datatype))\n",
        "  mask = np.zeros((dataset.RasterYSize, dataset.RasterXSize, 12),\n",
        "                  dtype=gdal_array.GDALTypeCodeToNumericTypeCode(image_datatype))\n",
        "\n",
        "  if use_only_band_index == -1:\n",
        "    if dataset.RasterCount != 12 and dataset.RasterCount != 1:\n",
        "      raise ValueError(f\"Expected 12 raster bands (one for each month) or one annual average, but found {dataset.RasterCount}\")\n",
        "    if dataset.RasterCount == 1:\n",
        "      use_only_band_index = 0\n",
        "\n",
        "  is_single_band = use_only_band_index != -1\n",
        "\n",
        "  if is_single_band and use_only_band_index >= dataset.RasterCount:\n",
        "    raise IndexError(f\"Specified raster band index {use_only_band_index}\"\n",
        "    f\" but there are only {dataset.RasterCount} rasters\")\n",
        "\n",
        "  for band_index in range(12):\n",
        "    band = dataset.GetRasterBand(use_only_band_index+1 if is_single_band else band_index+1)\n",
        "    image[:, :, band_index] = band.ReadAsArray()\n",
        "    mask[:, :, band_index] = band.GetMaskBand().ReadAsArray()\n",
        "  masked_image = np.ma.masked_where(mask == 0, image)\n",
        "  yearly_masked_image = masked_image.mean(axis=2)\n",
        "\n",
        "  return AmazonGeoTiff(dataset, image, mask, masked_image, yearly_masked_image)\n",
        "\n",
        "def get_extent(dataset):\n",
        "  geoTransform = dataset.GetGeoTransform()\n",
        "  minx = geoTransform[0]\n",
        "  maxy = geoTransform[3]\n",
        "  maxx = minx + geoTransform[1] * dataset.RasterXSize\n",
        "  miny = maxy + geoTransform[5] * dataset.RasterYSize\n",
        "  return Bounds(minx, maxx, miny, maxy, geoTransform[1], geoTransform[5], dataset.RasterXSize, dataset.RasterYSize)\n",
        "\n",
        "def plot_band(geotiff: AmazonGeoTiff, month_index, figsize=None):\n",
        "  if figsize:\n",
        "    plt.figure(figsize=figsize)\n",
        "  im = plt.imshow(geotiff.masked_image[:,:,month_index], extent=get_extent(geotiff.gdal_dataset).to_matplotlib(), interpolation='none')\n",
        "  plt.colorbar(im)\n",
        "\n",
        "def animate(geotiff: AmazonGeoTiff, nSeconds, fps):\n",
        "  fig = plt.figure( figsize=(8,8) )\n",
        "\n",
        "  months = []\n",
        "  labels = []\n",
        "  for m in range(12):\n",
        "    months.append(geotiff.masked_image[:,:,m])\n",
        "    labels.append(f\"Month: {m+1}\")\n",
        "  a = months[0]\n",
        "  extent = get_extent(geotiff.gdal_dataset).to_matplotlib()\n",
        "  ax = fig.add_subplot()\n",
        "  im = fig.axes[0].imshow(a, interpolation='none', aspect='auto', extent = extent)\n",
        "  txt = fig.text(0.3,0,\"\", fontsize=24)\n",
        "  fig.colorbar(im)\n",
        "\n",
        "  def animate_func(i):\n",
        "    if i % fps == 0:\n",
        "      print( '.', end ='' )\n",
        "\n",
        "    im.set_array(months[i])\n",
        "    txt.set_text(labels[i])\n",
        "    return [im, txt]\n",
        "\n",
        "  anim = animation.FuncAnimation(\n",
        "                                fig,\n",
        "                                animate_func,\n",
        "                                frames = nSeconds * fps,\n",
        "                                interval = 1000 / fps, # in ms\n",
        "                                )\n",
        "  plt.close()\n",
        "\n",
        "  return anim\n",
        "\n",
        "def save_numpy_to_geotiff(bounds: Bounds, prediction: np.ma.MaskedArray, path: str):\n",
        "  \"\"\"Copy metadata from a base geotiff and write raster data + mask from `data`\"\"\"\n",
        "  driver = gdal.GetDriverByName(\"GTiff\")\n",
        "  metadata = driver.GetMetadata()\n",
        "  if metadata.get(gdal.DCAP_CREATE) != \"YES\":\n",
        "      raise RuntimeError(\"GTiff driver does not support required method Create().\")\n",
        "  if metadata.get(gdal.DCAP_CREATECOPY) != \"YES\":\n",
        "      raise RuntimeError(\"GTiff driver does not support required method CreateCopy().\")\n",
        "\n",
        "  dataset = driver.Create(path, bounds.raster_size_x, bounds.raster_size_y, prediction.shape[2], eType=gdal.GDT_Float64)\n",
        "  dataset.SetGeoTransform([bounds.minx, bounds.pixel_size_x, 0, bounds.maxy, 0, bounds.pixel_size_y])\n",
        "  dataset.SetProjection('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433],AUTHORITY[\"EPSG\",\"4326\"]]')\n",
        "\n",
        "  #dataset = driver.CreateCopy(path, base.gdal_dataset, strict=0)\n",
        "  if len(prediction.shape) != 3 or prediction.shape[0] != bounds.raster_size_x or prediction.shape[1] != bounds.raster_size_y:\n",
        "    raise ValueError(\"Shape of prediction does not match base geotiff\")\n",
        "  #if prediction.shape[2] > base.gdal_dataset.RasterCount:\n",
        "  #  raise ValueError(f\"Expected fewer than {dataset.RasterCount} bands in prediction but found {prediction.shape[2]}\")\n",
        "\n",
        "  prediction_transformed = np.flip(np.transpose(prediction, axes=[1,0,2]), axis=0)\n",
        "  for band_index in range(dataset.RasterCount):\n",
        "    band = dataset.GetRasterBand(band_index+1)\n",
        "    if band.CreateMaskBand(0) == gdal.CE_Failure:\n",
        "      raise RuntimeError(\"Failed to create mask band\")\n",
        "    mask_band = band.GetMaskBand()\n",
        "    band.WriteArray(np.choose(prediction_transformed[:, :, band_index].mask, (prediction_transformed[:, :, band_index].data,np.array(band.GetNoDataValue()),)))\n",
        "    mask_band.WriteArray(np.logical_not(prediction_transformed[:, :, band_index].mask))\n",
        "\n",
        "def coords_to_indices(bounds: Bounds, x: float, y: float):\n",
        "  if x < bounds.minx or x > bounds.maxx or y < bounds.miny or y > bounds.maxy:\n",
        "    raise ValueError(\"Coordinates out of bounds\")\n",
        "\n",
        "  # X => lat, Y => lon\n",
        "  x_idx = bounds.raster_size_y - int(math.ceil((y - bounds.miny) / abs(bounds.pixel_size_y)))\n",
        "  y_idx = int((x - bounds.minx) / abs(bounds.pixel_size_x))\n",
        "\n",
        "  return x_idx, y_idx\n",
        "\n",
        "def test_coords_to_indices():\n",
        "  bounds = Bounds(50, 100, 50, 100, 1, 1, 50, 50)\n",
        "  x, y = coords_to_indices(bounds, 55, 55)\n",
        "  assert x == 45\n",
        "  assert y == 5\n",
        "\n",
        "  bounds = Bounds(-100, -50, -100, -50, 1, 1, 50, 50)\n",
        "  x, y = coords_to_indices(bounds, -55, -55)\n",
        "  assert x == 5\n",
        "  assert y == 45\n",
        "\n",
        "  bounds = Bounds(-10, 50, -10, 50, 1, 1, 60, 60)\n",
        "  x, y = coords_to_indices(bounds, -1, 13)\n",
        "  assert x == 37\n",
        "  assert y == 9\n",
        "\n",
        "  bounds = Bounds(minx=-73.97513931345594, maxx=-34.808472803053895, miny=-33.73347244751509, maxy=5.266527396029211, pixel_size_x=0.04166666650042771, pixel_size_y=-0.041666666499513144, raster_size_x=937, raster_size_y=941)\n",
        "  x, y = coords_to_indices(bounds, -67.14342073173958, -7.273271869467912e-05)\n",
        "  #print(x)\n",
        "  assert x == 131 # was: 132\n",
        "  assert y == 163\n",
        "\n",
        "test_coords_to_indices()\n",
        "\n",
        "def get_data_at_coords(dataset: AmazonGeoTiff, x: float, y: float, month: int) -> float:\n",
        "  # x = longitude\n",
        "  # y = latitude\n",
        "  bounds = get_extent(dataset.gdal_dataset)\n",
        "  x_idx, y_idx = coords_to_indices(bounds, x, y)\n",
        "  if month == -1:\n",
        "    value = dataset.yearly_masked_image[x_idx, y_idx]\n",
        "  else:\n",
        "    value = dataset.masked_image[x_idx, y_idx, month]\n",
        "  if np.ma.is_masked(value):\n",
        "    raise ValueError(\"Coordinates are masked\")\n",
        "  else:\n",
        "    return value"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Access data stored on Google Drive\n",
        "if GDRIVE_BASE:\n",
        "    from google.colab import drive\n",
        "    drive.mount(GDRIVE_BASE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PI3Enxboe1u"
      },
      "outputs": [],
      "source": [
        "brazil_map_geotiff = load_raster(get_raster_path(\"brasil_clim_raster.tiff\")) # mean annual precipitation\n",
        "# Will be used to compute isoscapes for carbon and nitrogen\n",
        "\n",
        "relative_humidity_geotiff = load_raster(get_raster_path(\"R.rh_Stack.tif\"))\n",
        "temperature_geotiff = load_raster(get_raster_path(\"Temperatura_Stack.tif\"))\n",
        "vapor_pressure_deficit_geotiff = load_raster(get_raster_path(\"R.vpd_Stack.tif\"))\n",
        "atmosphere_isoscape_geotiff = load_raster(get_raster_path(\"Iso_Oxi_Stack.tif\"))\n",
        "cellulose_isoscape_geotiff = load_raster(get_raster_path(\"iso_O_cellulose.tif\"))\n",
        "\n",
        "# Soil Geotiffs are not necessary to load, but required to build plant nitrogen geotiff.\n",
        "soil_plant_nitrogen_difference_isoscape_geotiff = load_raster(get_raster_path(\"raster_krig_d15N_soil_plant.tiff\"))\n",
        "soil_nitrogen_isoscape_geotiff = load_raster(get_raster_path(\"raster_krig_d15N_soil.tiff\"))\n",
        "plant_nitrogen_isoscape_geotiff = load_raster(get_raster_path(\"plant_nitrogen_isoscape.tiff\"))\n",
        "\n",
        "carbon_means_krig_isoscape_geotiff = load_raster(get_raster_path(\"Brasil_Raster_Krig_iso_d13C.tiff\"))\n",
        "\n",
        "land_water_mask_geotiff = load_raster(get_raster_path(\"Land_Water_Brazil_MODIS.tif\")) if LOAD_WATER_MASK_GEOTIFF else None\n",
        "possible_tree_mask_geotiff = load_raster(get_raster_path(\"Possible_Trees_Brazil_MODIS.tif\")) if LOAD_TREE_MASK_GEOTIFF else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwrDKcwmIu4f"
      },
      "outputs": [],
      "source": [
        "if REGENERATE_PLANT_NITROGEN_GEOTIFF:\n",
        "  plant_nitrogen_array = soil_nitrogen_isoscape_geotiff.yearly_masked_image - soil_plant_nitrogen_difference_isoscape_geotiff.yearly_masked_image\n",
        "  save_numpy_to_geotiff(soil_plant_nitrogen_difference_isoscape_geotiff,\n",
        "    np.expand_dims(np.flip(plant_nitrogen_array.T, axis=1), axis=2),\n",
        "    get_raster_path(\"plant_nitrogen_isoscape.tiff\"))\n",
        "plant_nitrogen_isoscape_geotiff = load_raster(get_raster_path(\"plant_nitrogen_isoscape.tiff\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3-CHLVHQyD9U"
      },
      "source": [
        "# Train Isoscape Models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YTieS4NxKetw"
      },
      "source": [
        "## Preprocess\n",
        "\n",
        "Sample data from Martinelli's map of measurement sites to train fake isoscape models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNXqjSsTsMYy"
      },
      "outputs": [],
      "source": [
        "def gen_tabular_dataset(monthly: bool, samples_per_site: int) -> pd.DataFrame:\n",
        "  sample_site_coordinates = [(-70,-5,),(-67.5,0,),(-66,-4.5,),(-63,-9.5,),(-63,-9,),(-62,-6,),(-60,-2.5,),(-60,1,),(-60,-12.5,),(-59,-2.5,),(-57.5,-4,),(-55,-3.5,),(-54,-1,),(-52.5,-13,),(-51.5,-2.5,)]\n",
        "  sample_radius = 0.5\n",
        "  features = [relative_humidity_geotiff, temperature_geotiff, vapor_pressure_deficit_geotiff, atmosphere_isoscape_geotiff, cellulose_isoscape_geotiff]\n",
        "  image_feature_names = [\"rh\", \"temp\", \"vpd\", \"atmosphere_oxygen_ratio\", \"cellulose_oxygen_ratio\"]\n",
        "  feature_names = [\"lat\", \"lon\", \"month_of_year\"] + image_feature_names\n",
        "  rs = RandomState(MT19937(SeedSequence(42)))\n",
        "\n",
        "  feature_values = {}\n",
        "  for name in feature_names:\n",
        "    feature_values[name] = []\n",
        "\n",
        "  for coord in tqdm(sample_site_coordinates):\n",
        "    month_start = 0 if monthly else -1\n",
        "    month_end = 12 if monthly else 0\n",
        "    for month in range(month_start, month_end):\n",
        "      samples_collected = 0\n",
        "      while samples_collected < samples_per_site:\n",
        "        row = {}\n",
        "        sample_x, sample_y = 2*(rs.rand(2) - 0.5) * sample_radius\n",
        "        sample_x += coord[0]\n",
        "        sample_y += coord[1]\n",
        "\n",
        "        try:\n",
        "          for feature, feature_name in zip(features, image_feature_names):\n",
        "            row[feature_name] = get_data_at_coords(feature, sample_x, sample_y, month)\n",
        "          row[\"month_of_year\"] = month\n",
        "          row[\"lon\"] = sample_x\n",
        "          row[\"lat\"] = sample_y\n",
        "          samples_collected += 1\n",
        "        except ValueError:\n",
        "          # masked and out-of-bounds coordinates\n",
        "          continue\n",
        "        for key, value in row.items():\n",
        "          feature_values[key].append(value)\n",
        "\n",
        "  samples = pd.DataFrame(feature_values)\n",
        "\n",
        "  if not monthly:\n",
        "    samples.drop(\"month_of_year\", axis=1, inplace=True)\n",
        "\n",
        "  return samples\n",
        "\n",
        "monthly_data_large = gen_tabular_dataset(monthly=True, samples_per_site=30)\n",
        "monthly_data_255_trees = gen_tabular_dataset(monthly=True, samples_per_site=17)\n",
        "yearly_data_large = gen_tabular_dataset(monthly=False, samples_per_site=30*12)\n",
        "yearly_data_255_trees = gen_tabular_dataset(monthly=False, samples_per_site=17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0dAH2eCmegR"
      },
      "outputs": [],
      "source": [
        "leaf_data = pd.read_csv(get_sample_db_path(\"pontos-vasp-cluster.csv\"))\n",
        "leaf_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eitP1hjX335"
      },
      "outputs": [],
      "source": [
        "def load_leaf_dataframe(db_path: str, isotope_col: str):\n",
        "  leaf_data = pd.read_csv(db_path)\n",
        "  leaf_data = leaf_data.rename(columns={\"latitude\": \"lat\", \"longitude\": \"lon\"})\n",
        "  leaf_df = leaf_data[[\"lon\", \"lat\", \"MAP\", \"MAT\", \"vap\", \"d15N_soil\", \"dem\", \"pa\", \"pet\", \"ph\", isotope_col]]\n",
        "  return leaf_df\n",
        "\n",
        "carbon_df = load_leaf_dataframe(get_sample_db_path(\"pontos-vasp-cluster.csv\"), \"d13C\") \n",
        "nitrogen_df = load_leaf_dataframe(get_sample_db_path(\"pontos-vasp-cluster.csv\"), \"d15N\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0EW3f_q_sQbL"
      },
      "source": [
        "### Partition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX1m5XvIsPyH"
      },
      "outputs": [],
      "source": [
        "def partition(df) -> PartitionedDataset:\n",
        "  train = df[df[\"lon\"] < -55]\n",
        "  test = df[(df[\"lon\"] >= -55) & (df[\"lat\"] > -2.85)]\n",
        "  validation = df[(df[\"lon\"] >= -55) & (df[\"lat\"] <= -2.85)]\n",
        "  return PartitionedDataset(train, test, validation)\n",
        "\n",
        "def print_split(dataset: PartitionedDataset) -> None:\n",
        "  total_len = len(dataset.train)+len(dataset.validation)+len(dataset.test)\n",
        "  print(f\"Train: {100*len(dataset.train)/total_len:.2f}% ({len(dataset.train)})\")\n",
        "  print(f\"Test: {100*len(dataset.test)/total_len:.2f}% ({len(dataset.test)})\")\n",
        "  print(f\"Validation: {100*len(dataset.validation)/total_len:.2f}% ({len(dataset.validation)})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FEXJ9fQtLM9"
      },
      "outputs": [],
      "source": [
        "yearly_large_partitioned = partition(yearly_data_large)\n",
        "print_split(yearly_large_partitioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DKNx1ckzaYN"
      },
      "outputs": [],
      "source": [
        "yearly_255_trees_partitioned = partition(yearly_data_255_trees)\n",
        "print_split(yearly_255_trees_partitioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06mCyO82w_n7"
      },
      "outputs": [],
      "source": [
        "monthly_large_partitioned = partition(monthly_data_large)\n",
        "print_split(monthly_large_partitioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9DQikZExELq"
      },
      "outputs": [],
      "source": [
        "monthly_255_trees_partitioned = partition(monthly_data_255_trees)\n",
        "print_split(monthly_255_trees_partitioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVTzinGguSFj"
      },
      "outputs": [],
      "source": [
        "nitrogen_df_partitioned = partition(nitrogen_df)\n",
        "print_split(nitrogen_df_partitioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T73lHHC9u6ol"
      },
      "outputs": [],
      "source": [
        "carbon_df_partitioned = partition(carbon_df)\n",
        "print_split(carbon_df_partitioned)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aIu9xdqPBI_v"
      },
      "source": [
        "## XGBoost: Train XGBoost Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyjRvlzG1J1m"
      },
      "outputs": [],
      "source": [
        "def train_xgb(data: PartitionedDataset, booster: str, rounds: int) -> xgb.XGBRegressor:\n",
        "  xgb_model = xgb.XGBRegressor(n_estimators=rounds, eta=0.1, max_depth=2, objective='reg:squarederror', booster=booster)\n",
        "  # split data into input and output columns\n",
        "  X, y = data.train.iloc[:, :-1], data.train.iloc[:, -1]\n",
        "  X_val, y_val = data.validation.iloc[:, :-1], data.validation.iloc[:, -1]\n",
        "  print(f\"Predicting: {data.train.columns[-1]}\")\n",
        "  xgb_model.fit(X, y, eval_set=[(X_val, y_val)], verbose=XGB_VERBOSITY_LEVEL)\n",
        "  return xgb_model\n",
        "\n",
        "def train_or_load_xgboost(basename: str, data: PartitionedDataset, rounds: int=100000):\n",
        "  if REBUILD_MODEL: \n",
        "    print(\"Training model\")\n",
        "    model = train_xgb(data, booster='gblinear', rounds=rounds)\n",
        "    with open(f\"{basename}_config_xgb.json\", \"w\") as f:\n",
        "      f.write(model.get_booster().save_config())\n",
        "    model.save_model(f\"{basename}_xgb.json\")\n",
        "  else:\n",
        "    print(\"Loading model\")\n",
        "    model = xgb.XGBRegressor()\n",
        "    model.load_model(f\"{basename}_xgb.json\")\n",
        "    with open(f\"{basename}_config_xgb.json\", \"r\") as f:\n",
        "      model.get_booster().load_config(f.read())\n",
        "  print(f\"RMSE (validation): {model.evals_result()['validation_0']['rmse'][-1]}\")\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validation RMSE xgboost: 0.306059 w/ 100,000 rounds\n",
        "# Validation RMSE google internal tooling: 0.39386\n",
        "yearly_255_trees_xgb_model = train_or_load_xgboost(\n",
        "  get_model_path(\"oxygen_isoscape_model\"),\n",
        "  yearly_255_trees_partitioned,\n",
        "  rounds=100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKHBDwd0w3SD"
      },
      "outputs": [],
      "source": [
        "# HMM, post-bugfix, Carbon might diverge too.\n",
        "carbon_isoscape_model = train_or_load_xgboost(get_model_path(\"carbon_isoscape_model\"), carbon_df_partitioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y56PuvBSySRG"
      },
      "outputs": [],
      "source": [
        "# Validation loss seems to diverge\n",
        "nitrogen_isoscape_model = train_or_load_xgboost(get_model_path(\"nitrogen_isoscape_model\"), nitrogen_df_partitioned, rounds=10000)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wLigqWVAEGiP"
      },
      "source": [
        "### Test XGBoost Model Code\n",
        "\n",
        "Test data created as follows:\n",
        "```python\n",
        "# Create data for unit tests\n",
        "from io import StringIO\n",
        "\n",
        "train_text = StringIO()\n",
        "yearly_255_trees_partitioned.validation.iloc[:10].to_csv(train_text, index=False)\n",
        "print(train_text.getvalue())\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLXAbth_CgDf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def create_test_data():\n",
        "  train_txt = \"\"\"lat,lon,rh,temp,vpd,atmosphere_oxygen_ratio,cellulose_oxygen_ratio\n",
        "  -4.880332787307218,-69.95800610699372,0.8044400215148926,26.225001017252605,0.6966667175292969,-4.451689084370931,37.122222900390625\n",
        "  -4.688096349322666,-70.44263021829333,0.80293075243632,26.35833485921224,0.7074999809265137,-4.41741943359375,37.17825063069662\n",
        "  -4.872397683046066,-69.63990597562567,0.8054693539937338,26.308329264322918,0.6958333651224772,-4.417543411254883,37.1220448811849\n",
        "  -4.8247274690858735,-69.81806665464333,0.8040264447530111,26.308331807454426,0.7016665935516357,-4.424846013387044,37.14974721272787\n",
        "  -4.765274838163909,-70.01923594475969,0.8022874991099039,26.337496439615887,0.709166685740153,-4.418321291605632,37.20004526774088\n",
        "  -4.771462642125715,-70.34365888186157,0.8011360963185629,26.508333841959637,0.7199999491373698,-4.385458946228027,37.247047424316406\n",
        "  -4.798305195940103,-70.28306090761369,0.802595059076945,26.366666158040363,0.709166685740153,-4.411936124165853,37.20568339029948\n",
        "  -5.223217462581197,-69.53591146126529,0.8077573776245117,26.10833231608073,0.6808333396911621,-4.4716800053914385,37.02557881673177\n",
        "  -4.613341938104102,-69.7943386465883,0.8022151788075765,26.400001525878906,0.7116666634877523,-4.423205057779948,37.17969512939453\n",
        "  -4.527212807226629,-69.8817482523093,0.8018482526143392,26.4499994913737,0.7149999936421713,-4.395961443583171,37.22857411702474\"\"\"\n",
        "  train_df = pd.read_csv(StringIO(train_txt))\n",
        "\n",
        "  val_txt = \"\"\"lat,lon,rh,temp,vpd,atmosphere_oxygen_ratio,cellulose_oxygen_ratio\n",
        "  -3.6696957825007046,-54.87948135669049,0.8142155011494955,25.770833333333332,0.6483333110809326,-3.3461217880249023,38.01644388834635\n",
        "  -3.8993546066489224,-54.86859101648585,0.805713415145874,26.149998982747395,0.6933333079020182,-3.261778195699056,38.26288604736328\n",
        "  -3.159593531700783,-54.93297940204632,0.819889227549235,25.912501017252605,0.6333333253860474,-3.287173271179199,37.95276896158854\n",
        "  -3.9960354096696906,-54.87203413927777,0.8026766777038574,26.149998982747395,0.7041666507720947,-3.2518555323282876,38.3813222249349\n",
        "  -3.80255400058822,-54.751942535999156,0.8073338667551676,26.054166158040363,0.6833333174387614,-3.302551587422689,38.1731923421224\n",
        "  -12.828852720078402,-52.607319143523036,0.7082154750823975,25.258333841959637,1.0200000603993733,-3.547501564025879,39.927050272623696\n",
        "  -12.532258968752565,-52.097391445126696,0.7110532919565836,25.520833333333332,1.019166628519694,-3.43365478515625,40.047627766927086\n",
        "  -13.427351375947753,-52.060761037543834,0.7014106909434,24.8249994913737,1.005833387374878,-3.589900334676107,40.11137390136719\n",
        "  -13.349866138079692,-52.65445230256682,0.7073808511098226,24.958333333333332,1.00083327293396,-3.5771010716756186,39.98369598388672\n",
        "  -12.730453380778542,-52.375592581693155,0.7120146751403809,25.2375005086263,1.0024999777475994,-3.494396209716797,40.02317810058594\"\"\"\n",
        "  val_df = pd.read_csv(StringIO(val_txt))\n",
        "\n",
        "  test_df = pd.DataFrame()\n",
        "\n",
        "  return PartitionedDataset(train=train_df, test=test_df, validation=val_df)\n",
        "\n",
        "# This function override REBUILD_MODEL for testing.\n",
        "def test_train_or_load_xgboost__load_succeeds():\n",
        "  for f in glob.glob(\"/tmp/foobar_model*\"):\n",
        "    os.remove(f)\n",
        "\n",
        "  # TODO: Probably better to have a function to load xgboost models instead of train and load sharing a function.\n",
        "  global REBUILD_MODEL\n",
        "  REBUILD_MODEL_tmp = REBUILD_MODEL\n",
        "  REBUILD_MODEL = True\n",
        "  model_under_test = yearly_255_trees_xgb_model = train_or_load_xgboost(\"/tmp/foobar_model\", create_test_data(), rounds=100)\n",
        "\n",
        "  final_loss = model_under_test.evals_result()['validation_0']['rmse'][-1]\n",
        "  initial_loss = model_under_test.evals_result()['validation_0']['rmse'][0]\n",
        "  assert final_loss < initial_loss\n",
        "\n",
        "  original_prediction = model_under_test.predict(pd.DataFrame(np.array([[1,2,3,4,5,6]], dtype=float), columns=['lat', 'lon', 'rh', 'temp', 'vpd', 'atmosphere_oxygen_ratio']))[0]\n",
        "  \n",
        "  REBUILD_MODEL = False\n",
        "  model_under_test = train_or_load_xgboost(\"/tmp/foobar_model\", create_test_data(), rounds=100)\n",
        "  REBUILD_MODEL = REBUILD_MODEL_tmp\n",
        "  \n",
        "  loaded_prediction = model_under_test.predict(pd.DataFrame(np.array([[1,2,3,4,5,6]], dtype=float), columns=['lat', 'lon', 'rh', 'temp', 'vpd', 'atmosphere_oxygen_ratio']))[0]\n",
        "  assert original_prediction == loaded_prediction\n",
        "  \n",
        "\n",
        "test_train_or_load_xgboost__load_succeeds()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UN9XREX4-zc-"
      },
      "source": [
        "**We also trained a model assuming 255 trees sampled monthly.**\n",
        "\n",
        "Preserving this as text only because it is not realistic as of 2023.\n",
        "\n",
        "Validation RMSE xgboost: 0.29072 \\\n",
        "Validation RMSE Google internal tooling: 0.29183 \\\n",
        "`monthly_255_trees_xgb_model = train_xgb(monthly_255_trees_partitioned, booster='gbtree', rounds=15000)`\n",
        "\n",
        "For the best results here, add `max_depth=2` to XGBRegressor params."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U_2b5nUNxhnB"
      },
      "source": [
        "# Data Validation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OxBjEf3iolSI"
      },
      "source": [
        "Do we use coordinates correctly?\n",
        "Ideally, we should create a sample image and make this a unit test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqebAozxoWKZ"
      },
      "outputs": [],
      "source": [
        "get_data_at_coords(relative_humidity_geotiff, -65, -5, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJFqC7iAoqZ9"
      },
      "outputs": [],
      "source": [
        "get_data_at_coords(relative_humidity_geotiff, -43, -10, 0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8hmczyhjomik"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3vwGZ-nP5FS"
      },
      "source": [
        "### GeoTIFFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnV5YP3uQ90j"
      },
      "outputs": [],
      "source": [
        "animate(land_water_mask_geotiff, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R55iMGRtJY7_"
      },
      "outputs": [],
      "source": [
        "animate(soil_nitrogen_isoscape_geotiff, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-pzCaR_OdEe"
      },
      "outputs": [],
      "source": [
        "animate(soil_plant_nitrogen_difference_isoscape_geotiff, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkiwF1_-Jqt6"
      },
      "outputs": [],
      "source": [
        "animate(plant_nitrogen_isoscape_geotiff, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf3S_mf08er0"
      },
      "outputs": [],
      "source": [
        "animate(relative_humidity_geotiff, 12, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnxur_QP4GEx"
      },
      "outputs": [],
      "source": [
        "animate(temperature_geotiff, 12, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt8rwiw14L25"
      },
      "outputs": [],
      "source": [
        "animate(vapor_pressure_deficit_geotiff, 12, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMYb_NWv4e0f"
      },
      "outputs": [],
      "source": [
        "animate(atmosphere_isoscape_geotiff, 12, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPaF1Is7-_3e"
      },
      "outputs": [],
      "source": [
        "animate(cellulose_isoscape_geotiff, 12, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoTYPwVGkWgb"
      },
      "outputs": [],
      "source": [
        "animate(relative_humidity_geotiff, 12, 1).save('/usr/local/google/home/nicholasroth/amazon_rainforest_gifs/relative_humidity.gif', writer='imagemagick', fps=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYLoEpMpkcqY"
      },
      "outputs": [],
      "source": [
        "animate(temperature_geotiff, 12, 1).save('/usr/local/google/home/nicholasroth/amazon_rainforest_gifs/temperature.gif', writer='imagemagick', fps=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KizIXcRAkemP"
      },
      "outputs": [],
      "source": [
        "animate(vapor_pressure_deficit_geotiff, 12, 1).save('/usr/local/google/home/nicholasroth/amazon_rainforest_gifs/vapor_pressure_deficit.gif', writer='imagemagick', fps=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taQx0Pe65Eu_"
      },
      "outputs": [],
      "source": [
        "animate(atmosphere_isoscape_geotiff, 12, 1).save('/usr/local/google/home/nicholasroth/amazon_rainforest_gifs/atmospheric_isoscape.gif', writer='imagemagick', fps=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGYcBrfJccBL"
      },
      "outputs": [],
      "source": [
        "animate(cellulose_isoscape_geotiff, 12, 1).save('/usr/local/google/home/nicholasroth/amazon_rainforest_gifs/cellulose_isoscape.gif', writer='imagemagick', fps=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXcLSWMs4430"
      },
      "outputs": [],
      "source": [
        "# Make sure this is Gaussian for the next step\n",
        "_ = plt.hist(cellulose_isoscape_geotiff.yearly_masked_image.data[cellulose_isoscape_geotiff.yearly_masked_image.mask == False], bins=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-tE4aDG6SVN"
      },
      "outputs": [],
      "source": [
        "_ = plt.hist(cellulose_isoscape_geotiff.masked_image.data[cellulose_isoscape_geotiff.masked_image.mask == False], bins=100)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VnuIe2bi67TC"
      },
      "source": [
        "If we squint, the distribution of monthly samples (bottom) looks like it could be Gaussian. NO WAY for the annual means (top). Additionally, when we measure an individual cellulose sample, we are sampling this monthly distribution, not a yearly distribution of means (which has an artificially lower std dev). For these reasons, we will compare samples against monthly point-in-time measurements instead of yearly means.\n",
        "\n",
        "\n",
        "**When we capture real training data, it will be important to also capture corresponding point-in-time measurements at the same location so our z-scores are coming from the same distribution.**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4tfIYeF_fXUx"
      },
      "source": [
        "## Investigate Tree Samples\n",
        "\n",
        "* Do they fit the Craig-Gordon model?\n",
        "* If so, how well?\n",
        "* RMSE, r, r^2, variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b10T4XuGfWUL"
      },
      "outputs": [],
      "source": [
        "samples = pd.read_csv(\"/usr/local/google/home/nicholasroth/Existing Samples - Jamari1_flona_tapajos_18sampes_Nicholas.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZYC4tbpuQJo"
      },
      "outputs": [],
      "source": [
        "plt.title(\"Expected Values (Craig-Gordon)\")\n",
        "im = plt.imshow(cellulose_isoscape_geotiff.yearly_masked_image,\n",
        "                extent=get_extent(cellulose_isoscape_geotiff.gdal_dataset).to_matplotlib(), interpolation='none')\n",
        "_ = plt.colorbar(im)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jrP7043i7Pj"
      },
      "outputs": [],
      "source": [
        "expected_values = []\n",
        "actual_values = []\n",
        "for _, row in samples.iterrows():\n",
        "  actual_value = float(row['sample_value'])\n",
        "  monthly_craig_gordon_value = get_data_at_coords(cellulose_isoscape_geotiff, row[\"long\"], row[\"lat\"], int(row[\"date\"].split(\"-\")[1])-1)\n",
        "  craig_gordon_values = []\n",
        "  for i in range(12):\n",
        "    craig_gordon_values.append(get_data_at_coords(cellulose_isoscape_geotiff, row[\"long\"], row[\"lat\"], i))\n",
        "  expected_craig_gordon_value = np.mean(craig_gordon_values)\n",
        "  expected_values.append(expected_craig_gordon_value)\n",
        "  actual_values.append(actual_value)\n",
        "  if False:\n",
        "    print(f\"Monthly: {monthly_craig_gordon_value:.05f}\")\n",
        "    print(f\"Expected: {expected_craig_gordon_value:.05f}\")\n",
        "    print(f\"Actual: {actual_value:.05f}\")\n",
        "    print()\n",
        "\n",
        "residuals = np.array(actual_values)-np.array(expected_values)\n",
        "rmse = np.sqrt(np.mean(residuals**2))\n",
        "r = np.corrcoef(np.array(actual_values), np.array(expected_values))\n",
        "print(f\"RMSE: {rmse:.05f}\")\n",
        "print(f\"r = {r[1,0]:.05f}\")\n",
        "print(f\"r^2 = {r[1,0]**2:.05f}\")\n",
        "\n",
        "plt.title(\"Expected (Craig-Gordon) vs Actual Sample Values w/ expected=actual Line\")\n",
        "plt.scatter(actual_values, expected_values)\n",
        "_ = plt.plot([min(actual_values), max(actual_values)],\n",
        "         [min(actual_values), max(actual_values)], color='black')\n",
        "plt.show()\n",
        "\n",
        "plt.title(\"Expected (Craig-Gordon) vs Actual Sample Values w/ Regression Line\")\n",
        "plt.scatter(actual_values, expected_values)\n",
        "b, a = np.polyfit(actual_values, expected_values, deg=1)\n",
        "xseq = np.linspace(min(actual_values), max(actual_values), num=100)\n",
        "plt.plot(xseq, a + b * xseq, color=\"k\", lw=2.5);\n",
        "plt.show()\n",
        "\n",
        "plt.title(\"Residuals\")\n",
        "_ = plt.hist(residuals, bins=5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn1bp9YzJXDW"
      },
      "source": [
        "# Compute Isoscapes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wSSdzTLbfBWS"
      },
      "source": [
        "## XGBoost: Compute AI-Predicted Isoscape\n",
        "\n",
        "Required: REGENERATE_OXYGEN_ISOSCAPE == true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBUsHhz2n51q"
      },
      "outputs": [],
      "source": [
        "def get_xgb_isoscape_prediction():\n",
        "  bounds = get_extent(cellulose_isoscape_geotiff.gdal_dataset)\n",
        "  features = [relative_humidity_geotiff, temperature_geotiff, vapor_pressure_deficit_geotiff, atmosphere_isoscape_geotiff]\n",
        "  image_feature_names = [\"rh\", \"temp\", \"vpd\", \"atmosphere_oxygen_ratio\"]\n",
        "  #feature_names = [\"lat\", \"lon\", \"month_of_year\"] + image_feature_names\n",
        "  feature_names = [\"lat\", \"lon\"] + image_feature_names\n",
        "  predicted_isoscape = np.ma.array(np.zeros([bounds.raster_size_x, bounds.raster_size_y, 1], dtype=float), mask=np.ones([bounds.raster_size_x, bounds.raster_size_y, 1], dtype=bool))\n",
        "\n",
        "  for x_idx, x in enumerate(tqdm(np.arange(bounds.minx, bounds.maxx, bounds.pixel_size_x, dtype=float))):\n",
        "    rows = []\n",
        "    row_indexes = []\n",
        "    for y_idx, y in enumerate(np.arange(bounds.miny, bounds.maxy, -bounds.pixel_size_y, dtype=float)):\n",
        "      #for month in range(12):\n",
        "      month = 0\n",
        "      row = {}\n",
        "      try:\n",
        "        for feature, feature_name in zip(features, image_feature_names):\n",
        "          row[feature_name] = get_data_at_coords(feature, x, y, month)\n",
        "        #row[\"month_of_year\"] = month\n",
        "        row[\"lon\"] = x\n",
        "        row[\"lat\"] = y\n",
        "      except ValueError:\n",
        "        # masked and out-of-bounds coordinates\n",
        "        continue\n",
        "      except IndexError:\n",
        "        continue\n",
        "      rows.append(row)\n",
        "      row_indexes.append((y_idx,month,))\n",
        "    if (len(rows) > 0):\n",
        "      reordered = pd.DataFrame(rows)[yearly_255_trees_xgb_model.get_booster().feature_names]\n",
        "      predictions = yearly_255_trees_xgb_model.predict(reordered)\n",
        "      predictions_np = predictions\n",
        "      for prediction, (y_idx, month_idx) in zip(predictions_np, row_indexes):\n",
        "        predicted_isoscape.mask[x_idx,y_idx,month_idx] = False # unmask since we have data\n",
        "        predicted_isoscape.data[x_idx,y_idx,month_idx] = prediction\n",
        "\n",
        "  return predicted_isoscape\n",
        "\n",
        "if REGENERATE_OXYGEN_XGB_ISOSCAPE:\n",
        "  xgb_isoscape_prediction = get_xgb_isoscape_prediction()\n",
        "  save_numpy_to_geotiff(get_extent(cellulose_isoscape_geotiff.gdal_dataset), xgb_isoscape_prediction, get_model_path(\"predicted_isoscape_xgboost.tiff\"))\n",
        "  plt.imshow(xgb_isoscape_prediction)\n",
        "\n",
        "# TODO: TESTME!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kuExDB3lEXV9"
      },
      "source": [
        "## Turn XGBoost isoscape into a Gaussian distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igc1BbzJEW7b"
      },
      "outputs": [],
      "source": [
        "predicted_cellulose_isoscape_geotiff = load_raster(get_model_path(\"predicted_isoscape_xgboost.tiff\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLOJhVNqFUV_"
      },
      "outputs": [],
      "source": [
        "plt.imshow(predicted_cellulose_isoscape_geotiff.yearly_masked_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jen3RErXobbq"
      },
      "outputs": [],
      "source": [
        "get_data_at_coords(predicted_cellulose_isoscape_geotiff, -72, -31, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YN8UnykFXs5"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "def get_2d_gaussian(center_lon: float, center_lat: float, stdev: float):\n",
        "  \"\"\"Quick-and-dirty function to get a PDF for sampling from an image\n",
        "  to turn it into a distribution. Intended for use with isoscapes.\n",
        "\n",
        "  Major room for improvement! This framing assumes no distortion from the\n",
        "  projection, i.e. that 1 deg latitude == 1 deg longitude == 111 km everywhere.\n",
        "  This should probably be fine for Brazil, for now, since it's near the Equator.\n",
        "  \"\"\"\n",
        "  rv = multivariate_normal([center_lat, center_lon], [[stdev, 0], [0, stdev]])\n",
        "\n",
        "  return rv\n",
        "\n",
        "# x = longitude\n",
        "# y = latitude\n",
        "def plot_gaussian(rv, shape: Bounds):\n",
        "  \"\"\"Informative, for debugging and visualizing get_2d_gaussian().\"\"\"\n",
        "  x = np.linspace(shape.minx, shape.maxx, shape.raster_size_x)\n",
        "  y = np.linspace(shape.maxy, shape.miny, shape.raster_size_y) # inverted y axis\n",
        "  X, Y = np.meshgrid(x,y)\n",
        "  target = np.empty((shape.raster_size_y, shape.raster_size_x,2,), dtype=float)\n",
        "  target[:, :, 0] = Y\n",
        "  target[:, :, 1] = X\n",
        "  pd = rv.pdf(target)\n",
        "  plt.imshow(pd)\n",
        "  plt.colorbar()\n",
        "\n",
        "# TODO: For each pixel in the predicted isoscape\n",
        "# Yeah, this is basically Gaussian blur, huh...\n",
        "# BUT, it does give us a distribution.\n",
        "def gaussian_kernel(input: AmazonGeoTiff, stdev_in_degrees: float=1):\n",
        "  bounds = get_extent(input.gdal_dataset)\n",
        "  means = np.ma.zeros((bounds.raster_size_x, bounds.raster_size_y,), dtype=float)\n",
        "  means.mask = np.ones((bounds.raster_size_x, bounds.raster_size_y), dtype=bool)\n",
        "  variances = np.ma.zeros((bounds.raster_size_x, bounds.raster_size_y,), dtype=float)\n",
        "  variances.mask = np.ones((bounds.raster_size_x, bounds.raster_size_y), dtype=bool)\n",
        "  for map_y in tqdm(range(0, bounds.raster_size_y, 1)):\n",
        "    y_coord = map_y * abs(bounds.pixel_size_y) + bounds.miny\n",
        "    for map_x in range(0, bounds.raster_size_x, 1):\n",
        "      x_coord = map_x * abs(bounds.pixel_size_x) + bounds.minx\n",
        "      rv = get_2d_gaussian(y_coord, x_coord, stdev_in_degrees)\n",
        "      rsamp = rv.rvs(1000)\n",
        "      values = []\n",
        "      for coordinate_pair in rsamp:\n",
        "        try:\n",
        "          #print(coordinate_pair)\n",
        "          values.append(get_data_at_coords(input, coordinate_pair[0], coordinate_pair[1], 0))\n",
        "        except ValueError:\n",
        "          pass\n",
        "        if len(values) == 30:\n",
        "          break\n",
        "      if len(values) == 30:\n",
        "        # Set the mean and stdev pixels\n",
        "        #print(x_coord, y_coord)\n",
        "        means[map_x, map_y] = np.mean(values)\n",
        "        variances[map_x, map_y] = np.var(values)\n",
        "        # Apply sample corrective factor to variance\n",
        "        variances[map_x, map_y] *= len(values) / (len(values)-1)\n",
        "        means.mask[map_x, map_y] = False\n",
        "        variances.mask[map_x, map_y] = False\n",
        "  return means, variances\n",
        "\n",
        "rv = get_2d_gaussian(-60.16, 4.11, 1)\n",
        "plot_gaussian(rv, get_extent(predicted_cellulose_isoscape_geotiff.gdal_dataset))\n",
        "\n",
        "if REGENERATE_OXYGEN_XGB_MEANS_VARIANCES:\n",
        "  xgb_means, xgb_variances = gaussian_kernel(predicted_cellulose_isoscape_geotiff, stdev_in_degrees=0.1)\n",
        "  save_numpy_to_geotiff(bds, np.expand_dims(xgb_means, axis=2), get_model_path(\"xgb_means_oxygen_isoscape.tiff\"))\n",
        "  save_numpy_to_geotiff(bds, np.expand_dims(xgb_variances, axis=2), get_model_path(\"xgb_variances_oxygen_isoscape.tiff\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-D1eG5P1T02"
      },
      "outputs": [],
      "source": [
        "xgb_means_oxygen_geotiff = load_raster(get_model_path(\"xgb_means_oxygen_isoscape.tiff\"))\n",
        "xgb_variances_oxygen_geotiff = load_raster(get_model_path(\"xgb_variances_oxygen_isoscape.tiff\"))\n",
        "\n",
        "# Until we re-generate the map\n",
        "xgb_variances_oxygen_geotiff.yearly_masked_image *= (5 / 4)\n",
        "xgb_variances_oxygen_geotiff.masked_image *= (5 / 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QqM7wL51X0o"
      },
      "outputs": [],
      "source": [
        "plot_band(xgb_means_oxygen_geotiff, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl_G90Be3nxn"
      },
      "outputs": [],
      "source": [
        "plot_band(xgb_variances_oxygen_geotiff, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxh8_X4mvY1F"
      },
      "outputs": [],
      "source": [
        "get_data_at_coords(xgb_means_oxygen_geotiff, -60, 4, 0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S5xxL_SG24E_"
      },
      "source": [
        "# Evaluate Precision, Recall From Samples"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eVzLYW_Pc-UM"
      },
      "source": [
        "## Oxygen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D18w0h0229GE"
      },
      "outputs": [],
      "source": [
        "real_samples = pd.read_csv(get_sample_db_path(\"38_ISOTOPE RESULTS _VARIABLES_10032023.csv\"), sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7D4JE2GJQEQ"
      },
      "outputs": [],
      "source": [
        "import scipy.stats\n",
        "g = real_samples.groupby(['lat','long'])['d18O sample']\n",
        "print('lat,long,mean,var,n,p_value,reject?')\n",
        "print(\"Real Coordinates, Real Samples\")\n",
        "all_coords = []\n",
        "for coords, x in g:\n",
        "  if x.size > 1:\n",
        "    lat = coords[0]\n",
        "    lon = coords[1]\n",
        "    all_coords.append(coords)\n",
        "    lab_samp_mean = x.mean()\n",
        "    lab_samp_var = x.var()*(x.size / (x.size - 1))\n",
        "    lab_samp_size = x.size\n",
        "    sumauma_samp_mean = get_data_at_coords(xgb_means_oxygen_geotiff, lon, lat, 0)\n",
        "    sumauma_samp_var = get_data_at_coords(xgb_variances_oxygen_geotiff, lon, lat, 0)\n",
        "    sumauma_samp_size = 5\n",
        "    _, p_value = scipy.stats.ttest_ind_from_stats(sumauma_samp_mean, math.sqrt(sumauma_samp_var), sumauma_samp_size, lab_samp_mean, math.sqrt(lab_samp_var), lab_samp_size, equal_var=False, alternative=\"two-sided\")\n",
        "    print(f\"{lat},{lon},{lab_samp_mean},{lab_samp_var},{lab_samp_size},{p_value},{p_value < 0.05}\")\n",
        "\n",
        "print(\"Fake Coordinates, Real Samples\")\n",
        "for _, x in g:\n",
        "  if x.size > 1:\n",
        "    coords = random.choice(all_coords)\n",
        "    lat = coords[0]\n",
        "    lon = coords[1]\n",
        "    lab_samp_mean = x.mean()\n",
        "    lab_samp_var = x.var()*(x.size / (x.size - 1))\n",
        "    lab_samp_size = x.size\n",
        "    sumauma_samp_mean = get_data_at_coords(xgb_means_oxygen_geotiff, lon, lat, 0)\n",
        "    sumauma_samp_var = get_data_at_coords(xgb_variances_oxygen_geotiff, lon, lat, 0)\n",
        "    sumauma_samp_size = 5\n",
        "    _, p_value = scipy.stats.ttest_ind_from_stats(sumauma_samp_mean, math.sqrt(sumauma_samp_var), sumauma_samp_size, lab_samp_mean, math.sqrt(lab_samp_var), lab_samp_size, equal_var=False, alternative=\"two-sided\")\n",
        "    print(f\"{lat},{lon},{lab_samp_mean},{lab_samp_var},{lab_samp_size},{p_value},{p_value < 0.05}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OULPrUeCdBNh"
      },
      "source": [
        "## Carbon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tyE1Zc7J3Lb"
      },
      "outputs": [],
      "source": [
        "%%substitute_globals\n",
        "!ls {SAMPLE_DATA_BASE}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNzQwj5zJ1WB"
      },
      "outputs": [],
      "source": [
        "real_samples_old = pd.read_csv(get_sample_db_path(\"pontos-vasp-cluster.csv\"), sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvzTB2gaGhXL"
      },
      "outputs": [],
      "source": [
        "carbon_means_geotiff = load_raster(get_raster_path(\"iso_d13C_map_wood_stack.tiff\"), use_only_band_index=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXLWv_n9IzUk"
      },
      "outputs": [],
      "source": [
        "carbon_variances_geotiff = load_raster(get_raster_path(\"iso_d13C_map_wood_stack.tiff\"), use_only_band_index=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XoTOHSwKCwp"
      },
      "outputs": [],
      "source": [
        "import scipy.stats\n",
        "g = real_samples_old.groupby(['latitude','longitude'])['d13C', 'd15N']\n",
        "# Assume sample variance == Kriging variance b/c we only have means for the samples in this CSV\n",
        "print('lat,long,mean,var,n,p_value,reject?')\n",
        "print(\"Real Coordinates, Real Samples\")\n",
        "all_coords = []\n",
        "for coords, x in g:\n",
        "  try:\n",
        "    if x.size >= 1:\n",
        "      lat = coords[0]\n",
        "      lon = coords[1]\n",
        "      all_coords.append(coords)\n",
        "      lab_samp_size = 5\n",
        "      sumauma_samp_size = 5 # for all we know\n",
        "\n",
        "      # d13C p-value\n",
        "      lab_samp_mean_d13c = x['d13C'].mean()\n",
        "      lab_samp_var_d13c = get_data_at_coords(carbon_variances_geotiff, lon, lat, 0)\n",
        "      sumauma_samp_mean_d13c = get_data_at_coords(carbon_means_geotiff, lon, lat, 0)\n",
        "      sumauma_samp_var_d13c = get_data_at_coords(carbon_variances_geotiff, lon, lat, 0)\n",
        "      _, p_value_d13c = scipy.stats.ttest_ind_from_stats(sumauma_samp_mean_d13c, math.sqrt(sumauma_samp_var_d13c), sumauma_samp_size, lab_samp_mean_d13c, math.sqrt(lab_samp_var_d13c), lab_samp_size, equal_var=True, alternative=\"two-sided\")\n",
        "\n",
        "      # d15N p-value\n",
        "      if False:\n",
        "        # Waiting for real d15N rasters\n",
        "        lab_samp_mean_d15n = x['d15N'].mean()\n",
        "        lab_samp_var_d15n = get_data_at_coords(nitrogen_variances_geotiff, lon, lat, 0)\n",
        "        sumauma_samp_mean_d15n = get_data_at_coords(nitrogen_means_geotiff, lon, lat, 0)\n",
        "        sumauma_samp_var_d15n = get_data_at_coords(nitrogen_variances_geotiff, lon, lat, 0)\n",
        "        _, p_value_d15n = scipy.stats.ttest_ind_from_stats(sumauma_samp_mean_d15n, math.sqrt(sumauma_samp_var_d15n), sumauma_samp_size, lab_samp_mean_d15n, math.sqrt(lab_samp_var_d15n), lab_samp_size, equal_var=True, alternative=\"two-sided\")\n",
        "      print(f\"{lat},{lon},{lab_samp_mean_d13c},{lab_samp_var_d13c},{lab_samp_size},{p_value_d13c},{p_value_d13c < 0.05}\")\n",
        "  except ValueError:\n",
        "    pass\n",
        "\n",
        "print(\"Fake Coordinates, Real Samples\")\n",
        "for real_coords, x in g:\n",
        "  try:\n",
        "    if x.size >= 1:\n",
        "      coords = random.choice(all_coords)\n",
        "      while coords[0] == real_coords[0] and coords[1] == real_coords[1]:\n",
        "        coords = random.choice(all_coords)\n",
        "      lat = coords[0]\n",
        "      lon = coords[1]\n",
        "      lab_samp_mean = x['d13C'].mean()\n",
        "      lab_samp_var = get_data_at_coords(carbon_variances_geotiff, lon, lat, 0)\n",
        "      lab_samp_size = x.size\n",
        "      sumauma_samp_mean = get_data_at_coords(carbon_means_geotiff, lon, lat, 0)\n",
        "      sumauma_samp_var = get_data_at_coords(carbon_variances_geotiff, lon, lat, 0)\n",
        "      sumauma_samp_size = 5\n",
        "      _, p_value = scipy.stats.ttest_ind_from_stats(sumauma_samp_mean, math.sqrt(sumauma_samp_var), sumauma_samp_size, lab_samp_mean, math.sqrt(lab_samp_var), lab_samp_size, equal_var=True, alternative=\"two-sided\")\n",
        "      print(f\"{lat},{lon},{lab_samp_mean},{lab_samp_var},{lab_samp_size},{p_value},{p_value < 0.05}\")\n",
        "  except ValueError:\n",
        "    pass\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5-3_Jvn90CCw"
      },
      "source": [
        "# A probability that a coordinate matches a sample given a predicted isoscape\n",
        "\n",
        "1. Compute t-intervals for a coordinate pixel on the paperwork based on the output of the AI isoscape model\n",
        "2. For a given sample, compute a z-score from the estimated t-distribution, and turn that into a p-value based on two-sided area under the curve: p($\\in$ isotope distribution | possible coordinates, isoscape). *This can be combined with other knowledge in the future to get, for example, p($\\in$ isoscape distribution $\\land$ tree rings look right for the area | possible coordinates, isoscape, tree ring knowledge).*\n",
        "3. Depending on the p-value, reject the null hypothesis that the paperwork is correct\n",
        "\n",
        "Key challenge with this approach: The statistical bound on false positives is per-pixel, and only guarantees that it falls in a range of isoscape values associated with that location rather than the location itself. We will likely have multiple possible points of origin for each sample."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2i6Ez45-C76Z"
      },
      "source": [
        "## Compute Sample Origin Given AI-Predicted Isoscapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ti0_jhDby1l"
      },
      "outputs": [],
      "source": [
        "predicted_cellulose_isoscape_geotiff = load_raster(get_model_path(\"predicted_isoscape_xgboost.tiff\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygeZCfWtWor8"
      },
      "outputs": [],
      "source": [
        "np.sum(predicted_cellulose_isoscape_geotiff.masked_image.mask[0,:,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HnJTO9Ib9hu"
      },
      "outputs": [],
      "source": [
        "plot_band(predicted_cellulose_isoscape_geotiff, 1, figsize=(12,12))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL9f3XYOAZma"
      },
      "outputs": [],
      "source": [
        "plt.imshow(predicted_cellulose_isoscape_geotiff.masked_image.data[:,:,3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-qNKsdKUWwr"
      },
      "outputs": [],
      "source": [
        "predicted_cellulose_isoscape_geotiff.masked_image.mask"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u7fjWcF96Eai"
      },
      "source": [
        "### Grab Land Water Mask From Earth Engine\n",
        "\n",
        "In Earth Engine code editor:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QEJOvnUj6KXS"
      },
      "source": [
        "```js\n",
        "// Load MODIS water_mask (250m) and select the water mask.\n",
        "var watermask = ee.ImageCollection('MODIS/006/MOD44W')\n",
        "  .select(['water_mask'])\n",
        "  .sort('system:time_start', false)\n",
        "  .first();\n",
        "print(watermask);\n",
        "\n",
        "// Load MODIS land cover (500m) and select\n",
        "// Land Cover Type 1: Annual International\n",
        "// Geosphere-Biosphere Programme (IGBP) classification\n",
        "// Key: http://www.eomf.ou.edu/static/IGBP.pdf\n",
        "// Selected: Forest and Cropland/natural vegetation mosaics\n",
        "var tree_selector = '(b(\"LC_Type1\") >= 1 && b(\"LC_Type1\") <= 5) || b(\"LC_Type1\") == 14';\n",
        "var treemask = ee.ImageCollection('MODIS/006/MCD12Q1')\n",
        "  .select(['LC_Type1'])\n",
        "  .sort('system:time_start', false)\n",
        "  .first()\n",
        "  .expression(tree_selector)\n",
        "  .rename('tree_mask');\n",
        "\n",
        "print(treemask);\n",
        "\n",
        "// Define the visualization parameters.\n",
        "var vizParamsWater = {\n",
        "  bands: ['water_mask'],\n",
        "  min: 0,\n",
        "  max: 0.5,\n",
        "  gamma: [1],\n",
        "  opacity: 0.5\n",
        "};\n",
        "var vizParamsTree = {\n",
        "  bands: ['tree_mask'],\n",
        "  min: 0,\n",
        "  max: 0.5,\n",
        "  gamma: [1],\n",
        "  opacity: 0.5\n",
        "};\n",
        "\n",
        "// Center the map and display the image.\n",
        "Map.setCenter(-50,-20,4); // Brazil\n",
        "Map.addLayer(watermask, vizParamsWater, 'grayscale');\n",
        "Map.addLayer(treemask, vizParamsTree, 'grayscale');\n",
        "\n",
        "// Reproject based on Martinelli's GeoTiffs\n",
        "var proj_str = 'EPSG:4326';\n",
        "var projection = ee.Projection(proj_str);\n",
        "//var output = watermask.reproject(projection);\n",
        "var output_projection_info = projection.getInfo();\n",
        "print(output_projection_info);\n",
        "\n",
        "// Save to GeoTIFFs\n",
        "print(\"Watermask CRS:\");\n",
        "var watermask_crs = watermask.getInfo().bands[0].crs;\n",
        "print(watermask_crs);\n",
        "\n",
        "// Note: export_region should specify a projection that matches its input.\n",
        "// Used with Export.image.toDrive, this should be the projection of the input image.\n",
        "// Used with image.clip(), this should be the projection of `image`.\n",
        "var export_region = ee.Geometry.Rectangle([[-73.975139313, -34.808472803], [-33.733472448, 5.266527396]], proj_str);\n",
        "print(\"Export area (m^2): \")\n",
        "print(export_region.area());\n",
        "\n",
        "watermask = watermask.reproject({crs: proj_str, scale: 250.0});\n",
        "treemask = treemask.reproject({crs: proj_str, scale: 250.0});\n",
        "\n",
        "// Export the images\n",
        "/*\n",
        "Export.image.toDrive({\n",
        "  image: watermask,\n",
        "  description: 'Land_Water_Brazil_MODIS',\n",
        "  crs: proj_str,\n",
        "  region: export_region,\n",
        "  scale: 250, // meters\n",
        "  maxPixels: 2100712614\n",
        "});*/\n",
        "\n",
        "Export.image.toDrive({\n",
        "  image: treemask,\n",
        "  description: 'Possible_Trees_Brazil_MODIS',\n",
        "  crs: proj_str,\n",
        "  region: export_region,\n",
        "  scale: 500, // meters\n",
        "  maxPixels: 2100712614\n",
        "});\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eJf8ol4x6ZFO"
      },
      "source": [
        "### Compute p-values for a sample\n",
        "Null hypotheses in these tests always involve equality. Our null hypothesis is that the sample could reasonably come from any given location. We reject that null hypothesis when it is very improbably that the sample came from a given location (i.e. p < c).\n",
        "\n",
        "Should be a two-sample test: One sample from the Craig-Gordon or ML approximation, one sample from the tree cellulose, ideally with N >= 30 for each.\n",
        "\n",
        "\\\n",
        "H0 (null hypothesis): pixel group == sample; i.e. the sample could be from x \\\n",
        "Ha (alternative hypothesis): pixel group != sample; i.e. the sample might not be from x\n",
        "\n",
        "\\\n",
        "Many coordinates work well like (-55,-5), but some-- especially those affected by the saturation-- don't, like (-55,-10). We would incorrectly rule those areas out as potential origins of a wood sample by rejecting the null hypothesis at reasonable p-values like 0.05. We could require substantially lower p-values to reject, but that would come at the cost of ruling very little out (i.e. not the most useful model).\n",
        "**TODO: To remedy this, consider using a larger sample size (> 1 pixel) for the isoscape side of the t-test**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HWX-undaaqab"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9jj3uRgqOoW"
      },
      "outputs": [],
      "source": [
        "def make_isoscape_with_pooled_sample_dimension_from_2d_isoscape(input: np.ma.MaskedArray, radius: int = 5):\n",
        "\n",
        "  canvas = np.zeros((radius*3, radius*3), dtype=float)\n",
        "  x_origin = int((radius*3)/2)\n",
        "  y_origin = int((radius*3)/2)\n",
        "  for y in range(-radius, radius):\n",
        "    for x in range(-radius, radius):\n",
        "      if (x*x+y*y <= radius*radius - radius):\n",
        "        x_pixel = min(max(x+x_origin, 0), radius*3-1)\n",
        "        y_pixel = min(max(y+y_origin, 0), radius*3-1)\n",
        "        canvas[x_pixel, y_pixel] = 1\n",
        "  area = int(sum(canvas.flatten()))\n",
        "\n",
        "  pooled = np.ma.MaskedArray(data=np.zeros((input.shape[0], input.shape[1], area), dtype=float),\n",
        "                             mask=np.repeat(input.mask[:, :, np.newaxis], area, axis=2))\n",
        "\n",
        "\n",
        "  for x_origin in tqdm(range(input.shape[0])):\n",
        "    for y_origin in range(input.shape[1]):\n",
        "      pooling_counter = 0\n",
        "      for y in range(-radius, radius):\n",
        "        for x in range(-radius, radius):\n",
        "          if (x*x+y*y <= radius*radius - radius):\n",
        "            x_pixel = x+x_origin\n",
        "            y_pixel = y+y_origin\n",
        "            x_pixel_clamped = min(max(x_pixel, 0), input.shape[0]-1)\n",
        "            y_pixel_clamped = min(max(y_pixel, 0), input.shape[1]-1)\n",
        "            if x_pixel_clamped == x_pixel and y_pixel_clamped == y_pixel:\n",
        "              pooled[x_origin, y_origin, pooling_counter] = input[x_pixel, y_pixel]\n",
        "              pooling_counter += 1\n",
        "      if pooling_counter < area:\n",
        "        samples_to_mask = area - pooling_counter\n",
        "        pooled.mask[x_origin, y_origin, pooling_counter:] = True\n",
        "  return pooled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS5Kc8_8uqkn"
      },
      "outputs": [],
      "source": [
        "plant_nitrogen_isoscape_pooled_samples = make_isoscape_with_pooled_sample_dimension_from_2d_isoscape(plant_nitrogen_isoscape_geotiff.yearly_masked_image, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivvLfayH9WDY"
      },
      "outputs": [],
      "source": [
        "np.save(\"/usr/local/google/home/nicholasroth/plant_nitrogen_isoscape_pooled_samples.numpy.mask\", plant_nitrogen_isoscape_pooled_samples.mask)\n",
        "np.save(\"/usr/local/google/home/nicholasroth/plant_nitrogen_isoscape_pooled_samples.numpy.data\", plant_nitrogen_isoscape_pooled_samples.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkCoMp6WpkRT"
      },
      "outputs": [],
      "source": [
        "mask = np.load(\"/usr/local/google/home/nicholasroth/plant_nitrogen_isoscape_pooled_samples.numpy.mask.npy\")\n",
        "data = np.load(\"/usr/local/google/home/nicholasroth/plant_nitrogen_isoscape_pooled_samples.numpy.data.npy\")\n",
        "plant_nitrogen_isoscape_pooled_samples = np.ma.MaskedArray(data, mask=mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XHaC-2JsTgs"
      },
      "outputs": [],
      "source": [
        "import dataclasses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MWjUAXASpJe"
      },
      "outputs": [],
      "source": [
        "# Take a sample from the \"real\" data\n",
        "# In practice, this would be multiple wood samples from the same furniture\n",
        "# Here, these come from slightly different pixels in a geotiff\n",
        "import scipy.stats\n",
        "import cv2\n",
        "\n",
        "def sample_from_geotiff(x, y, sample_geotiff, sample_radius, num_samples):\n",
        "  coord = (x, y,)\n",
        "  rs = RandomState(MT19937(SeedSequence(42)))\n",
        "  samples = []\n",
        "\n",
        "  for _ in range(num_samples):\n",
        "    sample_x, sample_y = 2*(rs.rand(2) - 0.5) * sample_radius\n",
        "    sample_x += coord[0]\n",
        "    sample_y += coord[1]\n",
        "    total_months = sample_geotiff.masked_image.shape[2]\n",
        "    monthly_readings = [get_data_at_coords(sample_geotiff, sample_x, sample_y, month) for month in range(total_months)]\n",
        "    samples.append(np.mean(monthly_readings))\n",
        "  return samples\n",
        "\n",
        "def fake_t_test(isoscape_geotiff_masked_image, cellulose_sample_x, cellulose_sample_y, cellulose_samples_geotiff, num_cellulose_samples):\n",
        "  \"\"\"Called a \"fake\" t-test because we randomly sample points close to\n",
        "     (cellulose_sample_x, cellulose_sample_y) from `cellulose_samples_geotiff`\n",
        "     to mimic taking samples out of the same piece of furniture.\n",
        "\n",
        "     We then perform a series of two-sample t-tests between this fake\n",
        "     furniture/timber sample and each coordinate of isoscape_geotiff\n",
        "  \"\"\"\n",
        "  fake_sample = sample_from_geotiff(cellulose_sample_x, cellulose_sample_y, cellulose_samples_geotiff, get_extent(cellulose_samples_geotiff.gdal_dataset).pixel_size_x*50, num_cellulose_samples)\n",
        "  shape = isoscape_geotiff_masked_image.shape\n",
        "  return scipy.stats.ttest_ind(isoscape_geotiff_masked_image, np.tile(fake_sample,(shape[0],shape[1],1)), axis=2, equal_var=False, alternative='two-sided')\n",
        "\n",
        "def crop_to_coordinates(original_extent: Bounds, new_extent: Bounds, original_image: np.ndarray):\n",
        "  x_off_deg = max((new_extent.minx - original_extent.minx), 0)\n",
        "  x_off_px = int(x_off_deg / abs(original_extent.pixel_size_x))\n",
        "  y_off_deg = max((new_extent.miny - original_extent.miny), 0)\n",
        "  y_off_px = int(y_off_deg / abs(original_extent.pixel_size_y))\n",
        "  x_max_deg = min(new_extent.maxx, original_extent.maxx)\n",
        "  x_max_px = int((x_max_deg - original_extent.minx) / abs(original_extent.pixel_size_x))\n",
        "  y_max_deg = min(new_extent.maxy, original_extent.maxy)\n",
        "  y_max_px = int((y_max_deg - original_extent.miny) / abs(original_extent.pixel_size_y))\n",
        "  # Pixels are stored in descending order if an axis has negative pixel size.\n",
        "  if original_extent.pixel_size_x < 0:\n",
        "    raise RuntimeError(\"Inverted X axis not supported\")\n",
        "  cropped_image = np.flip(original_image, axis=0) if original_extent.pixel_size_y < 0 else original_image\n",
        "  cropped_image = cropped_image[y_off_px:y_max_px, x_off_px:x_max_px]\n",
        "  cropped_image = np.flip(cropped_image, axis=0) if original_extent.pixel_size_y < 0 else cropped_image\n",
        "  resulting_bounds = Bounds(original_extent.minx + x_off_deg,\n",
        "                            x_max_deg,\n",
        "                            original_extent.miny + y_off_deg,\n",
        "                            y_max_deg,\n",
        "                            original_extent.pixel_size_x,\n",
        "                            original_extent.pixel_size_y,\n",
        "                            cropped_image.shape[1],\n",
        "                            cropped_image.shape[0])\n",
        "  return cropped_image, resulting_bounds\n",
        "\n",
        "def pad_to_coordinates(original_extent: Bounds, new_extent: Bounds, original_image: np.ndarray, with_ones: bool = False):\n",
        "  \"\"\"Pads an image to new coordinates larger than the original.\n",
        "  Precondition: Must not specify negative padding (i.e. a crop)\n",
        "  \"\"\"\n",
        "  x_size = max(abs(int((new_extent.maxx - new_extent.minx) / original_extent.pixel_size_x)), original_image.T.shape[0])\n",
        "  y_size = max(abs(int((new_extent.maxy - new_extent.miny) / original_extent.pixel_size_y)), original_image.T.shape[1])\n",
        "  if with_ones:\n",
        "    padded_image = np.ones((x_size, y_size,), dtype=original_image.dtype)\n",
        "  else:\n",
        "    padded_image = np.zeros((x_size, y_size,), dtype=original_image.dtype)\n",
        "  x_offset = max(int((original_extent.minx - new_extent.minx) / abs(original_extent.pixel_size_x)), 0)\n",
        "  y_offset = max(int((original_extent.miny - new_extent.miny) / abs(original_extent.pixel_size_y)), 0)\n",
        "  # Correct rounding errors to satisfy the following by adjusting offsets:\n",
        "  # * x_offset + original_extent.raster_size_x <= padded_image.shape[0]\n",
        "  # --> x_offset <= padded_image.shape[0] - original_extent.raster_size_x\n",
        "  # * y_offset + original_extent.raster_size_y <= padded_image.shape[1]\n",
        "  x_offset = min(x_offset, padded_image.shape[0] - original_image.T.shape[0])\n",
        "  y_offset = min(y_offset, padded_image.shape[1] - original_image.T.shape[1])\n",
        "\n",
        "  # Pixels are stored in descending order if an axis has negative pixel size.\n",
        "  if original_extent.pixel_size_x < 0:\n",
        "    raise RuntimeError(\"Inverted X axis not supported\")\n",
        "  padded_image = np.flip(padded_image, axis=1) if original_extent.pixel_size_y < 0 else padded_image\n",
        "  original_image = np.flip(original_image, axis=0) if original_extent.pixel_size_y < 0 else original_image\n",
        "  padded_image[x_offset:x_offset+original_image.T.shape[0], y_offset:y_offset+original_image.T.shape[1]] = original_image.T\n",
        "  padded_image = np.flip(padded_image, axis=1) if original_extent.pixel_size_y < 0 else padded_image\n",
        "  return padded_image.T\n",
        "\n",
        "def upscale(new_size_x: int, new_size_y: int, original_image: np.ndarray):\n",
        "  \"\"\"Rescales an image of the same geographical extent with nearest-neighbor sampling.\n",
        "\n",
        "  Useful to match pixel size between images.\n",
        "  \"\"\"\n",
        "  resized_data = cv2.resize(original_image, dsize=(new_size_y, new_size_x), interpolation=cv2.INTER_NEAREST_EXACT)\n",
        "  return resized_data\n",
        "\n",
        "def align_to_bounds(original_bounds: Bounds, new_bounds: Bounds, original_image: np.ndarray, pad_with_ones: bool):\n",
        "  # TODO: UNIT TESTS (if we want to use this version in production)!!!\n",
        "  cropped, cropped_bounds = crop_to_coordinates(original_bounds, new_bounds, original_image)\n",
        "  padded = pad_to_coordinates(cropped_bounds, new_bounds, cropped, with_ones=pad_with_ones)\n",
        "  scaled = upscale(new_bounds.raster_size_x, new_bounds.raster_size_y, padded)\n",
        "  return scaled\n",
        "\n",
        "def combine_pvalues(pvalue_nitrogen, pvalue_oxygen, nitrogen_extent, oxygen_extent, nitrogen_mask, oxygen_mask):\n",
        "  # TODO: implement combine_pvalues() in a generic way to combine any two maskedarrays of p-values\n",
        "  # using align_to_bounds()\n",
        "  pvalue_nitrogen = pad_to_coordinates(nitrogen_extent, oxygen_extent, pvalue_nitrogen)\n",
        "  nitrogen_mask = pad_to_coordinates(nitrogen_extent, oxygen_extent, nitrogen_mask.astype(int), True).astype(bool)\n",
        "  pvalue_oxygen = upscale(pvalue_nitrogen.shape[0], pvalue_nitrogen.shape[1], pvalue_oxygen)\n",
        "  oxygen_mask = upscale(pvalue_nitrogen.shape[0], pvalue_nitrogen.shape[1], oxygen_mask.astype(int)).astype(bool)\n",
        "  both_mask = np.logical_or(oxygen_mask, nitrogen_mask)\n",
        "  pvalues_combined = np.ma.MaskedArray(pvalue_oxygen*pvalue_nitrogen, mask=both_mask)\n",
        "  result = np.ma.masked_array(np.where(both_mask, pvalue_oxygen, pvalues_combined), mask=oxygen_mask)\n",
        "  return result\n",
        "\n",
        "def evalutate_on_sample_from_point(x, y, p_threshold=0.05, num_cellulose_samples=10):\n",
        "  print(predicted_cellulose_isoscape_geotiff.masked_image.mask.shape)\n",
        "  default_mask = predicted_cellulose_isoscape_geotiff.masked_image.mask[:,:,0]\n",
        "  oxygen_extent=get_extent(predicted_cellulose_isoscape_geotiff.gdal_dataset)\n",
        "  nitrogen_extent=get_extent(plant_nitrogen_isoscape_geotiff.gdal_dataset)\n",
        "  nitrogen_mask = plant_nitrogen_isoscape_geotiff.yearly_masked_image.mask\n",
        "  oxygen_mask = predicted_cellulose_isoscape_geotiff.yearly_masked_image.mask\n",
        "  # Pessimistic case: Cellulose sample is faked with Craig-Gordon model, and\n",
        "  # the isoscape comes from the AI model. Nitrogen comes from Martinelli's\n",
        "  # geotiff (unknown ultimate source).\n",
        "  statistic_oxygen, pvalue_oxygen = fake_t_test(predicted_cellulose_isoscape_geotiff.masked_image, x, y, cellulose_isoscape_geotiff, num_cellulose_samples)\n",
        "  statistic_nitrogen, pvalue_nitrogen = fake_t_test(plant_nitrogen_isoscape_pooled_samples, x, y, plant_nitrogen_isoscape_geotiff, num_cellulose_samples)\n",
        "  pvalues_combined = combine_pvalues(pvalue_nitrogen, pvalue_oxygen, nitrogen_extent, oxygen_extent, nitrogen_mask, oxygen_mask)\n",
        "\n",
        "  pvalues_bounds = dataclasses.replace(oxygen_extent,\n",
        "                                       raster_size_x = pvalues_combined.shape[0],\n",
        "                                       raster_size_y = pvalues_combined.shape[1],\n",
        "                                       pixel_size_x = nitrogen_extent.pixel_size_x,\n",
        "                                       pixel_size_y = nitrogen_extent.pixel_size_y)\n",
        "  land_water_mask = align_to_bounds(get_extent(land_water_mask_geotiff.gdal_dataset), pvalues_bounds, land_water_mask_geotiff.masked_image[:,:,0], pad_with_ones=True)\n",
        "  possible_tree_mask = align_to_bounds(get_extent(possible_tree_mask_geotiff.gdal_dataset), pvalues_bounds, possible_tree_mask_geotiff.masked_image[:,:,0], pad_with_ones=True)\n",
        "\n",
        "  fig, ax = plt.subplots(2,2, figsize=(30,30))\n",
        "  plt.colorbar(ax[0,0].imshow(pvalues_combined, extent=oxygen_extent.to_matplotlib()), ax=ax[0,0])\n",
        "  ax[0,0].set_title(\"p-values\")\n",
        "  invalid_terrain = np.logical_or(np.logical_or(pvalues_combined.mask, land_water_mask), np.logical_not(possible_tree_mask))\n",
        "  ax[0,1].imshow(np.ma.masked_array(pvalues_combined < p_threshold, mask=invalid_terrain), extent=oxygen_extent.to_matplotlib())\n",
        "  circle1 = plt.Circle((x, y), 1, color='r')\n",
        "  ax[0,1].add_patch(circle1)\n",
        "  ax[0,1].set_title(f\"p < {p_threshold}\")\n",
        "  ax[0,0].set_ylabel(\"Pessimistic Case\")\n",
        "\n",
        "  print(f\"Invalid p-values: {np.sum(pvalues_combined.flatten() > 1)}\")\n",
        "  # Optimistic case: Cellulose sample is faked with Craig-Gordon model, and\n",
        "  # the isoscape also comes from the Craig-Gordon model.\n",
        "  statistic_oxygen, pvalue_oxygen = fake_t_test(cellulose_isoscape_geotiff.masked_image, x, y, cellulose_isoscape_geotiff, num_cellulose_samples)\n",
        "  pvalues_combined = combine_pvalues(pvalue_nitrogen, pvalue_oxygen, nitrogen_extent, oxygen_extent, nitrogen_mask, oxygen_mask)\n",
        "  oxygen_extent=get_extent(cellulose_isoscape_geotiff.gdal_dataset)\n",
        "  plt.colorbar(ax[1,0].imshow(pvalues_combined, extent=oxygen_extent.to_matplotlib()), ax=ax[1,0])\n",
        "  #ax[1,0].imshow(possible_tree_mask, extent=oxygen_extent.to_matplotlib())\n",
        "  #ax[1,0].imshow(invalid_terrain.astype(bool), extent=oxygen_extent.to_matplotlib())\n",
        "  ax[1,1].imshow(np.ma.masked_array(pvalues_combined < p_threshold, invalid_terrain), extent=oxygen_extent.to_matplotlib())\n",
        "  circle1 = plt.Circle((x, y), 1, color='r')\n",
        "  ax[1,1].add_patch(circle1)\n",
        "  ax[1,0].set_ylabel(\"Optimistic Case\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yobubaTYasaj"
      },
      "source": [
        "#### Story\n",
        "\n",
        "**Yellow: We reject the null hypothesis that the sample might have come from here because it is so improbable**\n",
        "\n",
        "##### Pessimistic Example: Outlier point sampled from outlier area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o0KKQ2wa3s2"
      },
      "outputs": [],
      "source": [
        "evalutate_on_sample_from_point(x=-55, y=-10, p_threshold=0.01, num_cellulose_samples=10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uN40DClvbk-3"
      },
      "source": [
        "##### More Optimistic Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2CgtxSAbnKN"
      },
      "outputs": [],
      "source": [
        "evalutate_on_sample_from_point(x=-55, y=-5, p_threshold=0.01, num_cellulose_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFhsAHZcb7e3"
      },
      "outputs": [],
      "source": [
        "evalutate_on_sample_from_point(x=-65, y=-5, p_threshold=0.01, num_cellulose_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VjyYdLj2VGi"
      },
      "outputs": [],
      "source": [
        "# This is the Amazon biome shapefile we use:\n",
        "# https://services.arcgis.com/F7DSX1DSNSiWmOqh/arcgis/rest/services/lm_bioma_250/FeatureServer\n",
        "# http://geoftp.ibge.gov.br/informacoes_ambientais/estudos_ambientais/biomas/vetores/Biomas_250mil.zip\n",
        "\n",
        "# TODO: Figure out how to produce good isoscapes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N6JSQQ2g8gS3"
      },
      "source": [
        "### Compute mean absolute error in km\n",
        "*Compute based on min, max, and median AE for each sample.*\n",
        "Also compute proportion of samples for which nothing passed our threshold.\n",
        "Ideally, plot these against each other on a curve while varying $c$ (0.95, 0.99, etc.).\n",
        "\n",
        "This may not be what we actually want because ultimately this is a binary classification problem. That framing may be more useful as a business metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHmsrCWo8v4M"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4qCnFGsvx39s",
        "3-CHLVHQyD9U",
        "YTieS4NxKetw",
        "xR2Nz4gcsR6N",
        "ZMhPJFTo1FuI",
        "JbO_o5Gg1LHr",
        "U_2b5nUNxhnB",
        "wjHQ-Asj07XN",
        "i9M9pxxi1KOv",
        "lgfrhThP6KeY",
        "wSSdzTLbfBWS"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
